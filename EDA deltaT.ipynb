{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_data_types(df):\n",
    "    for col in df.columns:\n",
    "        unique_vals = df[col].unique()\n",
    "        if df[col].dtypes == 'object':\n",
    "            None\n",
    "#         elif (set(unique_vals) == {0, 1}) or col == 'userId':\n",
    "#             df[col] = df[col].astype('int64')\n",
    "#         else:\n",
    "#             df[col] = df[col].astype('float64')\n",
    "        elif (set(unique_vals) != {0, 1}):\n",
    "            df[col] = df[col].astype('float64')\n",
    "        else:\n",
    "            df[col] = df[col].astype('int64')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ho_t02['userId'] = ho_t02['userId'].astype('str')\n",
    "# ho_t02 = adjust_data_types(ho_t02)\n",
    "# ho_t02.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "UserConfigValidationException will be deprecated from dice_ml.utils. Please import UserConfigValidationException from raiutils.exceptions.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFE, RFECV, VarianceThreshold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, make_scorer, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, permutation_test_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "import shap\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "import dice_ml\n",
    "\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    return specificity\n",
    "\n",
    "specificity_scorer = make_scorer(specificity_score)\n",
    "\n",
    "def npv_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    npv = tn / (tn + fn)\n",
    "    return npv\n",
    "\n",
    "npv_scorer = make_scorer(npv_score)\n",
    "\n",
    "g_mean_scorer = make_scorer(geometric_mean_score)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(10, shuffle=True, random_state=0)\n",
    "\n",
    "# def get_cat_cols(df):\n",
    "#     one_hot_encoded_columns = []\n",
    "\n",
    "#     for idx, column in enumerate(df.columns):\n",
    "#         if df[column].nunique() == 2 and set(df[column].unique()) == {0, 1}:\n",
    "#             one_hot_encoded_columns.append(idx)\n",
    "\n",
    "#     return one_hot_encoded_columns\n",
    "\n",
    "def get_cat_cols(df):\n",
    "    return df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# get_cat_cols(ho_t02)\n",
    "# cat_cols = get_cat_cols(ho_t02)\n",
    "# num_cols = [col for col in ho_t02.columns if col not in cat_cols]\n",
    "# cat_cols, num_cols\n",
    "\n",
    "def efron_rsquare(y, y_pred):\n",
    "    n = float(len(y))\n",
    "    t1 = np.sum(np.power(y - y_pred, 2.0))\n",
    "    t2 = np.sum(np.power((y - (np.sum(y) / n)), 2.0))\n",
    "    return 1.0 - (t1 / t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, TomekLinks\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DropCategoricalFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Assuming X is a DataFrame\n",
    "        non_categorical_cols = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "        return X[non_categorical_cols]\n",
    "\n",
    "\n",
    "def knn_class2(df, scorer, label_col, scaler, random_state=0):\n",
    "    X = df.drop(['userId', 'lastFirstName', label_col], axis=1)\n",
    "    y = df.loc[:, label_col]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('resampling', None),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "#         'scaler': [StandardScaler(),\n",
    "#                    MinMaxScaler(),\n",
    "#                    RobustScaler()\n",
    "#                   ],\n",
    "#         'resampling': [SMOTE(random_state=random_state),\n",
    "#                        SMOTENC(categorical_features=get_cat_cols(X),\n",
    "#                                random_state=random_state),\n",
    "#                        ADASYN(random_state=random_state),\n",
    "# #                        RandomOverSampler(random_state=random_state),\n",
    "#                        RandomUnderSampler(random_state=random_state)],\n",
    "#         'scaler': [RobustScaler(),\n",
    "#                    PowerTransformer()],\n",
    "#         'resampling': [SMOTENC(categorical_features=get_cat_cols(X),\n",
    "#                                random_state=random_state),\n",
    "        'resampling': [\n",
    "            SMOTE(random_state=random_state),\n",
    "            ADASYN(random_state=random_state),\n",
    "#             SMOTEENN(random_state=random_state),\n",
    "#             SMOTETomek(random_state=random_state),\n",
    "            EditedNearestNeighbours(),\n",
    "#             TomekLinks(),\n",
    "            RandomOverSampler(random_state=random_state),\n",
    "            RandomUnderSampler(random_state=random_state)\n",
    "        ],\n",
    "        'classifier__n_neighbors': list(range(1, 16)),\n",
    "        'classifier__metric': ['euclidean', 'cosine', 'hamming', 'braycurtis',\n",
    "                               'chebyshev', 'canberra', 'cityblock', 'sqeuclidean'\n",
    "                              ]\n",
    "    }\n",
    "\n",
    "    start_time_cv = time.time()\n",
    "    grid_search = GridSearchCV(pipeline, param_grid,\n",
    "                               cv=skf, scoring=scorer,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=4, return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    end_time_cv = time.time()\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Validation Score (Precision):\", best_score)\n",
    "    print('Best Estimator:', best_estimator)\n",
    "    print('Mean Train Score (Precision)', np.mean(cv_results['mean_train_score']))\n",
    "    print('Std Train Score (Precision)', np.mean(cv_results['std_train_score']))\n",
    "    elapsed_time_cv = (end_time_cv - start_time_cv)\n",
    "    print(f'GridSearchCV Runtime: {elapsed_time_cv} secs')\n",
    "\n",
    "    num_runs = 5\n",
    "    metrics_per_run = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        modified_random_state = random_state + run\n",
    "\n",
    "        start_time = time.time()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            stratify=y,\n",
    "                                                            random_state=modified_random_state)\n",
    "\n",
    "        modified_estimator = clone(best_estimator)\n",
    "#         if not (isinstance(best_estimator.named_steps['resampling'], EditedNearestNeighbours) or\n",
    "#             isinstance(best_estimator.named_steps['resampling'], TomekLinks)):\n",
    "#             modified_estimator.named_steps['resampling'].set_params(random_state=modified_random_state)\n",
    "#         modified_estimator.named_steps['classifier'].set_params(random_state=modified_random_state)\n",
    "        modified_estimator.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = modified_estimator.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = (end_time - start_time)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        specificity = specificity_score(y_test, y_pred)\n",
    "        npv = npv_score(y_test, y_pred)\n",
    "        g_mean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Run {run + 1} - Random State: {modified_random_state}\")\n",
    "        print(f\"Test Precision: {precision}\")\n",
    "\n",
    "#         y_hat = modified_estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "#         gmeans = np.sqrt(tpr * (1-fpr))\n",
    "#         ix = np.argmax(gmeans)\n",
    "#         print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "#         plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "#         plt.plot(fpr, tpr, marker='.', label='Original')\n",
    "#         plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         plt.legend()\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "        metrics_per_run.append({\n",
    "            'Random State': modified_random_state,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Precision': precision,\n",
    "            'Test Recall': recall,\n",
    "            'Test F1 Score': f1,\n",
    "            'Test Specificity': specificity,\n",
    "            'Test G-mean': g_mean,\n",
    "            'Test NPV': npv,\n",
    "            'Runtime': elapsed_time\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_per_run)\n",
    "\n",
    "    avg_metrics = metrics_df.mean()\n",
    "    std_metrics = metrics_df.std()\n",
    "\n",
    "    print('Ave Test Precision:', avg_metrics['Test Precision'])\n",
    "    print('Stdev Test Precision:', std_metrics['Test Precision'])\n",
    "    print(\"Ave Test Accuracy:\", avg_metrics['Test Accuracy'])\n",
    "    print('Stdev Test Accuracy:', std_metrics['Test Accuracy'])\n",
    "    print(\"Ave Test Specificity:\", avg_metrics['Test Specificity'])\n",
    "    print(\"Ave Test Recall:\", avg_metrics['Test Recall'])\n",
    "    print('Ave Test NPV:', avg_metrics['Test NPV'])\n",
    "    print(\"Ave Test F1-Score:\", avg_metrics['Test F1 Score'])\n",
    "    print(\"Ave Test G-mean:\", avg_metrics['Test G-mean'])\n",
    "    print(\"Ave Runtime:\", avg_metrics['Runtime'])\n",
    "\n",
    "    model_info = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'mean_train_score': np.mean(cv_results['mean_train_score']),\n",
    "        'std_train_score': np.mean(cv_results['std_train_score']),\n",
    "        'average_test_precision': avg_metrics['Test Precision'],\n",
    "        'stdev_test_precision': std_metrics['Test Precision'],\n",
    "        'average_test_accuracy': avg_metrics['Test Accuracy'],\n",
    "        'stdev_test_accuracy': std_metrics['Test Accuracy'],\n",
    "        'average_runtime': avg_metrics['Runtime'],\n",
    "        'average_test_specificity': avg_metrics['Test Specificity'],\n",
    "        'average_test_recall': avg_metrics['Test Recall'],\n",
    "        'average_test_npv': avg_metrics['Test NPV'],\n",
    "        'average_test_f1_score': avg_metrics['Test F1 Score'],\n",
    "        'average_test_g_mean': avg_metrics['Test G-mean']\n",
    "    }\n",
    "\n",
    "    return grid_search, best_estimator, model_info, metrics_df\n",
    "\n",
    "def svm_class2(df, scorer, label_col, scaler, random_state=0):\n",
    "    X = df.drop(['userId', 'lastFirstName', label_col], axis=1)\n",
    "    y = df.loc[:, label_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('resampling', None),\n",
    "        ('classifier', SVC(random_state=random_state, max_iter=1000,\n",
    "                           verbose=True))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "#         'scaler': [StandardScaler(),\n",
    "#                    MinMaxScaler(),\n",
    "#                    RobustScaler()\n",
    "#         ],\n",
    "        'resampling': [SMOTE(random_state=random_state),\n",
    "                       ADASYN(random_state=random_state),\n",
    "#                        SMOTEENN(random_state=random_state),\n",
    "#                        SMOTETomek(random_state=random_state),\n",
    "                       EditedNearestNeighbours(),\n",
    "#                        TomekLinks(),\n",
    "                       RandomOverSampler(random_state=random_state),\n",
    "                       RandomUnderSampler(random_state=random_state)],\n",
    "        'classifier__C': list(np.logspace(-6, 6, num=13)),\n",
    "        'classifier__probability': [True, False],\n",
    "        'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }\n",
    "\n",
    "    start_time_cv = time.time()\n",
    "    grid_search = GridSearchCV(pipeline, param_grid,\n",
    "                               cv=skf, scoring=scorer,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=4, return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    end_time_cv = time.time()\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Validation Score (Precision):\", best_score)\n",
    "    print('Best Estimator:', best_estimator)\n",
    "    print('Mean Train Score (Precision)', np.mean(cv_results['mean_train_score']))\n",
    "    print('Std Train Score (Precision)', np.mean(cv_results['std_train_score']))\n",
    "    elapsed_time_cv = (end_time_cv - start_time_cv)\n",
    "    print(f'GridSearchCV Runtime: {elapsed_time_cv} secs')\n",
    "\n",
    "    num_runs = 5\n",
    "    metrics_per_run = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        modified_random_state = random_state + run\n",
    "\n",
    "        start_time = time.time()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            stratify=y,\n",
    "                                                            random_state=modified_random_state)\n",
    "\n",
    "        modified_estimator = clone(best_estimator)\n",
    "#         if not (isinstance(best_estimator.named_steps['resampling'], EditedNearestNeighbours) or\n",
    "#             isinstance(best_estimator.named_steps['resampling'], TomekLinks)):\n",
    "#             modified_estimator.named_steps['resampling'].set_params(random_state=modified_random_state)\n",
    "#         modified_estimator.named_steps['classifier'].set_params(random_state=modified_random_state)\n",
    "        modified_estimator.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = modified_estimator.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = (end_time - start_time)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        specificity = specificity_score(y_test, y_pred)\n",
    "        npv = npv_score(y_test, y_pred)\n",
    "        g_mean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Run {run + 1} - Random State: {modified_random_state}\")\n",
    "        print(f\"Test Precision: {precision}\")\n",
    "\n",
    "#         y_hat = modified_estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "#         gmeans = np.sqrt(tpr * (1-fpr))\n",
    "#         ix = np.argmax(gmeans)\n",
    "#         print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "#         plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "#         plt.plot(fpr, tpr, marker='.', label='Original')\n",
    "#         plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         plt.legend()\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "        metrics_per_run.append({\n",
    "            'Random State': modified_random_state,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Precision': precision,\n",
    "            'Test Recall': recall,\n",
    "            'Test F1 Score': f1,\n",
    "            'Test Specificity': specificity,\n",
    "            'Test G-mean': g_mean,\n",
    "            'Test NPV': npv,\n",
    "            'Runtime': elapsed_time\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_per_run)\n",
    "\n",
    "    avg_metrics = metrics_df.mean()\n",
    "    std_metrics = metrics_df.std()\n",
    "\n",
    "    print('Ave Test Precision:', avg_metrics['Test Precision'])\n",
    "    print('Stdev Test Precision:', std_metrics['Test Precision'])\n",
    "    print(\"Ave Test Accuracy:\", avg_metrics['Test Accuracy'])\n",
    "    print('Stdev Test Accuracy:', std_metrics['Test Accuracy'])\n",
    "    print(\"Ave Test Specificity:\", avg_metrics['Test Specificity'])\n",
    "    print(\"Ave Test Recall:\", avg_metrics['Test Recall'])\n",
    "    print('Ave Test NPV:', avg_metrics['Test NPV'])\n",
    "    print(\"Ave Test F1-Score:\", avg_metrics['Test F1 Score'])\n",
    "    print(\"Ave Test G-mean:\", avg_metrics['Test G-mean'])\n",
    "    print(\"Ave Runtime:\", avg_metrics['Runtime'])\n",
    "\n",
    "    model_info = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'mean_train_score': np.mean(cv_results['mean_train_score']),\n",
    "        'std_train_score': np.mean(cv_results['std_train_score']),\n",
    "        'average_test_precision': avg_metrics['Test Precision'],\n",
    "        'stdev_test_precision': std_metrics['Test Precision'],\n",
    "        'average_test_accuracy': avg_metrics['Test Accuracy'],\n",
    "        'stdev_test_accuracy': std_metrics['Test Accuracy'],\n",
    "        'average_runtime': avg_metrics['Runtime'],\n",
    "        'average_test_specificity': avg_metrics['Test Specificity'],\n",
    "        'average_test_recall': avg_metrics['Test Recall'],\n",
    "        'average_test_npv': avg_metrics['Test NPV'],\n",
    "        'average_test_f1_score': avg_metrics['Test F1 Score'],\n",
    "        'average_test_g_mean': avg_metrics['Test G-mean']\n",
    "    }\n",
    "\n",
    "    return grid_search, best_estimator, model_info, metrics_df\n",
    "\n",
    "def logreg_class2(df, scorer, label_col, scaler, random_state=0):\n",
    "    X = df.drop(['userId', 'lastFirstName', label_col], axis=1)\n",
    "    y = df.loc[:, label_col]\n",
    "\n",
    "#     scaler = RobustScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "#     pca = PCA().fit(X_scaled)\n",
    "\n",
    "#     # 2. Plot the cumulative explained variance\n",
    "#     explained_var_ratio = pca.explained_variance_ratio_\n",
    "#     cumulative_explained_var = np.cumsum(explained_var_ratio)\n",
    "\n",
    "#     # 3. Find the number of components that explain up to 90% of the variance\n",
    "#     num_components = np.argmax(cumulative_explained_var >= 0.90) + 1\n",
    "#     print(f\"Number of components that explain up to 90% of variance: {num_components}\")\n",
    "\n",
    "#     # Refit PCA with the selected number of components\n",
    "#     pca_90 = PCA(n_components=num_components)\n",
    "#     X = pca_90.fit_transform(X_scaled)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "#         ('drop_categorical', None),\n",
    "#         ('encoder', None),\n",
    "        ('scaler', scaler),\n",
    "        ('resampling', None),\n",
    "        ('classifier', LogisticRegression(random_state=random_state, max_iter=1000))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "#         'drop_categorical': [None, DropCategoricalFeatures()]\n",
    "#         'encoder': [None, OneHotEncoder(sparse_output=False)],\n",
    "#         'scaler': [\n",
    "#             StandardScaler(),\n",
    "#             MinMaxScaler(),\n",
    "#             RobustScaler()\n",
    "#         ],\n",
    "        'resampling': [SMOTE(random_state=random_state),\n",
    "                       ADASYN(random_state=random_state),\n",
    "#                        SMOTEENN(random_state=random_state),\n",
    "#                        SMOTETomek(random_state=random_state),\n",
    "                       EditedNearestNeighbours(),\n",
    "#                        TomekLinks(),\n",
    "                       RandomOverSampler(random_state=random_state),\n",
    "                       RandomUnderSampler(random_state=random_state)],\n",
    "        'classifier__C': list(np.logspace(-6, 6, num=13)),\n",
    "        'classifier__penalty': ['l2', None],\n",
    "        'classifier__solver': ['liblinear', 'lbfgs', 'newton-cholesky']\n",
    "#         'classifier__penalty': ['l1', l2', 'elasticnet', 'None']\n",
    "    }\n",
    "\n",
    "    start_time_cv = time.time()\n",
    "    grid_search = GridSearchCV(pipeline, param_grid,\n",
    "                               cv=skf, scoring=scorer,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=4, return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    train_score = best_estimator.score(X_train, y_train)\n",
    "    cv_results = grid_search.cv_results_\n",
    "    end_time_cv = time.time()\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Validation Score (Precision):\", best_score)\n",
    "    print('Best Estimator:', best_estimator)\n",
    "    # print('Mean Train Score (Precision)', np.mean(cv_results['mean_train_score']))\n",
    "    # print('Std Train Score (Precision)', np.mean(cv_results['std_train_score']))\n",
    "    print('Train Score of Best Model:', train_score)\n",
    "    elapsed_time_cv = (end_time_cv - start_time_cv)\n",
    "    print(f'GridSearchCV Runtime: {elapsed_time_cv} secs')\n",
    "\n",
    "    y_train_pred = best_estimator.predict_proba(X_train)[:, 1]\n",
    "    efron = efron_rsquare(y_train, y_train_pred)\n",
    "    print('Efron R-squared:', efron)\n",
    "\n",
    "\n",
    "    num_runs = 5\n",
    "    metrics_per_run = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        modified_random_state = random_state + run\n",
    "\n",
    "        start_time = time.time()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            stratify=y,\n",
    "                                                            random_state=modified_random_state)\n",
    "\n",
    "        modified_estimator = clone(best_estimator)\n",
    "#         if not (isinstance(best_estimator.named_steps['resampling'], EditedNearestNeighbours) or\n",
    "#             isinstance(best_estimator.named_steps['resampling'], TomekLinks)):\n",
    "#             modified_estimator.named_steps['resampling'].set_params(random_state=modified_random_state)\n",
    "#         modified_estimator.named_steps['classifier'].set_params(random_state=modified_random_state)\n",
    "        modified_estimator.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = modified_estimator.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = (end_time - start_time)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        specificity = specificity_score(y_test, y_pred)\n",
    "        npv = npv_score(y_test, y_pred)\n",
    "        g_mean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Run {run + 1} - Random State: {modified_random_state}\")\n",
    "        print(f\"Test Precision: {precision}\")\n",
    "\n",
    "#         y_hat = modified_estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "#         gmeans = np.sqrt(tpr * (1-fpr))\n",
    "#         ix = np.argmax(gmeans)\n",
    "#         print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "#         plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "#         plt.plot(fpr, tpr, marker='.', label='Original')\n",
    "#         plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         plt.legend()\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "        metrics_per_run.append({\n",
    "            'Random State': modified_random_state,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Precision': precision,\n",
    "            'Test Recall': recall,\n",
    "            'Test F1 Score': f1,\n",
    "            'Test Specificity': specificity,\n",
    "            'Test G-mean': g_mean,\n",
    "            'Test NPV': npv,\n",
    "            'Runtime': elapsed_time\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_per_run)\n",
    "\n",
    "    avg_metrics = metrics_df.mean()\n",
    "    std_metrics = metrics_df.std()\n",
    "\n",
    "    print('Ave Test Precision:', avg_metrics['Test Precision'])\n",
    "    print('Stdev Test Precision:', std_metrics['Test Precision'])\n",
    "    print(\"Ave Test Accuracy:\", avg_metrics['Test Accuracy'])\n",
    "    print('Stdev Test Accuracy:', std_metrics['Test Accuracy'])\n",
    "    print(\"Ave Test Specificity:\", avg_metrics['Test Specificity'])\n",
    "    print(\"Ave Test Recall:\", avg_metrics['Test Recall'])\n",
    "    print('Ave Test NPV:', avg_metrics['Test NPV'])\n",
    "    print(\"Ave Test F1-Score:\", avg_metrics['Test F1 Score'])\n",
    "    print(\"Ave Test G-mean:\", avg_metrics['Test G-mean'])\n",
    "    print(\"Ave Runtime:\", avg_metrics['Runtime'])\n",
    "\n",
    "    model_info = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'train_score': train_score,\n",
    "        'efron': efron,\n",
    "        # 'std_train_score': np.mean(cv_results['std_train_score']),\n",
    "        'average_test_precision': avg_metrics['Test Precision'],\n",
    "        'stdev_test_precision': std_metrics['Test Precision'],\n",
    "        'average_test_accuracy': avg_metrics['Test Accuracy'],\n",
    "        'stdev_test_accuracy': std_metrics['Test Accuracy'],\n",
    "        'average_runtime': avg_metrics['Runtime'],\n",
    "        'average_test_specificity': avg_metrics['Test Specificity'],\n",
    "        'average_test_recall': avg_metrics['Test Recall'],\n",
    "        'average_test_npv': avg_metrics['Test NPV'],\n",
    "        'average_test_f1_score': avg_metrics['Test F1 Score'],\n",
    "        'average_test_g_mean': avg_metrics['Test G-mean']\n",
    "    }\n",
    "\n",
    "    return grid_search, best_estimator, model_info, metrics_df\n",
    "\n",
    "\n",
    "\n",
    "def dt_class2(df, scorer, label_col, scaler, random_state=0):\n",
    "    X = df.drop(['userId', 'lastFirstName', label_col], axis=1)\n",
    "    y = df.loc[:, label_col]\n",
    "\n",
    "#     scaler = RobustScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "#     pca = PCA().fit(X_scaled)\n",
    "\n",
    "#     # 2. Plot the cumulative explained variance\n",
    "#     explained_var_ratio = pca.explained_variance_ratio_\n",
    "#     cumulative_explained_var = np.cumsum(explained_var_ratio)\n",
    "\n",
    "#     # 3. Find the number of components that explain up to 90% of the variance\n",
    "#     num_components = np.argmax(cumulative_explained_var >= 0.90) + 1\n",
    "#     print(f\"Number of components that explain up to 90% of variance: {num_components}\")\n",
    "\n",
    "#     # Refit PCA with the selected number of components\n",
    "#     pca_90 = PCA(n_components=num_components)\n",
    "#     X = pca_90.fit_transform(X_scaled)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    cat_cols = get_cat_cols(X_train)\n",
    "    num_cols = [col for col in X_train.columns if col not in cat_cols]\n",
    "\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('resampling', None),\n",
    "        ('classifier', DecisionTreeClassifier(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "#         'scaler': [\n",
    "#             StandardScaler(), \n",
    "#             RobustScaler(),\n",
    "#             MinMaxScaler()\n",
    "#         ],\n",
    "        'resampling': [\n",
    "            SMOTE(random_state=random_state),\n",
    "            ADASYN(random_state=random_state),\n",
    "            SMOTEENN(random_state=random_state),\n",
    "            SMOTETomek(random_state=random_state),\n",
    "            EditedNearestNeighbours(),\n",
    "            TomekLinks(),\n",
    "            RandomOverSampler(random_state=random_state),\n",
    "            RandomUnderSampler(random_state=random_state)\n",
    "        ],\n",
    "#         'classifier__max_depth': [3],\n",
    "# #         'classifier__min_samples_split': [1],\n",
    "#         'classifier__min_samples_leaf': [1],\n",
    "#         'classifier__max_depth': [2, 5],\n",
    "#         'classifier__min_samples_split': [3],\n",
    "#         'classifier__min_samples_leaf': [3],\n",
    "# #         'classifier__max_features': ['auto', 'sqrt', 'log2', None],\n",
    "#         'classifier__criterion': [\"entropy\", \"log_loss\"]\n",
    "#         'classifier__max_depth': [2, 3],\n",
    "#         'classifier__min_samples_split': [1],\n",
    "#         'classifier__min_samples_leaf': [2, 3],\n",
    "        'classifier__max_depth': [2, 3, 5],\n",
    "        'classifier__min_samples_split': [2, 3],\n",
    "        'classifier__min_samples_leaf': [1, 2, 3],\n",
    "        'classifier__max_features': ['auto', 'sqrt', 'log2', None],\n",
    "        'classifier__criterion': [\"gini\", \"entropy\", \"log_loss\"]\n",
    "    }\n",
    "\n",
    "    start_time_cv = time.time()\n",
    "    grid_search = GridSearchCV(pipeline, param_grid,\n",
    "#                                cv=10, scoring=scorer,\n",
    "                               cv=skf, scoring=scorer,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=4, return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    end_time_cv = time.time()\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Validation Score (Precision):\", best_score)\n",
    "    print('Best Estimator:', best_estimator)\n",
    "    print('Mean Train Score (Precision)', np.mean(cv_results['mean_train_score']))\n",
    "    print('Std Train Score (Precision)', np.mean(cv_results['std_train_score']))\n",
    "    elapsed_time_cv = (end_time_cv - start_time_cv)\n",
    "    print(f'GridSearchCV Runtime: {elapsed_time_cv} secs')\n",
    "\n",
    "    num_runs = 5\n",
    "    metrics_per_run = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        modified_random_state = random_state + run\n",
    "\n",
    "        start_time = time.time()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            stratify=y,\n",
    "                                                            random_state=modified_random_state)\n",
    "\n",
    "        modified_estimator = clone(best_estimator)\n",
    "#         if not (isinstance(best_estimator.named_steps['resampling'], EditedNearestNeighbours) or\n",
    "#             isinstance(best_estimator.named_steps['resampling'], TomekLinks)):\n",
    "#             modified_estimator.named_steps['resampling'].set_params(random_state=modified_random_state)\n",
    "#         modified_estimator.named_steps['classifier'].set_params(random_state=modified_random_state)\n",
    "        modified_estimator.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = modified_estimator.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = (end_time - start_time)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        specificity = specificity_score(y_test, y_pred)\n",
    "        npv = npv_score(y_test, y_pred)\n",
    "        g_mean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Run {run + 1} - Random State: {modified_random_state}\")\n",
    "        print(f\"Test Precision: {precision}\")\n",
    "\n",
    "#         y_hat = modified_estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "#         gmeans = np.sqrt(tpr * (1-fpr))\n",
    "#         ix = np.argmax(gmeans)\n",
    "#         print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "#         plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "#         plt.plot(fpr, tpr, marker='.', label='Original')\n",
    "#         plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         plt.legend()\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "        metrics_per_run.append({\n",
    "            'Random State': modified_random_state,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Precision': precision,\n",
    "            'Test Recall': recall,\n",
    "            'Test F1 Score': f1,\n",
    "            'Test Specificity': specificity,\n",
    "            'Test G-mean': g_mean,\n",
    "            'Test NPV': npv,\n",
    "            'Runtime': elapsed_time\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_per_run)\n",
    "\n",
    "    avg_metrics = metrics_df.mean()\n",
    "    std_metrics = metrics_df.std()\n",
    "\n",
    "    print('Ave Test Precision:', avg_metrics['Test Precision'])\n",
    "    print('Stdev Test Precision:', std_metrics['Test Precision'])\n",
    "    print(\"Ave Test Accuracy:\", avg_metrics['Test Accuracy'])\n",
    "    print('Stdev Test Accuracy:', std_metrics['Test Accuracy'])\n",
    "    print(\"Ave Test Specificity:\", avg_metrics['Test Specificity'])\n",
    "    print(\"Ave Test Recall:\", avg_metrics['Test Recall'])\n",
    "    print('Ave Test NPV:', avg_metrics['Test NPV'])\n",
    "    print(\"Ave Test F1-Score:\", avg_metrics['Test F1 Score'])\n",
    "    print(\"Ave Test G-mean:\", avg_metrics['Test G-mean'])\n",
    "    print(\"Ave Runtime:\", avg_metrics['Runtime'])\n",
    "\n",
    "    model_info = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'mean_train_score': np.mean(cv_results['mean_train_score']),\n",
    "        'std_train_score': np.mean(cv_results['std_train_score']),\n",
    "        'average_test_precision': avg_metrics['Test Precision'],\n",
    "        'stdev_test_precision': std_metrics['Test Precision'],\n",
    "        'average_test_accuracy': avg_metrics['Test Accuracy'],\n",
    "        'stdev_test_accuracy': std_metrics['Test Accuracy'],\n",
    "        'average_runtime': avg_metrics['Runtime'],\n",
    "        'average_test_specificity': avg_metrics['Test Specificity'],\n",
    "        'average_test_recall': avg_metrics['Test Recall'],\n",
    "        'average_test_npv': avg_metrics['Test NPV'],\n",
    "        'average_test_f1_score': avg_metrics['Test F1 Score'],\n",
    "        'average_test_g_mean': avg_metrics['Test G-mean']\n",
    "    }\n",
    "\n",
    "    return grid_search, best_estimator, model_info, metrics_df\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def rf_class2(df, scorer, label_col, scaler, random_state=0):\n",
    "#     df_majority = df[df.home_ownership_class == 0]\n",
    "#     df_minority = df[df.home_ownership_class == 1]\n",
    "\n",
    "# #     display(df_majority)\n",
    "# #     display(df_minority)\n",
    "# #     print(len(df_majority), len(df_minority))\n",
    "#     # Randomly downsample majority class\n",
    "#     df_majority_downsampled = df_majority.sample(frac=0.2,\n",
    "#                                                  random_state=random_state)\n",
    "#     df = pd.concat([df_majority_downsampled, df_minority], axis=0)\n",
    "\n",
    "    X = df.drop(['userId', 'lastFirstName', label_col], axis=1)\n",
    "    y = df.loc[:, label_col]\n",
    "\n",
    "#     scaler = RobustScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "#     pca = PCA().fit(X_scaled)\n",
    "\n",
    "#     # 2. Plot the cumulative explained variance\n",
    "#     explained_var_ratio = pca.explained_variance_ratio_\n",
    "#     cumulative_explained_var = np.cumsum(explained_var_ratio)\n",
    "\n",
    "#     # 3. Find the number of components that explain up to 90% of the variance\n",
    "#     num_components = np.argmax(cumulative_explained_var >= 0.90) + 1\n",
    "#     print(f\"Number of components that explain up to 90% of variance: {num_components}\")\n",
    "\n",
    "#     # Refit PCA with the selected number of components\n",
    "#     pca_90 = PCA(n_components=num_components)\n",
    "#     X = pca_90.fit_transform(X_scaled)\n",
    "\n",
    "#     scaler = RobustScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#     n_components = X_scaled.shape[1]  # Maximum number of components\n",
    "#     svd = TruncatedSVD(n_components=n_components)\n",
    "#     X_svd = svd.fit_transform(X_scaled)\n",
    "\n",
    "#     # Step 3: Find number of components that explain at least 90% variance\n",
    "#     cum_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "#     num_components = np.argmax(cum_variance >= 0.90) + 1  # +1 because Python indexing starts from 0\n",
    "#     X_selected = X_svd[:, :num_components]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "#     X_resampled, y_resampled = RandomOverSampler(random_state=random_state).fit_resample(X_train, y_train)\n",
    "#     class_counts_original = np.bincount(y_train.astype(int))\n",
    "#     class_counts_resampled = np.bincount(y_resampled.astype(int))\n",
    "\n",
    "    # Assuming 0 is the majority class, calculate its upweight factor\n",
    "#     majority_class_original = class_counts_original[0]\n",
    "#     majority_class_resampled = class_counts_resampled[0]\n",
    "#     upweight_factor = majority_class_original / majority_class_resampled\n",
    "\n",
    "#     minority_class_original = class_counts_original[1]\n",
    "#     minority_class_resampled = class_counts_resampled[1]\n",
    "#     downweight_factor = minority_class_original / minority_class_resampled\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('resampling', None),\n",
    "        ('classifier', RandomForestClassifier(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "#         'scaler': [\n",
    "#             StandardScaler(),\n",
    "#             MinMaxScaler(),\n",
    "#             RobustScaler()\n",
    "#         ],\n",
    "#         'resampling': [SMOTE(random_state=random_state),\n",
    "#                        SMOTENC(categorical_features=get_cat_cols(X),\n",
    "#                                random_state=random_state),\n",
    "#                        ADASYN(random_state=random_state),\n",
    "# #                        RandomOverSampler(random_state=random_state),\n",
    "#                        RandomUnderSampler(random_state=random_state)],\n",
    "#         'scaler': [RobustScaler(),\n",
    "#                    PowerTransformer()],\n",
    "#         'resampling': [SMOTENC(categorical_features=get_cat_cols(X),\n",
    "#                                random_state=random_state),\n",
    "        'resampling': [\n",
    "            SMOTE(random_state=random_state),\n",
    "            ADASYN(random_state=random_state),\n",
    "#             SMOTEENN(random_state=random_state),\n",
    "#             SMOTETomek(random_state=random_state),\n",
    "            EditedNearestNeighbours(),\n",
    "#             TomekLinks(),\n",
    "            RandomOverSampler(random_state=random_state),\n",
    "            RandomUnderSampler(random_state=random_state)\n",
    "        ],\n",
    "        'classifier__n_estimators': [200, 300],\n",
    "        'classifier__max_depth': [2, 3],\n",
    "        'classifier__min_samples_split': [2, 3],\n",
    "        'classifier__min_samples_leaf': [2, 3],\n",
    "        'classifier__oob_score': [True, False],\n",
    "        'classifier__criterion': [\"gini\", \"entropy\", \"log_loss\"]\n",
    "\n",
    "#         'classifier__n_estimators': [200, 250],\n",
    "#         'classifier__max_depth': [2, 3],\n",
    "#         'classifier__min_samples_split': [3, 4],\n",
    "#         'classifier__min_samples_leaf': [3, 4],\n",
    "#         'classifier__oob_score': [True, False],\n",
    "#         'classifier__criterion': [\"gini\", \"entropy\", \"log_loss\"]\n",
    "    }\n",
    "#         'classifier__n_estimators': [200, 250, 300],\n",
    "# #         'classifier__max_depth': [2, 3, 4],\n",
    "#         'classifier__max_depth': [2],\n",
    "#         'classifier__class_weight': [{0: 263/807, 1:1}],\n",
    "#         'classifier__oob_score': [True],\n",
    "# #         'classifier__class_weight': [{0: 1, 1:807/263}],\n",
    "#         'classifier__min_samples_split': [3, 4],\n",
    "#         'classifier__min_samples_leaf': [3, 4],\n",
    "#         'classifier__criterion': [\"gini\", \"entropy\", \"log_loss\"]\n",
    "#     }\n",
    "    start_time_cv = time.time()\n",
    "    grid_search = GridSearchCV(pipeline, param_grid,\n",
    "#                                cv=10, scoring=scorer,\n",
    "                               cv=skf, scoring=scorer,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=4, return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    end_time_cv = time.time()\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Validation Score (Precision):\", best_score)\n",
    "    print('Best Estimator:', best_estimator)\n",
    "    print('Mean Train Score (Precision)', np.mean(cv_results['mean_train_score']))\n",
    "    print('Std Train Score (Precision)', np.mean(cv_results['std_train_score']))\n",
    "    elapsed_time_cv = (end_time_cv - start_time_cv)\n",
    "    print(f'GridSearchCV Runtime: {elapsed_time_cv} secs')\n",
    "\n",
    "    num_runs = 5\n",
    "    metrics_per_run = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        modified_random_state = random_state + run\n",
    "\n",
    "        start_time = time.time()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            stratify=y,\n",
    "                                                            random_state=modified_random_state)\n",
    "\n",
    "        modified_estimator = clone(best_estimator)\n",
    "#         if not (isinstance(best_estimator.named_steps['resampling'], EditedNearestNeighbours) or\n",
    "#             isinstance(best_estimator.named_steps['resampling'], TomekLinks)):\n",
    "#             modified_estimator.named_steps['resampling'].set_params(random_state=modified_random_state)\n",
    "#         modified_estimator.named_steps['classifier'].set_params(random_state=modified_random_state)\n",
    "        modified_estimator.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = modified_estimator.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = (end_time - start_time)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        specificity = specificity_score(y_test, y_pred)\n",
    "        npv = npv_score(y_test, y_pred)\n",
    "        g_mean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Run {run + 1} - Random State: {modified_random_state}\")\n",
    "        print(f\"Test Precision: {precision}\")\n",
    "\n",
    "#         y_hat = modified_estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "#         gmeans = np.sqrt(tpr * (1-fpr))\n",
    "#         ix = np.argmax(gmeans)\n",
    "#         print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "#         plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "#         plt.plot(fpr, tpr, marker='.', label='Original')\n",
    "#         plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         plt.legend()\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "        metrics_per_run.append({\n",
    "            'Random State': modified_random_state,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Precision': precision,\n",
    "            'Test Recall': recall,\n",
    "            'Test F1 Score': f1,\n",
    "            'Test Specificity': specificity,\n",
    "            'Test G-mean': g_mean,\n",
    "            'Test NPV': npv,\n",
    "            'Runtime': elapsed_time\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_per_run)\n",
    "\n",
    "    avg_metrics = metrics_df.mean()\n",
    "    std_metrics = metrics_df.std()\n",
    "\n",
    "    print('Ave Test Precision:', avg_metrics['Test Precision'])\n",
    "    print('Stdev Test Precision:', std_metrics['Test Precision'])\n",
    "    print(\"Ave Test Accuracy:\", avg_metrics['Test Accuracy'])\n",
    "    print('Stdev Test Accuracy:', std_metrics['Test Accuracy'])\n",
    "    print(\"Ave Test Specificity:\", avg_metrics['Test Specificity'])\n",
    "    print(\"Ave Test Recall:\", avg_metrics['Test Recall'])\n",
    "    print('Ave Test NPV:', avg_metrics['Test NPV'])\n",
    "    print(\"Ave Test F1-Score:\", avg_metrics['Test F1 Score'])\n",
    "    print(\"Ave Test G-mean:\", avg_metrics['Test G-mean'])\n",
    "    print(\"Ave Runtime:\", avg_metrics['Runtime'])\n",
    "\n",
    "    model_info = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'mean_train_score': np.mean(cv_results['mean_train_score']),\n",
    "        'std_train_score': np.mean(cv_results['std_train_score']),\n",
    "        'average_test_precision': avg_metrics['Test Precision'],\n",
    "        'stdev_test_precision': std_metrics['Test Precision'],\n",
    "        'average_test_accuracy': avg_metrics['Test Accuracy'],\n",
    "        'stdev_test_accuracy': std_metrics['Test Accuracy'],\n",
    "        'average_runtime': avg_metrics['Runtime'],\n",
    "        'average_test_specificity': avg_metrics['Test Specificity'],\n",
    "        'average_test_recall': avg_metrics['Test Recall'],\n",
    "        'average_test_npv': avg_metrics['Test NPV'],\n",
    "        'average_test_f1_score': avg_metrics['Test F1 Score'],\n",
    "        'average_test_g_mean': avg_metrics['Test G-mean']\n",
    "    }\n",
    "\n",
    "    return grid_search, best_estimator, model_info, metrics_df\n",
    "\n",
    "def gbm_class2(df, scorer, label_col, scaler, random_state=0):\n",
    "    X = df.drop(['userId', 'lastFirstName', label_col], axis=1)\n",
    "    y = df.loc[:, label_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('resampling', None),\n",
    "        ('classifier', GradientBoostingClassifier(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "#         'scaler': [StandardScaler(),\n",
    "#                    MinMaxScaler(),\n",
    "#                    RobustScaler()\n",
    "#                   ],\n",
    "        'resampling': [\n",
    "            SMOTE(random_state=random_state),\n",
    "                       ADASYN(random_state=random_state),\n",
    "#                        SMOTEENN(random_state=random_state),\n",
    "#                        SMOTETomek(random_state=random_state),\n",
    "                       EditedNearestNeighbours(),\n",
    "#                        TomekLinks(),\n",
    "                       RandomOverSampler(random_state=random_state),\n",
    "                       RandomUnderSampler(random_state=random_state)],\n",
    "#         'classifier__n_estimators': [200],\n",
    "#         'classifier__max_depth': [None],\n",
    "#         'classifier__min_samples_leaf': [3],\n",
    "#         'classifier__min_samples_split': [5],\n",
    "#         'classifier__learning_rate': [0.1]\n",
    "        'classifier__n_estimators': [200, 250],\n",
    "        'classifier__max_depth': [2, 3],\n",
    "        'classifier__min_samples_leaf': [2, 3],\n",
    "        'classifier__min_samples_split': [2, 3],\n",
    "        'classifier__learning_rate': [0.1]\n",
    "    }\n",
    "#         'loss': ['log_loss', 'deviance', 'exponential']\n",
    "#         'min_samples_leaf': [1, 2, 3],\n",
    "#         'min_samples_split': [2, 3, 5],\n",
    "\n",
    "    start_time_cv = time.time()\n",
    "    grid_search = GridSearchCV(pipeline, param_grid,\n",
    "                               cv=skf, scoring=scorer,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=4, return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    end_time_cv = time.time()\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Validation Score (Precision):\", best_score)\n",
    "    print('Best Estimator:', best_estimator)\n",
    "    print('Mean Train Score (Precision)', np.mean(cv_results['mean_train_score']))\n",
    "    print('Std Train Score (Precision)', np.mean(cv_results['std_train_score']))\n",
    "    elapsed_time_cv = (end_time_cv - start_time_cv)\n",
    "    print(f'GridSearchCV Runtime: {elapsed_time_cv} secs')\n",
    "\n",
    "    num_runs = 5\n",
    "    metrics_per_run = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        modified_random_state = random_state + run\n",
    "\n",
    "        start_time = time.time()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            stratify=y,\n",
    "                                                            random_state=modified_random_state)\n",
    "\n",
    "        modified_estimator = clone(best_estimator)\n",
    "#         if not (isinstance(best_estimator.named_steps['resampling'], EditedNearestNeighbours) or\n",
    "#             isinstance(best_estimator.named_steps['resampling'], TomekLinks)):\n",
    "#             modified_estimator.named_steps['resampling'].set_params(random_state=modified_random_state)\n",
    "#         modified_estimator.named_steps['classifier'].set_params(random_state=modified_random_state)\n",
    "        modified_estimator.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = modified_estimator.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = (end_time - start_time)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        specificity = specificity_score(y_test, y_pred)\n",
    "        npv = npv_score(y_test, y_pred)\n",
    "        g_mean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Run {run + 1} - Random State: {modified_random_state}\")\n",
    "        print(f\"Test Precision: {precision}\")\n",
    "\n",
    "#         y_hat = modified_estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "#         gmeans = np.sqrt(tpr * (1-fpr))\n",
    "#         ix = np.argmax(gmeans)\n",
    "#         print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "#         plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "#         plt.plot(fpr, tpr, marker='.', label='Original')\n",
    "#         plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         plt.legend()\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "        metrics_per_run.append({\n",
    "            'Random State': modified_random_state,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Precision': precision,\n",
    "            'Test Recall': recall,\n",
    "            'Test F1 Score': f1,\n",
    "            'Test Specificity': specificity,\n",
    "            'Test G-mean': g_mean,\n",
    "            'Test NPV': npv,\n",
    "            'Runtime': elapsed_time\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_per_run)\n",
    "\n",
    "    avg_metrics = metrics_df.mean()\n",
    "    std_metrics = metrics_df.std()\n",
    "\n",
    "    print('Ave Test Precision:', avg_metrics['Test Precision'])\n",
    "    print('Stdev Test Precision:', std_metrics['Test Precision'])\n",
    "    print(\"Ave Test Accuracy:\", avg_metrics['Test Accuracy'])\n",
    "    print('Stdev Test Accuracy:', std_metrics['Test Accuracy'])\n",
    "    print(\"Ave Test Specificity:\", avg_metrics['Test Specificity'])\n",
    "    print(\"Ave Test Recall:\", avg_metrics['Test Recall'])\n",
    "    print('Ave Test NPV:', avg_metrics['Test NPV'])\n",
    "    print(\"Ave Test F1-Score:\", avg_metrics['Test F1 Score'])\n",
    "    print(\"Ave Test G-mean:\", avg_metrics['Test G-mean'])\n",
    "    print(\"Ave Runtime:\", avg_metrics['Runtime'])\n",
    "\n",
    "    model_info = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'mean_train_score': np.mean(cv_results['mean_train_score']),\n",
    "        'std_train_score': np.mean(cv_results['std_train_score']),\n",
    "        'average_test_precision': avg_metrics['Test Precision'],\n",
    "        'stdev_test_precision': std_metrics['Test Precision'],\n",
    "        'average_test_accuracy': avg_metrics['Test Accuracy'],\n",
    "        'stdev_test_accuracy': std_metrics['Test Accuracy'],\n",
    "        'average_runtime': avg_metrics['Runtime'],\n",
    "        'average_test_specificity': avg_metrics['Test Specificity'],\n",
    "        'average_test_recall': avg_metrics['Test Recall'],\n",
    "        'average_test_npv': avg_metrics['Test NPV'],\n",
    "        'average_test_f1_score': avg_metrics['Test F1 Score'],\n",
    "        'average_test_g_mean': avg_metrics['Test G-mean']\n",
    "    }\n",
    "\n",
    "    return grid_search, best_estimator, model_info, metrics_df\n",
    "\n",
    "def lgbm_class2(df, scorer, label_col, scaler, random_state=0):\n",
    "    X = df.drop(['userId', 'lastFirstName', label_col], axis=1)\n",
    "    y = df.loc[:, label_col]\n",
    "\n",
    "#     scaler = RobustScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "#     pca = PCA().fit(X_scaled)\n",
    "\n",
    "#     # 2. Plot the cumulative explained variance\n",
    "#     explained_var_ratio = pca.explained_variance_ratio_\n",
    "#     cumulative_explained_var = np.cumsum(explained_var_ratio)\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(range(1, len(explained_var_ratio)+1), cumulative_explained_var, marker='o', linestyle='--')\n",
    "#     plt.title('Cumulative Explained Variance Plot')\n",
    "#     plt.xlabel('Number of Components')\n",
    "#     plt.ylabel('Cumulative Explained Variance')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "#     # 3. Find the number of components that explain up to 90% of the variance\n",
    "#     num_components = np.argmax(cumulative_explained_var >= 0.90) + 1\n",
    "#     print(f\"Number of components that explain up to 90% of variance: {num_components}\")\n",
    "\n",
    "#     # Refit PCA with the selected number of components\n",
    "#     pca_90 = PCA(n_components=num_components)\n",
    "#     X = pca_90.fit_transform(X_scaled)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('resampling', None),\n",
    "        ('classifier', LGBMClassifier(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "#         'scaler': [StandardScaler(),\n",
    "#                    MinMaxScaler(),\n",
    "#                    RobustScaler()\n",
    "#                   ],\n",
    "#         'resampling': [SMOTE(random_state=random_state),\n",
    "#                        SMOTENC(categorical_features=get_cat_cols(X),\n",
    "#                                random_state=random_state),\n",
    "#                        ADASYN(random_state=random_state),\n",
    "# #                        RandomOverSampler(random_state=random_state),\n",
    "#                        RandomUnderSampler(random_state=random_state)],\n",
    "#         'scaler': [RobustScaler(),\n",
    "#                    PowerTransformer()],\n",
    "#         'resampling': [SMOTENC(categorical_features=get_cat_cols(X),\n",
    "#                                random_state=random_state),\n",
    "        'resampling': [SMOTE(random_state=random_state),\n",
    "                       ADASYN(random_state=random_state),\n",
    "#                        SMOTEENN(random_state=random_state),\n",
    "#                        SMOTETomek(random_state=random_state),\n",
    "                       EditedNearestNeighbours(),\n",
    "#                        TomekLinks(),\n",
    "                       RandomOverSampler(random_state=random_state),\n",
    "                       RandomUnderSampler(random_state=random_state)],\n",
    "#         'classifier__n_estimators': [400, 450],\n",
    "#         'classifier__learning_rate': [0.01],\n",
    "#         'classifier__num_leaves': [20, 25],\n",
    "#         'classifier__min_child_samples': [40, 45],\n",
    "#         'classifier__class_weight': [{0: 263/807, 1:1}, {0: 1, 1:807/263}],\n",
    "#         'classifier__class_weight': [{0: 1, 1:807/263}],\n",
    "#         'classifier__max_depth': [2, 3],\n",
    "#         'classifier__boosting_type': ['gbdt', 'dart', 'goss']\n",
    "\n",
    "        'classifier__n_estimators': [200, 300, 400],\n",
    "        'classifier__learning_rate': [0.1, 0.01],\n",
    "#         'classifier__num_leaves': [20, 25],\n",
    "#         'classifier__min_child_samples': [40, 45],\n",
    "#         'classifier__class_weight': [{0: 263/807, 1:1}, {0: 1, 1:807/263}],\n",
    "#         'classifier__class_weight': [{0: 1, 1:807/263}],\n",
    "        'classifier__max_depth': [2, 3],\n",
    "        'classifier__boosting_type': ['gbdt', 'dart', 'goss']\n",
    "    }\n",
    "#         'classifier__num_leaves': [20, 31, 40],\n",
    "#         'classifier__min_child_samples': [10, 20, 30],\n",
    "#         'classifier__max_depth': [-1, 5, 10],\n",
    "\n",
    "    start_time_cv = time.time()\n",
    "    grid_search = GridSearchCV(pipeline, param_grid,\n",
    "                               cv=skf, scoring=scorer,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=4, return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    end_time_cv = time.time()\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Validation Score (Precision):\", best_score)\n",
    "    print('Best Estimator:', best_estimator)\n",
    "    print('Mean Train Score (Precision)', np.mean(cv_results['mean_train_score']))\n",
    "    print('Std Train Score (Precision)', np.mean(cv_results['std_train_score']))\n",
    "    elapsed_time_cv = (end_time_cv - start_time_cv)\n",
    "    print(f'GridSearchCV Runtime: {elapsed_time_cv} secs')\n",
    "\n",
    "    num_runs = 5\n",
    "    metrics_per_run = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        modified_random_state = random_state + run\n",
    "\n",
    "        start_time = time.time()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            stratify=y,\n",
    "                                                            random_state=modified_random_state)\n",
    "\n",
    "        modified_estimator = clone(best_estimator)\n",
    "#         if not (isinstance(best_estimator.named_steps['resampling'], EditedNearestNeighbours) or\n",
    "#             isinstance(best_estimator.named_steps['resampling'], TomekLinks)):\n",
    "#             modified_estimator.named_steps['resampling'].set_params(random_state=modified_random_state)\n",
    "#         modified_estimator.named_steps['classifier'].set_params(random_state=modified_random_state)\n",
    "        modified_estimator.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = modified_estimator.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = (end_time - start_time)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        specificity = specificity_score(y_test, y_pred)\n",
    "        npv = npv_score(y_test, y_pred)\n",
    "        g_mean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Run {run + 1} - Random State: {modified_random_state}\")\n",
    "        print(f\"Test Precision: {precision}\")\n",
    "\n",
    "#         y_hat = modified_estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "#         gmeans = np.sqrt(tpr * (1-fpr))\n",
    "#         ix = np.argmax(gmeans)\n",
    "#         print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "#         plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "#         plt.plot(fpr, tpr, marker='.', label='Original')\n",
    "#         plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         plt.legend()\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "        metrics_per_run.append({\n",
    "            'Random State': modified_random_state,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Precision': precision,\n",
    "            'Test Recall': recall,\n",
    "            'Test F1 Score': f1,\n",
    "            'Test Specificity': specificity,\n",
    "            'Test G-mean': g_mean,\n",
    "            'Test NPV': npv,\n",
    "            'Runtime': elapsed_time\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_per_run)\n",
    "\n",
    "    avg_metrics = metrics_df.mean()\n",
    "    std_metrics = metrics_df.std()\n",
    "\n",
    "    print('Ave Test Precision:', avg_metrics['Test Precision'])\n",
    "    print('Stdev Test Precision:', std_metrics['Test Precision'])\n",
    "    print(\"Ave Test Accuracy:\", avg_metrics['Test Accuracy'])\n",
    "    print('Stdev Test Accuracy:', std_metrics['Test Accuracy'])\n",
    "    print(\"Ave Test Specificity:\", avg_metrics['Test Specificity'])\n",
    "    print(\"Ave Test Recall:\", avg_metrics['Test Recall'])\n",
    "    print('Ave Test NPV:', avg_metrics['Test NPV'])\n",
    "    print(\"Ave Test F1-Score:\", avg_metrics['Test F1 Score'])\n",
    "    print(\"Ave Test G-mean:\", avg_metrics['Test G-mean'])\n",
    "    print(\"Ave Runtime:\", avg_metrics['Runtime'])\n",
    "\n",
    "    model_info = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'mean_train_score': np.mean(cv_results['mean_train_score']),\n",
    "        'std_train_score': np.mean(cv_results['std_train_score']),\n",
    "        'average_test_precision': avg_metrics['Test Precision'],\n",
    "        'stdev_test_precision': std_metrics['Test Precision'],\n",
    "        'average_test_accuracy': avg_metrics['Test Accuracy'],\n",
    "        'stdev_test_accuracy': std_metrics['Test Accuracy'],\n",
    "        'average_runtime': avg_metrics['Runtime'],\n",
    "        'average_test_specificity': avg_metrics['Test Specificity'],\n",
    "        'average_test_recall': avg_metrics['Test Recall'],\n",
    "        'average_test_npv': avg_metrics['Test NPV'],\n",
    "        'average_test_f1_score': avg_metrics['Test F1 Score'],\n",
    "        'average_test_g_mean': avg_metrics['Test G-mean']\n",
    "    }\n",
    "\n",
    "    return grid_search, best_estimator, model_info, metrics_df\n",
    "\n",
    "def xgb_class2(df, scorer, label_col, scaler, random_state=0):\n",
    "    X = df.drop(['userId', 'lastFirstName', label_col], axis=1)\n",
    "    y = df.loc[:, label_col]\n",
    "\n",
    "#     scaler = RobustScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "#     pca = PCA().fit(X_scaled)\n",
    "\n",
    "#     # 2. Plot the cumulative explained variance\n",
    "#     explained_var_ratio = pca.explained_variance_ratio_\n",
    "#     cumulative_explained_var = np.cumsum(explained_var_ratio)\n",
    "\n",
    "#     # 3. Find the number of components that explain up to 90% of the variance\n",
    "#     num_components = np.argmax(cumulative_explained_var >= 0.90) + 1\n",
    "#     print(f\"Number of components that explain up to 90% of variance: {num_components}\")\n",
    "\n",
    "#     # Refit PCA with the selected number of components\n",
    "#     pca_90 = PCA(n_components=num_components)\n",
    "#     X = pca_90.fit_transform(X_scaled)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('resampling', None),\n",
    "        ('classifier', XGBClassifier(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "#         'scaler': [\n",
    "#             StandardScaler(),\n",
    "#             MinMaxScaler(),\n",
    "#             RobustScaler()\n",
    "#         ],\n",
    "#         'resampling': [SMOTE(random_state=random_state),\n",
    "#                        SMOTENC(categorical_features=get_cat_cols(X),\n",
    "#                                random_state=random_state),\n",
    "#                        ADASYN(random_state=random_state),\n",
    "# #                        RandomOverSampler(random_state=random_state),\n",
    "#                        RandomUnderSampler(random_state=random_state)],\n",
    "#         'scaler': [RobustScaler(),\n",
    "#                    PowerTransformer()],\n",
    "#         'resampling': [SMOTENC(categorical_features=get_cat_cols(X),\n",
    "#                                random_state=random_state),\n",
    "        'resampling': [SMOTE(random_state=random_state),\n",
    "                       ADASYN(random_state=random_state),\n",
    "#                        SMOTEENN(random_state=random_state),\n",
    "#                        SMOTETomek(random_state=random_state),\n",
    "                       EditedNearestNeighbours(),\n",
    "#                        TomekLinks(),\n",
    "                       RandomOverSampler(random_state=random_state),\n",
    "                       RandomUnderSampler(random_state=random_state)],\n",
    "# #         'classifier__n_estimators': [200, 250],\n",
    "#         'classifier__learning_rate': [2, 3],\n",
    "# #         'classifier__max_depth': [2, 3],\n",
    "#         'classifier__max_leaves': [5],\n",
    "#         'classifier__grow_policy': ['depthwise', 'lossguide']\n",
    "        'classifier__n_estimators': [200, 250],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 1],\n",
    "        'classifier__max_depth': [None, 2, 3],\n",
    "        'classifier__max_leaves': [10, 20, 30],\n",
    "        'classifier__grow_policy': ['depthwise', 'lossguide']\n",
    "    }\n",
    "#         'classifier__max_depth': [None, 5, 10],\n",
    "#         'classifier__max_leaves': [0, 10, 20],\n",
    "\n",
    "    start_time_cv = time.time()\n",
    "    grid_search = GridSearchCV(pipeline, param_grid,\n",
    "                               cv=skf, scoring=scorer,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=4, return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    end_time_cv = time.time()\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Validation Score (Precision):\", best_score)\n",
    "    print('Best Estimator:', best_estimator)\n",
    "    print('Mean Train Score (Precision)', np.mean(cv_results['mean_train_score']))\n",
    "    print('Std Train Score (Precision)', np.mean(cv_results['std_train_score']))\n",
    "    elapsed_time_cv = (end_time_cv - start_time_cv)\n",
    "    print(f'GridSearchCV Runtime: {elapsed_time_cv} secs')\n",
    "\n",
    "    num_runs = 5\n",
    "    metrics_per_run = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        modified_random_state = random_state + run\n",
    "\n",
    "        start_time = time.time()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            stratify=y,\n",
    "                                                            random_state=modified_random_state)\n",
    "\n",
    "        modified_estimator = clone(best_estimator)\n",
    "#         if not (isinstance(best_estimator.named_steps['resampling'], EditedNearestNeighbours) or\n",
    "#             isinstance(best_estimator.named_steps['resampling'], TomekLinks)):\n",
    "#             modified_estimator.named_steps['resampling'].set_params(random_state=modified_random_state)\n",
    "#         modified_estimator.named_steps['classifier'].set_params(random_state=modified_random_state)\n",
    "        modified_estimator.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = modified_estimator.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = (end_time - start_time)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        specificity = specificity_score(y_test, y_pred)\n",
    "        npv = npv_score(y_test, y_pred)\n",
    "        g_mean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Run {run + 1} - Random State: {modified_random_state}\")\n",
    "        print(f\"Test Precision: {precision}\")\n",
    "\n",
    "#         y_hat = modified_estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "#         gmeans = np.sqrt(tpr * (1-fpr))\n",
    "#         ix = np.argmax(gmeans)\n",
    "#         print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "#         plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "#         plt.plot(fpr, tpr, marker='.', label='Original')\n",
    "#         plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         plt.legend()\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "        metrics_per_run.append({\n",
    "            'Random State': modified_random_state,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Precision': precision,\n",
    "            'Test Recall': recall,\n",
    "            'Test F1 Score': f1,\n",
    "            'Test Specificity': specificity,\n",
    "            'Test G-mean': g_mean,\n",
    "            'Test NPV': npv,\n",
    "            'Runtime': elapsed_time\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_per_run)\n",
    "\n",
    "    avg_metrics = metrics_df.mean()\n",
    "    std_metrics = metrics_df.std()\n",
    "\n",
    "    print('Ave Test Precision:', avg_metrics['Test Precision'])\n",
    "    print('Stdev Test Precision:', std_metrics['Test Precision'])\n",
    "    print(\"Ave Test Accuracy:\", avg_metrics['Test Accuracy'])\n",
    "    print('Stdev Test Accuracy:', std_metrics['Test Accuracy'])\n",
    "    print(\"Ave Test Specificity:\", avg_metrics['Test Specificity'])\n",
    "    print(\"Ave Test Recall:\", avg_metrics['Test Recall'])\n",
    "    print('Ave Test NPV:', avg_metrics['Test NPV'])\n",
    "    print(\"Ave Test F1-Score:\", avg_metrics['Test F1 Score'])\n",
    "    print(\"Ave Test G-mean:\", avg_metrics['Test G-mean'])\n",
    "    print(\"Ave Runtime:\", avg_metrics['Runtime'])\n",
    "\n",
    "    model_info = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'mean_train_score': np.mean(cv_results['mean_train_score']),\n",
    "        'std_train_score': np.mean(cv_results['std_train_score']),\n",
    "        'average_test_precision': avg_metrics['Test Precision'],\n",
    "        'stdev_test_precision': std_metrics['Test Precision'],\n",
    "        'average_test_accuracy': avg_metrics['Test Accuracy'],\n",
    "        'stdev_test_accuracy': std_metrics['Test Accuracy'],\n",
    "        'average_runtime': avg_metrics['Runtime'],\n",
    "        'average_test_specificity': avg_metrics['Test Specificity'],\n",
    "        'average_test_recall': avg_metrics['Test Recall'],\n",
    "        'average_test_npv': avg_metrics['Test NPV'],\n",
    "        'average_test_f1_score': avg_metrics['Test F1 Score'],\n",
    "        'average_test_g_mean': avg_metrics['Test G-mean']\n",
    "    }\n",
    "\n",
    "    return grid_search, best_estimator, model_info, metrics_df\n",
    "\n",
    "def adaboost_class2(df, scorer, label_col, scaler, random_state=0):\n",
    "    X = df.drop(['userId', 'lastFirstName', label_col], axis=1)\n",
    "    y = df.loc[:, label_col]\n",
    "\n",
    "#     scaler = RobustScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "#     pca = PCA().fit(X_scaled)\n",
    "\n",
    "#     # 2. Plot the cumulative explained variance\n",
    "#     explained_var_ratio = pca.explained_variance_ratio_\n",
    "#     cumulative_explained_var = np.cumsum(explained_var_ratio)\n",
    "\n",
    "#     # 3. Find the number of components that explain up to 90% of the variance\n",
    "#     num_components = np.argmax(cumulative_explained_var >= 0.90) + 1\n",
    "#     print(f\"Number of components that explain up to 90% of variance: {num_components}\")\n",
    "\n",
    "#     # Refit PCA with the selected number of components\n",
    "#     pca_90 = PCA(n_components=num_components)\n",
    "#     X = pca_90.fit_transform(X_scaled)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    cat_cols = get_cat_cols(X_train)\n",
    "    num_cols = [col for col in X_train.columns if col not in cat_cols]\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('resampling', None),\n",
    "        ('classifier', AdaBoostClassifier(\n",
    "            random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "#         'scaler': [\n",
    "#             StandardScaler(),\n",
    "#             MinMaxScaler(),\n",
    "#             RobustScaler()\n",
    "#         ],\n",
    "        'resampling': [\n",
    "            SMOTE(random_state=random_state),\n",
    "            ADASYN(random_state=random_state),\n",
    "#             SMOTEENN(random_state=random_state),\n",
    "#             SMOTETomek(random_state=random_state),\n",
    "            EditedNearestNeighbours(),\n",
    "#             TomekLinks(),\n",
    "            RandomOverSampler(random_state=random_state),\n",
    "            RandomUnderSampler(random_state=random_state)],\n",
    "        'classifier__n_estimators': [200, 300],\n",
    "        'classifier__estimator': [DecisionTreeClassifier(\n",
    "            random_state=random_state,\n",
    "#             criterion='entropy', max_depth=3,\n",
    "#             min_samples_leaf=2, min_samples_split=2\n",
    "        ),\n",
    "                                  RandomForestClassifier(\n",
    "            random_state=random_state,\n",
    "#             max_depth=3, min_samples_leaf=2,\n",
    "#             min_samples_split=2, n_estimators=250,\n",
    "#             oob_score=True\n",
    "                                                        )],\n",
    "#         'classifier__learning_rate': [0.01, 0.1, 1],\n",
    "        'classifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "    }\n",
    "\n",
    "    start_time_cv = time.time()\n",
    "    grid_search = GridSearchCV(pipeline, param_grid,\n",
    "                               cv=skf, scoring=scorer,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=4, return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    end_time_cv = time.time()\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Validation Score (Precision):\", best_score)\n",
    "    print('Best Estimator:', best_estimator)\n",
    "    print('Mean Train Score (Precision)', np.mean(cv_results['mean_train_score']))\n",
    "    print('Std Train Score (Precision)', np.mean(cv_results['std_train_score']))\n",
    "    elapsed_time_cv = (end_time_cv - start_time_cv)\n",
    "    print(f'GridSearchCV Runtime: {elapsed_time_cv} secs')\n",
    "\n",
    "    num_runs = 5\n",
    "    metrics_per_run = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        modified_random_state = random_state + run\n",
    "\n",
    "        start_time = time.time()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            stratify=y,\n",
    "                                                            random_state=modified_random_state)\n",
    "\n",
    "        modified_estimator = clone(best_estimator)\n",
    "#         if not (isinstance(best_estimator.named_steps['resampling'], EditedNearestNeighbours) or\n",
    "#             isinstance(best_estimator.named_steps['resampling'], TomekLinks)):\n",
    "#             modified_estimator.named_steps['resampling'].set_params(random_state=modified_random_state)\n",
    "#         modified_estimator.named_steps['classifier'].set_params(random_state=modified_random_state)\n",
    "        modified_estimator.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = modified_estimator.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = (end_time - start_time)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        specificity = specificity_score(y_test, y_pred)\n",
    "        npv = npv_score(y_test, y_pred)\n",
    "        g_mean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Run {run + 1} - Random State: {modified_random_state}\")\n",
    "        print(f\"Test Precision: {precision}\")\n",
    "\n",
    "#         y_hat = modified_estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "#         gmeans = np.sqrt(tpr * (1-fpr))\n",
    "#         ix = np.argmax(gmeans)\n",
    "#         print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "#         plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "#         plt.plot(fpr, tpr, marker='.', label='Original')\n",
    "#         plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         plt.legend()\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "        metrics_per_run.append({\n",
    "            'Random State': modified_random_state,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Precision': precision,\n",
    "            'Test Recall': recall,\n",
    "            'Test F1 Score': f1,\n",
    "            'Test Specificity': specificity,\n",
    "            'Test G-mean': g_mean,\n",
    "            'Test NPV': npv,\n",
    "            'Runtime': elapsed_time\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_per_run)\n",
    "\n",
    "    avg_metrics = metrics_df.mean()\n",
    "    std_metrics = metrics_df.std()\n",
    "\n",
    "    print('Ave Test Precision:', avg_metrics['Test Precision'])\n",
    "    print('Stdev Test Precision:', std_metrics['Test Precision'])\n",
    "    print(\"Ave Test Accuracy:\", avg_metrics['Test Accuracy'])\n",
    "    print('Stdev Test Accuracy:', std_metrics['Test Accuracy'])\n",
    "    print(\"Ave Test Specificity:\", avg_metrics['Test Specificity'])\n",
    "    print(\"Ave Test Recall:\", avg_metrics['Test Recall'])\n",
    "    print('Ave Test NPV:', avg_metrics['Test NPV'])\n",
    "    print(\"Ave Test F1-Score:\", avg_metrics['Test F1 Score'])\n",
    "    print(\"Ave Test G-mean:\", avg_metrics['Test G-mean'])\n",
    "    print(\"Ave Runtime:\", avg_metrics['Runtime'])\n",
    "\n",
    "    model_info = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'mean_train_score': np.mean(cv_results['mean_train_score']),\n",
    "        'std_train_score': np.mean(cv_results['std_train_score']),\n",
    "        'average_test_precision': avg_metrics['Test Precision'],\n",
    "        'stdev_test_precision': std_metrics['Test Precision'],\n",
    "        'average_test_accuracy': avg_metrics['Test Accuracy'],\n",
    "        'stdev_test_accuracy': std_metrics['Test Accuracy'],\n",
    "        'average_runtime': avg_metrics['Runtime'],\n",
    "        'average_test_specificity': avg_metrics['Test Specificity'],\n",
    "        'average_test_recall': avg_metrics['Test Recall'],\n",
    "        'average_test_npv': avg_metrics['Test NPV'],\n",
    "        'average_test_f1_score': avg_metrics['Test F1 Score'],\n",
    "        'average_test_g_mean': avg_metrics['Test G-mean']\n",
    "    }\n",
    "\n",
    "    return grid_search, best_estimator, model_info, metrics_df\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "def ridge_class2(df, scorer, label_col, scaler, random_state=0):\n",
    "    X = df.drop(['userId', 'lastFirstName', label_col], axis=1)\n",
    "    y = df.loc[:, label_col]\n",
    "\n",
    "#     scaler = RobustScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "#     pca = PCA().fit(X_scaled)\n",
    "\n",
    "#     # 2. Plot the cumulative explained variance\n",
    "#     explained_var_ratio = pca.explained_variance_ratio_\n",
    "#     cumulative_explained_var = np.cumsum(explained_var_ratio)\n",
    "\n",
    "#     # 3. Find the number of components that explain up to 90% of the variance\n",
    "#     num_components = np.argmax(cumulative_explained_var >= 0.90) + 1\n",
    "#     print(f\"Number of components that explain up to 90% of variance: {num_components}\")\n",
    "\n",
    "#     # Refit PCA with the selected number of components\n",
    "#     pca_90 = PCA(n_components=num_components)\n",
    "#     X = pca_90.fit_transform(X_scaled)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    cat_cols = get_cat_cols(X_train)\n",
    "    num_cols = [col for col in X_train.columns if col not in cat_cols]\n",
    "    \n",
    "    cat_transformer = Pipeline([\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    num_transformer = Pipeline([\n",
    "        ('variance_threshold', VarianceThreshold(threshold=0.15))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers = [\n",
    "            ('num', num_transformer, num_cols),\n",
    "            ('cat', cat_transformer, cat_cols)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    preprocessor_num = ColumnTransformer(\n",
    "        transformers = [\n",
    "            ('num', num_transformer, num_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    preprocessor_drop_cat = ColumnTransformer(\n",
    "        transformers = [\n",
    "            ('num', 'passthrough', num_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "#         ('preprocessor', None),\n",
    "        ('preprocessor', preprocessor_drop_cat),\n",
    "        ('scaler', scaler),\n",
    "        ('resampling', None),\n",
    "        ('var_threshold', VarianceThreshold(threshold=0.15)),\n",
    "        ('classifier', RidgeClassifier(max_iter=1000, random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "#         'preprocessor': [\n",
    "#             preprocessor_drop_cat,\n",
    "#             preprocessor,\n",
    "#             preprocessor_num            \n",
    "#         ],\n",
    "        'resampling': [ADASYN(random_state=random_state),\n",
    "                       SMOTEENN(random_state=random_state),\n",
    "                       SMOTETomek(random_state=random_state),\n",
    "                       EditedNearestNeighbours(),\n",
    "                       TomekLinks(),\n",
    "                       RandomUnderSampler(random_state=random_state)],\n",
    "        'classifier__alpha': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'classifier__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sag', 'saga'],\n",
    "    }\n",
    "\n",
    "    start_time_cv = time.time()\n",
    "    grid_search = GridSearchCV(pipeline, param_grid,\n",
    "                               cv=10, scoring=scorer,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=4, return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    end_time_cv = time.time()\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Validation Score (Precision):\", best_score)\n",
    "    print('Best Estimator:', best_estimator)\n",
    "    print('Mean Train Score (Precision)', np.mean(cv_results['mean_train_score']))\n",
    "    print('Std Train Score (Precision)', np.mean(cv_results['std_train_score']))\n",
    "    elapsed_time_cv = (end_time_cv - start_time_cv)\n",
    "    print(f'GridSearchCV Runtime: {elapsed_time_cv} secs')\n",
    "\n",
    "    num_runs = 5\n",
    "    metrics_per_run = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        modified_random_state = random_state + run\n",
    "\n",
    "        start_time = time.time()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            stratify=y,\n",
    "                                                            random_state=modified_random_state)\n",
    "\n",
    "        modified_estimator = clone(best_estimator)\n",
    "#         if not (isinstance(best_estimator.named_steps['resampling'], EditedNearestNeighbours) or\n",
    "#             isinstance(best_estimator.named_steps['resampling'], TomekLinks)):\n",
    "#             modified_estimator.named_steps['resampling'].set_params(random_state=modified_random_state)\n",
    "#         modified_estimator.named_steps['classifier'].set_params(random_state=modified_random_state)\n",
    "        modified_estimator.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = modified_estimator.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = (end_time - start_time)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        specificity = specificity_score(y_test, y_pred)\n",
    "        npv = npv_score(y_test, y_pred)\n",
    "        g_mean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Run {run + 1} - Random State: {modified_random_state}\")\n",
    "        print(f\"Test Precision: {precision}\")\n",
    "\n",
    "#         y_hat = modified_estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "#         gmeans = np.sqrt(tpr * (1-fpr))\n",
    "#         ix = np.argmax(gmeans)\n",
    "#         print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "#         plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "#         plt.plot(fpr, tpr, marker='.', label='Original')\n",
    "#         plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         plt.legend()\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "        metrics_per_run.append({\n",
    "            'Random State': modified_random_state,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Precision': precision,\n",
    "            'Test Recall': recall,\n",
    "            'Test F1 Score': f1,\n",
    "            'Test Specificity': specificity,\n",
    "            'Test G-mean': g_mean,\n",
    "            'Test NPV': npv,\n",
    "            'Runtime': elapsed_time\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_per_run)\n",
    "\n",
    "    avg_metrics = metrics_df.mean()\n",
    "    std_metrics = metrics_df.std()\n",
    "\n",
    "    print('Ave Test Precision:', avg_metrics['Test Precision'])\n",
    "    print('Stdev Test Precision:', std_metrics['Test Precision'])\n",
    "    print(\"Ave Test Accuracy:\", avg_metrics['Test Accuracy'])\n",
    "    print('Stdev Test Accuracy:', std_metrics['Test Accuracy'])\n",
    "    print(\"Ave Test Specificity:\", avg_metrics['Test Specificity'])\n",
    "    print(\"Ave Test Recall:\", avg_metrics['Test Recall'])\n",
    "    print('Ave Test NPV:', avg_metrics['Test NPV'])\n",
    "    print(\"Ave Test F1-Score:\", avg_metrics['Test F1 Score'])\n",
    "    print(\"Ave Test G-mean:\", avg_metrics['Test G-mean'])\n",
    "    print(\"Ave Runtime:\", avg_metrics['Runtime'])\n",
    "\n",
    "    model_info = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'mean_train_score': np.mean(cv_results['mean_train_score']),\n",
    "        'std_train_score': np.mean(cv_results['std_train_score']),\n",
    "        'average_test_precision': avg_metrics['Test Precision'],\n",
    "        'stdev_test_precision': std_metrics['Test Precision'],\n",
    "        'average_test_accuracy': avg_metrics['Test Accuracy'],\n",
    "        'stdev_test_accuracy': std_metrics['Test Accuracy'],\n",
    "        'average_runtime': avg_metrics['Runtime'],\n",
    "        'average_test_specificity': avg_metrics['Test Specificity'],\n",
    "        'average_test_recall': avg_metrics['Test Recall'],\n",
    "        'average_test_npv': avg_metrics['Test NPV'],\n",
    "        'average_test_f1_score': avg_metrics['Test F1 Score'],\n",
    "        'average_test_g_mean': avg_metrics['Test G-mean']\n",
    "    }\n",
    "\n",
    "    return grid_search, best_estimator, model_info, metrics_df\n",
    "\n",
    "def catboost_class2(df, scorer, label_col, scaler, random_state=0):\n",
    "    X = df.drop(['userId', 'lastFirstName', label_col], axis=1)\n",
    "    y = df.loc[:, label_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('resampling', None),\n",
    "        ('classifier', CatBoostClassifier(random_state=random_state, ))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "#         'scaler': [StandardScaler(),\n",
    "#                    MinMaxScaler(),\n",
    "#                    RobustScaler()\n",
    "#                   ],\n",
    "        'resampling': [\n",
    "            SMOTE(random_state=random_state),\n",
    "                       ADASYN(random_state=random_state),\n",
    "#                        SMOTEENN(random_state=random_state),\n",
    "#                        SMOTETomek(random_state=random_state),\n",
    "                       EditedNearestNeighbours(),\n",
    "#                        TomekLinks(),\n",
    "                       RandomOverSampler(random_state=random_state),\n",
    "                       RandomUnderSampler(random_state=random_state)],\n",
    "#         'classifier__n_estimators': [200],\n",
    "#         'classifier__max_depth': [None],\n",
    "#         'classifier__min_samples_leaf': [3],\n",
    "#         'classifier__min_samples_split': [5],\n",
    "#         'classifier__learning_rate': [0.1]\n",
    "#         'classifier__n_estimators': [200, 250],\n",
    "#         'classifier__max_depth': [2, 3],\n",
    "#         'classifier__min_samples_leaf': [2, 3],\n",
    "#         'classifier__min_samples_split': [2, 3],\n",
    "#         'classifier__learning_rate': [0.1]\n",
    "    }\n",
    "#         'loss': ['log_loss', 'deviance', 'exponential']\n",
    "#         'min_samples_leaf': [1, 2, 3],\n",
    "#         'min_samples_split': [2, 3, 5],\n",
    "\n",
    "    start_time_cv = time.time()\n",
    "    grid_search = GridSearchCV(pipeline, param_grid,\n",
    "                               cv=skf, scoring=scorer,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=4, return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    end_time_cv = time.time()\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Validation Score (Precision):\", best_score)\n",
    "    print('Best Estimator:', best_estimator)\n",
    "    print('Mean Train Score (Precision)', np.mean(cv_results['mean_train_score']))\n",
    "    print('Std Train Score (Precision)', np.mean(cv_results['std_train_score']))\n",
    "    elapsed_time_cv = (end_time_cv - start_time_cv)\n",
    "    print(f'GridSearchCV Runtime: {elapsed_time_cv} secs')\n",
    "\n",
    "    num_runs = 5\n",
    "    metrics_per_run = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        modified_random_state = random_state + run\n",
    "\n",
    "        start_time = time.time()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=0.25,\n",
    "                                                            stratify=y,\n",
    "                                                            random_state=modified_random_state)\n",
    "\n",
    "        modified_estimator = clone(best_estimator)\n",
    "#         if not (isinstance(best_estimator.named_steps['resampling'], EditedNearestNeighbours) or\n",
    "#             isinstance(best_estimator.named_steps['resampling'], TomekLinks)):\n",
    "#             modified_estimator.named_steps['resampling'].set_params(random_state=modified_random_state)\n",
    "#         modified_estimator.named_steps['classifier'].set_params(random_state=modified_random_state)\n",
    "        modified_estimator.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = modified_estimator.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = (end_time - start_time)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        specificity = specificity_score(y_test, y_pred)\n",
    "        npv = npv_score(y_test, y_pred)\n",
    "        g_mean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Run {run + 1} - Random State: {modified_random_state}\")\n",
    "        print(f\"Test Precision: {precision}\")\n",
    "\n",
    "#         y_hat = modified_estimator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "#         gmeans = np.sqrt(tpr * (1-fpr))\n",
    "#         ix = np.argmax(gmeans)\n",
    "#         print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "#         plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "#         plt.plot(fpr, tpr, marker='.', label='Original')\n",
    "#         plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         plt.legend()\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "        metrics_per_run.append({\n",
    "            'Random State': modified_random_state,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Precision': precision,\n",
    "            'Test Recall': recall,\n",
    "            'Test F1 Score': f1,\n",
    "            'Test Specificity': specificity,\n",
    "            'Test G-mean': g_mean,\n",
    "            'Test NPV': npv,\n",
    "            'Runtime': elapsed_time\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_per_run)\n",
    "\n",
    "    avg_metrics = metrics_df.mean()\n",
    "    std_metrics = metrics_df.std()\n",
    "\n",
    "    print('Ave Test Precision:', avg_metrics['Test Precision'])\n",
    "    print('Stdev Test Precision:', std_metrics['Test Precision'])\n",
    "    print(\"Ave Test Accuracy:\", avg_metrics['Test Accuracy'])\n",
    "    print('Stdev Test Accuracy:', std_metrics['Test Accuracy'])\n",
    "    print(\"Ave Test Specificity:\", avg_metrics['Test Specificity'])\n",
    "    print(\"Ave Test Recall:\", avg_metrics['Test Recall'])\n",
    "    print('Ave Test NPV:', avg_metrics['Test NPV'])\n",
    "    print(\"Ave Test F1-Score:\", avg_metrics['Test F1 Score'])\n",
    "    print(\"Ave Test G-mean:\", avg_metrics['Test G-mean'])\n",
    "    print(\"Ave Runtime:\", avg_metrics['Runtime'])\n",
    "\n",
    "    model_info = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'mean_train_score': np.mean(cv_results['mean_train_score']),\n",
    "        'std_train_score': np.mean(cv_results['std_train_score']),\n",
    "        'average_test_precision': avg_metrics['Test Precision'],\n",
    "        'stdev_test_precision': std_metrics['Test Precision'],\n",
    "        'average_test_accuracy': avg_metrics['Test Accuracy'],\n",
    "        'stdev_test_accuracy': std_metrics['Test Accuracy'],\n",
    "        'average_runtime': avg_metrics['Runtime'],\n",
    "        'average_test_specificity': avg_metrics['Test Specificity'],\n",
    "        'average_test_recall': avg_metrics['Test Recall'],\n",
    "        'average_test_npv': avg_metrics['Test NPV'],\n",
    "        'average_test_f1_score': avg_metrics['Test F1 Score'],\n",
    "        'average_test_g_mean': avg_metrics['Test G-mean']\n",
    "    }\n",
    "\n",
    "    return grid_search, best_estimator, model_info, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CatBoostClassifier in module catboost.core:\n",
      "\n",
      "class CatBoostClassifier(CatBoost)\n",
      " |  CatBoostClassifier(iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function=None, border_count=None, feature_border_type=None, per_float_feature_quantization=None, input_borders=None, output_borders=None, fold_permutation_block=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, best_model_min_trees=None, verbose=None, silent=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, allow_const_label=None, target_border=None, classes_count=None, class_weights=None, auto_class_weights=None, class_names=None, one_hot_max_size=None, random_strength=None, random_score_type=None, name=None, ignored_features=None, train_dir=None, custom_loss=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, pinned_memory_size=None, allow_writing_files=None, final_ctr_computation_mode=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, ctr_description=None, ctr_target_border_count=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, mvs_reg=None, sampling_unit=None, sampling_frequency=None, dev_score_calc_obj_block_size=None, dev_efb_max_buckets=None, sparse_features_conflict_fraction=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, scale_pos_weight=None, gpu_cat_features_storage=None, data_partition=None, metadata=None, early_stopping_rounds=None, cat_features=None, grow_policy=None, min_data_in_leaf=None, min_child_samples=None, max_leaves=None, num_leaves=None, score_function=None, leaf_estimation_backtracking=None, ctr_history_unit=None, monotone_constraints=None, feature_weights=None, penalties_coefficient=None, first_feature_use_penalties=None, per_object_feature_penalties=None, model_shrink_rate=None, model_shrink_mode=None, langevin=None, diffusion_temperature=None, posterior_sampling=None, boost_from_average=None, text_features=None, tokenizers=None, dictionaries=None, feature_calcers=None, text_processing=None, embedding_features=None, callback=None, eval_fraction=None, fixed_binary_splits=None)\n",
      " |  \n",
      " |  Implementation of the scikit-learn API for CatBoost classification.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  iterations : int, [default=500]\n",
      " |      Max count of trees.\n",
      " |      range: [1,+inf)\n",
      " |  learning_rate : float, [default value is selected automatically for binary classification with other parameters set to default. In all other cases default is 0.03]\n",
      " |      Step size shrinkage used in update to prevents overfitting.\n",
      " |      range: (0,1]\n",
      " |  depth : int, [default=6]\n",
      " |      Depth of a tree. All trees are the same depth.\n",
      " |      range: [1,16]\n",
      " |  l2_leaf_reg : float, [default=3.0]\n",
      " |      Coefficient at the L2 regularization term of the cost function.\n",
      " |      range: [0,+inf)\n",
      " |  model_size_reg : float, [default=None]\n",
      " |      Model size regularization coefficient.\n",
      " |      range: [0,+inf)\n",
      " |  rsm : float, [default=None]\n",
      " |      Subsample ratio of columns when constructing each tree.\n",
      " |      range: (0,1]\n",
      " |  loss_function : string or object, [default='Logloss']\n",
      " |      The metric to use in training and also selector of the machine learning\n",
      " |      problem to solve. If string, then the name of a supported metric,\n",
      " |      optionally suffixed with parameter description.\n",
      " |      If object, it shall provide methods 'calc_ders_range' or 'calc_ders_multi'.\n",
      " |  border_count : int, [default = 254 for training on CPU or 128 for training on GPU]\n",
      " |      The number of partitions in numeric features binarization. Used in the preliminary calculation.\n",
      " |      range: [1,65535] on CPU, [1,255] on GPU\n",
      " |  feature_border_type : string, [default='GreedyLogSum']\n",
      " |      The binarization mode in numeric features binarization. Used in the preliminary calculation.\n",
      " |      Possible values:\n",
      " |          - 'Median'\n",
      " |          - 'Uniform'\n",
      " |          - 'UniformAndQuantiles'\n",
      " |          - 'GreedyLogSum'\n",
      " |          - 'MaxLogSum'\n",
      " |          - 'MinEntropy'\n",
      " |  per_float_feature_quantization : list of strings, [default=None]\n",
      " |      List of float binarization descriptions.\n",
      " |      Format : described in documentation on catboost.ai\n",
      " |      Example 1: ['0:1024'] means that feature 0 will have 1024 borders.\n",
      " |      Example 2: ['0:border_count=1024', '1:border_count=1024', ...] means that two first features have 1024 borders.\n",
      " |      Example 3: ['0:nan_mode=Forbidden,border_count=32,border_type=GreedyLogSum',\n",
      " |                  '1:nan_mode=Forbidden,border_count=32,border_type=GreedyLogSum'] - defines more quantization properties for first two features.\n",
      " |  input_borders : string or pathlib.Path, [default=None]\n",
      " |      input file with borders used in numeric features binarization.\n",
      " |  output_borders : string, [default=None]\n",
      " |      output file for borders that were used in numeric features binarization.\n",
      " |  fold_permutation_block : int, [default=1]\n",
      " |      To accelerate the learning.\n",
      " |      The recommended value is within [1, 256]. On small samples, must be set to 1.\n",
      " |      range: [1,+inf)\n",
      " |  od_pval : float, [default=None]\n",
      " |      Use overfitting detector to stop training when reaching a specified threshold.\n",
      " |      Can be used only with eval_set.\n",
      " |      range: [0,1]\n",
      " |  od_wait : int, [default=None]\n",
      " |      Number of iterations which overfitting detector will wait after new best error.\n",
      " |  od_type : string, [default=None]\n",
      " |      Type of overfitting detector which will be used in program.\n",
      " |      Posible values:\n",
      " |          - 'IncToDec'\n",
      " |          - 'Iter'\n",
      " |      For 'Iter' type od_pval must not be set.\n",
      " |      If None, then od_type=IncToDec.\n",
      " |  nan_mode : string, [default=None]\n",
      " |      Way to process missing values for numeric features.\n",
      " |      Possible values:\n",
      " |          - 'Forbidden' - raises an exception if there is a missing value for a numeric feature in a dataset.\n",
      " |          - 'Min' - each missing value will be processed as the minimum numerical value.\n",
      " |          - 'Max' - each missing value will be processed as the maximum numerical value.\n",
      " |      If None, then nan_mode=Min.\n",
      " |  counter_calc_method : string, [default=None]\n",
      " |      The method used to calculate counters for dataset with Counter type.\n",
      " |      Possible values:\n",
      " |          - 'PrefixTest' - only objects up to current in the test dataset are considered\n",
      " |          - 'FullTest' - all objects are considered in the test dataset\n",
      " |          - 'SkipTest' - Objects from test dataset are not considered\n",
      " |          - 'Full' - all objects are considered for both learn and test dataset\n",
      " |      If None, then counter_calc_method=PrefixTest.\n",
      " |  leaf_estimation_iterations : int, [default=None]\n",
      " |      The number of steps in the gradient when calculating the values in the leaves.\n",
      " |      If None, then leaf_estimation_iterations=1.\n",
      " |      range: [1,+inf)\n",
      " |  leaf_estimation_method : string, [default=None]\n",
      " |      The method used to calculate the values in the leaves.\n",
      " |      Possible values:\n",
      " |          - 'Newton'\n",
      " |          - 'Gradient'\n",
      " |  thread_count : int, [default=None]\n",
      " |      Number of parallel threads used to run CatBoost.\n",
      " |      If None or -1, then the number of threads is set to the number of CPU cores.\n",
      " |      range: [1,+inf)\n",
      " |  random_seed : int, [default=None]\n",
      " |      Random number seed.\n",
      " |      If None, 0 is used.\n",
      " |      range: [0,+inf)\n",
      " |  use_best_model : bool, [default=None]\n",
      " |      To limit the number of trees in predict() using information about the optimal value of the error function.\n",
      " |      Can be used only with eval_set.\n",
      " |  best_model_min_trees : int, [default=None]\n",
      " |      The minimal number of trees the best model should have.\n",
      " |  verbose: bool\n",
      " |      When set to True, logging_level is set to 'Verbose'.\n",
      " |      When set to False, logging_level is set to 'Silent'.\n",
      " |  silent: bool, synonym for verbose\n",
      " |  logging_level : string, [default='Verbose']\n",
      " |      Possible values:\n",
      " |          - 'Silent'\n",
      " |          - 'Verbose'\n",
      " |          - 'Info'\n",
      " |          - 'Debug'\n",
      " |  metric_period : int, [default=1]\n",
      " |      The frequency of iterations to print the information to stdout. The value should be a positive integer.\n",
      " |  simple_ctr: list of strings, [default=None]\n",
      " |      Binarization settings for categorical features.\n",
      " |          Format : see documentation\n",
      " |          Example: ['Borders:CtrBorderCount=5:Prior=0:Prior=0.5', 'BinarizedTargetMeanValue:TargetBorderCount=10:TargetBorderType=MinEntropy', ...]\n",
      " |          CTR types:\n",
      " |              CPU and GPU\n",
      " |              - 'Borders'\n",
      " |              - 'Buckets'\n",
      " |              CPU only\n",
      " |              - 'BinarizedTargetMeanValue'\n",
      " |              - 'Counter'\n",
      " |              GPU only\n",
      " |              - 'FloatTargetMeanValue'\n",
      " |              - 'FeatureFreq'\n",
      " |          Number_of_borders, binarization type, target borders and binarizations, priors are optional parametrs\n",
      " |  combinations_ctr: list of strings, [default=None]\n",
      " |  per_feature_ctr: list of strings, [default=None]\n",
      " |  ctr_target_border_count: int, [default=None]\n",
      " |      Maximum number of borders used in target binarization for categorical features that need it.\n",
      " |      If TargetBorderCount is specified in 'simple_ctr', 'combinations_ctr' or 'per_feature_ctr' option it\n",
      " |      overrides this value.\n",
      " |      range: [1, 255]\n",
      " |  ctr_leaf_count_limit : int, [default=None]\n",
      " |      The maximum number of leaves with categorical features.\n",
      " |      If the number of leaves exceeds the specified limit, some leaves are discarded.\n",
      " |      The leaves to be discarded are selected as follows:\n",
      " |          - The leaves are sorted by the frequency of the values.\n",
      " |          - The top N leaves are selected, where N is the value specified in the parameter.\n",
      " |          - All leaves starting from N+1 are discarded.\n",
      " |      This option reduces the resulting model size\n",
      " |      and the amount of memory required for training.\n",
      " |      Note that the resulting quality of the model can be affected.\n",
      " |      range: [1,+inf) (for zero limit use ignored_features)\n",
      " |  store_all_simple_ctr : bool, [default=None]\n",
      " |      Ignore categorical features, which are not used in feature combinations,\n",
      " |      when choosing candidates for exclusion.\n",
      " |      Use this parameter with ctr_leaf_count_limit only.\n",
      " |  max_ctr_complexity : int, [default=4]\n",
      " |      The maximum number of Categ features that can be combined.\n",
      " |      range: [0,+inf)\n",
      " |  has_time : bool, [default=False]\n",
      " |      To use the order in which objects are represented in the input data\n",
      " |      (do not perform a random permutation of the dataset at the preprocessing stage).\n",
      " |  allow_const_label : bool, [default=False]\n",
      " |      To allow the constant label value in dataset.\n",
      " |  target_border: float, [default=None]\n",
      " |      Border for target binarization.\n",
      " |  classes_count : int, [default=None]\n",
      " |      The upper limit for the numeric class label.\n",
      " |      Defines the number of classes for multiclassification.\n",
      " |      Only non-negative integers can be specified.\n",
      " |      The given integer should be greater than any of the target values.\n",
      " |      If this parameter is specified the labels for all classes in the input dataset\n",
      " |      should be smaller than the given value.\n",
      " |      If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n",
      " |      the numbers of classes specified by each of them must be equal.\n",
      " |  class_weights : list or dict, [default=None]\n",
      " |      Classes weights. The values are used as multipliers for the object weights.\n",
      " |      If None, all classes are supposed to have weight one.\n",
      " |      If list - class weights in order of class_names or sequential classes if class_names is undefined\n",
      " |      If dict - dict of class_name -> class_weight.\n",
      " |      If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n",
      " |      the numbers of classes specified by each of them must be equal.\n",
      " |  auto_class_weights : string [default=None]\n",
      " |      Enables automatic class weights calculation. Possible values:\n",
      " |          - Balanced  # weight = maxSummaryClassWeight / summaryClassWeight, statistics determined from train pool\n",
      " |          - SqrtBalanced  # weight = sqrt(maxSummaryClassWeight / summaryClassWeight)\n",
      " |  class_names: list of strings, [default=None]\n",
      " |      Class names. Allows to redefine the default values for class labels (integer numbers).\n",
      " |      If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n",
      " |      the numbers of classes specified by each of them must be equal.\n",
      " |  one_hot_max_size : int, [default=None]\n",
      " |      Convert the feature to float\n",
      " |      if the number of different values that it takes exceeds the specified value.\n",
      " |      Ctrs are not calculated for such features.\n",
      " |  random_strength : float, [default=1]\n",
      " |      Score standard deviation multiplier.\n",
      " |  random_score_type : string [default=None]\n",
      " |      Type of random noise added to scores.\n",
      " |      Possible values:\n",
      " |          - 'Gumbel' - Gumbel-distributed\n",
      " |          - 'NormalWithModelSizeDecrease' - Normally-distributed with deviation decreasing with model iteration count\n",
      " |      If None than 'NormalWithModelSizeDecrease' will be used by default.\n",
      " |  name : string, [default='experiment']\n",
      " |      The name that should be displayed in the visualization tools.\n",
      " |  ignored_features : list, [default=None]\n",
      " |      Indices or names of features that should be excluded when training.\n",
      " |  train_dir : string or pathlib.Path, [default=None]\n",
      " |      The directory in which you want to record generated in the process of learning files.\n",
      " |  custom_metric : string or list of strings, [default=None]\n",
      " |      To use your own metric function.\n",
      " |  custom_loss: alias to custom_metric\n",
      " |  eval_metric : string or object, [default=None]\n",
      " |      To optimize your custom metric in loss.\n",
      " |  bagging_temperature : float, [default=None]\n",
      " |      Controls intensity of Bayesian bagging. The higher the temperature the more aggressive bagging is.\n",
      " |      Typical values are in range [0, 1] (0 - no bagging, 1 - default).\n",
      " |  save_snapshot : bool, [default=None]\n",
      " |      Enable progress snapshotting for restoring progress after crashes or interruptions\n",
      " |  snapshot_file : string or pathlib.Path, [default=None]\n",
      " |      Learn progress snapshot file path, if None will use default filename\n",
      " |  snapshot_interval: int, [default=600]\n",
      " |      Interval between saving snapshots (seconds)\n",
      " |  fold_len_multiplier : float, [default=None]\n",
      " |      Fold length multiplier. Should be greater than 1\n",
      " |  used_ram_limit : string or number, [default=None]\n",
      " |      Set a limit on memory consumption (value like '1.2gb' or 1.2e9).\n",
      " |      WARNING: Currently this option affects CTR memory usage only.\n",
      " |  gpu_ram_part : float, [default=0.95]\n",
      " |      Fraction of the GPU RAM to use for training, a value from (0, 1].\n",
      " |  pinned_memory_size: int [default=None]\n",
      " |      Size of additional CPU pinned memory used for GPU learning,\n",
      " |      usually is estimated automatically, thus usually should not be set.\n",
      " |  allow_writing_files : bool, [default=True]\n",
      " |      If this flag is set to False, no files with different diagnostic info will be created during training.\n",
      " |      With this flag no snapshotting can be done. Plus visualisation will not\n",
      " |      work, because visualisation uses files that are created and updated during training.\n",
      " |  final_ctr_computation_mode : string, [default='Default']\n",
      " |      Possible values:\n",
      " |          - 'Default' - Compute final ctrs for all pools.\n",
      " |          - 'Skip' - Skip final ctr computation. WARNING: model without ctrs can't be applied.\n",
      " |  approx_on_full_history : bool, [default=False]\n",
      " |      If this flag is set to True, each approximated value is calculated using all the preceeding rows in the fold (slower, more accurate).\n",
      " |      If this flag is set to False, each approximated value is calculated using only the beginning 1/fold_len_multiplier fraction of the fold (faster, slightly less accurate).\n",
      " |  boosting_type : string, default value depends on object count and feature count in train dataset and on learning mode.\n",
      " |      Boosting scheme.\n",
      " |      Possible values:\n",
      " |          - 'Ordered' - Gives better quality, but may slow down the training.\n",
      " |          - 'Plain' - The classic gradient boosting scheme. May result in quality degradation, but does not slow down the training.\n",
      " |  task_type : string, [default=None]\n",
      " |      The calcer type used to train the model.\n",
      " |      Possible values:\n",
      " |          - 'CPU'\n",
      " |          - 'GPU'\n",
      " |  device_config : string, [default=None], deprecated, use devices instead\n",
      " |  devices : list or string, [default=None], GPU devices to use.\n",
      " |      String format is: '0' for 1 device or '0:1:3' for multiple devices or '0-3' for range of devices.\n",
      " |      List format is : [0] for 1 device or [0,1,3] for multiple devices.\n",
      " |  \n",
      " |  bootstrap_type : string, Bayesian, Bernoulli, Poisson, MVS.\n",
      " |      Default bootstrap is Bayesian for GPU and MVS for CPU.\n",
      " |      Poisson bootstrap is supported only on GPU.\n",
      " |      MVS bootstrap is supported only on GPU.\n",
      " |  \n",
      " |  subsample : float, [default=None]\n",
      " |      Sample rate for bagging. This parameter can be used Poisson or Bernoully bootstrap types.\n",
      " |  \n",
      " |  mvs_reg : float, [default is set automatically at each iteration based on gradient distribution]\n",
      " |      Regularization parameter for MVS sampling algorithm\n",
      " |  \n",
      " |  monotone_constraints : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Monotone constraints for features.\n",
      " |  \n",
      " |  feature_weights : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Coefficient to multiply split gain with specific feature use. Should be non-negative.\n",
      " |  \n",
      " |  penalties_coefficient : float, [default=1]\n",
      " |      Common coefficient for all penalties. Should be non-negative.\n",
      " |  \n",
      " |  first_feature_use_penalties : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Penalties to first use of specific feature in model. Should be non-negative.\n",
      " |  \n",
      " |  per_object_feature_penalties : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Penalties for first use of feature for each object. Should be non-negative.\n",
      " |  \n",
      " |  sampling_frequency : string, [default=PerTree]\n",
      " |      Frequency to sample weights and objects when building trees.\n",
      " |      Possible values:\n",
      " |          - 'PerTree' - Before constructing each new tree\n",
      " |          - 'PerTreeLevel' - Before choosing each new split of a tree\n",
      " |  \n",
      " |  sampling_unit : string, [default='Object'].\n",
      " |      Possible values:\n",
      " |          - 'Object'\n",
      " |          - 'Group'\n",
      " |      The parameter allows to specify the sampling scheme:\n",
      " |      sample weights for each object individually or for an entire group of objects together.\n",
      " |  \n",
      " |  dev_score_calc_obj_block_size: int, [default=5000000]\n",
      " |      CPU only. Size of block of samples in score calculation. Should be > 0\n",
      " |      Used only for learning speed tuning.\n",
      " |      Changing this parameter can affect results due to numerical accuracy differences\n",
      " |  \n",
      " |  dev_efb_max_buckets : int, [default=1024]\n",
      " |      CPU only. Maximum bucket count in exclusive features bundle. Should be in an integer between 0 and 65536.\n",
      " |      Used only for learning speed tuning.\n",
      " |  \n",
      " |  sparse_features_conflict_fraction : float, [default=0.0]\n",
      " |      CPU only. Maximum allowed fraction of conflicting non-default values for features in exclusive features bundle.\n",
      " |      Should be a real value in [0, 1) interval.\n",
      " |  \n",
      " |  grow_policy : string, [SymmetricTree,Lossguide,Depthwise], [default=SymmetricTree]\n",
      " |      The tree growing policy. It describes how to perform greedy tree construction.\n",
      " |  \n",
      " |  min_data_in_leaf : int, [default=1].\n",
      " |      The minimum training samples count in leaf.\n",
      " |      CatBoost will not search for new splits in leaves with samples count less than min_data_in_leaf.\n",
      " |      This parameter is used only for Depthwise and Lossguide growing policies.\n",
      " |  \n",
      " |  max_leaves : int, [default=31],\n",
      " |      The maximum leaf count in resulting tree.\n",
      " |      This parameter is used only for Lossguide growing policy.\n",
      " |  \n",
      " |  score_function : string, possible values L2, Cosine, NewtonL2, NewtonCosine, [default=Cosine]\n",
      " |      For growing policy Lossguide default=NewtonL2.\n",
      " |      GPU only. Score that is used during tree construction to select the next tree split.\n",
      " |  \n",
      " |  max_depth : int, Synonym for depth.\n",
      " |  \n",
      " |  n_estimators : int, synonym for iterations.\n",
      " |  \n",
      " |  num_trees : int, synonym for iterations.\n",
      " |  \n",
      " |  num_boost_round : int, synonym for iterations.\n",
      " |  \n",
      " |  colsample_bylevel : float, synonym for rsm.\n",
      " |  \n",
      " |  random_state : int, synonym for random_seed.\n",
      " |  \n",
      " |  reg_lambda : float, synonym for l2_leaf_reg.\n",
      " |  \n",
      " |  objective : string, synonym for loss_function.\n",
      " |  \n",
      " |  num_leaves : int, synonym for max_leaves.\n",
      " |  \n",
      " |  min_child_samples : int, synonym for min_data_in_leaf\n",
      " |  \n",
      " |  eta : float, synonym for learning_rate.\n",
      " |  \n",
      " |  max_bin : float, synonym for border_count.\n",
      " |  \n",
      " |  scale_pos_weight : float, synonym for class_weights.\n",
      " |      Can be used only for binary classification. Sets weight multiplier for\n",
      " |      class 1 to scale_pos_weight value.\n",
      " |  \n",
      " |  metadata : dict, string to string key-value pairs to be stored in model metadata storage\n",
      " |  \n",
      " |  early_stopping_rounds : int\n",
      " |      Synonym for od_wait. Only one of these parameters should be set.\n",
      " |  \n",
      " |  cat_features : list or numpy.ndarray, [default=None]\n",
      " |      If not None, giving the list of Categ features indices or names (names are represented as strings).\n",
      " |      If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n",
      " |  \n",
      " |  text_features : list or numpy.ndarray, [default=None]\n",
      " |      If not None, giving the list of Text features indices or names (names are represented as strings).\n",
      " |      If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n",
      " |  \n",
      " |  embedding_features : list or numpy.ndarray, [default=None]\n",
      " |      If not None, giving the list of Embedding features indices or names (names are represented as strings).\n",
      " |      If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n",
      " |  \n",
      " |  leaf_estimation_backtracking : string, [default=None]\n",
      " |      Type of backtracking during gradient descent.\n",
      " |      Possible values:\n",
      " |          - 'No' - never backtrack; supported on CPU and GPU\n",
      " |          - 'AnyImprovement' - reduce the descent step until the value of loss function is less than before the step; supported on CPU and GPU\n",
      " |          - 'Armijo' - reduce the descent step until Armijo condition is satisfied; supported on GPU only\n",
      " |  \n",
      " |  model_shrink_rate : float, [default=0]\n",
      " |      This parameter enables shrinkage of model at the start of each iteration. CPU only.\n",
      " |      For Constant mode shrinkage coefficient is calculated as (1 - model_shrink_rate * learning_rate).\n",
      " |      For Decreasing mode shrinkage coefficient is calculated as (1 - model_shrink_rate / iteration).\n",
      " |      Shrinkage coefficient should be in [0, 1).\n",
      " |  \n",
      " |  model_shrink_mode : string, [default=None]\n",
      " |      Mode of shrinkage coefficient calculation. CPU only.\n",
      " |      Possible values:\n",
      " |          - 'Constant' - Shrinkage coefficient is constant at each iteration.\n",
      " |          - 'Decreasing' - Shrinkage coefficient decreases at each iteration.\n",
      " |  \n",
      " |  langevin : bool, [default=False]\n",
      " |      Enables the Stochastic Gradient Langevin Boosting. CPU only.\n",
      " |  \n",
      " |  diffusion_temperature : float, [default=0]\n",
      " |      Langevin boosting diffusion temperature. CPU only.\n",
      " |  \n",
      " |  posterior_sampling : bool, [default=False]\n",
      " |      Set group of parameters for further use Uncertainty prediction:\n",
      " |          - Langevin = True\n",
      " |          - Model Shrink Rate = 1/(2N), where N is dataset size\n",
      " |          - Model Shrink Mode = Constant\n",
      " |          - Diffusion-temperature = N, where N is dataset size. CPU only.\n",
      " |  \n",
      " |  boost_from_average : bool, [default=True for RMSE, False for other losses]\n",
      " |      Enables to initialize approx values by best constant value for specified loss function.\n",
      " |      Available for RMSE, Logloss, CrossEntropy, Quantile and MAE.\n",
      " |  \n",
      " |  tokenizers : list of dicts,\n",
      " |      Each dict is a tokenizer description. Example:\n",
      " |      ```\n",
      " |      [\n",
      " |          {\n",
      " |              'tokenizer_id': 'Tokenizer',  # Tokeinzer identifier.\n",
      " |              'lowercasing': 'false',  # Possible values: 'true', 'false'.\n",
      " |              'number_process_policy': 'LeaveAsIs',  # Possible values: 'Skip', 'LeaveAsIs', 'Replace'.\n",
      " |              'number_token': '%',  # Rarely used character. Used in conjunction with Replace NumberProcessPolicy.\n",
      " |              'separator_type': 'ByDelimiter',  # Possible values: 'ByDelimiter', 'BySense'.\n",
      " |              'delimiter': ' ',  # Used in conjunction with ByDelimiter SeparatorType.\n",
      " |              'split_by_set': 'false',  # Each single character in delimiter used as individual delimiter.\n",
      " |              'skip_empty': 'true',  # Possible values: 'true', 'false'.\n",
      " |              'token_types': ['Word', 'Number', 'Unknown'],  # Used in conjunction with BySense SeparatorType.\n",
      " |                  # Possible values: 'Word', 'Number', 'Punctuation', 'SentenceBreak', 'ParagraphBreak', 'Unknown'.\n",
      " |              'subtokens_policy': 'SingleToken',  # Possible values:\n",
      " |                  # 'SingleToken' - All subtokens are interpreted as single token).\n",
      " |                  # 'SeveralTokens' - All subtokens are interpreted as several token.\n",
      " |          },\n",
      " |          ...\n",
      " |      ]\n",
      " |      ```\n",
      " |  \n",
      " |  dictionaries : list of dicts,\n",
      " |      Each dict is a tokenizer description. Example:\n",
      " |      ```\n",
      " |      [\n",
      " |          {\n",
      " |              'dictionary_id': 'Dictionary',  # Dictionary identifier.\n",
      " |              'token_level_type': 'Word',  # Possible values: 'Word', 'Letter'.\n",
      " |              'gram_order': '1',  # 1 for Unigram, 2 for Bigram, ...\n",
      " |              'skip_step': '0',  # 1 for 1-skip-gram, ...\n",
      " |              'end_of_word_token_policy': 'Insert',  # Possible values: 'Insert', 'Skip'.\n",
      " |              'end_of_sentence_token_policy': 'Skip',  # Possible values: 'Insert', 'Skip'.\n",
      " |              'occurrence_lower_bound': '3',  # The lower bound of token occurrences in the text to include it in the dictionary.\n",
      " |              'max_dictionary_size': '50000',  # The max dictionary size.\n",
      " |          },\n",
      " |          ...\n",
      " |      ]\n",
      " |      ```\n",
      " |  \n",
      " |  feature_calcers : list of strings,\n",
      " |      Each string is a calcer description. Example:\n",
      " |      ```\n",
      " |      [\n",
      " |          'NaiveBayes',\n",
      " |          'BM25',\n",
      " |          'BoW:top_tokens_count=2000',\n",
      " |      ]\n",
      " |      ```\n",
      " |  \n",
      " |  text_processing : dict,\n",
      " |      Text processging description.\n",
      " |  \n",
      " |  eval_fraction : float, [default=None]\n",
      " |      Fraction of the train dataset to be used as the evaluation dataset.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CatBoostClassifier\n",
      " |      CatBoost\n",
      " |      _CatBoostBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function=None, border_count=None, feature_border_type=None, per_float_feature_quantization=None, input_borders=None, output_borders=None, fold_permutation_block=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, best_model_min_trees=None, verbose=None, silent=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, allow_const_label=None, target_border=None, classes_count=None, class_weights=None, auto_class_weights=None, class_names=None, one_hot_max_size=None, random_strength=None, random_score_type=None, name=None, ignored_features=None, train_dir=None, custom_loss=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, pinned_memory_size=None, allow_writing_files=None, final_ctr_computation_mode=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, ctr_description=None, ctr_target_border_count=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, mvs_reg=None, sampling_unit=None, sampling_frequency=None, dev_score_calc_obj_block_size=None, dev_efb_max_buckets=None, sparse_features_conflict_fraction=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, scale_pos_weight=None, gpu_cat_features_storage=None, data_partition=None, metadata=None, early_stopping_rounds=None, cat_features=None, grow_policy=None, min_data_in_leaf=None, min_child_samples=None, max_leaves=None, num_leaves=None, score_function=None, leaf_estimation_backtracking=None, ctr_history_unit=None, monotone_constraints=None, feature_weights=None, penalties_coefficient=None, first_feature_use_penalties=None, per_object_feature_penalties=None, model_shrink_rate=None, model_shrink_mode=None, langevin=None, diffusion_temperature=None, posterior_sampling=None, boost_from_average=None, text_features=None, tokenizers=None, dictionaries=None, feature_calcers=None, text_processing=None, embedding_features=None, callback=None, eval_fraction=None, fixed_binary_splits=None)\n",
      " |      Initialize the CatBoost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : dict\n",
      " |          Parameters for CatBoost.\n",
      " |          If  None, all params are set to their defaults.\n",
      " |          If  dict, overriding parameters present in dict.\n",
      " |  \n",
      " |  fit(self, X, y=None, cat_features=None, text_features=None, embedding_features=None, sample_weight=None, baseline=None, use_best_model=None, eval_set=None, verbose=None, logging_level=None, plot=False, plot_file=None, column_description=None, verbose_eval=None, metric_period=None, silent=None, early_stopping_rounds=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, init_model=None, callbacks=None, log_cout=<ipykernel.iostream.OutStream object at 0x000001C449FA7FD0>, log_cerr=<ipykernel.iostream.OutStream object at 0x000001C449FA7EB0>)\n",
      " |      Fit the CatBoostClassifier model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          If not catboost.Pool, 2 dimensional Feature matrix or string - file with dataset.\n",
      " |      \n",
      " |      y : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Labels, 1 dimensional array like.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      cat_features : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving the list of Categ columns indices.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      text_features : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving the list of Text columns indices.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      embedding_features : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving the list of Embedding columns indices.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      sample_weight : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Instance weights, 1 dimensional array like.\n",
      " |      \n",
      " |      baseline : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving 2 dimensional array like data.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      use_best_model : bool, optional (default=None)\n",
      " |          Flag to use best model\n",
      " |      \n",
      " |      eval_set : catboost.Pool or list of catboost.Pool or tuple (X, y) or list [(X, y)], optional (default=None)\n",
      " |          Validation dataset or datasets for metrics calculation and possibly early stopping.\n",
      " |      \n",
      " |      metric_period : int\n",
      " |          Frequency of evaluating metrics.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If verbose is bool, then if set to True, logging_level is set to Verbose,\n",
      " |          if set to False, logging_level is set to Silent.\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output and\n",
      " |          logging_level is set to Verbose.\n",
      " |      \n",
      " |      silent : bool\n",
      " |          If silent is True, logging_level is set to Silent.\n",
      " |          If silent is False, logging_level is set to Verbose.\n",
      " |      \n",
      " |      logging_level : string, optional (default=None)\n",
      " |          Possible values:\n",
      " |              - 'Silent'\n",
      " |              - 'Verbose'\n",
      " |              - 'Info'\n",
      " |              - 'Debug'\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error graphs to file\n",
      " |      \n",
      " |      verbose_eval : bool or int\n",
      " |          Synonym for verbose. Only one of these parameters should be set.\n",
      " |      \n",
      " |      early_stopping_rounds : int\n",
      " |          Activates Iter overfitting detector with od_wait set to early_stopping_rounds.\n",
      " |      \n",
      " |      save_snapshot : bool, [default=None]\n",
      " |          Enable progress snapshotting for restoring progress after crashes or interruptions\n",
      " |      \n",
      " |      snapshot_file : string or pathlib.Path, [default=None]\n",
      " |          Learn progress snapshot file path, if None will use default filename\n",
      " |      \n",
      " |      snapshot_interval: int, [default=600]\n",
      " |          Interval between saving snapshots (seconds)\n",
      " |      \n",
      " |      init_model : CatBoost class or string or pathlib.Path, [default=None]\n",
      " |          Continue training starting from the existing model.\n",
      " |          If this parameter is a string or pathlib.Path, load initial model from the path specified by this string.\n",
      " |      \n",
      " |      callbacks : list, optional (default=None)\n",
      " |          List of callback objects that are applied at end of each iteration.\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model : CatBoost\n",
      " |  \n",
      " |  get_probability_threshold(self)\n",
      " |      Get a threshold for class separation in binary classification task\n",
      " |  \n",
      " |  predict(self, data, prediction_type='Class', ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type='CPU')\n",
      " |      Predict with data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      prediction_type : string, optional (default='Class')\n",
      " |          Can be:\n",
      " |          - 'RawFormulaVal' : return raw formula value.\n",
      " |          - 'Class' : return class label.\n",
      " |          - 'Probability' : return probability for every class.\n",
      " |          - 'LogProbability' : return log probability for every class.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool, optional (default=False)\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction:\n",
      " |          If data is for a single object, the return value depends on prediction_type value:\n",
      " |              - 'RawFormulaVal' : return raw formula value.\n",
      " |              - 'Class' : return class label.\n",
      " |              - 'Probability' : return one-dimensional numpy.ndarray with probability for every class.\n",
      " |              - 'LogProbability' : return one-dimensional numpy.ndarray with\n",
      " |                log probability for every class.\n",
      " |          otherwise numpy.ndarray, with values that depend on prediction_type value:\n",
      " |              - 'RawFormulaVal' : one-dimensional array of raw formula value for each object.\n",
      " |              - 'Class' : one-dimensional array of class label for each object.\n",
      " |              - 'Probability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with probability for every class for each object.\n",
      " |              - 'LogProbability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with log probability for every class for each object.\n",
      " |  \n",
      " |  predict_log_proba(self, data, ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type='CPU')\n",
      " |      Predict class log probability with data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          If data is for a single object\n",
      " |              return one-dimensional numpy.ndarray with log probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with log probability for every class for each object.\n",
      " |  \n",
      " |  predict_proba(self, X, ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type='CPU')\n",
      " |      Predict class probability with X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If X is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          If X is for a single object\n",
      " |              return one-dimensional numpy.ndarray with probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with probability for every class for each object.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Calculate accuracy.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          Data to apply model on.\n",
      " |      y : list or numpy.ndarray\n",
      " |          True labels.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      accuracy : float\n",
      " |  \n",
      " |  set_probability_threshold(self, binclass_probability_threshold=None)\n",
      " |      Set a threshold for class separation in binary classification task for a trained model.\n",
      " |      :param binclass_probability_threshold: float number in [0, 1] or None to discard it\n",
      " |  \n",
      " |  staged_predict(self, data, prediction_type='Class', ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, verbose=None)\n",
      " |      Predict target at each stage for data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      prediction_type : string, optional (default='Class')\n",
      " |          Can be:\n",
      " |          - 'RawFormulaVal' : return raw formula value.\n",
      " |          - 'Class' : return class label.\n",
      " |          - 'Probability' : return probability for every class.\n",
      " |          - 'LogProbability' : return log probability for every class.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : generator for each iteration that generates:\n",
      " |          If data is for a single object, the return value depends on prediction_type value:\n",
      " |              - 'RawFormulaVal' : return raw formula value.\n",
      " |              - 'Class' : return majority vote class.\n",
      " |              - 'Probability' : return one-dimensional numpy.ndarray with probability for every class.\n",
      " |              - 'LogProbability' : return one-dimensional numpy.ndarray with\n",
      " |                log probability for every class.\n",
      " |          otherwise numpy.ndarray, with values that depend on prediction_type value:\n",
      " |              - 'RawFormulaVal' : one-dimensional array of raw formula value for each object.\n",
      " |              - 'Class' : one-dimensional array of class label for each object.\n",
      " |              - 'Probability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with probability for every class for each object.\n",
      " |              - 'LogProbability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with log probability for every class for each object.\n",
      " |  \n",
      " |  staged_predict_log_proba(self, data, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, verbose=None)\n",
      " |      Predict classification target at each stage for data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : generator for each iteration that generates:\n",
      " |          If data is for a single object\n",
      " |              return one-dimensional numpy.ndarray with log probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with log probability for every class for each object.\n",
      " |  \n",
      " |  staged_predict_proba(self, data, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, verbose=None)\n",
      " |      Predict classification target at each stage for data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : generator for each iteration that generates:\n",
      " |          If data is for a single object\n",
      " |              return one-dimensional numpy.ndarray with probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with probability for every class for each object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from CatBoost:\n",
      " |  \n",
      " |  calc_feature_statistics(self, data, target=None, feature=None, prediction_type=None, cat_feature_values=None, plot=True, max_cat_features_on_plot=10, thread_count=-1, plot_file=None)\n",
      " |      Get statistics for the feature using the model, dataset and target.\n",
      " |      To use this function, you should install plotly.\n",
      " |      \n",
      " |      The catboost model has borders for the float features used in it. The borders divide\n",
      " |      feature values into bins, and the model's prediction depends on the number of the bin where the\n",
      " |      feature value falls in.\n",
      " |      \n",
      " |      For float features this function takes model's borders and computes\n",
      " |      1) Mean target value for every bin;\n",
      " |      2) Mean model prediction for every bin;\n",
      " |      3) The number of objects in dataset which fall into each bin;\n",
      " |      4) Predictions on varying feature. For every object, varies the feature value\n",
      " |      so that it falls into bin #0, bin #1, ... and counts model predictions.\n",
      " |      Then counts average prediction for each bin.\n",
      " |      \n",
      " |      For categorical features (only one-hot supported) does the same, but takes feature values\n",
      " |      provided in cat_feature_values instead of borders.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost. Pool or dict {'pool_name': pool} if you want several pools\n",
      " |          Data to compute statistics on\n",
      " |      target: numpy.ndarray or pandas.Series or dict {'pool_name': target} if you want several pools or None\n",
      " |          Target corresponding to data\n",
      " |          Use only if data is not catboost.Pool.\n",
      " |      feature: None, int, string, or list of int or strings\n",
      " |          Features indexes or names in pd.DataFrame for which you want to get statistics.\n",
      " |          None, if you need statistics for all features.\n",
      " |      prediction_type: str\n",
      " |          Prediction type used for counting mean_prediction: 'Class', 'Probability' or 'RawFormulaVal'.\n",
      " |          If not specified, is derived from the model.\n",
      " |      cat_feature_values: list or numpy.ndarray or pandas.Series or\n",
      " |                          dict: int or string to list or numpy.ndarray or pandas.Series\n",
      " |          Contains categorical feature values you need to get statistics on.\n",
      " |          Use dict, when parameter 'feature' is a list to specify cat values for different features.\n",
      " |          When parameter 'feature' is int or str, you can just pass list of cat values.\n",
      " |      plot: bool\n",
      " |          Plot statistics.\n",
      " |      max_cat_features_on_plot: int\n",
      " |          If categorical feature takes more than max_cat_features_on_plot different unique values,\n",
      " |          output result on several plots, not more than max_cat_features_on_plot feature values on each.\n",
      " |          Used only if plot=True or plot_file is not None.\n",
      " |      thread_count: int\n",
      " |          Number of threads to use for getting statistics.\n",
      " |      plot_file: str\n",
      " |          Output file for plot statistics.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict if parameter 'feature' is int or string, else dict of dicts:\n",
      " |          For each unique feature contain\n",
      " |          python dict with binarized feature statistics.\n",
      " |          For float feature, includes\n",
      " |                  'borders' -- borders for the specified feature in model\n",
      " |                  'binarized_feature' -- numbers of bins where feature values fall\n",
      " |                  'mean_target' -- mean value of target over each bin\n",
      " |                  'mean_prediction' -- mean value of model prediction over each bin\n",
      " |                  'objects_per_bin' -- number of objects per bin\n",
      " |                  'predictions_on_varying_feature' -- averaged over dataset predictions for\n",
      " |                  varying feature (see above)\n",
      " |          For one-hot feature, returns the same, but with 'cat_values' instead of 'borders'\n",
      " |  \n",
      " |  calc_leaf_indexes(self, data, ntree_start=0, ntree_end=0, thread_count=-1, verbose=False)\n",
      " |      Returns indexes of leafs to which objects from pool are mapped by model trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Index of first tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Index of the tree after last tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool (default=False)\n",
      " |          Enable debug logging level.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_indexes : 2-dimensional numpy.ndarray of numpy.uint32 with shape (object count, ntree_end - ntree_start).\n",
      " |          i-th row is an array of leaf indexes for i-th object.\n",
      " |  \n",
      " |  compare(self, model, data, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x000001C449FA7FD0>, log_cerr=<ipykernel.iostream.OutStream object at 0x000001C449FA7EB0>)\n",
      " |      Draw train and eval errors in Jupyter notebook for both models\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model: CatBoost model\n",
      " |          Another model to draw metrics\n",
      " |      \n",
      " |      data : catboost.Pool\n",
      " |          Data to evaluate metrics on.\n",
      " |      \n",
      " |      metrics : list of strings or catboost.metrics.BuiltinMetric\n",
      " |          List of evaluated metrics.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      tmp_dir : string or pathlib.Path (default=None)\n",
      " |          The name of the temporary directory for intermediate results.\n",
      " |          If None, then the name will be generated.\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save eval error graphs to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |  \n",
      " |  create_metric_calcer(self, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None)\n",
      " |      Create batch metric calcer. Could be used to aggregate metric on several pools\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |          Same as in eval_metrics except data\n",
      " |      Returns\n",
      " |      -------\n",
      " |          BatchMetricCalcer object\n",
      " |      \n",
      " |      Usage example\n",
      " |      -------\n",
      " |      # Large dataset is partitioned into parts [part1, part2]\n",
      " |      model.fit(params)\n",
      " |      batch_calcer = model.create_metric_calcer(['Logloss'])\n",
      " |      batch_calcer.add(part1)\n",
      " |      batch_calcer.add(part2)\n",
      " |      metrics = batch_calcer.eval_metrics()\n",
      " |  \n",
      " |  drop_unused_features(self)\n",
      " |      Drop unused features information from model\n",
      " |  \n",
      " |  eval_metrics(self, data, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None, plot=False, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x000001C449FA7FD0>, log_cerr=<ipykernel.iostream.OutStream object at 0x000001C449FA7EB0>)\n",
      " |      Calculate metrics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool\n",
      " |          Data to evaluate metrics on.\n",
      " |      \n",
      " |      metrics : list of strings or catboost.metrics.BuiltinMetric\n",
      " |          List of evaluated metrics.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      tmp_dir : string or pathlib.Path (default=None)\n",
      " |          The name of the temporary directory for intermediate results.\n",
      " |          If None, then the name will be generated.\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error graphs to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : dict: metric -> array of shape [(ntree_end - ntree_start) / eval_period]\n",
      " |  \n",
      " |  get_all_params(self)\n",
      " |      Get all params (specified by user and default params) that were set in training from CatBoost model.\n",
      " |      Full parameters documentation could be found here: https://catboost.ai/docs/concepts/python-reference_parameters-list.html\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : dict\n",
      " |          Dictionary of {param_key: param_value}.\n",
      " |  \n",
      " |  get_borders(self)\n",
      " |      Return map feature_index: borders for float features.\n",
      " |  \n",
      " |  get_cat_feature_indices(self)\n",
      " |  \n",
      " |  get_embedding_feature_indices(self)\n",
      " |  \n",
      " |  get_feature_importance(self, data=None, type=<EFstrType.FeatureImportance: 2>, prettified=False, thread_count=-1, verbose=False, fstr_type=None, shap_mode='Auto', model_output='Raw', interaction_indices=None, shap_calc_type='Regular', reference_data=None, sage_n_samples=128, sage_batch_size=512, sage_detect_convergence=True, log_cout=<ipykernel.iostream.OutStream object at 0x000001C449FA7FD0>, log_cerr=<ipykernel.iostream.OutStream object at 0x000001C449FA7EB0>)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data :\n",
      " |          Data to get feature importance.\n",
      " |          If type in ('LossFunctionChange', 'ShapValues', 'ShapInteractionValues') data must of Pool type.\n",
      " |              For every object in this dataset feature importances will be calculated.\n",
      " |          if type == 'SageValues' data must of Pool type.\n",
      " |              For every feature in this dataset importance will be calculated.\n",
      " |          If type == 'PredictionValuesChange', data is None or a dataset of Pool type\n",
      " |              Dataset specification is needed only in case if the model does not contain leaf weight information (trained with CatBoost v < 0.9).\n",
      " |          If type == 'PredictionDiff' data must contain a matrix of feature values of shape (2, n_features).\n",
      " |              Possible types are catboost.Pool or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData or pandas.SparseDataFrame or scipy.sparse.spmatrix\n",
      " |          If type == 'FeatureImportance'\n",
      " |              See 'PredictionValuesChange' for non-ranking metrics and 'LossFunctionChange' for ranking metrics.\n",
      " |          If type == 'Interaction'\n",
      " |              This parameter is not used.\n",
      " |      \n",
      " |      type : EFstrType or string (converted to EFstrType), optional\n",
      " |                  (default=EFstrType.FeatureImportance)\n",
      " |          Possible values:\n",
      " |              - PredictionValuesChange\n",
      " |                  Calculate score for every feature.\n",
      " |              - LossFunctionChange\n",
      " |                  Calculate score for every feature by loss.\n",
      " |              - FeatureImportance\n",
      " |                  PredictionValuesChange for non-ranking metrics and LossFunctionChange for ranking metrics\n",
      " |              - ShapValues\n",
      " |                  Calculate SHAP Values for every object.\n",
      " |              - ShapInteractionValues\n",
      " |                  Calculate SHAP Interaction Values between each pair of features for every object\n",
      " |              - Interaction\n",
      " |                  Calculate pairwise score between every feature.\n",
      " |              - PredictionDiff\n",
      " |                  Calculate most important features explaining difference in predictions for a pair of documents.\n",
      " |              - SageValues\n",
      " |                  Calculate SAGE value for every feature\n",
      " |      \n",
      " |      prettified : bool, optional (default=False)\n",
      " |          change returned data format to the list of (feature_id, importance) pairs sorted by importance\n",
      " |      \n",
      " |      thread_count : int, optional (default=-1)\n",
      " |          Number of threads.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If False, then evaluation is not logged. If True, then each possible iteration is logged.\n",
      " |          If a positive integer, then it stands for the size of batch N. After processing each batch, print progress\n",
      " |          and remaining time.\n",
      " |      \n",
      " |      fstr_type : string, deprecated, use type instead\n",
      " |      \n",
      " |      shap_mode : string, optional (default=\"Auto\")\n",
      " |          used only for ShapValues type\n",
      " |          Possible values:\n",
      " |              - \"Auto\"\n",
      " |                  Use direct SHAP Values calculation only if data size is smaller than average leaves number\n",
      " |                  (the best of two strategies below is chosen).\n",
      " |              - \"UsePreCalc\"\n",
      " |                  Calculate SHAP Values for every leaf in preprocessing. Final complexity is\n",
      " |                  O(NT(D+F))+O(TL^2 D^2) where N is the number of documents(objects), T - number of trees,\n",
      " |                  D - average tree depth, F - average number of features in tree, L - average number of leaves in tree\n",
      " |                  This is much faster (because of a smaller constant) than direct calculation when N >> L\n",
      " |              - \"NoPreCalc\"\n",
      " |                  Use direct SHAP Values calculation calculation with complexity O(NTLD^2). Direct algorithm\n",
      " |                  is faster when N < L (algorithm from https://arxiv.org/abs/1802.03888)\n",
      " |      \n",
      " |      shap_calc_type : EShapCalcType or string, optional (default=\"Regular\")\n",
      " |          used only for ShapValues type\n",
      " |          Possible values:\n",
      " |              - \"Regular\"\n",
      " |                  Calculate regular SHAP values\n",
      " |              - \"Approximate\"\n",
      " |                  Calculate approximate SHAP values\n",
      " |              - \"Exact\"\n",
      " |                  Calculate exact SHAP values\n",
      " |      \n",
      " |      interaction_indices : list of int or string (feature_idx_1, feature_idx_2), optional (default=None)\n",
      " |          used only for ShapInteractionValues type\n",
      " |          Calculate SHAP Interaction Values between pair of features feature_idx_1 and feature_idx_2 for every object\n",
      " |      \n",
      " |      reference_data: catboost.Pool or None\n",
      " |          Reference data for Independent Tree SHAP values from https://arxiv.org/abs/1905.04610v1\n",
      " |          if type == 'ShapValues' and reference_data is not None, then Independent Tree SHAP values are calculated\n",
      " |      \n",
      " |      sage_n_samples: int, optional (default=32)\n",
      " |          Number of outer samples used in SAGE values approximation algorithm\n",
      " |      sage_batch_size: int, optional (default=min(512, number of samples in dataset))\n",
      " |          Number of samples used on each step of SAGE values approximation algorithm\n",
      " |      sage_detect_convergence: bool, optional (default=False)\n",
      " |          If set True, sage values calculation will be stopped either when sage values converge\n",
      " |          or when sage_n_samples iterations of algorithm pass\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      depends on type:\n",
      " |          - FeatureImportance\n",
      " |              See PredictionValuesChange for non-ranking metrics and LossFunctionChange for ranking metrics.\n",
      " |          - PredictionValuesChange, LossFunctionChange, PredictionDiff, SageValues with prettified=False (default)\n",
      " |              list of length [n_features] with feature_importance values (float) for feature\n",
      " |          - PredictionValuesChange, LossFunctionChange, PredictionDiff, SageValues with prettified=True\n",
      " |              list of length [n_features] with (feature_id (string), feature_importance (float)) pairs, sorted by feature_importance in descending order\n",
      " |          - ShapValues\n",
      " |              np.ndarray of shape (n_objects, n_features + 1) with Shap values (float) for (object, feature).\n",
      " |              In case of multiclass the returned value is np.ndarray of shape\n",
      " |              (n_objects, classes_count, n_features + 1). For each object it contains Shap values (float).\n",
      " |              Values are calculated for RawFormulaVal predictions.\n",
      " |          - ShapInteractionValues\n",
      " |              np.ndarray of shape (n_objects, n_features + 1, n_features + 1) with Shap interaction values (float) for (object, feature(i), feature(j)).\n",
      " |              In case of multiclass the returned value is np.ndarray of shape\n",
      " |              (n_objects, classes_count, n_features + 1, n_features + 1). For each object it contains Shap interaction values (float).\n",
      " |              Values are calculated for RawFormulaVal predictions.\n",
      " |          - Interaction\n",
      " |              list of length [n_features] of 3-element lists of (first_feature_index, second_feature_index, interaction_score (float))\n",
      " |  \n",
      " |  get_object_importance(self, pool, train_pool, top_size=-1, type='Average', update_method='SinglePoint', importance_values_sign='All', thread_count=-1, verbose=False, ostr_type=None, log_cout=<ipykernel.iostream.OutStream object at 0x000001C449FA7FD0>, log_cerr=<ipykernel.iostream.OutStream object at 0x000001C449FA7EB0>)\n",
      " |      This is the implementation of the LeafInfluence algorithm from the following paper:\n",
      " |      https://arxiv.org/pdf/1802.06640.pdf\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pool : Pool\n",
      " |          The pool for which you want to evaluate the object importances.\n",
      " |      \n",
      " |      train_pool : Pool\n",
      " |          The pool on which the model has been trained.\n",
      " |      \n",
      " |      top_size : int (default=-1)\n",
      " |          Method returns the result of the top_size most important train objects.\n",
      " |          If -1, then the top size is not limited.\n",
      " |      \n",
      " |      type : string, optional (default='Average')\n",
      " |          Possible values:\n",
      " |              - Average (Method returns the mean train objects scores for all input objects)\n",
      " |              - PerObject (Method returns the train objects scores for every input object)\n",
      " |      \n",
      " |      importance_values_sign : string, optional (default='All')\n",
      " |          Method returns only Positive, Negative or All values.\n",
      " |          Possible values:\n",
      " |              - Positive\n",
      " |              - Negative\n",
      " |              - All\n",
      " |      \n",
      " |      update_method : string, optional (default='SinglePoint')\n",
      " |          Possible values:\n",
      " |              - SinglePoint\n",
      " |              - TopKLeaves (It is posible to set top size : TopKLeaves:top=2)\n",
      " |              - AllPoints\n",
      " |          Description of the update set methods are given in section 3.1.3 of the paper.\n",
      " |      \n",
      " |      thread_count : int, optional (default=-1)\n",
      " |          Number of threads.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If False, then evaluation is not logged. If True, then each possible iteration is logged.\n",
      " |          If a positive integer, then it stands for the size of batch N. After processing each batch, print progress\n",
      " |          and remaining time.\n",
      " |      \n",
      " |      ostr_type : string, deprecated, use type instead\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object_importances : tuple of two arrays (indices and scores) of shape = [top_size]\n",
      " |  \n",
      " |  get_param(self, key)\n",
      " |      Get param value from CatBoost model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : string\n",
      " |          The key to get param value from.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      value :\n",
      " |          The param value of the key, returns None if param do not exist.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get all params from CatBoost model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : dict\n",
      " |          Dictionary of {param_key: param_value}.\n",
      " |  \n",
      " |  get_text_feature_indices(self)\n",
      " |  \n",
      " |  grid_search(self, param_grid, X, y=None, cv=3, partition_random_seed=0, calc_cv_statistics=True, search_by_train_test_split=True, refit=True, shuffle=True, stratified=None, train_size=0.8, verbose=True, plot=False, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x000001C449FA7FD0>, log_cerr=<ipykernel.iostream.OutStream object at 0x000001C449FA7EB0>)\n",
      " |      Exhaustive search over specified parameter values for a model.\n",
      " |      Aafter calling this method model is fitted and can be used, if not specified otherwise (refit=False).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      param_grid: dict or list of dictionaries\n",
      " |          Dictionary with parameters names (string) as keys and lists of parameter settings\n",
      " |          to try as values, or a list of such dictionaries, in which case the grids spanned by each\n",
      " |          dictionary in the list are explored.\n",
      " |          This enables searching over any sequence of parameter settings.\n",
      " |      \n",
      " |      X: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |          Data to compute statistics on\n",
      " |      \n",
      " |      y: numpy.ndarray or pandas.Series or None\n",
      " |          Target corresponding to data\n",
      " |          Use only if data is not catboost.Pool.\n",
      " |      \n",
      " |      cv: int, cross-validation generator or an iterable, optional (default=None)\n",
      " |          Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
      " |          - None, to use the default 3-fold cross validation,\n",
      " |          - integer, to specify the number of folds in a (Stratified)KFold\n",
      " |          - one of the scikit-learn splitter classes\n",
      " |              (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\n",
      " |          - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |      \n",
      " |      partition_random_seed: int, optional (default=0)\n",
      " |          Use this as the seed value for random permutation of the data.\n",
      " |          Permutation is performed before splitting the data for cross validation.\n",
      " |          Each seed generates unique data splits.\n",
      " |          Used only when cv is None or int.\n",
      " |      \n",
      " |      search_by_train_test_split: bool, optional (default=True)\n",
      " |          If True, source dataset is splitted into train and test parts, models are trained\n",
      " |          on the train part and parameters are compared by loss function score on the test part.\n",
      " |          After that, if calc_cv_statistics=true, statistics on metrics are calculated\n",
      " |          using cross-validation using best parameters and the model is fitted with these parameters.\n",
      " |      \n",
      " |          If False, every iteration of grid search evaluates results on cross-validation.\n",
      " |          It is recommended to set parameter to True for large datasets, and to False for small datasets.\n",
      " |      \n",
      " |      calc_cv_statistics: bool, optional (default=True)\n",
      " |          The parameter determines whether quality should be estimated.\n",
      " |          using cross-validation with the found best parameters. Used only when search_by_train_test_split=True.\n",
      " |      \n",
      " |      refit: bool (default=True)\n",
      " |          Refit an estimator using the best found parameters on the whole dataset.\n",
      " |      \n",
      " |      shuffle: bool, optional (default=True)\n",
      " |          Shuffle the dataset objects before parameters searching.\n",
      " |      \n",
      " |      stratified: bool, optional (default=None)\n",
      " |          Perform stratified sampling. True for classification and False otherwise.\n",
      " |          Currently supported only for final cross-validation.\n",
      " |      \n",
      " |      train_size: float, optional (default=0.8)\n",
      " |          Should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.\n",
      " |      \n",
      " |      verbose: bool or int, optional (default=True)\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output\n",
      " |          verbose==True is equal to verbose==1\n",
      " |          When verbose==False, there is no messages\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error for every set of parameters in Jupyter notebook\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error for every set of parameters to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with two fields:\n",
      " |          'params': dict of best found parameters\n",
      " |          'cv_results': dict or pandas.core.frame.DataFrame with cross-validation results\n",
      " |              columns are: test-error-mean  test-error-std  train-error-mean  train-error-std\n",
      " |  \n",
      " |  iterate_leaf_indexes(self, data, ntree_start=0, ntree_end=0)\n",
      " |      Returns indexes of leafs to which objects from pool are mapped by model trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Index of first tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Index of the tree after last tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_indexes : generator. For each object in pool yields one-dimensional numpy.ndarray of leaf indexes.\n",
      " |  \n",
      " |  load_model(self, fname=None, format='cbm', stream=None, blob=None)\n",
      " |      Load model from a file, stream or blob.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Input file name.\n",
      " |  \n",
      " |  plot_partial_dependence(self, data, features, plot=True, plot_file=None, thread_count=-1)\n",
      " |      To use this function, you should install plotly.\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |      features: int, str, list<int>, tuple<int>, list<string>, tuple<string>\n",
      " |          Float features to calculate partial dependence for. Number of features should be 1 or 2.\n",
      " |      plot: bool\n",
      " |          Plot predictions.\n",
      " |      plot_file: str\n",
      " |          Output file for plot predictions.\n",
      " |      thread_count: int\n",
      " |          Number of threads to use. If -1 use maximum available number of threads.\n",
      " |      Returns\n",
      " |      -------\n",
      " |          If number of features is one - 1d numpy array and figure with line plot.\n",
      " |          If number of features is two - 2d numpy array and figure with 2d heatmap.\n",
      " |  \n",
      " |  plot_predictions(self, data, features_to_change, plot=True, plot_file=None)\n",
      " |      To use this function, you should install plotly.\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |      feature:\n",
      " |          Float features indexes in pd.DataFrame for which you want vary prediction value.\n",
      " |      plot: bool\n",
      " |          Plot predictions.\n",
      " |      plot_file: str\n",
      " |          Output file for plot predictions.\n",
      " |      Returns\n",
      " |      -------\n",
      " |          List of list of predictions for all buckets for all documents in data\n",
      " |  \n",
      " |  plot_tree(self, tree_idx, pool=None)\n",
      " |  \n",
      " |  randomized_search(self, param_distributions, X, y=None, cv=3, n_iter=10, partition_random_seed=0, calc_cv_statistics=True, search_by_train_test_split=True, refit=True, shuffle=True, stratified=None, train_size=0.8, verbose=True, plot=False, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x000001C449FA7FD0>, log_cerr=<ipykernel.iostream.OutStream object at 0x000001C449FA7EB0>)\n",
      " |      Randomized search on hyper parameters.\n",
      " |      After calling this method model is fitted and can be used, if not specified otherwise (refit=False).\n",
      " |      \n",
      " |      In contrast to grid_search, not all parameter values are tried out,\n",
      " |      but rather a fixed number of parameter settings is sampled from the specified distributions.\n",
      " |      The number of parameter settings that are tried is given by n_iter.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      param_distributions: dict\n",
      " |          Dictionary with parameters names (string) as keys and distributions or lists of parameters to try.\n",
      " |          Distributions must provide a rvs method for sampling (such as those from scipy.stats.distributions).\n",
      " |          If a list is given, it is sampled uniformly.\n",
      " |      \n",
      " |      X: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |          Data to compute statistics on\n",
      " |      \n",
      " |      y: numpy.ndarray or pandas.Series or None\n",
      " |          Target corresponding to data\n",
      " |          Use only if data is not catboost.Pool.\n",
      " |      \n",
      " |      cv: int, cross-validation generator or an iterable, optional (default=None)\n",
      " |          Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
      " |          - None, to use the default 3-fold cross validation,\n",
      " |          - integer, to specify the number of folds in a (Stratified)KFold\n",
      " |          - one of the scikit-learn splitter classes\n",
      " |              (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\n",
      " |          - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |      \n",
      " |      n_iter: int\n",
      " |          Number of parameter settings that are sampled.\n",
      " |          n_iter trades off runtime vs quality of the solution.\n",
      " |      \n",
      " |      partition_random_seed: int, optional (default=0)\n",
      " |          Use this as the seed value for random permutation of the data.\n",
      " |          Permutation is performed before splitting the data for cross validation.\n",
      " |          Each seed generates unique data splits.\n",
      " |          Used only when cv is None or int.\n",
      " |      \n",
      " |      search_by_train_test_split: bool, optional (default=True)\n",
      " |          If True, source dataset is splitted into train and test parts, models are trained\n",
      " |          on the train part and parameters are compared by loss function score on the test part.\n",
      " |          After that, if calc_cv_statistics=true, statistics on metrics are calculated\n",
      " |          using cross-validation using best parameters and the model is fitted with these parameters.\n",
      " |      \n",
      " |          If False, every iteration of grid search evaluates results on cross-validation.\n",
      " |          It is recommended to set parameter to True for large datasets, and to False for small datasets.\n",
      " |      \n",
      " |      calc_cv_statistics: bool, optional (default=True)\n",
      " |          The parameter determines whether quality should be estimated.\n",
      " |          using cross-validation with the found best parameters. Used only when search_by_train_test_split=True.\n",
      " |      \n",
      " |      refit: bool (default=True)\n",
      " |          Refit an estimator using the best found parameters on the whole dataset.\n",
      " |      \n",
      " |      shuffle: bool, optional (default=True)\n",
      " |          Shuffle the dataset objects before parameters searching.\n",
      " |      \n",
      " |      stratified: bool, optional (default=None)\n",
      " |          Perform stratified sampling. True for classification and False otherwise.\n",
      " |          Currently supported only for cross-validation.\n",
      " |      \n",
      " |      train_size: float, optional (default=0.8)\n",
      " |          Should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.\n",
      " |      \n",
      " |      verbose: bool or int, optional (default=True)\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output\n",
      " |          verbose==True is equal to verbose==1\n",
      " |          When verbose==False, there is no messages\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error for every set of parameters in Jupyter notebook\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error for every set of parameters to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with two fields:\n",
      " |          'params': dict of best found parameters\n",
      " |          'cv_results': dict or pandas.core.frame.DataFrame with cross-validation results\n",
      " |              columns are: test-error-mean  test-error-std  train-error-mean  train-error-std\n",
      " |  \n",
      " |  save_borders(self, fname)\n",
      " |      Save the model borders to a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or pathlib.Path\n",
      " |          Output file name.\n",
      " |  \n",
      " |  save_model(self, fname, format='cbm', export_parameters=None, pool=None)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Output file name.\n",
      " |      format : string\n",
      " |          Possible values:\n",
      " |              * 'cbm' for catboost binary format,\n",
      " |              * 'coreml' to export into Apple CoreML format\n",
      " |              * 'onnx' to export into ONNX-ML format\n",
      " |              * 'pmml' to export into PMML format\n",
      " |              * 'cpp' to export as C++ code\n",
      " |              * 'python' to export as Python code.\n",
      " |      export_parameters : dict\n",
      " |          Parameters for CoreML export:\n",
      " |              * prediction_type : string - either 'probability' or 'raw'\n",
      " |              * coreml_description : string\n",
      " |              * coreml_model_version : string\n",
      " |              * coreml_model_author : string\n",
      " |              * coreml_model_license: string\n",
      " |          Parameters for PMML export:\n",
      " |              * pmml_copyright : string\n",
      " |              * pmml_description : string\n",
      " |              * pmml_model_version : string\n",
      " |      pool : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series or catboost.FeaturesData\n",
      " |          Training pool.\n",
      " |  \n",
      " |  select_features(self, X, y=None, eval_set=None, features_for_select=None, num_features_to_select=None, algorithm=None, steps=None, shap_calc_type=None, train_final_model=True, verbose=None, logging_level=None, plot=False, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x000001C449FA7FD0>, log_cerr=<ipykernel.iostream.OutStream object at 0x000001C449FA7EB0>, grouping=None, features_tags_for_select=None, num_features_tags_to_select=None)\n",
      " |      Select best features from pool according to loss value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          If not catboost.Pool, 2 dimensional Feature matrix or string - file with dataset.\n",
      " |      \n",
      " |      y : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Labels, 1 dimensional array like.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      eval_set : catboost.Pool or list of catboost.Pool or tuple (X, y) or list [(X, y)], optional (default=None)\n",
      " |          Validation dataset or datasets for metrics calculation and possibly early stopping.\n",
      " |      \n",
      " |      features_for_select : str or list of feature indices, names or ranges\n",
      " |          (for grouping = Individual)\n",
      " |          Which features should participate in the selection.\n",
      " |          Format examples:\n",
      " |              - [0, 2, 3, 4, 17]\n",
      " |              - [0, \"2-4\", 17] (both ends in ranges are inclusive)\n",
      " |              - \"0,2-4,20\"\n",
      " |              - [\"Name0\", \"Name2\", \"Name3\", \"Name4\", \"Name20\"]\n",
      " |      \n",
      " |      num_features_to_select : positive int\n",
      " |          (for grouping = Individual)\n",
      " |          How many features to select from features_for_select.\n",
      " |      \n",
      " |      algorithm : EFeaturesSelectionAlgorithm or string, optional (default=RecursiveByShapValues)\n",
      " |          Which algorithm to use for features selection.\n",
      " |          Possible values:\n",
      " |              - RecursiveByPredictionValuesChange\n",
      " |                  Use prediction values change as feature strength, eliminate batch of features at once.\n",
      " |              - RecursiveByLossFunctionChange\n",
      " |                  Use loss function change as feature strength, eliminate batch of features at each step.\n",
      " |              - RecursiveByShapValues\n",
      " |                  Use shap values to estimate loss function change, eliminate features one by one.\n",
      " |      \n",
      " |      steps : positive int, optional (default=1)\n",
      " |          How many steps should be performed. In other words, how many times a full model will be trained.\n",
      " |          More steps give more accurate results.\n",
      " |      \n",
      " |      shap_calc_type : EShapCalcType or string, optional (default=Regular)\n",
      " |          Which method to use for calculation of shap values.\n",
      " |          Possible values:\n",
      " |              - Regular\n",
      " |                  Calculate regular SHAP values\n",
      " |              - Approximate\n",
      " |                  Calculate approximate SHAP values\n",
      " |              - Exact\n",
      " |                  Calculate exact SHAP values\n",
      " |      \n",
      " |      train_final_model : bool, optional (default=True)\n",
      " |          Need to fit model with selected features.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If verbose is bool, then if set to True, logging_level is set to Verbose,\n",
      " |          if set to False, logging_level is set to Silent.\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output and\n",
      " |          logging_level is set to Verbose.\n",
      " |      \n",
      " |      logging_level : string, optional (default=None)\n",
      " |          Possible values:\n",
      " |              - 'Silent'\n",
      " |              - 'Verbose'\n",
      " |              - 'Info'\n",
      " |              - 'Debug'\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook.\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error graphs to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      grouping : EFeaturesSelectionGrouping or string, optional (default=Individual)\n",
      " |          Which grouping to use for features selection.\n",
      " |          Possible values:\n",
      " |              - Individual\n",
      " |                  Select individual features\n",
      " |              - ByTags\n",
      " |                  Select feature groups (marked by tags)\n",
      " |      \n",
      " |      features_tags_for_select : list of strings\n",
      " |          (for grouping = ByTags)\n",
      " |          Which features tags should participate in the selection.\n",
      " |      \n",
      " |      num_features_tags_to_select : positive int\n",
      " |          (for grouping = ByTags)\n",
      " |          How many features tags to select from features_tags_for_select.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with fields:\n",
      " |          'selected_features': list of selected features indices\n",
      " |          'eliminated_features': list of eliminated features indices\n",
      " |          'selected_features_tags': list of selected features tags (optional, present if grouping == ByTags)\n",
      " |          'eliminated_features_tags': list of selected features tags (optional, present if grouping == ByTags)\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set parameters into CatBoost model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : key=value format\n",
      " |          List of key=value paris. Example: model.set_params(iterations=500, thread_count=2).\n",
      " |  \n",
      " |  shrink(self, ntree_end, ntree_start=0)\n",
      " |      Shrink the model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ntree_end: int\n",
      " |          Leave the trees with indices from the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Leave the trees with indices from the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |  \n",
      " |  virtual_ensembles_predict(self, data, prediction_type='VirtEnsembles', ntree_end=0, virtual_ensembles_count=10, thread_count=-1, verbose=None)\n",
      " |      Predict with data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      prediction_type : string, optional (default='RawFormulaVal')\n",
      " |          Can be:\n",
      " |          - 'VirtEnsembles': return V (virtual_ensembles_count) predictions.\n",
      " |              k-th virtEnsemle consists of trees [0, T/2] + [T/2 + T/(2V) * k, T/2 + T/(2V) * (k + 1)]  * constant.\n",
      " |          - 'TotalUncertainty': see returned predictions format in 'Returns' part\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      virtual_ensembles_count: int, optional (default=10)\n",
      " |          virtual ensembles count for 'TotalUncertainty' and 'VirtEnsembles' prediction types.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool, optional (default=False)\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          (with V as virtual_ensembles_count and T as trees count,\n",
      " |          k-th virtEnsemle consists of trees [0, T/2] + [T/2 + T/(2V) * k, T/2 + T/(2V) * (k + 1)]  * constant)\n",
      " |          If data is for a single object, return 1-dimensional array of predictions with size depends on prediction type,\n",
      " |          otherwise return 2-dimensional numpy.ndarray with shape (number_of_objects x size depends on prediction type);\n",
      " |          Returned predictions depends on prediction type:\n",
      " |          If loss-function was RMSEWithUncertainty:\n",
      " |              - 'VirtEnsembles': [mean0, var0, mean1, var1, ..., vark-1].\n",
      " |              - 'TotalUncertainty': [mean_predict, KnowledgeUnc, DataUnc].\n",
      " |          otherwise for regression:\n",
      " |              - 'VirtEnsembles':  [mean0, mean1, ...].\n",
      " |              - 'TotalUncertainty': [mean_predicts, KnowledgeUnc].\n",
      " |          otherwise for binary classification:\n",
      " |              - 'VirtEnsembles':  [ApproxRawFormulaVal0, ApproxRawFormulaVal1, ..., ApproxRawFormulaValk-1].\n",
      " |              - 'TotalUncertainty':  [DataUnc, TotalUnc].\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from CatBoost:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _CatBoostBase:\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __deepcopy__(self, _)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  copy(self)\n",
      " |  \n",
      " |  get_best_iteration(self)\n",
      " |  \n",
      " |  get_best_score(self)\n",
      " |  \n",
      " |  get_evals_result(self)\n",
      " |  \n",
      " |  get_leaf_values(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_values : 1d-array of leaf values for all trees.\n",
      " |      Value corresponding to j-th leaf of i-th tree is at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |  \n",
      " |  get_leaf_weights(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_weights : 1d-array of leaf weights for all trees.\n",
      " |      Weight of j-th leaf of i-th tree is at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |  \n",
      " |  get_metadata(self)\n",
      " |  \n",
      " |  get_n_features_in(self)\n",
      " |  \n",
      " |  get_scale_and_bias(self)\n",
      " |  \n",
      " |  get_test_eval(self)\n",
      " |  \n",
      " |  get_test_evals(self)\n",
      " |  \n",
      " |  get_tree_leaf_counts(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      tree_leaf_counts : 1d-array of numpy.uint32 of size tree_count_.\n",
      " |      tree_leaf_counts[i] equals to the number of leafs in i-th tree of the ensemble.\n",
      " |  \n",
      " |  is_fitted(self)\n",
      " |  \n",
      " |  set_feature_names(self, feature_names)\n",
      " |      Sets feature names equal to feature_names\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      feature_names: 1-d array of strings with new feature names in the same order as in pool\n",
      " |  \n",
      " |  set_leaf_values(self, new_leaf_values)\n",
      " |      Sets values at tree leafs of ensemble equal to new_leaf_values.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      new_leaf_values : 1d-array with new leaf values for all trees.\n",
      " |      It's size should be equal to sum(get_tree_leaf_counts()).\n",
      " |      Value corresponding to j-th leaf of i-th tree should be at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |  \n",
      " |  set_scale_and_bias(self, scale, bias)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _CatBoostBase:\n",
      " |  \n",
      " |  best_iteration_\n",
      " |  \n",
      " |  best_score_\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  evals_result_\n",
      " |  \n",
      " |  feature_names_\n",
      " |  \n",
      " |  learning_rate_\n",
      " |  \n",
      " |  n_features_in_\n",
      " |  \n",
      " |  random_seed_\n",
      " |  \n",
      " |  tree_count_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _CatBoostBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _CatBoostBase:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CatBoostClassifier)\n",
    "CatBoostClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def plot_hist_ee(df, bins=1_000, xlim=None):\n",
    "    df_X = df.copy().drop(['userId', 'lastFirstName', 'home_ownership_class'], axis=1)\n",
    "    df_names = df.copy()[['userId', 'lastFirstName']]\n",
    "    n_samples = df.shape[0]\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     scaled_data = scaler.fit_transform(df_X)\n",
    "    scaled_data = df_X\n",
    "\n",
    "    model = EllipticEnvelope(random_state=0)\n",
    "    model.fit(scaled_data)\n",
    "    \n",
    "    model_scores = model.score_samples(scaled_data)\n",
    "    \n",
    "    plt.hist(model_scores, bins=bins)\n",
    "    plt.xlabel('Elliptic Envelope scores')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Elliptic Envelope score histogram')\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim, 0)\n",
    "    plt.show()\n",
    "\n",
    "    return model_scores\n",
    "\n",
    "def plot_ee(df, scores, threshold):\n",
    "    model_scores = scores\n",
    "\n",
    "    outlier_mask = model_scores < threshold\n",
    "    plt.scatter(df.iloc[outlier_mask, df.columns.get_loc('basicMonthlySalary')],\n",
    "                df.iloc[outlier_mask, df.columns.get_loc('monthlyFamilyIncome')], c='purple',\n",
    "                label='Predicted Outliers')\n",
    "    plt.scatter(df.iloc[~outlier_mask, df.columns.get_loc('basicMonthlySalary')],\n",
    "                df.iloc[~outlier_mask, df.columns.get_loc('monthlyFamilyIncome')], c='yellow',\n",
    "                label='Predicted Inliers')\n",
    "    plt.xlabel('Basic Monthly Salary')\n",
    "    plt.ylabel('Monthly Family Income')\n",
    "    plt.title(f'Elliptic Envelope, Threshold = {threshold}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    df2 = df.copy()\n",
    "    df2['pred_outlier'] = (model_scores.reshape(-1, 1) < threshold)\n",
    "\n",
    "    print('Number of predicted outliers:',\n",
    "          df2[df2['pred_outlier'] == True].shape[0])\n",
    "    display(df2[df2['pred_outlier'] == True])\n",
    "\n",
    "def plot_hist_gmm(df, n_components, covariance_type='full', bins=1_000, xlim=None):\n",
    "    df_X = df.copy().drop(['userId', 'lastFirstName', 'home_ownership_class'], axis=1)\n",
    "    df_names = df.copy()[['userId', 'lastFirstName']]\n",
    "    n_samples = df.shape[0]\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     scaled_data = scaler.fit_transform(df_X)\n",
    "    scaled_data = df_X\n",
    "\n",
    "    model = GaussianMixture(n_components=n_components,\n",
    "                            covariance_type=covariance_type,\n",
    "                            random_state=0)\n",
    "    model.fit(scaled_data)\n",
    "    \n",
    "    model_scores = model.score_samples(scaled_data)\n",
    "    \n",
    "    plt.hist(model_scores, bins=bins)\n",
    "    plt.xlabel(f'{n_components}-component Gaussian Mixture Model scores')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{n_components}-component Gaussian Mixture Model score histogram')\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim, 0)\n",
    "    plt.show()\n",
    "\n",
    "    return model_scores\n",
    "\n",
    "def plot_gmm(df, n_components, scores, threshold):\n",
    "    model_scores = scores\n",
    "\n",
    "    outlier_mask = model_scores < threshold\n",
    "    plt.scatter(df.iloc[outlier_mask, df.columns.get_loc('basicMonthlySalary')],\n",
    "                df.iloc[outlier_mask, df.columns.get_loc('monthlyFamilyIncome')], c='purple',\n",
    "                label='Predicted Outliers')\n",
    "    plt.scatter(df.iloc[~outlier_mask, df.columns.get_loc('basicMonthlySalary')],\n",
    "                df.iloc[~outlier_mask, df.columns.get_loc('monthlyFamilyIncome')], c='yellow',\n",
    "                label='Predicted Inliers')\n",
    "    plt.xlabel('Basic Monthly Salary')\n",
    "    plt.ylabel('Monthly Family Income')\n",
    "    plt.title(f'{n_components}-component Gaussian Mixture Model, Threshold = {threshold}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    df2 = df.copy()\n",
    "    df2['pred_outlier'] = (model_scores.reshape(-1, 1) < threshold)\n",
    "\n",
    "    print('Number of predicted outliers:',\n",
    "          df2[df2['pred_outlier'] == True].shape[0])\n",
    "    display(df2[df2['pred_outlier'] == True])\n",
    "\n",
    "def plot_hist_lof(df, n_neighbors=20, bins=1_000, xlim=None):\n",
    "    df_X = df.copy().drop(['userId', 'lastFirstName', 'home_ownership_class'], axis=1)\n",
    "    df_names = df.copy()[['userId', 'lastFirstName']]\n",
    "    n_samples = df.shape[0]\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     scaled_data = scaler.fit_transform(df_X)\n",
    "    scaled_data = df_X\n",
    "\n",
    "    model = LocalOutlierFactor(n_neighbors=n_neighbors)\n",
    "    model.fit(scaled_data)\n",
    "    \n",
    "#     model_scores = model.score_samples(scaled_data)\n",
    "    model_scores = model.negative_outlier_factor_\n",
    "    \n",
    "    plt.hist(model_scores, bins=bins)\n",
    "    plt.xlabel(f'{n_neighbors}-neighbor Local Outlier Factor (LOF) scores')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{n_neighbors}-neighbor Local Outlier Factor (LOF) score histogram')\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim, 0)\n",
    "    plt.show()\n",
    "\n",
    "    return model_scores\n",
    "\n",
    "def plot_lof(df, n_neighbors, scores, threshold):\n",
    "    model_scores = scores\n",
    "\n",
    "    outlier_mask = model_scores < threshold\n",
    "    plt.scatter(df.iloc[outlier_mask, df.columns.get_loc('basicMonthlySalary')],\n",
    "                df.iloc[outlier_mask, df.columns.get_loc('monthlyFamilyIncome')], c='purple',\n",
    "                label='Predicted Outliers')\n",
    "    plt.scatter(df.iloc[~outlier_mask, df.columns.get_loc('basicMonthlySalary')],\n",
    "                df.iloc[~outlier_mask, df.columns.get_loc('monthlyFamilyIncome')], c='yellow',\n",
    "                label='Predicted Inliers')\n",
    "    plt.xlabel('Basic Monthly Salary')\n",
    "    plt.ylabel('Monthly Family Income')\n",
    "    plt.title(f'{n_neighbors}-neighbor Local Outlier Factor (LOF), Threshold = {threshold}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    df2 = df.copy()\n",
    "    df2['pred_outlier'] = (model_scores.reshape(-1, 1) < threshold)\n",
    "\n",
    "    print('Number of predicted outliers:',\n",
    "          df2[df2['pred_outlier'] == True].shape[0])\n",
    "    display(df2[df2['pred_outlier'] == True])\n",
    "\n",
    "def plot_hist_ocsvm(df, kernel='rbf', bins=1_000, xlim=None):\n",
    "    df_X = df.copy().drop(['userId', 'lastFirstName', 'home_ownership_class'], axis=1)\n",
    "    df_names = df.copy()[['userId', 'lastFirstName']]\n",
    "    n_samples = df.shape[0]\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     scaled_data = scaler.fit_transform(df_X)\n",
    "    scaled_data = df_X\n",
    "\n",
    "    model = OneClassSVM(kernel=kernel, degree=4)\n",
    "    model.fit(scaled_data)\n",
    "    \n",
    "    model_scores = model.score_samples(scaled_data)\n",
    "    \n",
    "    plt.hist(model_scores, bins=bins)\n",
    "    plt.xlabel(f'{kernel}-kernel One-Class SVM (OCSVM) scores')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{kernel}-kernel One-Class SVM (OCSVM) score histogram')\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim, 0)\n",
    "    plt.show()\n",
    "\n",
    "    return model_scores\n",
    "\n",
    "def plot_hist_ocsvm2(df, kernel='rbf', bins=1_000, xlim=None):\n",
    "    df_X = df.copy().drop(['userId', 'lastFirstName', 'ageing_class'], axis=1)\n",
    "    df_names = df.copy()[['userId', 'lastFirstName']]\n",
    "    n_samples = df.shape[0]\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     scaled_data = scaler.fit_transform(df_X)\n",
    "    scaled_data = df_X\n",
    "\n",
    "    model = OneClassSVM(kernel=kernel, degree=4)\n",
    "    model.fit(scaled_data)\n",
    "    \n",
    "    model_scores = model.score_samples(scaled_data)\n",
    "    \n",
    "    plt.hist(model_scores, bins=bins)\n",
    "    plt.xlabel(f'{kernel}-kernel One-Class SVM (OCSVM) scores')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{kernel}-kernel One-Class SVM (OCSVM) score histogram')\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim, 0)\n",
    "    plt.show()\n",
    "\n",
    "    return model_scores\n",
    "\n",
    "def plot_ocsvm(df, kernel, scores, threshold):\n",
    "    model_scores = scores\n",
    "\n",
    "    outlier_mask = model_scores < threshold\n",
    "    plt.scatter(df.iloc[outlier_mask, df.columns.get_loc('basicMonthlySalary')],\n",
    "                df.iloc[outlier_mask, df.columns.get_loc('monthlyFamilyIncome')], c='purple',\n",
    "                label='Predicted Outliers')\n",
    "    plt.scatter(df.iloc[~outlier_mask, df.columns.get_loc('basicMonthlySalary')],\n",
    "                df.iloc[~outlier_mask, df.columns.get_loc('monthlyFamilyIncome')], c='yellow',\n",
    "                label='Predicted Inliers')\n",
    "    plt.xlabel('Basic Monthly Salary')\n",
    "    plt.ylabel('Monthly Family Income')\n",
    "    plt.title(f'{kernel}-kernel One-Class SVM (OCSVM), Threshold = {threshold}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    df2 = df.copy()\n",
    "    df2['pred_outlier'] = (model_scores.reshape(-1, 1) < threshold)\n",
    "\n",
    "    print('Number of predicted outliers:',\n",
    "          df2[df2['pred_outlier'] == True].shape[0])\n",
    "    display(df2[df2['pred_outlier'] == True])\n",
    "\n",
    "    return list(df2[df2['pred_outlier'] == True].index)\n",
    "\n",
    "def plot_hist_if(df, n_estimators=100, bins=1_000, xlim=None):\n",
    "    df_X = df.copy().drop(['userId', 'lastFirstName', 'home_ownership_class'], axis=1)\n",
    "    df_names = df.copy()[['userId', 'lastFirstName']]\n",
    "    n_samples = df.shape[0]\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     scaled_data = scaler.fit_transform(df_X)\n",
    "    scaled_data = df_X\n",
    "\n",
    "    model = IsolationForest(n_estimators=n_estimators, random_state=0)\n",
    "    model.fit(scaled_data)\n",
    "    \n",
    "    model_scores = model.score_samples(scaled_data)\n",
    "    \n",
    "    plt.hist(model_scores, bins=bins)\n",
    "    plt.xlabel(f'{n_estimators}-estimator Isolation Forest (IF) scores')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{n_estimators}-estimator Isolation Forest (IF) score histogram')\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim, 0)\n",
    "    plt.show()\n",
    "\n",
    "    return model_scores\n",
    "\n",
    "def plot_if(df, n_estimators, scores, threshold):\n",
    "    model_scores = scores\n",
    "\n",
    "    outlier_mask = model_scores < threshold\n",
    "    plt.scatter(df.iloc[outlier_mask, df.columns.get_loc('basicMonthlySalary')],\n",
    "                df.iloc[outlier_mask, df.columns.get_loc('monthlyFamilyIncome')], c='purple',\n",
    "                label='Predicted Outliers')\n",
    "    plt.scatter(df.iloc[~outlier_mask, df.columns.get_loc('basicMonthlySalary')],\n",
    "                df.iloc[~outlier_mask, df.columns.get_loc('monthlyFamilyIncome')], c='yellow',\n",
    "                label='Predicted Inliers')\n",
    "    plt.xlabel('Basic Monthly Salary')\n",
    "    plt.ylabel('Monthly Family Income')\n",
    "    plt.title(f'{n_estimators}-estimator Isolation Forest (IF), Threshold = {threshold}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    df2 = df.copy()\n",
    "    df2['pred_outlier'] = (model_scores.reshape(-1, 1) < threshold)\n",
    "\n",
    "    print('Number of predicted outliers:',\n",
    "          df2[df2['pred_outlier'] == True].shape[0])\n",
    "    display(df2[df2['pred_outlier'] == True])\n",
    "\n",
    "    return list(df2[df2['pred_outlier'] == True].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>lastFirstName</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>province</th>\n",
       "      <th>job</th>\n",
       "      <th>basicMonthlySalary</th>\n",
       "      <th>preferredNetDisposableIncomeId</th>\n",
       "      <th>workingFamilyCount</th>\n",
       "      <th>residentsCount</th>\n",
       "      <th>monthlyFamilyIncome</th>\n",
       "      <th>food</th>\n",
       "      <th>hygiene</th>\n",
       "      <th>houseCleaning</th>\n",
       "      <th>fare</th>\n",
       "      <th>parking</th>\n",
       "      <th>gasoline</th>\n",
       "      <th>tuition</th>\n",
       "      <th>allowance</th>\n",
       "      <th>uniform</th>\n",
       "      <th>otherEducation</th>\n",
       "      <th>emergency</th>\n",
       "      <th>medicine</th>\n",
       "      <th>water</th>\n",
       "      <th>electricity</th>\n",
       "      <th>rent</th>\n",
       "      <th>repair</th>\n",
       "      <th>cinema</th>\n",
       "      <th>dineOut</th>\n",
       "      <th>leisure</th>\n",
       "      <th>personalCare</th>\n",
       "      <th>clothing</th>\n",
       "      <th>mobileLoad</th>\n",
       "      <th>internet</th>\n",
       "      <th>vehicleLoan</th>\n",
       "      <th>informalLenders</th>\n",
       "      <th>companyLoan</th>\n",
       "      <th>privateLoans</th>\n",
       "      <th>governmentLoans</th>\n",
       "      <th>smoking</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>gambling</th>\n",
       "      <th>smallLottery</th>\n",
       "      <th>otherVices</th>\n",
       "      <th>savings</th>\n",
       "      <th>loanOthers</th>\n",
       "      <th>payInsurance</th>\n",
       "      <th>loanSSS</th>\n",
       "      <th>payFamilySupport</th>\n",
       "      <th>loanPagIbig</th>\n",
       "      <th>loanGSIS</th>\n",
       "      <th>loanPersonal</th>\n",
       "      <th>houseHasPensioner</th>\n",
       "      <th>houseHasPrivateEmployee</th>\n",
       "      <th>houseHasBusiness</th>\n",
       "      <th>houseHasFreelancer</th>\n",
       "      <th>houseHasGovtEmployee</th>\n",
       "      <th>houseHasOFW</th>\n",
       "      <th>houseOnlyFamily</th>\n",
       "      <th>houseExtendedFamily</th>\n",
       "      <th>home_ownership_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>370</td>\n",
       "      <td>IBALI, HOWARD</td>\n",
       "      <td>24</td>\n",
       "      <td>MALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>19000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1025</td>\n",
       "      <td>PATALINGHOG, KIMBERLY</td>\n",
       "      <td>26</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>15000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1105</td>\n",
       "      <td>TEMILLOSO, DENNIS</td>\n",
       "      <td>40</td>\n",
       "      <td>MALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>3000</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1719</td>\n",
       "      <td>OSCARES, ELMER</td>\n",
       "      <td>32</td>\n",
       "      <td>MALE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>MACHINE OPERATOR</td>\n",
       "      <td>21200</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>392</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>876</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2081</td>\n",
       "      <td>LEGASPI, MANNY</td>\n",
       "      <td>39</td>\n",
       "      <td>MALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>CLERICAL SUPPORT</td>\n",
       "      <td>19800</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>300</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1500</td>\n",
       "      <td>1500</td>\n",
       "      <td>500</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>12850</td>\n",
       "      <td>LIPANGO, ARVIN</td>\n",
       "      <td>34</td>\n",
       "      <td>MALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>ASSOCIATE PROFESSIONAL</td>\n",
       "      <td>19000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>1500</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>12852</td>\n",
       "      <td>TEJADA, JERIC</td>\n",
       "      <td>27</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LAGUNA</td>\n",
       "      <td>CRAFT AND TRADE</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>500</td>\n",
       "      <td>400</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>12853</td>\n",
       "      <td>RAMOS, RHEALYN</td>\n",
       "      <td>27</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>CAVITE</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>13962</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30000</td>\n",
       "      <td>1000</td>\n",
       "      <td>150</td>\n",
       "      <td>1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>12854</td>\n",
       "      <td>BURGOS, LIEZEL</td>\n",
       "      <td>48</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>ILOILO</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>51000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>3000</td>\n",
       "      <td>1500</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15000</td>\n",
       "      <td>9000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>300</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>12857</td>\n",
       "      <td>SEÑERES, JENNY MAE</td>\n",
       "      <td>24</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>98800</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>143800.0</td>\n",
       "      <td>7000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6000</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>200</td>\n",
       "      <td>1650</td>\n",
       "      <td>17000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1070 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId          lastFirstName  age  gender      province  \\\n",
       "0        370          IBALI, HOWARD   24    MALE  METRO MANILA   \n",
       "1       1025  PATALINGHOG, KIMBERLY   26  FEMALE  METRO MANILA   \n",
       "2       1105      TEMILLOSO, DENNIS   40    MALE  METRO MANILA   \n",
       "3       1719         OSCARES, ELMER   32    MALE       UNKNOWN   \n",
       "4       2081         LEGASPI, MANNY   39    MALE  METRO MANILA   \n",
       "...      ...                    ...  ...     ...           ...   \n",
       "1065   12850         LIPANGO, ARVIN   34    MALE  METRO MANILA   \n",
       "1066   12852          TEJADA, JERIC   27    MALE        LAGUNA   \n",
       "1067   12853         RAMOS, RHEALYN   27  FEMALE        CAVITE   \n",
       "1068   12854         BURGOS, LIEZEL   48  FEMALE        ILOILO   \n",
       "1069   12857     SEÑERES, JENNY MAE   24  FEMALE  METRO MANILA   \n",
       "\n",
       "                         job  basicMonthlySalary  \\\n",
       "0          SERVICE AND SALES               19000   \n",
       "1          SERVICE AND SALES               15000   \n",
       "2          SERVICE AND SALES               20000   \n",
       "3           MACHINE OPERATOR               21200   \n",
       "4           CLERICAL SUPPORT               19800   \n",
       "...                      ...                 ...   \n",
       "1065  ASSOCIATE PROFESSIONAL               19000   \n",
       "1066         CRAFT AND TRADE                  20   \n",
       "1067       SERVICE AND SALES               13962   \n",
       "1068       SERVICE AND SALES               51000   \n",
       "1069       SERVICE AND SALES               98800   \n",
       "\n",
       "      preferredNetDisposableIncomeId  workingFamilyCount  residentsCount  \\\n",
       "0                                  2                   3               3   \n",
       "1                                  3                   2               4   \n",
       "2                                  2                   0               3   \n",
       "3                                  2                   0               5   \n",
       "4                                  2                   0               0   \n",
       "...                              ...                 ...             ...   \n",
       "1065                               2                   2               4   \n",
       "1066                               2                   1               1   \n",
       "1067                               1                   2               4   \n",
       "1068                               2                   0               5   \n",
       "1069                               3                   3               3   \n",
       "\n",
       "      monthlyFamilyIncome  food  hygiene  houseCleaning  fare  parking  \\\n",
       "0                 40000.0  1000      500            200   200        0   \n",
       "1                 30000.0  5000     3000           2000  2000        0   \n",
       "2                 30000.0  3000      500            300   200       50   \n",
       "3                 12000.0  2000      500            200   392        0   \n",
       "4                 25000.0  5000     2000           1000  1500        0   \n",
       "...                   ...   ...      ...            ...   ...      ...   \n",
       "1065              19000.0  5000     1000           1000     1        1   \n",
       "1066              20000.0  5000      500            400   280        0   \n",
       "1067              40000.0     4        2              1     3        1   \n",
       "1068              51000.0  5000     3000           1500  2000        0   \n",
       "1069             143800.0  7000     3000           2000     0        0   \n",
       "\n",
       "      gasoline  tuition  allowance  uniform  otherEducation  emergency  \\\n",
       "0            0        0         50        0               0        500   \n",
       "1            0        0          0        0               0       2000   \n",
       "2           50      500        100      100              50        500   \n",
       "3            0        0          0        0               0        100   \n",
       "4            0     1000        500     1500               0          0   \n",
       "...        ...      ...        ...      ...             ...        ...   \n",
       "1065         1     1000        500        1            1000       1000   \n",
       "1066         0        0          0        0               0          0   \n",
       "1067         1        0          0        0               0      30000   \n",
       "1068         0    15000       9000        0               0       1000   \n",
       "1069      6000        0          0        0               0       5000   \n",
       "\n",
       "      medicine  water  electricity  rent  repair  cinema  dineOut  leisure  \\\n",
       "0          450      0            0     0       0       0        0        0   \n",
       "1            0    200          200  1000     500       0        0        0   \n",
       "2          100    200          500     0     200       0      100      100   \n",
       "3            0    200          200  3000       0       0        0        0   \n",
       "4         1000    300         1900     0    1000       0     1000     1000   \n",
       "...        ...    ...          ...   ...     ...     ...      ...      ...   \n",
       "1065       500    200         1500  5000    1000       1        1        1   \n",
       "1066         0      0            0     0       0       0        0        0   \n",
       "1067      1000    150         1000  2000     500       0     1000        0   \n",
       "1068      1000    300          800     0       0       0      500      500   \n",
       "1069      1000      0            0  6000       0     500     2000     1000   \n",
       "\n",
       "      personalCare  clothing  mobileLoad  internet  vehicleLoan  \\\n",
       "0                0         0          50         0            0   \n",
       "1                0         0           0         0            0   \n",
       "2              100       200         200       200            0   \n",
       "3              100       200         400         0            0   \n",
       "4             1500      1500         500      1600            0   \n",
       "...            ...       ...         ...       ...          ...   \n",
       "1065             1         1         500         1            1   \n",
       "1066             0         0         200         0            0   \n",
       "1067             0         0         300         0            0   \n",
       "1068           300       500         300       800            0   \n",
       "1069          1000      1000         200      1650        17000   \n",
       "\n",
       "      informalLenders  companyLoan  privateLoans  governmentLoans  smoking  \\\n",
       "0                   0            0             0                0        0   \n",
       "1                   0            0             0                0        0   \n",
       "2                   0            0             0                0        0   \n",
       "3                   0            0             0              876        0   \n",
       "4                   0            0             0                0        0   \n",
       "...               ...          ...           ...              ...      ...   \n",
       "1065                1            1             1                1        1   \n",
       "1066                0            0             0             1672        0   \n",
       "1067                0          500             0                0        0   \n",
       "1068                0            0             0                0        0   \n",
       "1069                0            0             0                0        0   \n",
       "\n",
       "      alcohol  gambling  smallLottery  otherVices  savings  loanOthers  \\\n",
       "0           0         0             0           0     2000           0   \n",
       "1           0         0             0           0     2000           0   \n",
       "2           0         0             0           0     1000           0   \n",
       "3           0         0             0           0      500           0   \n",
       "4           0         0             0           0    10000           0   \n",
       "...       ...       ...           ...         ...      ...         ...   \n",
       "1065        1         1             1           1     2000           0   \n",
       "1066        0         0             0           0      500           0   \n",
       "1067        0         0             0           0     2000           0   \n",
       "1068        0         0             0           0     1000           0   \n",
       "1069        0         0             0           0    10000           0   \n",
       "\n",
       "      payInsurance  loanSSS  payFamilySupport  loanPagIbig  loanGSIS  \\\n",
       "0                0        0                 0            0         0   \n",
       "1                0        1                 1            1         0   \n",
       "2                0        0                 0            0         0   \n",
       "3                0        1                 0            0         0   \n",
       "4                0        0                 0            0         0   \n",
       "...            ...      ...               ...          ...       ...   \n",
       "1065             0        0                 0            0         0   \n",
       "1066             0        1                 0            1         0   \n",
       "1067             0        0                 0            0         0   \n",
       "1068             0        0                 0            0         0   \n",
       "1069             0        0                 0            0         0   \n",
       "\n",
       "      loanPersonal  houseHasPensioner  houseHasPrivateEmployee  \\\n",
       "0                0                  0                        1   \n",
       "1                0                  0                        1   \n",
       "2                0                  0                        1   \n",
       "3                0                  0                        1   \n",
       "4                0                  0                        0   \n",
       "...            ...                ...                      ...   \n",
       "1065             1                  0                        1   \n",
       "1066             0                  0                        1   \n",
       "1067             0                  0                        1   \n",
       "1068             0                  0                        0   \n",
       "1069             0                  0                        1   \n",
       "\n",
       "      houseHasBusiness  houseHasFreelancer  houseHasGovtEmployee  houseHasOFW  \\\n",
       "0                    0                   0                     0            1   \n",
       "1                    0                   0                     0            0   \n",
       "2                    0                   1                     0            0   \n",
       "3                    0                   0                     0            0   \n",
       "4                    0                   0                     0            0   \n",
       "...                ...                 ...                   ...          ...   \n",
       "1065                 0                   0                     0            0   \n",
       "1066                 0                   0                     0            0   \n",
       "1067                 0                   0                     0            0   \n",
       "1068                 0                   0                     0            1   \n",
       "1069                 0                   1                     0            1   \n",
       "\n",
       "      houseOnlyFamily  houseExtendedFamily  home_ownership_class  \n",
       "0                   0                    1                     1  \n",
       "1                   1                    0                     0  \n",
       "2                   0                    1                     0  \n",
       "3                   1                    0                     1  \n",
       "4                   0                    0                     0  \n",
       "...               ...                  ...                   ...  \n",
       "1065                1                    0                     0  \n",
       "1066                0                    0                     0  \n",
       "1067                1                    0                     0  \n",
       "1068                0                    0                     0  \n",
       "1069                1                    0                     0  \n",
       "\n",
       "[1070 rows x 61 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset = pd.read_csv('data/df_merged_no_nans.csv')\n",
    "# dataset = dataset.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Creating 'daysFromDateEntryStart'\n",
    "# dataset['dateEntry'] = pd.to_datetime(dataset['dateEntry'], format='%Y-%m-%d')\n",
    "# dateEntryStart = pd.to_datetime('2022-03-28', format='%Y-%m-%d')\n",
    "# dataset['daysFromDateEntryStart'] = (dataset['dateEntry'] - dateEntryStart).dt.days\n",
    "\n",
    "# Dropping irrelevant columns\n",
    "cols_to_drop = [\n",
    "    'id', 'fullName', 'firstName', 'lastName', 'address', 'occupation', 'dateEntry'\n",
    "]\n",
    "dataset = dataset.drop(cols_to_drop, axis=1)\n",
    "\n",
    "boolean_columns = [col for col in dataset.columns if dataset[col].dtype == bool]\n",
    "dataset[boolean_columns] = dataset[boolean_columns].apply(lambda x: x.astype('int'))\n",
    "\n",
    "columns_to_move = ['lastFirstName', 'age', 'gender', 'province', 'job']\n",
    "columns_remaining = [col for col in dataset.columns if col not in columns_to_move]\n",
    "\n",
    "new_column_order = columns_to_move + columns_remaining\n",
    "dataset = dataset[new_column_order]\n",
    "dataset.insert(0, 'userId', dataset.pop('userId'))\n",
    "\n",
    "def map_ageing_class(row):\n",
    "    found_in_hdmf = row['foundInHDMF']\n",
    "    home_ownership_class = row['home_ownership_class']\n",
    "\n",
    "    if home_ownership_class == 0:\n",
    "        return np.nan\n",
    "    elif found_in_hdmf in [' Current', 'FP', '01 mos', '02 mos', '03 mos']:\n",
    "        return 0\n",
    "    elif found_in_hdmf in ['04 mos', '05 mos']:\n",
    "        return 1\n",
    "\n",
    "# One-hot encoding of categorical columns\n",
    "# dataset = pd.get_dummies(dataset, columns=['gender'], prefix='gender', drop_first='True')\n",
    "# dataset = pd.get_dummies(dataset, columns=['province'], prefix='province')\n",
    "# dataset = pd.get_dummies(dataset, columns=['job'], prefix='job')\n",
    "\n",
    "dataset['home_ownership_class'] = ((dataset['foundInOS'] != 'False') |\n",
    "                                   (dataset['foundInHDMF'] != 'False')).astype('int')\n",
    "\n",
    "# dataset['ageing_class'] = dataset['home_ownership_class'].astype('int64')\n",
    "# dataset['ageing_class'] = dataset.apply(map_ageing_class, axis=1)\n",
    "\n",
    "cols_to_drop = ['foundInOS', 'foundInHDMF']\n",
    "dataset = dataset.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Selecting only Cluster 1 from the dataset\n",
    "# dataset = dataset[dataset['cluster'] == 1].drop(columns=['cluster'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>lastFirstName</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>province</th>\n",
       "      <th>job</th>\n",
       "      <th>basicMonthlySalary</th>\n",
       "      <th>preferredNetDisposableIncomeId</th>\n",
       "      <th>workingFamilyCount</th>\n",
       "      <th>residentsCount</th>\n",
       "      <th>monthlyFamilyIncome</th>\n",
       "      <th>food</th>\n",
       "      <th>hygiene</th>\n",
       "      <th>houseCleaning</th>\n",
       "      <th>fare</th>\n",
       "      <th>parking</th>\n",
       "      <th>gasoline</th>\n",
       "      <th>tuition</th>\n",
       "      <th>allowance</th>\n",
       "      <th>uniform</th>\n",
       "      <th>otherEducation</th>\n",
       "      <th>emergency</th>\n",
       "      <th>medicine</th>\n",
       "      <th>water</th>\n",
       "      <th>electricity</th>\n",
       "      <th>rent</th>\n",
       "      <th>repair</th>\n",
       "      <th>cinema</th>\n",
       "      <th>dineOut</th>\n",
       "      <th>leisure</th>\n",
       "      <th>personalCare</th>\n",
       "      <th>clothing</th>\n",
       "      <th>mobileLoad</th>\n",
       "      <th>internet</th>\n",
       "      <th>vehicleLoan</th>\n",
       "      <th>informalLenders</th>\n",
       "      <th>companyLoan</th>\n",
       "      <th>privateLoans</th>\n",
       "      <th>governmentLoans</th>\n",
       "      <th>smoking</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>gambling</th>\n",
       "      <th>smallLottery</th>\n",
       "      <th>otherVices</th>\n",
       "      <th>savings</th>\n",
       "      <th>loanOthers</th>\n",
       "      <th>payInsurance</th>\n",
       "      <th>loanSSS</th>\n",
       "      <th>payFamilySupport</th>\n",
       "      <th>loanPagIbig</th>\n",
       "      <th>loanGSIS</th>\n",
       "      <th>loanPersonal</th>\n",
       "      <th>houseHasPensioner</th>\n",
       "      <th>houseHasPrivateEmployee</th>\n",
       "      <th>houseHasBusiness</th>\n",
       "      <th>houseHasFreelancer</th>\n",
       "      <th>houseHasGovtEmployee</th>\n",
       "      <th>houseHasOFW</th>\n",
       "      <th>houseOnlyFamily</th>\n",
       "      <th>houseExtendedFamily</th>\n",
       "      <th>home_ownership_class</th>\n",
       "      <th>monthlyUtilityBills</th>\n",
       "      <th>monthlyVices</th>\n",
       "      <th>monthlyExpenses</th>\n",
       "      <th>monthlySoloNetIncome</th>\n",
       "      <th>positiveMonthlySoloNetIncome</th>\n",
       "      <th>monthlyFamilyNetIncome</th>\n",
       "      <th>positiveMonthlyFamilyNetIncome</th>\n",
       "      <th>monthlySoloNetIncomeWithSavings</th>\n",
       "      <th>positiveMonthlySoloNetIncomeWithSavings</th>\n",
       "      <th>monthlyFamilyNetIncomeWithSavings</th>\n",
       "      <th>positiveMonthlyFamilyNetIncomeWithSavings</th>\n",
       "      <th>monthlyFamilyIncome - basicMonthlySalary</th>\n",
       "      <th>positive monthlyFamilyIncome - basicMonthlySalary</th>\n",
       "      <th>basicMonthlySalary - monthlyExpenses</th>\n",
       "      <th>positive basicMonthlySalary - monthlyExpenses</th>\n",
       "      <th>monthlyFamilyIncome - monthlyExpenses</th>\n",
       "      <th>positive monthlyFamilyIncome - monthlyExpenses</th>\n",
       "      <th>basicMonthlySalary / monthlyFamilyIncome</th>\n",
       "      <th>monthlyExpenses / basicMonthlySalary</th>\n",
       "      <th>monthlyExpenses / monthlyFamilyIncome</th>\n",
       "      <th>monthlyVices / basicMonthlySalary</th>\n",
       "      <th>monthlyVices / monthlyFamilyIncome</th>\n",
       "      <th>basicMonthlySalary / workingFamilyCount</th>\n",
       "      <th>basicMonthlySalary / residentsCount</th>\n",
       "      <th>monthlyFamilyIncome / workingFamilyCount</th>\n",
       "      <th>monthlyFamilyIncome / residentsCount</th>\n",
       "      <th>monthlyExpenses / workingFamilyCount</th>\n",
       "      <th>monthlyExpenses / residentsCount</th>\n",
       "      <th>monthlyUtilityBills / workingFamilyCount</th>\n",
       "      <th>monthlyUtilityBills / residentsCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>370</td>\n",
       "      <td>IBALI, HOWARD</td>\n",
       "      <td>24</td>\n",
       "      <td>MALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>19000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>2950</td>\n",
       "      <td>16050</td>\n",
       "      <td>1</td>\n",
       "      <td>37050.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18050</td>\n",
       "      <td>1</td>\n",
       "      <td>39050.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16050</td>\n",
       "      <td>1</td>\n",
       "      <td>37050.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.155263</td>\n",
       "      <td>0.073750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6333.333333</td>\n",
       "      <td>6333.333333</td>\n",
       "      <td>13333.333333</td>\n",
       "      <td>13333.333333</td>\n",
       "      <td>983.333333</td>\n",
       "      <td>983.333333</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1025</td>\n",
       "      <td>PATALINGHOG, KIMBERLY</td>\n",
       "      <td>26</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>15000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>15900</td>\n",
       "      <td>-900</td>\n",
       "      <td>0</td>\n",
       "      <td>14100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1100</td>\n",
       "      <td>1</td>\n",
       "      <td>16100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-900</td>\n",
       "      <td>0</td>\n",
       "      <td>14100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7500.000000</td>\n",
       "      <td>3750.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>7500.000000</td>\n",
       "      <td>7950.000000</td>\n",
       "      <td>3975.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>350.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1105</td>\n",
       "      <td>TEMILLOSO, DENNIS</td>\n",
       "      <td>40</td>\n",
       "      <td>MALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>3000</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1100</td>\n",
       "      <td>0</td>\n",
       "      <td>7250</td>\n",
       "      <td>12750</td>\n",
       "      <td>1</td>\n",
       "      <td>22750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13750</td>\n",
       "      <td>1</td>\n",
       "      <td>23750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12750</td>\n",
       "      <td>1</td>\n",
       "      <td>22750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.241667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6666.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2416.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>366.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1719</td>\n",
       "      <td>OSCARES, ELMER</td>\n",
       "      <td>32</td>\n",
       "      <td>MALE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>MACHINE OPERATOR</td>\n",
       "      <td>21200</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>392</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>876</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3800</td>\n",
       "      <td>0</td>\n",
       "      <td>8168</td>\n",
       "      <td>13032</td>\n",
       "      <td>1</td>\n",
       "      <td>3832.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13532</td>\n",
       "      <td>1</td>\n",
       "      <td>4332.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-9200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13032</td>\n",
       "      <td>1</td>\n",
       "      <td>3832.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.766667</td>\n",
       "      <td>0.385283</td>\n",
       "      <td>0.680667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4240.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1633.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>760.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2081</td>\n",
       "      <td>LEGASPI, MANNY</td>\n",
       "      <td>39</td>\n",
       "      <td>MALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>CLERICAL SUPPORT</td>\n",
       "      <td>19800</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>300</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1500</td>\n",
       "      <td>1500</td>\n",
       "      <td>500</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4300</td>\n",
       "      <td>0</td>\n",
       "      <td>23800</td>\n",
       "      <td>-4000</td>\n",
       "      <td>0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6000</td>\n",
       "      <td>1</td>\n",
       "      <td>11200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-4000</td>\n",
       "      <td>0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>1.202020</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>12850</td>\n",
       "      <td>LIPANGO, ARVIN</td>\n",
       "      <td>34</td>\n",
       "      <td>MALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>ASSOCIATE PROFESSIONAL</td>\n",
       "      <td>19000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>1500</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7201</td>\n",
       "      <td>5</td>\n",
       "      <td>19220</td>\n",
       "      <td>-220</td>\n",
       "      <td>0</td>\n",
       "      <td>-220.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1780</td>\n",
       "      <td>1</td>\n",
       "      <td>1780.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-220</td>\n",
       "      <td>0</td>\n",
       "      <td>-220.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.011579</td>\n",
       "      <td>1.011579</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>9500.000000</td>\n",
       "      <td>4750.000000</td>\n",
       "      <td>9500.000000</td>\n",
       "      <td>4750.000000</td>\n",
       "      <td>9610.000000</td>\n",
       "      <td>4805.000000</td>\n",
       "      <td>3600.500000</td>\n",
       "      <td>1800.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>12852</td>\n",
       "      <td>TEJADA, JERIC</td>\n",
       "      <td>27</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LAGUNA</td>\n",
       "      <td>CRAFT AND TRADE</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>500</td>\n",
       "      <td>400</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>8052</td>\n",
       "      <td>-8032</td>\n",
       "      <td>0</td>\n",
       "      <td>11948.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-7532</td>\n",
       "      <td>0</td>\n",
       "      <td>12448.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19980.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-8032</td>\n",
       "      <td>0</td>\n",
       "      <td>11948.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>402.600000</td>\n",
       "      <td>0.402600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>8052.000000</td>\n",
       "      <td>8052.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>12853</td>\n",
       "      <td>RAMOS, RHEALYN</td>\n",
       "      <td>27</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>CAVITE</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>13962</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30000</td>\n",
       "      <td>1000</td>\n",
       "      <td>150</td>\n",
       "      <td>1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3450</td>\n",
       "      <td>0</td>\n",
       "      <td>36462</td>\n",
       "      <td>-22500</td>\n",
       "      <td>0</td>\n",
       "      <td>3538.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-20500</td>\n",
       "      <td>0</td>\n",
       "      <td>5538.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26038.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-22500</td>\n",
       "      <td>0</td>\n",
       "      <td>3538.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.349050</td>\n",
       "      <td>2.611517</td>\n",
       "      <td>0.911550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6981.000000</td>\n",
       "      <td>3490.500000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>18231.000000</td>\n",
       "      <td>9115.500000</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>862.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>12854</td>\n",
       "      <td>BURGOS, LIEZEL</td>\n",
       "      <td>48</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>ILOILO</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>51000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>3000</td>\n",
       "      <td>1500</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15000</td>\n",
       "      <td>9000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>300</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2200</td>\n",
       "      <td>0</td>\n",
       "      <td>41500</td>\n",
       "      <td>9500</td>\n",
       "      <td>1</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10500</td>\n",
       "      <td>1</td>\n",
       "      <td>10500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9500</td>\n",
       "      <td>1</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10200.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10200.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8300.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>440.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>12857</td>\n",
       "      <td>SEÑERES, JENNY MAE</td>\n",
       "      <td>24</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>98800</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>143800.0</td>\n",
       "      <td>7000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6000</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>200</td>\n",
       "      <td>1650</td>\n",
       "      <td>17000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7850</td>\n",
       "      <td>0</td>\n",
       "      <td>54350</td>\n",
       "      <td>44450</td>\n",
       "      <td>1</td>\n",
       "      <td>89450.0</td>\n",
       "      <td>1</td>\n",
       "      <td>54450</td>\n",
       "      <td>1</td>\n",
       "      <td>99450.0</td>\n",
       "      <td>1</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>44450</td>\n",
       "      <td>1</td>\n",
       "      <td>89450.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.687065</td>\n",
       "      <td>0.550101</td>\n",
       "      <td>0.377955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32933.333333</td>\n",
       "      <td>32933.333333</td>\n",
       "      <td>47933.333333</td>\n",
       "      <td>47933.333333</td>\n",
       "      <td>18116.666667</td>\n",
       "      <td>18116.666667</td>\n",
       "      <td>2616.666667</td>\n",
       "      <td>2616.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1070 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     userId          lastFirstName  age  gender      province  \\\n",
       "0       370          IBALI, HOWARD   24    MALE  METRO MANILA   \n",
       "1      1025  PATALINGHOG, KIMBERLY   26  FEMALE  METRO MANILA   \n",
       "2      1105      TEMILLOSO, DENNIS   40    MALE  METRO MANILA   \n",
       "3      1719         OSCARES, ELMER   32    MALE       UNKNOWN   \n",
       "4      2081         LEGASPI, MANNY   39    MALE  METRO MANILA   \n",
       "...     ...                    ...  ...     ...           ...   \n",
       "1065  12850         LIPANGO, ARVIN   34    MALE  METRO MANILA   \n",
       "1066  12852          TEJADA, JERIC   27    MALE        LAGUNA   \n",
       "1067  12853         RAMOS, RHEALYN   27  FEMALE        CAVITE   \n",
       "1068  12854         BURGOS, LIEZEL   48  FEMALE        ILOILO   \n",
       "1069  12857     SEÑERES, JENNY MAE   24  FEMALE  METRO MANILA   \n",
       "\n",
       "                         job  basicMonthlySalary  \\\n",
       "0          SERVICE AND SALES               19000   \n",
       "1          SERVICE AND SALES               15000   \n",
       "2          SERVICE AND SALES               20000   \n",
       "3           MACHINE OPERATOR               21200   \n",
       "4           CLERICAL SUPPORT               19800   \n",
       "...                      ...                 ...   \n",
       "1065  ASSOCIATE PROFESSIONAL               19000   \n",
       "1066         CRAFT AND TRADE                  20   \n",
       "1067       SERVICE AND SALES               13962   \n",
       "1068       SERVICE AND SALES               51000   \n",
       "1069       SERVICE AND SALES               98800   \n",
       "\n",
       "      preferredNetDisposableIncomeId  workingFamilyCount  residentsCount  \\\n",
       "0                                  2                   3               3   \n",
       "1                                  3                   2               4   \n",
       "2                                  2                   0               3   \n",
       "3                                  2                   0               5   \n",
       "4                                  2                   0               0   \n",
       "...                              ...                 ...             ...   \n",
       "1065                               2                   2               4   \n",
       "1066                               2                   1               1   \n",
       "1067                               1                   2               4   \n",
       "1068                               2                   0               5   \n",
       "1069                               3                   3               3   \n",
       "\n",
       "      monthlyFamilyIncome  food  hygiene  houseCleaning  fare  parking  \\\n",
       "0                 40000.0  1000      500            200   200        0   \n",
       "1                 30000.0  5000     3000           2000  2000        0   \n",
       "2                 30000.0  3000      500            300   200       50   \n",
       "3                 12000.0  2000      500            200   392        0   \n",
       "4                 25000.0  5000     2000           1000  1500        0   \n",
       "...                   ...   ...      ...            ...   ...      ...   \n",
       "1065              19000.0  5000     1000           1000     1        1   \n",
       "1066              20000.0  5000      500            400   280        0   \n",
       "1067              40000.0     4        2              1     3        1   \n",
       "1068              51000.0  5000     3000           1500  2000        0   \n",
       "1069             143800.0  7000     3000           2000     0        0   \n",
       "\n",
       "      gasoline  tuition  allowance  uniform  otherEducation  emergency  \\\n",
       "0            0        0         50        0               0        500   \n",
       "1            0        0          0        0               0       2000   \n",
       "2           50      500        100      100              50        500   \n",
       "3            0        0          0        0               0        100   \n",
       "4            0     1000        500     1500               0          0   \n",
       "...        ...      ...        ...      ...             ...        ...   \n",
       "1065         1     1000        500        1            1000       1000   \n",
       "1066         0        0          0        0               0          0   \n",
       "1067         1        0          0        0               0      30000   \n",
       "1068         0    15000       9000        0               0       1000   \n",
       "1069      6000        0          0        0               0       5000   \n",
       "\n",
       "      medicine  water  electricity  rent  repair  cinema  dineOut  leisure  \\\n",
       "0          450      0            0     0       0       0        0        0   \n",
       "1            0    200          200  1000     500       0        0        0   \n",
       "2          100    200          500     0     200       0      100      100   \n",
       "3            0    200          200  3000       0       0        0        0   \n",
       "4         1000    300         1900     0    1000       0     1000     1000   \n",
       "...        ...    ...          ...   ...     ...     ...      ...      ...   \n",
       "1065       500    200         1500  5000    1000       1        1        1   \n",
       "1066         0      0            0     0       0       0        0        0   \n",
       "1067      1000    150         1000  2000     500       0     1000        0   \n",
       "1068      1000    300          800     0       0       0      500      500   \n",
       "1069      1000      0            0  6000       0     500     2000     1000   \n",
       "\n",
       "      personalCare  clothing  mobileLoad  internet  vehicleLoan  \\\n",
       "0                0         0          50         0            0   \n",
       "1                0         0           0         0            0   \n",
       "2              100       200         200       200            0   \n",
       "3              100       200         400         0            0   \n",
       "4             1500      1500         500      1600            0   \n",
       "...            ...       ...         ...       ...          ...   \n",
       "1065             1         1         500         1            1   \n",
       "1066             0         0         200         0            0   \n",
       "1067             0         0         300         0            0   \n",
       "1068           300       500         300       800            0   \n",
       "1069          1000      1000         200      1650        17000   \n",
       "\n",
       "      informalLenders  companyLoan  privateLoans  governmentLoans  smoking  \\\n",
       "0                   0            0             0                0        0   \n",
       "1                   0            0             0                0        0   \n",
       "2                   0            0             0                0        0   \n",
       "3                   0            0             0              876        0   \n",
       "4                   0            0             0                0        0   \n",
       "...               ...          ...           ...              ...      ...   \n",
       "1065                1            1             1                1        1   \n",
       "1066                0            0             0             1672        0   \n",
       "1067                0          500             0                0        0   \n",
       "1068                0            0             0                0        0   \n",
       "1069                0            0             0                0        0   \n",
       "\n",
       "      alcohol  gambling  smallLottery  otherVices  savings  loanOthers  \\\n",
       "0           0         0             0           0     2000           0   \n",
       "1           0         0             0           0     2000           0   \n",
       "2           0         0             0           0     1000           0   \n",
       "3           0         0             0           0      500           0   \n",
       "4           0         0             0           0    10000           0   \n",
       "...       ...       ...           ...         ...      ...         ...   \n",
       "1065        1         1             1           1     2000           0   \n",
       "1066        0         0             0           0      500           0   \n",
       "1067        0         0             0           0     2000           0   \n",
       "1068        0         0             0           0     1000           0   \n",
       "1069        0         0             0           0    10000           0   \n",
       "\n",
       "      payInsurance  loanSSS  payFamilySupport  loanPagIbig  loanGSIS  \\\n",
       "0                0        0                 0            0         0   \n",
       "1                0        1                 1            1         0   \n",
       "2                0        0                 0            0         0   \n",
       "3                0        1                 0            0         0   \n",
       "4                0        0                 0            0         0   \n",
       "...            ...      ...               ...          ...       ...   \n",
       "1065             0        0                 0            0         0   \n",
       "1066             0        1                 0            1         0   \n",
       "1067             0        0                 0            0         0   \n",
       "1068             0        0                 0            0         0   \n",
       "1069             0        0                 0            0         0   \n",
       "\n",
       "      loanPersonal  houseHasPensioner  houseHasPrivateEmployee  \\\n",
       "0                0                  0                        1   \n",
       "1                0                  0                        1   \n",
       "2                0                  0                        1   \n",
       "3                0                  0                        1   \n",
       "4                0                  0                        0   \n",
       "...            ...                ...                      ...   \n",
       "1065             1                  0                        1   \n",
       "1066             0                  0                        1   \n",
       "1067             0                  0                        1   \n",
       "1068             0                  0                        0   \n",
       "1069             0                  0                        1   \n",
       "\n",
       "      houseHasBusiness  houseHasFreelancer  houseHasGovtEmployee  houseHasOFW  \\\n",
       "0                    0                   0                     0            1   \n",
       "1                    0                   0                     0            0   \n",
       "2                    0                   1                     0            0   \n",
       "3                    0                   0                     0            0   \n",
       "4                    0                   0                     0            0   \n",
       "...                ...                 ...                   ...          ...   \n",
       "1065                 0                   0                     0            0   \n",
       "1066                 0                   0                     0            0   \n",
       "1067                 0                   0                     0            0   \n",
       "1068                 0                   0                     0            1   \n",
       "1069                 0                   1                     0            1   \n",
       "\n",
       "      houseOnlyFamily  houseExtendedFamily  home_ownership_class  \\\n",
       "0                   0                    1                     1   \n",
       "1                   1                    0                     0   \n",
       "2                   0                    1                     0   \n",
       "3                   1                    0                     1   \n",
       "4                   0                    0                     0   \n",
       "...               ...                  ...                   ...   \n",
       "1065                1                    0                     0   \n",
       "1066                0                    0                     0   \n",
       "1067                1                    0                     0   \n",
       "1068                0                    0                     0   \n",
       "1069                1                    0                     0   \n",
       "\n",
       "      monthlyUtilityBills  monthlyVices  monthlyExpenses  \\\n",
       "0                      50             0             2950   \n",
       "1                    1400             0            15900   \n",
       "2                    1100             0             7250   \n",
       "3                    3800             0             8168   \n",
       "4                    4300             0            23800   \n",
       "...                   ...           ...              ...   \n",
       "1065                 7201             5            19220   \n",
       "1066                  200             0             8052   \n",
       "1067                 3450             0            36462   \n",
       "1068                 2200             0            41500   \n",
       "1069                 7850             0            54350   \n",
       "\n",
       "      monthlySoloNetIncome  positiveMonthlySoloNetIncome  \\\n",
       "0                    16050                             1   \n",
       "1                     -900                             0   \n",
       "2                    12750                             1   \n",
       "3                    13032                             1   \n",
       "4                    -4000                             0   \n",
       "...                    ...                           ...   \n",
       "1065                  -220                             0   \n",
       "1066                 -8032                             0   \n",
       "1067                -22500                             0   \n",
       "1068                  9500                             1   \n",
       "1069                 44450                             1   \n",
       "\n",
       "      monthlyFamilyNetIncome  positiveMonthlyFamilyNetIncome  \\\n",
       "0                    37050.0                               1   \n",
       "1                    14100.0                               1   \n",
       "2                    22750.0                               1   \n",
       "3                     3832.0                               1   \n",
       "4                     1200.0                               1   \n",
       "...                      ...                             ...   \n",
       "1065                  -220.0                               0   \n",
       "1066                 11948.0                               1   \n",
       "1067                  3538.0                               1   \n",
       "1068                  9500.0                               1   \n",
       "1069                 89450.0                               1   \n",
       "\n",
       "      monthlySoloNetIncomeWithSavings  \\\n",
       "0                               18050   \n",
       "1                                1100   \n",
       "2                               13750   \n",
       "3                               13532   \n",
       "4                                6000   \n",
       "...                               ...   \n",
       "1065                             1780   \n",
       "1066                            -7532   \n",
       "1067                           -20500   \n",
       "1068                            10500   \n",
       "1069                            54450   \n",
       "\n",
       "      positiveMonthlySoloNetIncomeWithSavings  \\\n",
       "0                                           1   \n",
       "1                                           1   \n",
       "2                                           1   \n",
       "3                                           1   \n",
       "4                                           1   \n",
       "...                                       ...   \n",
       "1065                                        1   \n",
       "1066                                        0   \n",
       "1067                                        0   \n",
       "1068                                        1   \n",
       "1069                                        1   \n",
       "\n",
       "      monthlyFamilyNetIncomeWithSavings  \\\n",
       "0                               39050.0   \n",
       "1                               16100.0   \n",
       "2                               23750.0   \n",
       "3                                4332.0   \n",
       "4                               11200.0   \n",
       "...                                 ...   \n",
       "1065                             1780.0   \n",
       "1066                            12448.0   \n",
       "1067                             5538.0   \n",
       "1068                            10500.0   \n",
       "1069                            99450.0   \n",
       "\n",
       "      positiveMonthlyFamilyNetIncomeWithSavings  \\\n",
       "0                                             1   \n",
       "1                                             1   \n",
       "2                                             1   \n",
       "3                                             1   \n",
       "4                                             1   \n",
       "...                                         ...   \n",
       "1065                                          1   \n",
       "1066                                          1   \n",
       "1067                                          1   \n",
       "1068                                          1   \n",
       "1069                                          1   \n",
       "\n",
       "      monthlyFamilyIncome - basicMonthlySalary  \\\n",
       "0                                      21000.0   \n",
       "1                                      15000.0   \n",
       "2                                      10000.0   \n",
       "3                                      -9200.0   \n",
       "4                                       5200.0   \n",
       "...                                        ...   \n",
       "1065                                       0.0   \n",
       "1066                                   19980.0   \n",
       "1067                                   26038.0   \n",
       "1068                                       0.0   \n",
       "1069                                   45000.0   \n",
       "\n",
       "      positive monthlyFamilyIncome - basicMonthlySalary  \\\n",
       "0                                                     1   \n",
       "1                                                     1   \n",
       "2                                                     1   \n",
       "3                                                     0   \n",
       "4                                                     1   \n",
       "...                                                 ...   \n",
       "1065                                                  0   \n",
       "1066                                                  1   \n",
       "1067                                                  1   \n",
       "1068                                                  0   \n",
       "1069                                                  1   \n",
       "\n",
       "      basicMonthlySalary - monthlyExpenses  \\\n",
       "0                                    16050   \n",
       "1                                     -900   \n",
       "2                                    12750   \n",
       "3                                    13032   \n",
       "4                                    -4000   \n",
       "...                                    ...   \n",
       "1065                                  -220   \n",
       "1066                                 -8032   \n",
       "1067                                -22500   \n",
       "1068                                  9500   \n",
       "1069                                 44450   \n",
       "\n",
       "      positive basicMonthlySalary - monthlyExpenses  \\\n",
       "0                                                 1   \n",
       "1                                                 0   \n",
       "2                                                 1   \n",
       "3                                                 1   \n",
       "4                                                 0   \n",
       "...                                             ...   \n",
       "1065                                              0   \n",
       "1066                                              0   \n",
       "1067                                              0   \n",
       "1068                                              1   \n",
       "1069                                              1   \n",
       "\n",
       "      monthlyFamilyIncome - monthlyExpenses  \\\n",
       "0                                   37050.0   \n",
       "1                                   14100.0   \n",
       "2                                   22750.0   \n",
       "3                                    3832.0   \n",
       "4                                    1200.0   \n",
       "...                                     ...   \n",
       "1065                                 -220.0   \n",
       "1066                                11948.0   \n",
       "1067                                 3538.0   \n",
       "1068                                 9500.0   \n",
       "1069                                89450.0   \n",
       "\n",
       "      positive monthlyFamilyIncome - monthlyExpenses  \\\n",
       "0                                                  1   \n",
       "1                                                  1   \n",
       "2                                                  1   \n",
       "3                                                  1   \n",
       "4                                                  1   \n",
       "...                                              ...   \n",
       "1065                                               0   \n",
       "1066                                               1   \n",
       "1067                                               1   \n",
       "1068                                               1   \n",
       "1069                                               1   \n",
       "\n",
       "      basicMonthlySalary / monthlyFamilyIncome  \\\n",
       "0                                     0.475000   \n",
       "1                                     0.500000   \n",
       "2                                     0.666667   \n",
       "3                                     1.766667   \n",
       "4                                     0.792000   \n",
       "...                                        ...   \n",
       "1065                                  1.000000   \n",
       "1066                                  0.001000   \n",
       "1067                                  0.349050   \n",
       "1068                                  1.000000   \n",
       "1069                                  0.687065   \n",
       "\n",
       "      monthlyExpenses / basicMonthlySalary  \\\n",
       "0                                 0.155263   \n",
       "1                                 1.060000   \n",
       "2                                 0.362500   \n",
       "3                                 0.385283   \n",
       "4                                 1.202020   \n",
       "...                                    ...   \n",
       "1065                              1.011579   \n",
       "1066                            402.600000   \n",
       "1067                              2.611517   \n",
       "1068                              0.813725   \n",
       "1069                              0.550101   \n",
       "\n",
       "      monthlyExpenses / monthlyFamilyIncome  \\\n",
       "0                                  0.073750   \n",
       "1                                  0.530000   \n",
       "2                                  0.241667   \n",
       "3                                  0.680667   \n",
       "4                                  0.952000   \n",
       "...                                     ...   \n",
       "1065                               1.011579   \n",
       "1066                               0.402600   \n",
       "1067                               0.911550   \n",
       "1068                               0.813725   \n",
       "1069                               0.377955   \n",
       "\n",
       "      monthlyVices / basicMonthlySalary  monthlyVices / monthlyFamilyIncome  \\\n",
       "0                              0.000000                            0.000000   \n",
       "1                              0.000000                            0.000000   \n",
       "2                              0.000000                            0.000000   \n",
       "3                              0.000000                            0.000000   \n",
       "4                              0.000000                            0.000000   \n",
       "...                                 ...                                 ...   \n",
       "1065                           0.000263                            0.000263   \n",
       "1066                           0.000000                            0.000000   \n",
       "1067                           0.000000                            0.000000   \n",
       "1068                           0.000000                            0.000000   \n",
       "1069                           0.000000                            0.000000   \n",
       "\n",
       "      basicMonthlySalary / workingFamilyCount  \\\n",
       "0                                 6333.333333   \n",
       "1                                 7500.000000   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "...                                       ...   \n",
       "1065                              9500.000000   \n",
       "1066                                20.000000   \n",
       "1067                              6981.000000   \n",
       "1068                                      NaN   \n",
       "1069                             32933.333333   \n",
       "\n",
       "      basicMonthlySalary / residentsCount  \\\n",
       "0                             6333.333333   \n",
       "1                             3750.000000   \n",
       "2                             6666.666667   \n",
       "3                             4240.000000   \n",
       "4                                     NaN   \n",
       "...                                   ...   \n",
       "1065                          4750.000000   \n",
       "1066                            20.000000   \n",
       "1067                          3490.500000   \n",
       "1068                         10200.000000   \n",
       "1069                         32933.333333   \n",
       "\n",
       "      monthlyFamilyIncome / workingFamilyCount  \\\n",
       "0                                 13333.333333   \n",
       "1                                 15000.000000   \n",
       "2                                          NaN   \n",
       "3                                          NaN   \n",
       "4                                          NaN   \n",
       "...                                        ...   \n",
       "1065                               9500.000000   \n",
       "1066                              20000.000000   \n",
       "1067                              20000.000000   \n",
       "1068                                       NaN   \n",
       "1069                              47933.333333   \n",
       "\n",
       "      monthlyFamilyIncome / residentsCount  \\\n",
       "0                             13333.333333   \n",
       "1                              7500.000000   \n",
       "2                             10000.000000   \n",
       "3                              2400.000000   \n",
       "4                                      NaN   \n",
       "...                                    ...   \n",
       "1065                           4750.000000   \n",
       "1066                          20000.000000   \n",
       "1067                          10000.000000   \n",
       "1068                          10200.000000   \n",
       "1069                          47933.333333   \n",
       "\n",
       "      monthlyExpenses / workingFamilyCount  monthlyExpenses / residentsCount  \\\n",
       "0                               983.333333                        983.333333   \n",
       "1                              7950.000000                       3975.000000   \n",
       "2                                      NaN                       2416.666667   \n",
       "3                                      NaN                       1633.600000   \n",
       "4                                      NaN                               NaN   \n",
       "...                                    ...                               ...   \n",
       "1065                           9610.000000                       4805.000000   \n",
       "1066                           8052.000000                       8052.000000   \n",
       "1067                          18231.000000                       9115.500000   \n",
       "1068                                   NaN                       8300.000000   \n",
       "1069                          18116.666667                      18116.666667   \n",
       "\n",
       "      monthlyUtilityBills / workingFamilyCount  \\\n",
       "0                                    16.666667   \n",
       "1                                   700.000000   \n",
       "2                                          NaN   \n",
       "3                                          NaN   \n",
       "4                                          NaN   \n",
       "...                                        ...   \n",
       "1065                               3600.500000   \n",
       "1066                                200.000000   \n",
       "1067                               1725.000000   \n",
       "1068                                       NaN   \n",
       "1069                               2616.666667   \n",
       "\n",
       "      monthlyUtilityBills / residentsCount  \n",
       "0                                16.666667  \n",
       "1                               350.000000  \n",
       "2                               366.666667  \n",
       "3                               760.000000  \n",
       "4                                      NaN  \n",
       "...                                    ...  \n",
       "1065                           1800.250000  \n",
       "1066                            200.000000  \n",
       "1067                            862.500000  \n",
       "1068                            440.000000  \n",
       "1069                           2616.666667  \n",
       "\n",
       "[1070 rows x 91 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.copy()\n",
    "\n",
    "df['monthlyUtilityBills'] = (\n",
    "    df['water'] + df['electricity'] + df['rent'] +\n",
    "    df['internet'] + df['mobileLoad']\n",
    ")\n",
    "df['monthlyVices'] = (\n",
    "    df['smoking'] + df['alcohol'] + df['gambling'] +\n",
    "    df['smallLottery'] + df['otherVices']\n",
    ")\n",
    "df['monthlyExpenses'] = (\n",
    "    df['food'] + df['hygiene'] + df['houseCleaning'] +\n",
    "    df['fare'] + df['parking'] + df['gasoline'] +\n",
    "    df['tuition'] + df['allowance'] + df['uniform'] +\n",
    "    df['otherEducation'] + df['emergency'] + df['medicine'] +\n",
    "    df['repair'] + df['cinema'] + df['dineOut'] +\n",
    "    df['leisure'] + df['personalCare'] + df['clothing'] +\n",
    "    df['vehicleLoan'] + df['monthlyUtilityBills'] +\n",
    "    df['informalLenders'] + df['companyLoan'] + df['privateLoans'] +\n",
    "    df['governmentLoans'] + df['monthlyVices']\n",
    ")\n",
    "df['monthlySoloNetIncome'] = (\n",
    "    df['basicMonthlySalary'] - df['monthlyExpenses']\n",
    ")\n",
    "df['positiveMonthlySoloNetIncome'] = (\n",
    "    df['monthlySoloNetIncome'] > 0\n",
    ").astype(int)\n",
    "df['monthlyFamilyNetIncome'] = (\n",
    "    df['monthlyFamilyIncome'] - df['monthlyExpenses']\n",
    ")\n",
    "df['positiveMonthlyFamilyNetIncome'] = (\n",
    "    df['monthlyFamilyNetIncome'] > 0\n",
    ").astype(int)\n",
    "df['monthlySoloNetIncomeWithSavings'] = (\n",
    "    df['basicMonthlySalary'] + df['savings'] - df['monthlyExpenses']\n",
    ")\n",
    "df['positiveMonthlySoloNetIncomeWithSavings'] = (\n",
    "    df['monthlySoloNetIncomeWithSavings'] > 0\n",
    ").astype(int)\n",
    "df['monthlyFamilyNetIncomeWithSavings'] = (\n",
    "    df['monthlyFamilyIncome'] + df['savings'] - df['monthlyExpenses']\n",
    ")\n",
    "df['positiveMonthlyFamilyNetIncomeWithSavings'] = (\n",
    "    df['monthlyFamilyNetIncomeWithSavings'] > 0\n",
    ").astype(int)\n",
    "df['monthlyFamilyIncome - basicMonthlySalary'] = (\n",
    "    df['monthlyFamilyIncome'] - df['basicMonthlySalary']\n",
    ")\n",
    "df['positive monthlyFamilyIncome - basicMonthlySalary'] = (\n",
    "    df['monthlyFamilyIncome - basicMonthlySalary'] > 0\n",
    ").astype(int)\n",
    "df['basicMonthlySalary - monthlyExpenses'] = (\n",
    "    df['basicMonthlySalary'] - df['monthlyExpenses']\n",
    ")\n",
    "df['positive basicMonthlySalary - monthlyExpenses'] = (\n",
    "    df['basicMonthlySalary - monthlyExpenses'] > 0\n",
    ").astype(int)\n",
    "df['monthlyFamilyIncome - monthlyExpenses'] = (\n",
    "    df['monthlyFamilyIncome'] - df['monthlyExpenses']\n",
    ")\n",
    "df['positive monthlyFamilyIncome - monthlyExpenses'] = (\n",
    "    df['monthlyFamilyIncome - monthlyExpenses'] > 0\n",
    ").astype(int)\n",
    "df['basicMonthlySalary / monthlyFamilyIncome'] = np.where(\n",
    "    df['monthlyFamilyIncome'] == 0,\n",
    "    np.nan,\n",
    "    df['basicMonthlySalary'] / df['monthlyFamilyIncome']\n",
    ")\n",
    "df['monthlyExpenses / basicMonthlySalary'] = np.where(\n",
    "    df['basicMonthlySalary'] == 0,\n",
    "    np.nan,\n",
    "    df['monthlyExpenses'] / df['basicMonthlySalary']\n",
    ")\n",
    "df['monthlyExpenses / monthlyFamilyIncome'] = np.where(\n",
    "    df['monthlyFamilyIncome'] == 0,\n",
    "    np.nan,\n",
    "    df['monthlyExpenses'] / df['monthlyFamilyIncome']\n",
    ")\n",
    "df['monthlyVices / basicMonthlySalary'] = np.where(\n",
    "    df['basicMonthlySalary'] == 0,\n",
    "    np.nan,\n",
    "    df['monthlyVices'] / df['basicMonthlySalary']\n",
    ")\n",
    "df['monthlyVices / monthlyFamilyIncome'] = np.where(\n",
    "    df['monthlyFamilyIncome'] == 0,\n",
    "    np.nan,\n",
    "    df['monthlyVices'] / df['monthlyFamilyIncome']\n",
    ")\n",
    "df['basicMonthlySalary / workingFamilyCount'] = np.where(\n",
    "    df['workingFamilyCount'] == 0,\n",
    "    np.nan,\n",
    "    df['basicMonthlySalary'] / df['workingFamilyCount']\n",
    ")\n",
    "df['basicMonthlySalary / residentsCount'] = np.where(\n",
    "    df['residentsCount'] == 0,\n",
    "    np.nan,\n",
    "    df['basicMonthlySalary'] / df['residentsCount']\n",
    ")\n",
    "df['monthlyFamilyIncome / workingFamilyCount'] = np.where(\n",
    "    df['workingFamilyCount'] == 0,\n",
    "    np.nan,\n",
    "    df['monthlyFamilyIncome'] / df['workingFamilyCount']\n",
    ")\n",
    "df['monthlyFamilyIncome / residentsCount'] = np.where(\n",
    "    df['residentsCount'] == 0,\n",
    "    np.nan,\n",
    "    df['monthlyFamilyIncome'] / df['residentsCount']\n",
    ")\n",
    "df['monthlyExpenses / workingFamilyCount'] = np.where(\n",
    "    df['workingFamilyCount'] == 0,\n",
    "    np.nan,\n",
    "    df['monthlyExpenses'] / df['workingFamilyCount']\n",
    ")\n",
    "df['monthlyExpenses / residentsCount'] = np.where(\n",
    "    df['residentsCount'] == 0,\n",
    "    np.nan,\n",
    "    df['monthlyExpenses'] / df['residentsCount']\n",
    ")\n",
    "df['monthlyUtilityBills / workingFamilyCount'] = np.where(\n",
    "    df['workingFamilyCount'] == 0,\n",
    "    np.nan,\n",
    "    df['monthlyUtilityBills'] / df['workingFamilyCount']\n",
    ")\n",
    "df['monthlyUtilityBills / residentsCount'] = np.where(\n",
    "    df['residentsCount'] == 0,\n",
    "    np.nan,\n",
    "    df['monthlyUtilityBills'] / df['residentsCount']\n",
    ")\n",
    "dataset = df.copy()\n",
    "\n",
    "# Convert all int32 columns to int64\n",
    "for col in dataset.select_dtypes(include='int32').columns:\n",
    "    dataset[col] = dataset[col].astype('int64')\n",
    "\n",
    "dataset['userId'] = dataset['userId'].astype(str)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1070 entries, 0 to 1069\n",
      "Data columns (total 91 columns):\n",
      " #   Column                                             Non-Null Count  Dtype  \n",
      "---  ------                                             --------------  -----  \n",
      " 0   userId                                             1070 non-null   object \n",
      " 1   lastFirstName                                      1070 non-null   object \n",
      " 2   age                                                1070 non-null   int64  \n",
      " 3   gender                                             1070 non-null   object \n",
      " 4   province                                           1070 non-null   object \n",
      " 5   job                                                1070 non-null   object \n",
      " 6   basicMonthlySalary                                 1070 non-null   int64  \n",
      " 7   preferredNetDisposableIncomeId                     1070 non-null   int64  \n",
      " 8   workingFamilyCount                                 1070 non-null   int64  \n",
      " 9   residentsCount                                     1070 non-null   int64  \n",
      " 10  monthlyFamilyIncome                                1070 non-null   float64\n",
      " 11  food                                               1070 non-null   int64  \n",
      " 12  hygiene                                            1070 non-null   int64  \n",
      " 13  houseCleaning                                      1070 non-null   int64  \n",
      " 14  fare                                               1070 non-null   int64  \n",
      " 15  parking                                            1070 non-null   int64  \n",
      " 16  gasoline                                           1070 non-null   int64  \n",
      " 17  tuition                                            1070 non-null   int64  \n",
      " 18  allowance                                          1070 non-null   int64  \n",
      " 19  uniform                                            1070 non-null   int64  \n",
      " 20  otherEducation                                     1070 non-null   int64  \n",
      " 21  emergency                                          1070 non-null   int64  \n",
      " 22  medicine                                           1070 non-null   int64  \n",
      " 23  water                                              1070 non-null   int64  \n",
      " 24  electricity                                        1070 non-null   int64  \n",
      " 25  rent                                               1070 non-null   int64  \n",
      " 26  repair                                             1070 non-null   int64  \n",
      " 27  cinema                                             1070 non-null   int64  \n",
      " 28  dineOut                                            1070 non-null   int64  \n",
      " 29  leisure                                            1070 non-null   int64  \n",
      " 30  personalCare                                       1070 non-null   int64  \n",
      " 31  clothing                                           1070 non-null   int64  \n",
      " 32  mobileLoad                                         1070 non-null   int64  \n",
      " 33  internet                                           1070 non-null   int64  \n",
      " 34  vehicleLoan                                        1070 non-null   int64  \n",
      " 35  informalLenders                                    1070 non-null   int64  \n",
      " 36  companyLoan                                        1070 non-null   int64  \n",
      " 37  privateLoans                                       1070 non-null   int64  \n",
      " 38  governmentLoans                                    1070 non-null   int64  \n",
      " 39  smoking                                            1070 non-null   int64  \n",
      " 40  alcohol                                            1070 non-null   int64  \n",
      " 41  gambling                                           1070 non-null   int64  \n",
      " 42  smallLottery                                       1070 non-null   int64  \n",
      " 43  otherVices                                         1070 non-null   int64  \n",
      " 44  savings                                            1070 non-null   int64  \n",
      " 45  loanOthers                                         1070 non-null   int64  \n",
      " 46  payInsurance                                       1070 non-null   int64  \n",
      " 47  loanSSS                                            1070 non-null   int64  \n",
      " 48  payFamilySupport                                   1070 non-null   int64  \n",
      " 49  loanPagIbig                                        1070 non-null   int64  \n",
      " 50  loanGSIS                                           1070 non-null   int64  \n",
      " 51  loanPersonal                                       1070 non-null   int64  \n",
      " 52  houseHasPensioner                                  1070 non-null   int64  \n",
      " 53  houseHasPrivateEmployee                            1070 non-null   int64  \n",
      " 54  houseHasBusiness                                   1070 non-null   int64  \n",
      " 55  houseHasFreelancer                                 1070 non-null   int64  \n",
      " 56  houseHasGovtEmployee                               1070 non-null   int64  \n",
      " 57  houseHasOFW                                        1070 non-null   int64  \n",
      " 58  houseOnlyFamily                                    1070 non-null   int64  \n",
      " 59  houseExtendedFamily                                1070 non-null   int64  \n",
      " 60  home_ownership_class                               1070 non-null   int64  \n",
      " 61  monthlyUtilityBills                                1070 non-null   int64  \n",
      " 62  monthlyVices                                       1070 non-null   int64  \n",
      " 63  monthlyExpenses                                    1070 non-null   int64  \n",
      " 64  monthlySoloNetIncome                               1070 non-null   int64  \n",
      " 65  positiveMonthlySoloNetIncome                       1070 non-null   int64  \n",
      " 66  monthlyFamilyNetIncome                             1070 non-null   float64\n",
      " 67  positiveMonthlyFamilyNetIncome                     1070 non-null   int64  \n",
      " 68  monthlySoloNetIncomeWithSavings                    1070 non-null   int64  \n",
      " 69  positiveMonthlySoloNetIncomeWithSavings            1070 non-null   int64  \n",
      " 70  monthlyFamilyNetIncomeWithSavings                  1070 non-null   float64\n",
      " 71  positiveMonthlyFamilyNetIncomeWithSavings          1070 non-null   int64  \n",
      " 72  monthlyFamilyIncome - basicMonthlySalary           1070 non-null   float64\n",
      " 73  positive monthlyFamilyIncome - basicMonthlySalary  1070 non-null   int64  \n",
      " 74  basicMonthlySalary - monthlyExpenses               1070 non-null   int64  \n",
      " 75  positive basicMonthlySalary - monthlyExpenses      1070 non-null   int64  \n",
      " 76  monthlyFamilyIncome - monthlyExpenses              1070 non-null   float64\n",
      " 77  positive monthlyFamilyIncome - monthlyExpenses     1070 non-null   int64  \n",
      " 78  basicMonthlySalary / monthlyFamilyIncome           1070 non-null   float64\n",
      " 79  monthlyExpenses / basicMonthlySalary               1069 non-null   float64\n",
      " 80  monthlyExpenses / monthlyFamilyIncome              1070 non-null   float64\n",
      " 81  monthlyVices / basicMonthlySalary                  1069 non-null   float64\n",
      " 82  monthlyVices / monthlyFamilyIncome                 1070 non-null   float64\n",
      " 83  basicMonthlySalary / workingFamilyCount            935 non-null    float64\n",
      " 84  basicMonthlySalary / residentsCount                995 non-null    float64\n",
      " 85  monthlyFamilyIncome / workingFamilyCount           935 non-null    float64\n",
      " 86  monthlyFamilyIncome / residentsCount               995 non-null    float64\n",
      " 87  monthlyExpenses / workingFamilyCount               935 non-null    float64\n",
      " 88  monthlyExpenses / residentsCount                   995 non-null    float64\n",
      " 89  monthlyUtilityBills / workingFamilyCount           935 non-null    float64\n",
      " 90  monthlyUtilityBills / residentsCount               995 non-null    float64\n",
      "dtypes: float64(18), int64(68), object(5)\n",
      "memory usage: 760.8+ KB\n"
     ]
    }
   ],
   "source": [
    "cols_with_nulls = [\n",
    "    'monthlyExpenses / basicMonthlySalary',\n",
    "    'monthlyVices / basicMonthlySalary',\n",
    "    'basicMonthlySalary / workingFamilyCount',\n",
    "    'basicMonthlySalary / residentsCount',\n",
    "    'monthlyFamilyIncome / workingFamilyCount',\n",
    "    'monthlyFamilyIncome / residentsCount',\n",
    "    'monthlyExpenses / workingFamilyCount',\n",
    "    'monthlyExpenses / residentsCount',\n",
    "    'monthlyUtilityBills / workingFamilyCount',\n",
    "    'monthlyUtilityBills / residentsCount'\n",
    "]\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>lastFirstName</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>province</th>\n",
       "      <th>job</th>\n",
       "      <th>basicMonthlySalary</th>\n",
       "      <th>preferredNetDisposableIncomeId</th>\n",
       "      <th>workingFamilyCount</th>\n",
       "      <th>residentsCount</th>\n",
       "      <th>monthlyFamilyIncome</th>\n",
       "      <th>food</th>\n",
       "      <th>hygiene</th>\n",
       "      <th>houseCleaning</th>\n",
       "      <th>fare</th>\n",
       "      <th>parking</th>\n",
       "      <th>gasoline</th>\n",
       "      <th>tuition</th>\n",
       "      <th>allowance</th>\n",
       "      <th>uniform</th>\n",
       "      <th>otherEducation</th>\n",
       "      <th>emergency</th>\n",
       "      <th>medicine</th>\n",
       "      <th>water</th>\n",
       "      <th>electricity</th>\n",
       "      <th>rent</th>\n",
       "      <th>repair</th>\n",
       "      <th>cinema</th>\n",
       "      <th>dineOut</th>\n",
       "      <th>leisure</th>\n",
       "      <th>personalCare</th>\n",
       "      <th>clothing</th>\n",
       "      <th>mobileLoad</th>\n",
       "      <th>internet</th>\n",
       "      <th>vehicleLoan</th>\n",
       "      <th>informalLenders</th>\n",
       "      <th>companyLoan</th>\n",
       "      <th>privateLoans</th>\n",
       "      <th>governmentLoans</th>\n",
       "      <th>smoking</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>gambling</th>\n",
       "      <th>smallLottery</th>\n",
       "      <th>otherVices</th>\n",
       "      <th>savings</th>\n",
       "      <th>loanOthers</th>\n",
       "      <th>payInsurance</th>\n",
       "      <th>loanSSS</th>\n",
       "      <th>payFamilySupport</th>\n",
       "      <th>loanPagIbig</th>\n",
       "      <th>loanGSIS</th>\n",
       "      <th>loanPersonal</th>\n",
       "      <th>houseHasPensioner</th>\n",
       "      <th>houseHasPrivateEmployee</th>\n",
       "      <th>houseHasBusiness</th>\n",
       "      <th>houseHasFreelancer</th>\n",
       "      <th>houseHasGovtEmployee</th>\n",
       "      <th>houseHasOFW</th>\n",
       "      <th>houseOnlyFamily</th>\n",
       "      <th>houseExtendedFamily</th>\n",
       "      <th>home_ownership_class</th>\n",
       "      <th>monthlyUtilityBills</th>\n",
       "      <th>monthlyVices</th>\n",
       "      <th>monthlyExpenses</th>\n",
       "      <th>monthlySoloNetIncome</th>\n",
       "      <th>positiveMonthlySoloNetIncome</th>\n",
       "      <th>monthlyFamilyNetIncome</th>\n",
       "      <th>positiveMonthlyFamilyNetIncome</th>\n",
       "      <th>monthlySoloNetIncomeWithSavings</th>\n",
       "      <th>positiveMonthlySoloNetIncomeWithSavings</th>\n",
       "      <th>monthlyFamilyNetIncomeWithSavings</th>\n",
       "      <th>positiveMonthlyFamilyNetIncomeWithSavings</th>\n",
       "      <th>monthlyFamilyIncome - basicMonthlySalary</th>\n",
       "      <th>positive monthlyFamilyIncome - basicMonthlySalary</th>\n",
       "      <th>basicMonthlySalary - monthlyExpenses</th>\n",
       "      <th>positive basicMonthlySalary - monthlyExpenses</th>\n",
       "      <th>monthlyFamilyIncome - monthlyExpenses</th>\n",
       "      <th>positive monthlyFamilyIncome - monthlyExpenses</th>\n",
       "      <th>basicMonthlySalary / monthlyFamilyIncome</th>\n",
       "      <th>monthlyExpenses / basicMonthlySalary</th>\n",
       "      <th>monthlyExpenses / monthlyFamilyIncome</th>\n",
       "      <th>monthlyVices / basicMonthlySalary</th>\n",
       "      <th>monthlyVices / monthlyFamilyIncome</th>\n",
       "      <th>basicMonthlySalary / workingFamilyCount</th>\n",
       "      <th>basicMonthlySalary / residentsCount</th>\n",
       "      <th>monthlyFamilyIncome / workingFamilyCount</th>\n",
       "      <th>monthlyFamilyIncome / residentsCount</th>\n",
       "      <th>monthlyExpenses / workingFamilyCount</th>\n",
       "      <th>monthlyExpenses / residentsCount</th>\n",
       "      <th>monthlyUtilityBills / workingFamilyCount</th>\n",
       "      <th>monthlyUtilityBills / residentsCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>370</td>\n",
       "      <td>IBALI, HOWARD</td>\n",
       "      <td>24</td>\n",
       "      <td>MALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>19000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>2950</td>\n",
       "      <td>16050</td>\n",
       "      <td>1</td>\n",
       "      <td>37050.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18050</td>\n",
       "      <td>1</td>\n",
       "      <td>39050.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16050</td>\n",
       "      <td>1</td>\n",
       "      <td>37050.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.155263</td>\n",
       "      <td>0.073750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6333.333333</td>\n",
       "      <td>6333.333333</td>\n",
       "      <td>13333.333333</td>\n",
       "      <td>13333.333333</td>\n",
       "      <td>983.333333</td>\n",
       "      <td>983.333333</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1025</td>\n",
       "      <td>PATALINGHOG, KIMBERLY</td>\n",
       "      <td>26</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>15000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>15900</td>\n",
       "      <td>-900</td>\n",
       "      <td>0</td>\n",
       "      <td>14100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1100</td>\n",
       "      <td>1</td>\n",
       "      <td>16100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-900</td>\n",
       "      <td>0</td>\n",
       "      <td>14100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7500.000000</td>\n",
       "      <td>3750.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>7500.000000</td>\n",
       "      <td>7950.000000</td>\n",
       "      <td>3975.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>350.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1105</td>\n",
       "      <td>TEMILLOSO, DENNIS</td>\n",
       "      <td>40</td>\n",
       "      <td>MALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>3000</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1100</td>\n",
       "      <td>0</td>\n",
       "      <td>7250</td>\n",
       "      <td>12750</td>\n",
       "      <td>1</td>\n",
       "      <td>22750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13750</td>\n",
       "      <td>1</td>\n",
       "      <td>23750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12750</td>\n",
       "      <td>1</td>\n",
       "      <td>22750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.241667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6666.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2416.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>366.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1719</td>\n",
       "      <td>OSCARES, ELMER</td>\n",
       "      <td>32</td>\n",
       "      <td>MALE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>MACHINE OPERATOR</td>\n",
       "      <td>21200</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>392</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>876</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3800</td>\n",
       "      <td>0</td>\n",
       "      <td>8168</td>\n",
       "      <td>13032</td>\n",
       "      <td>1</td>\n",
       "      <td>3832.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13532</td>\n",
       "      <td>1</td>\n",
       "      <td>4332.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-9200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13032</td>\n",
       "      <td>1</td>\n",
       "      <td>3832.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.766667</td>\n",
       "      <td>0.385283</td>\n",
       "      <td>0.680667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4240.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1633.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>760.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2081</td>\n",
       "      <td>LEGASPI, MANNY</td>\n",
       "      <td>39</td>\n",
       "      <td>MALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>CLERICAL SUPPORT</td>\n",
       "      <td>19800</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>300</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1500</td>\n",
       "      <td>1500</td>\n",
       "      <td>500</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4300</td>\n",
       "      <td>0</td>\n",
       "      <td>23800</td>\n",
       "      <td>-4000</td>\n",
       "      <td>0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6000</td>\n",
       "      <td>1</td>\n",
       "      <td>11200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-4000</td>\n",
       "      <td>0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>1.202020</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>12850</td>\n",
       "      <td>LIPANGO, ARVIN</td>\n",
       "      <td>34</td>\n",
       "      <td>MALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>ASSOCIATE PROFESSIONAL</td>\n",
       "      <td>19000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>1500</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7201</td>\n",
       "      <td>5</td>\n",
       "      <td>19220</td>\n",
       "      <td>-220</td>\n",
       "      <td>0</td>\n",
       "      <td>-220.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1780</td>\n",
       "      <td>1</td>\n",
       "      <td>1780.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-220</td>\n",
       "      <td>0</td>\n",
       "      <td>-220.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.011579</td>\n",
       "      <td>1.011579</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>9500.000000</td>\n",
       "      <td>4750.000000</td>\n",
       "      <td>9500.000000</td>\n",
       "      <td>4750.000000</td>\n",
       "      <td>9610.000000</td>\n",
       "      <td>4805.000000</td>\n",
       "      <td>3600.500000</td>\n",
       "      <td>1800.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>12852</td>\n",
       "      <td>TEJADA, JERIC</td>\n",
       "      <td>27</td>\n",
       "      <td>MALE</td>\n",
       "      <td>LAGUNA</td>\n",
       "      <td>CRAFT AND TRADE</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>500</td>\n",
       "      <td>400</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>8052</td>\n",
       "      <td>-8032</td>\n",
       "      <td>0</td>\n",
       "      <td>11948.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-7532</td>\n",
       "      <td>0</td>\n",
       "      <td>12448.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19980.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-8032</td>\n",
       "      <td>0</td>\n",
       "      <td>11948.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>402.600000</td>\n",
       "      <td>0.402600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>8052.000000</td>\n",
       "      <td>8052.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>12853</td>\n",
       "      <td>RAMOS, RHEALYN</td>\n",
       "      <td>27</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>CAVITE</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>13962</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30000</td>\n",
       "      <td>1000</td>\n",
       "      <td>150</td>\n",
       "      <td>1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3450</td>\n",
       "      <td>0</td>\n",
       "      <td>36462</td>\n",
       "      <td>-22500</td>\n",
       "      <td>0</td>\n",
       "      <td>3538.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-20500</td>\n",
       "      <td>0</td>\n",
       "      <td>5538.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26038.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-22500</td>\n",
       "      <td>0</td>\n",
       "      <td>3538.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.349050</td>\n",
       "      <td>2.611517</td>\n",
       "      <td>0.911550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6981.000000</td>\n",
       "      <td>3490.500000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>18231.000000</td>\n",
       "      <td>9115.500000</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>862.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>12854</td>\n",
       "      <td>BURGOS, LIEZEL</td>\n",
       "      <td>48</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>ILOILO</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>51000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>3000</td>\n",
       "      <td>1500</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15000</td>\n",
       "      <td>9000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>300</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2200</td>\n",
       "      <td>0</td>\n",
       "      <td>41500</td>\n",
       "      <td>9500</td>\n",
       "      <td>1</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10500</td>\n",
       "      <td>1</td>\n",
       "      <td>10500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9500</td>\n",
       "      <td>1</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10200.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10200.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8300.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>440.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>12857</td>\n",
       "      <td>SEÑERES, JENNY MAE</td>\n",
       "      <td>24</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>METRO MANILA</td>\n",
       "      <td>SERVICE AND SALES</td>\n",
       "      <td>98800</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>143800.0</td>\n",
       "      <td>7000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6000</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>200</td>\n",
       "      <td>1650</td>\n",
       "      <td>17000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7850</td>\n",
       "      <td>0</td>\n",
       "      <td>54350</td>\n",
       "      <td>44450</td>\n",
       "      <td>1</td>\n",
       "      <td>89450.0</td>\n",
       "      <td>1</td>\n",
       "      <td>54450</td>\n",
       "      <td>1</td>\n",
       "      <td>99450.0</td>\n",
       "      <td>1</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>44450</td>\n",
       "      <td>1</td>\n",
       "      <td>89450.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.687065</td>\n",
       "      <td>0.550101</td>\n",
       "      <td>0.377955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32933.333333</td>\n",
       "      <td>32933.333333</td>\n",
       "      <td>47933.333333</td>\n",
       "      <td>47933.333333</td>\n",
       "      <td>18116.666667</td>\n",
       "      <td>18116.666667</td>\n",
       "      <td>2616.666667</td>\n",
       "      <td>2616.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1070 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     userId          lastFirstName  age  gender      province  \\\n",
       "0       370          IBALI, HOWARD   24    MALE  METRO MANILA   \n",
       "1      1025  PATALINGHOG, KIMBERLY   26  FEMALE  METRO MANILA   \n",
       "2      1105      TEMILLOSO, DENNIS   40    MALE  METRO MANILA   \n",
       "3      1719         OSCARES, ELMER   32    MALE       UNKNOWN   \n",
       "4      2081         LEGASPI, MANNY   39    MALE  METRO MANILA   \n",
       "...     ...                    ...  ...     ...           ...   \n",
       "1065  12850         LIPANGO, ARVIN   34    MALE  METRO MANILA   \n",
       "1066  12852          TEJADA, JERIC   27    MALE        LAGUNA   \n",
       "1067  12853         RAMOS, RHEALYN   27  FEMALE        CAVITE   \n",
       "1068  12854         BURGOS, LIEZEL   48  FEMALE        ILOILO   \n",
       "1069  12857     SEÑERES, JENNY MAE   24  FEMALE  METRO MANILA   \n",
       "\n",
       "                         job  basicMonthlySalary  \\\n",
       "0          SERVICE AND SALES               19000   \n",
       "1          SERVICE AND SALES               15000   \n",
       "2          SERVICE AND SALES               20000   \n",
       "3           MACHINE OPERATOR               21200   \n",
       "4           CLERICAL SUPPORT               19800   \n",
       "...                      ...                 ...   \n",
       "1065  ASSOCIATE PROFESSIONAL               19000   \n",
       "1066         CRAFT AND TRADE                  20   \n",
       "1067       SERVICE AND SALES               13962   \n",
       "1068       SERVICE AND SALES               51000   \n",
       "1069       SERVICE AND SALES               98800   \n",
       "\n",
       "      preferredNetDisposableIncomeId  workingFamilyCount  residentsCount  \\\n",
       "0                                  2                   3               3   \n",
       "1                                  3                   2               4   \n",
       "2                                  2                   0               3   \n",
       "3                                  2                   0               5   \n",
       "4                                  2                   0               0   \n",
       "...                              ...                 ...             ...   \n",
       "1065                               2                   2               4   \n",
       "1066                               2                   1               1   \n",
       "1067                               1                   2               4   \n",
       "1068                               2                   0               5   \n",
       "1069                               3                   3               3   \n",
       "\n",
       "      monthlyFamilyIncome  food  hygiene  houseCleaning  fare  parking  \\\n",
       "0                 40000.0  1000      500            200   200        0   \n",
       "1                 30000.0  5000     3000           2000  2000        0   \n",
       "2                 30000.0  3000      500            300   200       50   \n",
       "3                 12000.0  2000      500            200   392        0   \n",
       "4                 25000.0  5000     2000           1000  1500        0   \n",
       "...                   ...   ...      ...            ...   ...      ...   \n",
       "1065              19000.0  5000     1000           1000     1        1   \n",
       "1066              20000.0  5000      500            400   280        0   \n",
       "1067              40000.0     4        2              1     3        1   \n",
       "1068              51000.0  5000     3000           1500  2000        0   \n",
       "1069             143800.0  7000     3000           2000     0        0   \n",
       "\n",
       "      gasoline  tuition  allowance  uniform  otherEducation  emergency  \\\n",
       "0            0        0         50        0               0        500   \n",
       "1            0        0          0        0               0       2000   \n",
       "2           50      500        100      100              50        500   \n",
       "3            0        0          0        0               0        100   \n",
       "4            0     1000        500     1500               0          0   \n",
       "...        ...      ...        ...      ...             ...        ...   \n",
       "1065         1     1000        500        1            1000       1000   \n",
       "1066         0        0          0        0               0          0   \n",
       "1067         1        0          0        0               0      30000   \n",
       "1068         0    15000       9000        0               0       1000   \n",
       "1069      6000        0          0        0               0       5000   \n",
       "\n",
       "      medicine  water  electricity  rent  repair  cinema  dineOut  leisure  \\\n",
       "0          450      0            0     0       0       0        0        0   \n",
       "1            0    200          200  1000     500       0        0        0   \n",
       "2          100    200          500     0     200       0      100      100   \n",
       "3            0    200          200  3000       0       0        0        0   \n",
       "4         1000    300         1900     0    1000       0     1000     1000   \n",
       "...        ...    ...          ...   ...     ...     ...      ...      ...   \n",
       "1065       500    200         1500  5000    1000       1        1        1   \n",
       "1066         0      0            0     0       0       0        0        0   \n",
       "1067      1000    150         1000  2000     500       0     1000        0   \n",
       "1068      1000    300          800     0       0       0      500      500   \n",
       "1069      1000      0            0  6000       0     500     2000     1000   \n",
       "\n",
       "      personalCare  clothing  mobileLoad  internet  vehicleLoan  \\\n",
       "0                0         0          50         0            0   \n",
       "1                0         0           0         0            0   \n",
       "2              100       200         200       200            0   \n",
       "3              100       200         400         0            0   \n",
       "4             1500      1500         500      1600            0   \n",
       "...            ...       ...         ...       ...          ...   \n",
       "1065             1         1         500         1            1   \n",
       "1066             0         0         200         0            0   \n",
       "1067             0         0         300         0            0   \n",
       "1068           300       500         300       800            0   \n",
       "1069          1000      1000         200      1650        17000   \n",
       "\n",
       "      informalLenders  companyLoan  privateLoans  governmentLoans  smoking  \\\n",
       "0                   0            0             0                0        0   \n",
       "1                   0            0             0                0        0   \n",
       "2                   0            0             0                0        0   \n",
       "3                   0            0             0              876        0   \n",
       "4                   0            0             0                0        0   \n",
       "...               ...          ...           ...              ...      ...   \n",
       "1065                1            1             1                1        1   \n",
       "1066                0            0             0             1672        0   \n",
       "1067                0          500             0                0        0   \n",
       "1068                0            0             0                0        0   \n",
       "1069                0            0             0                0        0   \n",
       "\n",
       "      alcohol  gambling  smallLottery  otherVices  savings  loanOthers  \\\n",
       "0           0         0             0           0     2000           0   \n",
       "1           0         0             0           0     2000           0   \n",
       "2           0         0             0           0     1000           0   \n",
       "3           0         0             0           0      500           0   \n",
       "4           0         0             0           0    10000           0   \n",
       "...       ...       ...           ...         ...      ...         ...   \n",
       "1065        1         1             1           1     2000           0   \n",
       "1066        0         0             0           0      500           0   \n",
       "1067        0         0             0           0     2000           0   \n",
       "1068        0         0             0           0     1000           0   \n",
       "1069        0         0             0           0    10000           0   \n",
       "\n",
       "      payInsurance  loanSSS  payFamilySupport  loanPagIbig  loanGSIS  \\\n",
       "0                0        0                 0            0         0   \n",
       "1                0        1                 1            1         0   \n",
       "2                0        0                 0            0         0   \n",
       "3                0        1                 0            0         0   \n",
       "4                0        0                 0            0         0   \n",
       "...            ...      ...               ...          ...       ...   \n",
       "1065             0        0                 0            0         0   \n",
       "1066             0        1                 0            1         0   \n",
       "1067             0        0                 0            0         0   \n",
       "1068             0        0                 0            0         0   \n",
       "1069             0        0                 0            0         0   \n",
       "\n",
       "      loanPersonal  houseHasPensioner  houseHasPrivateEmployee  \\\n",
       "0                0                  0                        1   \n",
       "1                0                  0                        1   \n",
       "2                0                  0                        1   \n",
       "3                0                  0                        1   \n",
       "4                0                  0                        0   \n",
       "...            ...                ...                      ...   \n",
       "1065             1                  0                        1   \n",
       "1066             0                  0                        1   \n",
       "1067             0                  0                        1   \n",
       "1068             0                  0                        0   \n",
       "1069             0                  0                        1   \n",
       "\n",
       "      houseHasBusiness  houseHasFreelancer  houseHasGovtEmployee  houseHasOFW  \\\n",
       "0                    0                   0                     0            1   \n",
       "1                    0                   0                     0            0   \n",
       "2                    0                   1                     0            0   \n",
       "3                    0                   0                     0            0   \n",
       "4                    0                   0                     0            0   \n",
       "...                ...                 ...                   ...          ...   \n",
       "1065                 0                   0                     0            0   \n",
       "1066                 0                   0                     0            0   \n",
       "1067                 0                   0                     0            0   \n",
       "1068                 0                   0                     0            1   \n",
       "1069                 0                   1                     0            1   \n",
       "\n",
       "      houseOnlyFamily  houseExtendedFamily  home_ownership_class  \\\n",
       "0                   0                    1                     1   \n",
       "1                   1                    0                     0   \n",
       "2                   0                    1                     0   \n",
       "3                   1                    0                     1   \n",
       "4                   0                    0                     0   \n",
       "...               ...                  ...                   ...   \n",
       "1065                1                    0                     0   \n",
       "1066                0                    0                     0   \n",
       "1067                1                    0                     0   \n",
       "1068                0                    0                     0   \n",
       "1069                1                    0                     0   \n",
       "\n",
       "      monthlyUtilityBills  monthlyVices  monthlyExpenses  \\\n",
       "0                      50             0             2950   \n",
       "1                    1400             0            15900   \n",
       "2                    1100             0             7250   \n",
       "3                    3800             0             8168   \n",
       "4                    4300             0            23800   \n",
       "...                   ...           ...              ...   \n",
       "1065                 7201             5            19220   \n",
       "1066                  200             0             8052   \n",
       "1067                 3450             0            36462   \n",
       "1068                 2200             0            41500   \n",
       "1069                 7850             0            54350   \n",
       "\n",
       "      monthlySoloNetIncome  positiveMonthlySoloNetIncome  \\\n",
       "0                    16050                             1   \n",
       "1                     -900                             0   \n",
       "2                    12750                             1   \n",
       "3                    13032                             1   \n",
       "4                    -4000                             0   \n",
       "...                    ...                           ...   \n",
       "1065                  -220                             0   \n",
       "1066                 -8032                             0   \n",
       "1067                -22500                             0   \n",
       "1068                  9500                             1   \n",
       "1069                 44450                             1   \n",
       "\n",
       "      monthlyFamilyNetIncome  positiveMonthlyFamilyNetIncome  \\\n",
       "0                    37050.0                               1   \n",
       "1                    14100.0                               1   \n",
       "2                    22750.0                               1   \n",
       "3                     3832.0                               1   \n",
       "4                     1200.0                               1   \n",
       "...                      ...                             ...   \n",
       "1065                  -220.0                               0   \n",
       "1066                 11948.0                               1   \n",
       "1067                  3538.0                               1   \n",
       "1068                  9500.0                               1   \n",
       "1069                 89450.0                               1   \n",
       "\n",
       "      monthlySoloNetIncomeWithSavings  \\\n",
       "0                               18050   \n",
       "1                                1100   \n",
       "2                               13750   \n",
       "3                               13532   \n",
       "4                                6000   \n",
       "...                               ...   \n",
       "1065                             1780   \n",
       "1066                            -7532   \n",
       "1067                           -20500   \n",
       "1068                            10500   \n",
       "1069                            54450   \n",
       "\n",
       "      positiveMonthlySoloNetIncomeWithSavings  \\\n",
       "0                                           1   \n",
       "1                                           1   \n",
       "2                                           1   \n",
       "3                                           1   \n",
       "4                                           1   \n",
       "...                                       ...   \n",
       "1065                                        1   \n",
       "1066                                        0   \n",
       "1067                                        0   \n",
       "1068                                        1   \n",
       "1069                                        1   \n",
       "\n",
       "      monthlyFamilyNetIncomeWithSavings  \\\n",
       "0                               39050.0   \n",
       "1                               16100.0   \n",
       "2                               23750.0   \n",
       "3                                4332.0   \n",
       "4                               11200.0   \n",
       "...                                 ...   \n",
       "1065                             1780.0   \n",
       "1066                            12448.0   \n",
       "1067                             5538.0   \n",
       "1068                            10500.0   \n",
       "1069                            99450.0   \n",
       "\n",
       "      positiveMonthlyFamilyNetIncomeWithSavings  \\\n",
       "0                                             1   \n",
       "1                                             1   \n",
       "2                                             1   \n",
       "3                                             1   \n",
       "4                                             1   \n",
       "...                                         ...   \n",
       "1065                                          1   \n",
       "1066                                          1   \n",
       "1067                                          1   \n",
       "1068                                          1   \n",
       "1069                                          1   \n",
       "\n",
       "      monthlyFamilyIncome - basicMonthlySalary  \\\n",
       "0                                      21000.0   \n",
       "1                                      15000.0   \n",
       "2                                      10000.0   \n",
       "3                                      -9200.0   \n",
       "4                                       5200.0   \n",
       "...                                        ...   \n",
       "1065                                       0.0   \n",
       "1066                                   19980.0   \n",
       "1067                                   26038.0   \n",
       "1068                                       0.0   \n",
       "1069                                   45000.0   \n",
       "\n",
       "      positive monthlyFamilyIncome - basicMonthlySalary  \\\n",
       "0                                                     1   \n",
       "1                                                     1   \n",
       "2                                                     1   \n",
       "3                                                     0   \n",
       "4                                                     1   \n",
       "...                                                 ...   \n",
       "1065                                                  0   \n",
       "1066                                                  1   \n",
       "1067                                                  1   \n",
       "1068                                                  0   \n",
       "1069                                                  1   \n",
       "\n",
       "      basicMonthlySalary - monthlyExpenses  \\\n",
       "0                                    16050   \n",
       "1                                     -900   \n",
       "2                                    12750   \n",
       "3                                    13032   \n",
       "4                                    -4000   \n",
       "...                                    ...   \n",
       "1065                                  -220   \n",
       "1066                                 -8032   \n",
       "1067                                -22500   \n",
       "1068                                  9500   \n",
       "1069                                 44450   \n",
       "\n",
       "      positive basicMonthlySalary - monthlyExpenses  \\\n",
       "0                                                 1   \n",
       "1                                                 0   \n",
       "2                                                 1   \n",
       "3                                                 1   \n",
       "4                                                 0   \n",
       "...                                             ...   \n",
       "1065                                              0   \n",
       "1066                                              0   \n",
       "1067                                              0   \n",
       "1068                                              1   \n",
       "1069                                              1   \n",
       "\n",
       "      monthlyFamilyIncome - monthlyExpenses  \\\n",
       "0                                   37050.0   \n",
       "1                                   14100.0   \n",
       "2                                   22750.0   \n",
       "3                                    3832.0   \n",
       "4                                    1200.0   \n",
       "...                                     ...   \n",
       "1065                                 -220.0   \n",
       "1066                                11948.0   \n",
       "1067                                 3538.0   \n",
       "1068                                 9500.0   \n",
       "1069                                89450.0   \n",
       "\n",
       "      positive monthlyFamilyIncome - monthlyExpenses  \\\n",
       "0                                                  1   \n",
       "1                                                  1   \n",
       "2                                                  1   \n",
       "3                                                  1   \n",
       "4                                                  1   \n",
       "...                                              ...   \n",
       "1065                                               0   \n",
       "1066                                               1   \n",
       "1067                                               1   \n",
       "1068                                               1   \n",
       "1069                                               1   \n",
       "\n",
       "      basicMonthlySalary / monthlyFamilyIncome  \\\n",
       "0                                     0.475000   \n",
       "1                                     0.500000   \n",
       "2                                     0.666667   \n",
       "3                                     1.766667   \n",
       "4                                     0.792000   \n",
       "...                                        ...   \n",
       "1065                                  1.000000   \n",
       "1066                                  0.001000   \n",
       "1067                                  0.349050   \n",
       "1068                                  1.000000   \n",
       "1069                                  0.687065   \n",
       "\n",
       "      monthlyExpenses / basicMonthlySalary  \\\n",
       "0                                 0.155263   \n",
       "1                                 1.060000   \n",
       "2                                 0.362500   \n",
       "3                                 0.385283   \n",
       "4                                 1.202020   \n",
       "...                                    ...   \n",
       "1065                              1.011579   \n",
       "1066                            402.600000   \n",
       "1067                              2.611517   \n",
       "1068                              0.813725   \n",
       "1069                              0.550101   \n",
       "\n",
       "      monthlyExpenses / monthlyFamilyIncome  \\\n",
       "0                                  0.073750   \n",
       "1                                  0.530000   \n",
       "2                                  0.241667   \n",
       "3                                  0.680667   \n",
       "4                                  0.952000   \n",
       "...                                     ...   \n",
       "1065                               1.011579   \n",
       "1066                               0.402600   \n",
       "1067                               0.911550   \n",
       "1068                               0.813725   \n",
       "1069                               0.377955   \n",
       "\n",
       "      monthlyVices / basicMonthlySalary  monthlyVices / monthlyFamilyIncome  \\\n",
       "0                              0.000000                            0.000000   \n",
       "1                              0.000000                            0.000000   \n",
       "2                              0.000000                            0.000000   \n",
       "3                              0.000000                            0.000000   \n",
       "4                              0.000000                            0.000000   \n",
       "...                                 ...                                 ...   \n",
       "1065                           0.000263                            0.000263   \n",
       "1066                           0.000000                            0.000000   \n",
       "1067                           0.000000                            0.000000   \n",
       "1068                           0.000000                            0.000000   \n",
       "1069                           0.000000                            0.000000   \n",
       "\n",
       "      basicMonthlySalary / workingFamilyCount  \\\n",
       "0                                 6333.333333   \n",
       "1                                 7500.000000   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "...                                       ...   \n",
       "1065                              9500.000000   \n",
       "1066                                20.000000   \n",
       "1067                              6981.000000   \n",
       "1068                                      NaN   \n",
       "1069                             32933.333333   \n",
       "\n",
       "      basicMonthlySalary / residentsCount  \\\n",
       "0                             6333.333333   \n",
       "1                             3750.000000   \n",
       "2                             6666.666667   \n",
       "3                             4240.000000   \n",
       "4                                     NaN   \n",
       "...                                   ...   \n",
       "1065                          4750.000000   \n",
       "1066                            20.000000   \n",
       "1067                          3490.500000   \n",
       "1068                         10200.000000   \n",
       "1069                         32933.333333   \n",
       "\n",
       "      monthlyFamilyIncome / workingFamilyCount  \\\n",
       "0                                 13333.333333   \n",
       "1                                 15000.000000   \n",
       "2                                          NaN   \n",
       "3                                          NaN   \n",
       "4                                          NaN   \n",
       "...                                        ...   \n",
       "1065                               9500.000000   \n",
       "1066                              20000.000000   \n",
       "1067                              20000.000000   \n",
       "1068                                       NaN   \n",
       "1069                              47933.333333   \n",
       "\n",
       "      monthlyFamilyIncome / residentsCount  \\\n",
       "0                             13333.333333   \n",
       "1                              7500.000000   \n",
       "2                             10000.000000   \n",
       "3                              2400.000000   \n",
       "4                                      NaN   \n",
       "...                                    ...   \n",
       "1065                           4750.000000   \n",
       "1066                          20000.000000   \n",
       "1067                          10000.000000   \n",
       "1068                          10200.000000   \n",
       "1069                          47933.333333   \n",
       "\n",
       "      monthlyExpenses / workingFamilyCount  monthlyExpenses / residentsCount  \\\n",
       "0                               983.333333                        983.333333   \n",
       "1                              7950.000000                       3975.000000   \n",
       "2                                      NaN                       2416.666667   \n",
       "3                                      NaN                       1633.600000   \n",
       "4                                      NaN                               NaN   \n",
       "...                                    ...                               ...   \n",
       "1065                           9610.000000                       4805.000000   \n",
       "1066                           8052.000000                       8052.000000   \n",
       "1067                          18231.000000                       9115.500000   \n",
       "1068                                   NaN                       8300.000000   \n",
       "1069                          18116.666667                      18116.666667   \n",
       "\n",
       "      monthlyUtilityBills / workingFamilyCount  \\\n",
       "0                                    16.666667   \n",
       "1                                   700.000000   \n",
       "2                                          NaN   \n",
       "3                                          NaN   \n",
       "4                                          NaN   \n",
       "...                                        ...   \n",
       "1065                               3600.500000   \n",
       "1066                                200.000000   \n",
       "1067                               1725.000000   \n",
       "1068                                       NaN   \n",
       "1069                               2616.666667   \n",
       "\n",
       "      monthlyUtilityBills / residentsCount  \n",
       "0                                16.666667  \n",
       "1                               350.000000  \n",
       "2                               366.666667  \n",
       "3                               760.000000  \n",
       "4                                      NaN  \n",
       "...                                    ...  \n",
       "1065                           1800.250000  \n",
       "1066                            200.000000  \n",
       "1067                            862.500000  \n",
       "1068                            440.000000  \n",
       "1069                           2616.666667  \n",
       "\n",
       "[1070 rows x 91 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset[dataset['monthlyExpenses / basicMonthlySalary'].isnull()]\n",
    "# dataset = dataset.copy().drop(columns=['Unnamed: 0'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      36.000000\n",
       "mean     -291.333333\n",
       "std       711.310581\n",
       "min     -2158.000000\n",
       "25%      -347.500000\n",
       "50%        17.500000\n",
       "75%       200.500000\n",
       "max       275.000000\n",
       "Name: OSDate - dateEntry, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['OSDate - dateEntry'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAHFCAYAAAAHcXhbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAABDr0lEQVR4nO3deXhTZfrG8TvpkqalZWmltFAooiwCgoALLiwyIKuKojColMVtEEXBQXAGKSIgMlRGXHAtLgOoM6AMA2hVNgWVzbrAoOxbEVlsgZaQNu/vD6f5GbqXtM2B7+e6cs3kzTlvnvMksTdnSWzGGCMAAACLsld1AQAAAGeDMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMAMAACyNMIOANmfOHNlsNq1fv77Qx3v37q3ExESfscTERA0ePLhMz7NmzRolJyfr119/LV+h56F3331XzZs3l9PplM1m0zfffFPs8lu2bNHgwYNVv359hYaGKiYmRj179tTSpUuLXP6uu+7ShRdeqLCwMMXExKhNmzYaMWKEsrKyvMsNHjxYNpvNe4uIiFBiYqJuvPFGpaamyuVylXsblyxZouTk5HKvX5F27dolm82mOXPmlHndzZs3Kzk5Wbt27Sr38+d/Nou6rVixosxz8jlEeRFmcM5ZuHChxo8fX6Z11qxZo4kTJ/If0VL65ZdfdNddd6lRo0ZatmyZ1q5dq8aNGxe5/IIFC3TZZZfp66+/1vjx4/XJJ5/opZdekiT17NlTY8aM8Vl+06ZNatu2rTZv3qwnnnhCy5Yt0+zZs9WrVy999NFHOnr0qM/yTqdTa9eu1dq1a7V48WI9+eSTioiI0D333KO2bdtq37595drOJUuWaOLEieVaN5Bt3rxZEydOPKswky81NdXb+9/f2rRpU+a5+ByivIKrugDA3y677LKqLqHM3G63bDabgoOt8ZH88ccf5Xa7deedd6pjx47FLrt9+3bdddddatmypVasWKGIiAjvY7fddpv+9Kc/afr06WrTpo0GDBggSZo5c6bsdrtWrFihyMhI7/L9+vXTpEmTdOZPytntdl111VU+Y4MGDdKQIUPUu3dv9evXT19++eXZbjYK0aJFC7Vr165KnjsnJ0dOp7NKnhuBhT0zOOeceZjJ4/HoqaeeUpMmTeR0OlWjRg1deuml+vvf/y5JSk5O1p///GdJUsOGDQvsJvd4PHrmmWfUtGlTORwO1a5dW4MGDSrwr31jjKZMmaIGDRooLCxM7dq1U1pamjp16qROnTp5l1uxYoVsNpvefvttjR49WnXr1pXD4dC2bdv0yy+/aPjw4brkkktUrVo11a5dW9dff71Wr17t81z5hximT5+uadOmKTExUU6nU506dfIGjbFjxyo+Pl7Vq1dX3759dejQoVL1b9GiRWrfvr3Cw8MVGRmprl27au3atd7HBw8erGuvvVaS1L9/f9lsNp/tO9Ozzz6r7OxszZo1yyfI5JsxY4Zq1KihyZMne8eOHDmiqKgoVatWrdA5bTZbqbalW7duuueee/TVV19p1apV3vF3331X3bp1U1xcnJxOp5o1a6axY8fq5MmTPtv5wgsveJ8v/5a/N8MYoxdffFGtW7eW0+lUzZo11a9fP+3YsaNUtZXFgQMHdPvttysyMlLVq1dX//79dfDgwQLLrV+/XgMGDPC+HxITE/XHP/5Ru3fv9i4zZ84c3XbbbZKkzp07e7fr94erPvnkE3Xp0kVRUVEKDw/XNddco08//bTc9dtsNo0YMUJvv/22mjVrpvDwcLVq1UqLFy/2LlPS5zAxMVG9e/f27uULCwvTxIkT1aVLFzVt2rRAwDXG6KKLLlKvXr3KXTcsxAABLDU11UgyX375pXG73QVuPXv2NA0aNPBZp0GDBiYpKcl7f+rUqSYoKMhMmDDBfPrpp2bZsmVm5syZJjk52RhjzN69e82DDz5oJJkFCxaYtWvXmrVr15rMzExjjDH33nuvkWRGjBhhli1bZmbPnm0uuOACk5CQYH755Rfv84wbN85IMvfee69ZtmyZefXVV039+vVNXFyc6dixo3e55cuXG0mmbt26pl+/fmbRokVm8eLF5siRI+a///2v+dOf/mTmz59vVqxYYRYvXmyGDRtm7Ha7Wb58uXeOnTt3GkmmQYMGpk+fPmbx4sXmnXfeMbGxsaZx48bmrrvuMkOHDjVLly41s2fPNtWqVTN9+vQpsd//+Mc/jCTTrVs388EHH5h3333XtG3b1oSGhprVq1cbY4zZtm2beeGFF4wkM2XKFLN27Vrzww8/FDln48aNTWxsbLHPe/vttxtJJiMjwxhjzFNPPWUkmT/+8Y9mxYoVJjs7u8h1k5KSTERERJGPL1u2zEgykyZN8o5NmjTJPPvss+Y///mPWbFihZk9e7Zp2LCh6dy5s3eZbdu2mX79+hlJ3vfE2rVrzalTp4wxxtxzzz0mJCTEjB492ixbtszMnTvXNG3a1MTGxpqDBw8Wu71lkZ2dbZo1a2aqV69uZs2aZT766CPz0EMPmfr16xtJJjU11bvs+++/b5544gmzcOFCs3LlSjN//nzTsWNHc8EFF3jfq4cOHTJTpkwxkswLL7zg3a5Dhw4ZY4x5++23jc1mMzfffLNZsGCB+fe//2169+5tgoKCzCeffOJ9ruI+m7m5uT7bIMkkJiaaK664wrz33ntmyZIlplOnTiY4ONhs377dGFPy57BBgwYmLi7OXHjhheaNN94wy5cvN19//bX58MMPjSSTlpbm85z/+c9/jCTzn//8x2+vBQIXYQYBLf8/mMXdSgozvXv3Nq1bty72eaZPn24kmZ07d/qMb9myxUgyw4cP9xn/6quvjCTz+OOPG2OMOXr0qHE4HKZ///4+y61du9ZIKjTMdOjQocTtz83NNW6323Tp0sX07dvXO54fZlq1amXy8vK84zNnzjSSzI033ugzz8MPP2wkef8wFCYvL8/Ex8ebli1b+sx5/PhxU7t2bXP11VcX2Ib333+/xG0ICwszV111VbHLPPbYY0aS+eqrr4wxxpw6dcrcfPPN3tc4KCjIXHbZZeYvf/mL949uvpLCTP5r+Kc//anQxz0ej3G73WblypVGkklPT/c+9sADD5jC/s2X/7rOmDHDZ3zv3r3G6XSaMWPGFLu9ZfHSSy8ZSebDDz/0Gb/nnnsKhJkz5ebmmhMnTpiIiAjz97//3Tv+/vvvG0k+AdkYY06ePGlq1apVIPjm5eWZVq1amSuuuMI7VtxnMygoyGd9SSY2NtZkZWV5xw4ePGjsdruZOnWqd6yoz6Exv32ug4KCzNatWwvUduGFF5qbbrrJZ7xHjx6mUaNGxuPxFNkfnDs4zARLeOutt7Ru3boCt/zDHcW54oorlJ6eruHDh+ujjz7yuRKmJMuXL5ekAldHXXHFFWrWrJl31/uXX34pl8ul22+/3We5q666qsDVVvluvfXWQsdnz56tNm3aKCwsTMHBwQoJCdGnn36qLVu2FFi2Z8+estv//2PcrFkzSSqwaz1/fM+ePUVsqbR161YdOHBAd911l8+c1apV06233qovv/xS2dnZRa5/Nsz/DhHkHz5yOBxauHChNm/erGeffVYDBgzQL7/8osmTJ6tZs2baunVrmef+vR07dmjgwIGqU6eOgoKCFBIS4j33p7A+n2nx4sWy2Wy68847lZub673VqVNHrVq1KvFKnt+vk5ubW2iN+ZYvX67IyEjdeOONPuMDBw4ssOyJEyf02GOP6aKLLlJwcLCCg4NVrVo1nTx5slTbtWbNGh09elRJSUk+9Xk8HnXv3l3r1q3zORQnFf7Z/OqrrwrM3blzZ5/zn2JjY1W7dm2fQ2AlufTSSwucaG632zVixAgtXrzY+/7evn27li1bpuHDh5f6kCSszRpnG+K816xZs0JPMqxevbr27t1b7Lrjxo1TRESE3nnnHc2ePVtBQUHq0KGDpk2bVuKJi0eOHJEkxcXFFXgsPj7e+x/i/OViY2MLLFfYWFFzpqSkaPTo0br//vs1adIkxcTEKCgoSOPHjy/0j1GtWrV87oeGhhY7furUqUJr+f02FLWtHo9Hx44dU3h4eJFzFKZ+/frauXNnscvkn4eSkJDgM96sWTNvEDPGaObMmRo1apTGjx+v9957r1TPn/8axcfHS/rtD/51112nsLAwPfXUU2rcuLHCw8O1d+9e3XLLLcrJySlxzp9//lnGmCJf2wsvvLDIdXft2qWGDRv6jC1fvrzI846OHDlS6PPUqVOnwNjAgQP16aefavz48br88ssVFRUlm82mnj17lnq7pN9OtC7K0aNHfc59Kuqzeabo6OgCYw6Ho1R15SvsvSlJQ4cO1RNPPKHZs2drypQpeuGFF+R0OjV06NBSzw1rI8zgnBccHKxRo0Zp1KhR+vXXX/XJJ5/o8ccf1w033KC9e/cW+8c5/z/AGRkZqlevns9jBw4cUExMjM9y+X8Mfu/gwYOF7p0p7F+M77zzjjp16uS9bDnf8ePHi99IP/j9tp7pwIEDstvtqlmzZpnn7dq1q1544QV9+eWXBa44kqTs7GylpaWpRYsWhf6Bzmez2fTII4/oySef1Pfff1/q51+0aJEkecPCZ599pgMHDmjFihU+V2KV5XLgmJgY2Ww2rV69Wg6Ho8DjhY3li4+P17p163zGmjRpUuTy0dHR+vrrrwuMn3kCcGZmphYvXqwJEyZo7Nix3nGXy1XgUvai5L+fZ82aVehrJRUdzitDUXtZqlevrqSkJL322mt69NFHlZqaqoEDB6pGjRqVWyCqDIeZcF6pUaOG+vXrpwceeEBHjx717hHI/+Nz5r8Sr7/+ekm/hYzfW7dunbZs2aIuXbpIkq688ko5HA69++67Pst9+eWXZdqNbrPZCvwh/Pbbb32uJqooTZo0Ud26dTV37lyfwx4nT57Uv/71L+8VTmX1yCOPyOl06sEHHyxwiEKSHn30UR07dkx//etfvWOFBSrpt1CVlZXl3ctSkrS0NL322mu6+uqrvYckf38o6/defvnlAusX9b7o3bu3jDHav3+/2rVrV+DWsmXLImsKDQ0tsPzvD7+cqXPnzjp+/Lg3lOWbO3euz32bzSZjTIHteu2115SXl1eq7brmmmtUo0YNbd68udDtateunXcvX0Uoqq7SeOihh3T48GH169dPv/76q0aMGOHv8hDA2DODc16fPn2834VxwQUXaPfu3Zo5c6YaNGigiy++WJK8f3z+/ve/KykpSSEhIWrSpImaNGmie++9V7NmzZLdblePHj20a9cujR8/XgkJCXrkkUck/XZYZ9SoUZo6dapq1qypvn37at++fZo4caLi4uJ8zkEpTu/evTVp0iRNmDBBHTt21NatW/Xkk0+qYcOGys3NrZgG/Y/dbtczzzyjO+64Q71799Z9990nl8ul6dOn69dff9XTTz9drnkbNWqkt99+W3fccYcuv/xyjRo1Sk2aNNHPP/+sN954Q0uXLtWjjz6q/v37e9e599579euvv+rWW29VixYtFBQUpP/+97969tlnZbfb9dhjj/k8h8fj8X6PjMvl0p49e7R06VK99957atasmc8hqauvvlo1a9bU/fffrwkTJigkJET/+Mc/lJ6eXqD2/PfFtGnT1KNHDwUFBenSSy/VNddco3vvvVdDhgzR+vXr1aFDB0VERCgjI0Off/65WrZsqT/96U/l6teZBg0apGeffVaDBg3S5MmTdfHFF2vJkiX66KOPfJaLiopShw4dNH36dMXExCgxMVErV67U66+/XmAPRYsWLSRJr7zyiiIjIxUWFqaGDRsqOjpas2bNUlJSko4ePap+/fqpdu3a+uWXX5Senq5ffvmlwF7D77//vtD3ZqNGjXTBBReUaVuL+hwWF/byNW7cWN27d9fSpUt17bXXqlWrVmV6blhc1Z17DJQs/4qJdevWFfp4r169SryaacaMGebqq682MTExJjQ01NSvX98MGzbM7Nq1y2e9cePGmfj4eGO3232u9MjLyzPTpk0zjRs3NiEhISYmJsbceeedZu/evT7rezwe89RTT5l69eqZ0NBQc+mll5rFixebVq1a+VyJVNyVQC6Xyzz66KOmbt26JiwszLRp08Z88MEHJikpyWc7869mmj59us/6Rc1dUh9/74MPPjBXXnmlCQsLMxEREaZLly7miy++KNXzFOeHH34wSUlJpl69eiYkJMTUqlXLdO/evdBLZz/66CMzdOhQc8kll5jq1aub4OBgExcXZ2655Razdu1an2WTkpJ8rqRxOp2mfv36pk+fPuaNN94wLperwPxr1qwx7du3N+Hh4eaCCy4wd999t9m4cWOBq4NcLpe5++67zQUXXGBsNluBK23eeOMNc+WVV5qIiAjjdDpNo0aNzKBBg8z69etL3ZfS2Ldvn7n11ltNtWrVTGRkpLn11lvNmjVrCtSbv1zNmjVNZGSk6d69u/n+++8LfCaM+e3Kt4YNG5qgoKAC86xcudL06tXL1KpVy4SEhJi6deuaXr16+bzeJV1p+Oqrr3qXlWQeeOCBAttVWF1FfQ4bNGhgevXqVWyf5syZYySZ+fPnF99QnHNsxhRzGj2As7Jz5041bdpUEyZM0OOPP17V5QDntPyr7nbt2qWQkJCqLgeViMNMgJ+kp6dr3rx5uvrqqxUVFaWtW7fqmWeeUVRUlIYNG1bV5QHnJJfLpY0bN+rrr7/WwoULlZKSQpA5DxFmAD+JiIjQ+vXr9frrr+vXX39V9erV1alTJ02ePLlKrwABzmUZGRnef0Dcd999evDBB6u6JFQBDjMBAABL49JsAABgaYQZAABgaYQZAABgaef8CcAej0cHDhxQZGQkPzgGAIBFGGN0/PhxxcfHl/jFo+d8mDlw4ECBH68DAADWsHfv3gK/jXemcz7M5H8N9t69exUVFVXF1Zw9t9utjz/+WN26deO7FCoJPa9c9Lvy0fPKR89LlpWVpYSEhFL9nMU5H2byDy1FRUWdM2EmPDxcUVFRfAAqCT2vXPS78tHzykfPS680p4hwAjAAALA0wgwAALA0wgwAALA0wgwAALA0wgwAALA0wgwAALA0wgwAALA0wgwAALA0wgwAALA0wgwAALC0Kg0zq1atUp8+fRQfHy+bzaYPPvjA+5jb7dZjjz2mli1bKiIiQvHx8Ro0aJAOHDhQdQUDAICAU6Vh5uTJk2rVqpWef/75Ao9lZ2dr48aNGj9+vDZu3KgFCxboxx9/1I033lgFlQIAgEBVpT802aNHD/Xo0aPQx6pXr660tDSfsVmzZumKK67Qnj17VL9+/cooEQAABDhLnTOTmZkpm82mGjVqVHUpAAAgQFTpnpmyOHXqlMaOHauBAwcqKiqqyOVcLpdcLpf3flZWlqTfzsFxu90VXmdFy9+Gc2FbrIKeVy76XfnoeeWrip7v27dPR44c8fu80dHRqlevnt/nLUtvbMYY4/cKysFms2nhwoW6+eabCzzmdrt12223ac+ePVqxYkWxYSY5OVkTJ04sMD537lyFh4f7s2QAAFBBsrOzNXDgQGVmZhb7d1+yQJhxu926/fbbtWPHDn322WeKjo4udp7C9swkJCTo8OHDJTbDCtxut9LS0tS1a1eFhIRUdTnnBXpeueh35aPnla+ye56enq4OHTqoVvcHFVKrrt/mdR/dr6PLZmnVqlVq1aqV3+aVfvv7HRMTU6owE9CHmfKDzE8//aTly5eXGGQkyeFwyOFwFBgPCQk5pz6k59r2WAE9r1z0u/LR88pXWT232+3KyclRXlS8gmMa+W3evFyjnJwc2e12v29HWear0jBz4sQJbdu2zXt/586d+uabb1SrVi3Fx8erX79+2rhxoxYvXqy8vDwdPHhQklSrVi2FhoZWVdkAACCAVGmYWb9+vTp37uy9P2rUKElSUlKSkpOTtWjRIklS69atfdZbvny5OnXqVFllAgCAAFalYaZTp04q7pSdADmdBwAABDBLfc8MAADAmQgzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0ggzAADA0qo0zKxatUp9+vRRfHy8bDabPvjgA5/HjTFKTk5WfHy8nE6nOnXqpB9++KFqigUAAAGpSsPMyZMn1apVKz3//POFPv7MM88oJSVFzz//vNatW6c6deqoa9euOn78eCVXCgAAAlVwVT55jx491KNHj0IfM8Zo5syZ+stf/qJbbrlFkvTmm28qNjZWc+fO1X333VeZpQIAgABVpWGmODt37tTBgwfVrVs375jD4VDHjh21Zs2aIsOMy+WSy+Xy3s/KypIkud1uud3uii26EuRvw7mwLVZBzysX/a589LzyVXbPPR6PnE6nwoJtCg0yfpvXFmyT0+mUx+Px+7aUZb6ADTMHDx6UJMXGxvqMx8bGavfu3UWuN3XqVE2cOLHA+Mcff6zw8HD/FlmF0tLSqrqE8w49r1z0u/LR88pXmT2fN2/e//5fnh9nbSD1maf9+/dr//79fpxXys7OLvWyARtm8tlsNp/7xpgCY783btw4jRo1yns/KytLCQkJ6tatm6KioiqszsridruVlpamrl27KiQkpKrLOS/Q88pFvysfPa98ld3z9PR0dejQQbEDn1Zo7IV+m/f0zzv089yxWrVqlVq1auW3eaX/P7JSGgEbZurUqSPptz00cXFx3vFDhw4V2Fvzew6HQw6Ho8B4SEjIOfUhPde2xwroeeWi35WPnle+yuq53W5XTk6OTuUambyidwiUlSvXKCcnR3a73e/bUZb5AvZ7Zho2bKg6der47II7ffq0Vq5cqauvvroKKwMAAIGkSvfMnDhxQtu2bfPe37lzp7755hvVqlVL9evX18MPP6wpU6bo4osv1sUXX6wpU6YoPDxcAwcOrMKqAQBAIKnSMLN+/Xp17tzZez//XJekpCTNmTNHY8aMUU5OjoYPH65jx47pyiuv1Mcff6zIyMiqKhkAAASYKg0znTp1kjFFXyJms9mUnJys5OTkyisKAABYSsCeMwMAAFAahBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBphBkAAGBpAR1mcnNz9de//lUNGzaU0+nUhRdeqCeffFIej6eqSwMAAAEiuKoLKM60adM0e/Zsvfnmm2revLnWr1+vIUOGqHr16ho5cmRVlwcAAAJAQIeZtWvX6qabblKvXr0kSYmJiZo3b57Wr19fxZUBAIBAEdBh5tprr9Xs2bP1448/qnHjxkpPT9fnn3+umTNnFrmOy+WSy+Xy3s/KypIkud1uud3uii65wuVvw7mwLVZBzysX/a589LzyVXbPPR6PnE6nwoJtCg0yfpvXFmyT0+mUx+Px+7aUZT6bMcZ/W+Vnxhg9/vjjmjZtmoKCgpSXl6fJkydr3LhxRa6TnJysiRMnFhifO3euwsPDK7JcAADgJ9nZ2Ro4cKAyMzMVFRVV7LIBHWbmz5+vP//5z5o+fbqaN2+ub775Rg8//LBSUlKUlJRU6DqF7ZlJSEjQ4cOHS2yGFbjdbqWlpalr164KCQmp6nLOC/S8ctHvykfPK19l9zw9PV0dOnRQ7MCnFRp7od/mPf3zDv08d6xWrVqlVq1a+W1e6be/3zExMaUKMwF9mOnPf/6zxo4dqwEDBkiSWrZsqd27d2vq1KlFhhmHwyGHw1FgPCQk5Jz6kJ5r22MF9Lxy0e/KR88rX2X13G63KycnR6dyjUyezW/zunKNcnJyZLfb/b4dZZkvoC/Nzs7Olt3uW2JQUBCXZgMAAK+A3jPTp08fTZ48WfXr11fz5s21adMmpaSkaOjQoVVdGgAACBABHWZmzZql8ePHa/jw4Tp06JDi4+N133336Yknnqjq0gAAQIAI6DATGRmpmTNnFnspNgAAOL8F9DkzAAAAJSHMAAAASyPMAAAASyPMAAAASyPMAAAASyPMAAAASyPMAAAASyPMAAAASyPMAAAASyPMAAAASytXmNm5c6e/6wAAACiXcoWZiy66SJ07d9Y777yjU6dO+bsmAACAUitXmElPT9dll12m0aNHq06dOrrvvvv09ddf+7s2AACAEpUrzLRo0UIpKSnav3+/UlNTdfDgQV177bVq3ry5UlJS9Msvv/i7TgAAgEKd1QnAwcHB6tu3r9577z1NmzZN27dv16OPPqp69epp0KBBysjI8FedAAAAhTqrMLN+/XoNHz5ccXFxSklJ0aOPPqrt27frs88+0/79+3XTTTf5q04AAIBCBZdnpZSUFKWmpmrr1q3q2bOn3nrrLfXs2VN2+2/ZqGHDhnr55ZfVtGlTvxYLAABwpnKFmZdeeklDhw7VkCFDVKdOnUKXqV+/vl5//fWzKg4AAKAk5QozP/30U4nLhIaGKikpqTzTAwAAlFq5zplJTU3V+++/X2D8/fff15tvvnnWRQEAAJRWucLM008/rZiYmALjtWvX1pQpU866KAAAgNIqV5jZvXu3GjZsWGC8QYMG2rNnz1kXBQAAUFrlCjO1a9fWt99+W2A8PT1d0dHRZ10UAABAaZUrzAwYMEAPPfSQli9frry8POXl5emzzz7TyJEjNWDAAH/XCAAAUKRyXc301FNPaffu3erSpYuCg3+bwuPxaNCgQZwzAwAAKlW5wkxoaKjeffddTZo0Senp6XI6nWrZsqUaNGjg7/oAAACKVa4wk69x48Zq3Lixv2oBAAAos3KFmby8PM2ZM0effvqpDh06JI/H4/P4Z5995pfiAAAASlKuMDNy5EjNmTNHvXr1UosWLWSz2fxdFwAAQKmUK8zMnz9f7733nnr27OnvegAAAMqkXJdmh4aG6qKLLvJ3LQAAAGVWrjAzevRo/f3vf5cxxt/1AAAAlEm5DjN9/vnnWr58uZYuXarmzZsrJCTE5/EFCxb4pTgAAICSlCvM1KhRQ3379vV3LQAAAGVWrjCTmprq7zoAAADKpVznzEhSbm6uPvnkE7388ss6fvy4JOnAgQM6ceKE34oDAAAoSbn2zOzevVvdu3fXnj175HK51LVrV0VGRuqZZ57RqVOnNHv2bH/XCQAAUKhy7ZkZOXKk2rVrp2PHjsnpdHrH+/btq08//dRvxQEAAJSk3FczffHFFwoNDfUZb9Cggfbv3++XwgAAAEqjXHtmPB6P8vLyCozv27dPkZGRZ10UAABAaZUrzHTt2lUzZ8703rfZbDpx4oQmTJjATxwAAIBKVa7DTM8++6w6d+6sSy65RKdOndLAgQP1008/KSYmRvPmzfN3jQAAAEUqV5iJj4/XN998o3nz5mnjxo3yeDwaNmyY7rjjDp8TggEAACpaucKMJDmdTg0dOlRDhw71Zz0AAABlUq4w89ZbbxX7+KBBg8pVDAAAQFmVK8yMHDnS577b7VZ2drZCQ0MVHh5OmAEAAJWmXFczHTt2zOd24sQJbd26Vddeey0nAAMAgEpV7t9mOtPFF1+sp59+usBem7O1f/9+3XnnnYqOjlZ4eLhat26tDRs2+PU5AACAdZX7BODCBAUF6cCBA36b79ixY7rmmmvUuXNnLV26VLVr19b27dtVo0YNvz0HAACwtnKFmUWLFvncN8YoIyNDzz//vK655hq/FCZJ06ZNU0JCglJTU71jiYmJfpsfAABYX7nCzM033+xz32az6YILLtD111+vGTNm+KMuSb+FphtuuEG33XabVq5cqbp162r48OG65557/PYcAADA2soVZjwej7/rKNSOHTv00ksvadSoUXr88cf19ddf66GHHpLD4SjyiimXyyWXy+W9n5WVJem3K67cbnel1F2R8rfhXNgWq6DnlYt+Vz56Xnb79u3TkSNHyr1+/t/RTZs2yW7//9NXo6OjVa9evbOur7DnczqdCgu2KTTI+G1eW7BNTqdTHo/H7++fssxnM8b4b6v8LDQ0VO3atdOaNWu8Yw899JDWrVuntWvXFrpOcnKyJk6cWGB87ty5Cg8Pr7BaAQCA/2RnZ2vgwIHKzMxUVFRUscuWa8/MqFGjSr1sSkpKeZ5CkhQXF6dLLrnEZ6xZs2b617/+VeQ648aN86kvKytLCQkJ6tatW4nNsAK32620tDR17dpVISEhVV3OeYGeVy76Xfnoedmkp6erQ4cOqtX9QYXUqluuORzBNk3rUV+PLd0jV+5v+xTcR/fr6LJZWrVqlVq1auXPkr01xw58WqGxF/pt3tM/79DPc8dWSM35R1ZKo1xhZtOmTdq4caNyc3PVpEkTSdKPP/6ooKAgtWnTxruczWYrz/Re11xzjbZu3eoz9uOPP6pBgwZFruNwOORwOAqMh4SEnFMf0nNte6yAnlcu+l356Hnp2O125eTkKC8qXsExjco1hwkykvJkohvK5P32tzIv1ygnJ0d2u93vr0N+zadyjff5/MFVgTWXZb5yhZk+ffooMjJSb775pmrWrCnpt8uohwwZouuuu06jR48uz7QFPPLII7r66qs1ZcoU3X777fr666/1yiuv6JVXXvHL/AAAwPrK9aV5M2bM0NSpU71BRpJq1qypp556yq9XM11++eVauHCh5s2bpxYtWmjSpEmaOXOm7rjjDr89BwAAsLZy7ZnJysrSzz//rObNm/uMHzp0SMePH/dLYfl69+6t3r17+3VOAABw7ijXnpm+fftqyJAh+uc//6l9+/Zp3759+uc//6lhw4bplltu8XeNAAAARSrXnpnZs2fr0Ucf1Z133um9Djw4OFjDhg3T9OnT/VogAABAccoVZsLDw/Xiiy9q+vTp2r59u4wxuuiiixQREeHv+gAAAIp1Vr+anZGRoYyMDDVu3FgREREK4O/fAwAA56hyhZkjR46oS5cuaty4sXr27KmMjAxJ0t133+23y7IBAABKo1xh5pFHHlFISIj27Nnj8xMB/fv317Jly/xWHAAAQEnKdc7Mxx9/rI8++qjAj2FdfPHF2r17t18KAwAAKI1y7Zk5efJkoT/aePjw4UJ/SgAAAKCilCvMdOjQQW+99Zb3vs1mk8fj0fTp09W5c2e/FQcAAFCSch1mmj59ujp16qT169fr9OnTGjNmjH744QcdPXpUX3zxhb9rBAAAKFK59sxccskl+vbbb3XFFVeoa9euOnnypG655RZt2rRJjRqV7xdEAQAAyqPMe2bcbre6deuml19+WRMnTqyImgAAAEqtzHtmQkJC9P3338tms1VEPQAAAGVSrsNMgwYN0uuvv+7vWgAAAMqsXCcAnz59Wq+99prS0tLUrl27Ar/JlJKS4pfiAAAASlKmMLNjxw4lJibq+++/V5s2bSRJP/74o88yHH4CAACVqUxh5uKLL1ZGRoaWL18u6befL3juuecUGxtbIcUBAACUpEznzJz5q9hLly7VyZMn/VoQAABAWZTrBOB8Z4YbAACAylamMGOz2QqcE8M5MgAAoCqV6ZwZY4wGDx7s/THJU6dO6f777y9wNdOCBQv8VyEAAEAxyhRmkpKSfO7feeedfi0GAACgrMoUZlJTUyuqDgAAIGnLli2WmDOQlOtL8wAAgH/lnTgm2Wwc9SgHwgwAAAHA4zohGaPo3qMVEp3g17lzdqxX5up3/DpnICHMAAAQQEKiE+Soc5Ff53Qf2evX+QLNWX3PDAAAQFUjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEsjzAAAAEuzVJiZOnWqbDabHn744aouBQAABAjLhJl169bplVde0aWXXlrVpQAAgABiiTBz4sQJ3XHHHXr11VdVs2bNqi4HAAAEkOCqLqA0HnjgAfXq1Ut/+MMf9NRTTxW7rMvlksvl8t7PysqSJLndbrnd7gqtszLkb8O5sC1WQc8rF/2ufPS8bDwej5xOp8KCbQoNMuWaw2E3Pv8rSbkhQWc9b1Eqam5bsE1Op1Mej8fv75+yzGczxvi3Y342f/58TZ48WevWrVNYWJg6deqk1q1ba+bMmYUun5ycrIkTJxYYnzt3rsLDwyu4WgAA4A/Z2dkaOHCgMjMzFRUVVeyyAR1m9u7dq3bt2unjjz9Wq1atJKnEMFPYnpmEhAQdPny4xGZYgdvtVlpamrp27aqQkJCqLue8QM8rF/2ufPS8bNLT09WhQwfFDnxaobEXlmsOh91oUjuPxq+3y+WxSZJOblmto8tmndW8RamouU//vEM/zx2rVatWef9O+0tWVpZiYmJKFWYC+jDThg0bdOjQIbVt29Y7lpeXp1WrVun555+Xy+VSUFCQzzoOh0MOh6PAXCEhIefUh/Rc2x4roOeVi35XPnpeOna7XTk5OTqVa2TybGc1l8tjk+t/c5xy5/lt3jNV1NyuXKOcnBzZ7Xa/v3fKMl9Ah5kuXbrou+++8xkbMmSImjZtqscee6xAkAEAAOefgA4zkZGRatGihc9YRESEoqOjC4wDAIDzkyUuzQYAAChKQO+ZKcyKFSuqugQAABBA2DMDAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsLbiqCwAAoCLs2bNHhw8f9vu8W7Zs8fucODuEGQDAOWfPnj1q0rSZTuVkV3UpqASEGQDAOefw4cM6lZOt6N6jFRKd4Ne5c3asV+bqd/w6J84OYQYAcM4KiU6Qo85Ffp3TfWSvX+fD2eMEYAAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGmEGQAAYGkBHWamTp2qyy+/XJGRkapdu7Zuvvlmbd26tarLAgAAASSgw8zKlSv1wAMP6Msvv1RaWppyc3PVrVs3nTx5sqpLAwAAASK4qgsozrJly3zup6amqnbt2tqwYYM6dOhQRVUBAIBAEtBh5kyZmZmSpFq1ahW5jMvlksvl8t7PysqSJLndbrnd7ootsBLkb8O5sC1WcT71fN++fTpy5Ijf542Ojla9evVKtez51O9AcS723OPxyOl0KizYptAg49e5c0OCznpuh934/K+/5i1KRc1tC7bJ6XTK4/H4/f1Tlvlsxhj/dqyCGGN000036dixY1q9enWRyyUnJ2vixIkFxufOnavw8PCKLBEAAPhJdna2Bg4cqMzMTEVFRRW7rGXCzAMPPKD//Oc/+vzzz4v9F15he2YSEhJ0+PDhEpthBW63W2lpaeratatCQkKqupzzwvnS8/T0dHXo0EG1uj+okFp1/Tav++h+HV02S6tWrVKrVq1KXv486XcgORd7nv9+jh34tEJjL/Tr3Ce3rNbRZbPOam6H3WhSO4/Gr7fL5bH5bd6iVNTcp3/eoZ/nji3157sssrKyFBMTU6owY4nDTA8++KAWLVqkVatWlbir2uFwyOFwFBgPCQk5Zz6k0rm3PVZwrvfcbrcrJydHeVHxCo5p5Ld583KNcnJyZLfby9S/c73fgehc6nn++/lUrpHJs/l17lPuPL/N7fLY5PrfHP6c90wVNbernJ/v0ijLfAEdZowxevDBB7Vw4UKtWLFCDRs2rOqSAABAgAnoMPPAAw9o7ty5+vDDDxUZGamDBw9KkqpXry6n01nF1QEAgEAQ0N8z89JLLykzM1OdOnVSXFyc9/buu+9WdWkAACBABPSeGYucmwwAAKpQQO+ZAQAAKAlhBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWBphBgAAWFpwVRdgdXv27NHhw4f9Pq/L5ZLD4Sgw7vF4JEnp6emy28uXRWNiYlS/fv2zqq8wFdULqeh+VMa85e15RfXZqrZs2VKq5crab/pcOaz2+S7t+w3nBsLMWdizZ4+aNG2mUznZ/p/cZpeMp8Cw0+nUvHnz1KFDB+Xk5JRr6jBnuLb+d4tf/wBUaC+kIvtRGfOWt+cV0WcryjtxTLLZdOedd5Zq+bL2mz5XPMt+vnHeIMychcOHD+tUTraie49WSHSC3+bN2bFemavfKXTesGCbJCl24NM6lWvKPLf7yF4dWTxDhw8f9ut//CuqF1Lx/aiMecvT84rqsxV5XCckY0r9+pWl3/S5clj5843zA2HGD0KiE+Soc5Hf5nMf2VvkvKFBRlKeQmMvlMmz+e05/cXfvZCK70dlzBvoPbeK0r5+9DtwWfHzjfMDJwADAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLs0SYefHFF9WwYUOFhYWpbdu2Wr16dVWXBAAAAkTAh5l3331XDz/8sP7yl79o06ZNuu6669SjRw/t2bOnqksDAAABIODDTEpKioYNG6a7775bzZo108yZM5WQkKCXXnqpqksDAAABIKDDzOnTp7VhwwZ169bNZ7xbt25as2ZNFVUFAAACSXBVF1Ccw4cPKy8vT7GxsT7jsbGxOnjwYKHruFwuuVwu7/3MzExJ0tGjR+V2u/1aX1ZWlsLCwmQ7slPG4yp5hVKyH88ocl5PsJSdnSBPxl6Z3LLPbTt2QGFhYdqwYYOysrL8VLH0008/VUgvpOL7URnzlqfnFdVnSbLb7fJ4PH6dU6q417Csr19Z+l2RfZYqrtcVNW955/Z4PMrOztbq1atltxf8N+65/PmuqrkLe58Hes2Fyf8MZmVl6ciRI36bV5KOHz8uSTLGlLywCWD79+83ksyaNWt8xp966inTpEmTQteZMGGCkcSNGzdu3LhxOwdue/fuLTEvBPSemZiYGAUFBRXYC3Po0KECe2vyjRs3TqNGjfLe93g8Onr0qKKjo2Wz2Sq03sqQlZWlhIQE7d27V1FRUVVdznmBnlcu+l356Hnlo+clM8bo+PHjio+PL3HZgA4zoaGhatu2rdLS0tS3b1/veFpamm666aZC13E4HHI4HD5jNWrUqMgyq0RUVBQfgEpGzysX/a589Lzy0fPiVa9evVTLBXSYkaRRo0bprrvuUrt27dS+fXu98sor2rNnj+6///6qLg0AAASAgA8z/fv315EjR/Tkk08qIyNDLVq00JIlS9SgQYOqLg0AAASAgA8zkjR8+HANHz68qssICA6HQxMmTChwKA0Vh55XLvpd+eh55aPn/mUzpjTXPAEAAASmgP7SPAAAgJIQZgAAgKURZgAAgKURZgAAgKURZgLArl27NGzYMDVs2FBOp1ONGjXShAkTdPr0aZ/l9uzZoz59+igiIkIxMTF66KGHCizz3XffqWPHjnI6napbt66efPLJAr9rsXLlSrVt21ZhYWG68MILNXv27ArfxkAzefJkXX311QoPDy/ySxVtNluB25m9ot+lV5qe8x6vWImJiQXe02PHjvVZxl+vAYr24osvqmHDhgoLC1Pbtm21evXqqi7J+s7u15PgD0uXLjWDBw82H330kdm+fbv58MMPTe3atc3o0aO9y+Tm5poWLVqYzp07m40bN5q0tDQTHx9vRowY4V0mMzPTxMbGmgEDBpjvvvvO/Otf/zKRkZHmb3/7m3eZHTt2mPDwcDNy5EizefNm8+qrr5qQkBDzz3/+s1K3uao98cQTJiUlxYwaNcpUr1690GUkmdTUVJORkeG9ZWdnex+n32VTUs95j1e8Bg0amCeffNLnPX38+HHv4/56DVC0+fPnm5CQEPPqq6+azZs3m5EjR5qIiAize/fuqi7N0ggzAeqZZ54xDRs29N5fsmSJsdvtZv/+/d6xefPmGYfDYTIzM40xxrz44oumevXq5tSpU95lpk6dauLj443H4zHGGDNmzBjTtGlTn+e67777zFVXXVWRmxOwUlNTiw0zCxcuLHJd+l0+RfWc93jFa9CggXn22WeLfNxfrwGKdsUVV5j777/fZ6xp06Zm7NixVVTRuYHDTAEqMzNTtWrV8t5fu3atWrRo4fODWzfccINcLpc2bNjgXaZjx44+X8J0ww036MCBA9q1a5d3mW7duvk81w033KD169fL7XZX4BZZ04gRIxQTE6PLL79cs2fPlsfj8T5Gv/2L93jlmDZtmqKjo9W6dWtNnjzZ5xCSv14DFO706dPasGFDgfdnt27dtGbNmiqq6txAmAlA27dv16xZs3x+f+rgwYMFfim8Zs2aCg0N9f6qeGHL5N8vaZnc3FwdPnzY79tiZZMmTdL777+vTz75RAMGDNDo0aM1ZcoU7+P02794j1e8kSNHav78+Vq+fLlGjBihmTNn+ny7ur9eAxTu8OHDysvLK7R/9O7sEGYqUHJycqEnkf7+tn79ep91Dhw4oO7du+u2227T3Xff7fOYzWYr8BzGGJ/xM5cx/zspr6zLWFF5+l2cv/71r2rfvr1at26t0aNH68knn9T06dN9ljmf+y35v+e8x8uuLK/BI488oo4dO+rSSy/V3XffrdmzZ+v111/XkSNHvPP56zVA0QrrH707O5b4bSarGjFihAYMGFDsMomJid7/f+DAAXXu3Nn76+C/V6dOHX311Vc+Y8eOHZPb7fam/Dp16hRI94cOHZKkEpcJDg5WdHR06TcuAJW132V11VVXKSsrSz///LNiY2PP+35L/u057/HyOZvX4KqrrpIkbdu2TdHR0X57DVC4mJgYBQUFFdo/end2CDMVKCYmRjExMaVadv/+/ercubPatm2r1NRU2e2+O83at2+vyZMnKyMjQ3FxcZKkjz/+WA6HQ23btvUu8/jjj+v06dMKDQ31LhMfH+/9j1n79u3173//22fujz/+WO3atVNISMjZbG6VK0u/y2PTpk0KCwvzXlZ8vvdb8m/PeY+Xz9m8Bps2bZIkb7/99RqgcKGhoWrbtq3S0tLUt29f73haWppuuummKqzsHFBFJx7jd/bv328uuugic/3115t9+/b5XDaZL/+SyS5dupiNGzeaTz75xNSrV8/nkslff/3VxMbGmj/+8Y/mu+++MwsWLDBRUVGFXrb6yCOPmM2bN5vXX3/9vLxsdffu3WbTpk1m4sSJplq1ambTpk1m06ZN3stUFy1aZF555RXz3XffmW3btplXX33VREVFmYceesg7B/0um5J6znu8Yq1Zs8akpKSYTZs2mR07dph3333XxMfHmxtvvNG7jL9eAxQt/9Ls119/3WzevNk8/PDDJiIiwuzatauqS7M0wkwASE1NNZIKvf3e7t27Ta9evYzT6TS1atUyI0aM8Lk80hhjvv32W3PdddcZh8Nh6tSpY5KTkwtcLrlixQpz2WWXmdDQUJOYmGheeumlCt/GQJOUlFRov5cvX26M+e27f1q3bm2qVatmwsPDTYsWLczMmTON2+32mYd+l15JPTeG93hF2rBhg7nyyitN9erVTVhYmGnSpImZMGGCOXnypM9y/noNULQXXnjBNGjQwISGhpo2bdqYlStXVnVJlmczhq9tBAAA1sXVTAAAwNIIMwAAwNIIMwAAwNIIMwAAwNIIMwAAwNIIMwAAwNIIMwAAwNIIMwAC0uDBg3XzzTd773fq1EkPP/yw9352drZuvfVWRUVFyWaz6ddffy10DMC5jzADnCcGDx7s/RXlkJAQxcbGqmvXrnrjjTfk8XiqurwSLViwQJMmTfLef/PNN7V69WqtWbNGGRkZql69eqFjAM59hBngPNK9e3dlZGRo165dWrp0qTp37qyRI0eqd+/eys3NreryilWrVi1FRkZ672/fvl3NmjVTixYtVKdOHdlstkLHyiovL88S4Q7A/yPMAOcRh8OhOnXqqG7dumrTpo0ef/xxffjhh1q6dKnmzJnjXS4lJUUtW7ZURESEEhISNHz4cJ04cUKSdPLkSUVFRemf//ynz9z//ve/FRERoePHj+v06dMaMWKE4uLiFBYWpsTERE2dOrXIuvLy8jRq1CjVqFFD0dHRGjNmjM78pZXfH2bq1KmTZsyYoVWrVslms6lTp06FjknS6dOnNWbMGNWtW1cRERG68sortWLFCu+8c+bMUY0aNbR48WJdcsklcjgc2r17d6nX++ijj9SsWTNVq1bNGxZ/74033lDz5s3lcDgUFxenESNGeB/LzMzUvffeq9q1aysqKkrXX3+90tPTS3oZAZyBMAOc566//nq1atVKCxYs8I7Z7XY999xz+v777/Xmm2/qs88+05gxYyRJERERGjBggFJTU33mSU1NVb9+/RQZGannnntOixYt0nvvvaetW7fqnXfeUWJiYpE1zJgxQ2+88YZef/11ff755zp69KgWLlxY5PILFizQPffco/bt2ysjI0MLFiwodEyShgwZoi+++ELz58/Xt99+q9tuu03du3fXTz/95J0vOztbU6dO1WuvvaYffvhBtWvXLvV6f/vb3/T2229r1apV2rNnjx599FHv4y+99JIeeOAB3Xvvvfruu++0aNEiXXTRRZIkY4x69eqlgwcPasmSJdqwYYPatGmjLl266OjRo6V45QB4Ve3vXAKoLElJSeamm24q9LH+/fubZs2aFbnue++9Z6Kjo733v/rqKxMUFGT2799vjDHml19+MSEhIWbFihXGGGMefPBBc/3115f6l5Tj4uLM008/7b3vdrtNvXr1fOrt2LGjGTlypPf+yJEjTceOHX3mOXNs27ZtxmazeevM16VLFzNu3DhjzP//av0333xTrvW2bdvmffyFF14wsbGx3vvx8fHmL3/5S6Hb/Omnn5qoqKgCv0jdqFEj8/LLLxe6DoDCBVd1mAJQ9YwxPueXLF++XFOmTNHmzZuVlZWl3NxcnTp1SidPnlRERISuuOIKNW/eXG+99ZbGjh2rt99+W/Xr11eHDh0k/XaycdeuXdWkSRN1795dvXv3Vrdu3Qp97szMTGVkZKh9+/beseDgYLVr167Aoaay2rhxo4wxaty4sc+4y+VSdHS0935oaKguvfTSMq8XHh6uRo0aee/HxcXp0KFDkqRDhw7pwIED6tKlS6G1bdiwQSdOnPCZT5JycnK0ffv2Mm4pcH4jzADQli1b1LBhQ0nS7t271bNnT91///2aNGmSatWqpc8//1zDhg2T2+32rnP33Xfr+eef19ixY5WamqohQ4Z4A1GbNm20c+dOLV26VJ988oluv/12/eEPfyhwnk1F83g8CgoK0oYNGxQUFOTzWLVq1bz/3+l0+oS50q4XEhLi85jNZvMGMKfTWWJtcXFxPufh5KtRo0ax6wLwRZgBznOfffaZvvvuOz3yyCOSpPXr1ys3N1czZsyQ3f7baXXvvfdegfXuvPNOjRkzRs8995x++OEHJSUl+TweFRWl/v37q3///urXr5+6d++uo0ePqlatWj7LVa9eXXFxcfryyy+9e3Zyc3O955Ccjcsuu0x5eXk6dOiQrrvuugpf7/ciIyOVmJioTz/9VJ07dy7weJs2bXTw4EEFBwcXez4RgJIRZoDziMvl0sGDB5WXl6eff/5Zy5Yt09SpU9W7d28NGjRIktSoUSPl5uZq1qxZ6tOnj7744gvNnj27wFw1a9bULbfcoj//+c/q1q2b6tWr533s2WefVVxcnFq3bi273a73339fderUKXKPw8iRI/X000/r4osvVrNmzZSSkuKXL7xr3Lix7rjjDg0aNEgzZszQZZddpsOHD+uzzz5Ty5Yt1bNnT7+ud6bk5GTdf//9ql27tnr06KHjx4/riy++0IMPPqg//OEPat++vW6++WZNmzZNTZo00YEDB7RkyRLdfPPNateu3VlvP3C+4Gom4DyybNkyxcXFKTExUd27d9fy5cv13HPP6cMPP/QeTmndurVSUlI0bdo0tWjRQv/4xz+KvKx62LBhOn36tIYOHeozXq1aNU2bNk3t2rXT5Zdfrl27dmnJkiXePT1nGj16tAYNGqTBgwerffv2ioyMVN++ff2yzampqRo0aJBGjx6tJk2a6MYbb9RXX32lhISEClnv95KSkjRz5ky9+OKLat68uXr37u29Gspms2nJkiXq0KGDhg4dqsaNG2vAgAHatWuXYmNjz2qbgfONzZztGXYAzlv/+Mc/NHLkSB04cEChoaFVXQ6A8xSHmQCUWXZ2tnbu3KmpU6fqvvvuI8gAqFIcZgJQZs8884xat26t2NhYjRs3rqrLAXCe4zATAACwNPbMAAAASyPMAAAASyPMAAAASyPMAAAASyPMAAAASyPMAAAASyPMAAAASyPMAAAASyPMAAAAS/s/NdKoGYf3B0sAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['OSDate - dateEntry'], bins=20, edgecolor='k')  # You can adjust the number of bins as needed\n",
    "plt.xlabel('Days difference')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of OSDate - dateEntry')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     23.000000\n",
       "mean     128.391304\n",
       "std      101.220165\n",
       "min        4.000000\n",
       "25%       28.500000\n",
       "50%      179.000000\n",
       "75%      223.000000\n",
       "max      275.000000\n",
       "Name: OSDate - dateEntry, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['OSDate - dateEntry'][df['OSDate - dateEntry'] > 0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAHFCAYAAADcytJ5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+y0lEQVR4nO3dd3hUZd7/8c9MMqkklARCQo0oiJQoYIFVpAgPVUFREJVQLEhZFFhsjwsICsgDolhwLcGGgLugLELYUAIooICIiCyrSCfUCAEShiRz//7wl1mG9GHiHMj7dV1z6dznnHu+554zJx9OmbEZY4wAAAAsyO7vAgAAAApDUAEAAJZFUAEAAJZFUAEAAJZFUAEAAJZFUAEAAJZFUAEAAJZFUAEAAJZFUAEAAJZFUIHfzJ49WzabTZs2bSpwerdu3VS3bl2Ptrp166p///6lep1169Zp3LhxOnnypHeFlkPz5s1To0aNFBoaKpvNpu+//77I+Xfs2KH+/furdu3aCgoKUnR0tLp06aKlS5cWOv9DDz2kq666SiEhIYqOjlazZs00bNgwZWRkuOfr37+/bDab+xEeHq66devqzjvvVFJSkpxOp9fruGTJEo0bN87r5cvSnj17ZLPZNHv27FIv+9NPP2ncuHHas2eP16+f99ks7JGamlrqPvkcwlsEFVxWFi5cqOeff75Uy6xbt07jx49nB1lCx44d00MPPaR69eopOTlZ69evV/369Qudf8GCBbrhhhv07bff6vnnn9fy5cv11ltvSZK6dOmiMWPGeMy/ZcsWNW/eXD/99JP++te/Kjk5WbNmzVLXrl21bNkypaene8wfGhqq9evXa/369Vq8eLFeeOEFhYeH65FHHlHz5s114MABr9ZzyZIlGj9+vFfLWtlPP/2k8ePHX1JQyZOUlOQe+wsfzZo1K3VffA7hrUB/FwCUxg033ODvEkotOztbNptNgYGXx8ftP//5j7Kzs/Xggw/q9ttvL3LeXbt26aGHHlKTJk2Umpqq8PBw97R7771Xjz/+uKZOnapmzZqpT58+kqQZM2bIbrcrNTVVERER7vl79eqlCRMm6OKfH7Pb7brllls82vr166cBAwaoW7du6tWrlzZs2HCpq40CNG7cWC1atPDLa2dlZSk0NNQvrw1r4YgKLisXn/pxuVyaOHGiGjRooNDQUFWqVElNmzbVq6++KkkaN26c/vKXv0iS4uPj8x26drlcevnll3XttdcqODhY1apVU79+/fL9K90Yo5deekl16tRRSEiIWrRooZSUFLVp00Zt2rRxz5eamiqbzaaPPvpIo0aNUo0aNRQcHKxffvlFx44d05AhQ3TdddepQoUKqlatmtq1a6e1a9d6vFbeYf+pU6dqypQpqlu3rkJDQ9WmTRt3iHj66acVFxenihUrqmfPnjp69GiJxm/RokVq2bKlwsLCFBERoQ4dOmj9+vXu6f3799ett94qSerdu7dsNpvH+l3slVdeUWZmpmbOnOkRUvJMmzZNlSpV0osvvuhuO3HihCIjI1WhQoUC+7TZbCVal44dO+qRRx7RN998ozVr1rjb582bp44dOyo2NlahoaFq2LChnn76aZ09e9ZjPd944w336+U98o5CGGP05ptv6vrrr1doaKgqV66sXr166ddffy1RbaVx6NAh3XfffYqIiFDFihXVu3dvHT58ON98mzZtUp8+fdzbQ926dXX//fdr79697nlmz56te++9V5LUtm1b93pdeApp+fLlat++vSIjIxUWFqY//elPWrFihdf122w2DRs2TB999JEaNmyosLAwJSQkaPHixe55ivsc1q1bV926dXMfnQsJCdH48ePVvn17XXvttfnCqzFGV199tbp27ep13biMGMBPkpKSjCSzYcMGk52dne/RpUsXU6dOHY9l6tSpYxITE93PJ02aZAICAszYsWPNihUrTHJyspkxY4YZN26cMcaY/fv3m+HDhxtJZsGCBWb9+vVm/fr15tSpU8YYYx599FEjyQwbNswkJyebWbNmmapVq5patWqZY8eOuV/nmWeeMZLMo48+apKTk80777xjateubWJjY83tt9/unm/VqlVGkqlRo4bp1auXWbRokVm8eLE5ceKE+fe//20ef/xxM3fuXJOammoWL15sBg0aZOx2u1m1apW7j927dxtJpk6dOqZ79+5m8eLF5uOPPzYxMTGmfv365qGHHjIDBw40S5cuNbNmzTIVKlQw3bt3L3a8P/nkEyPJdOzY0Xz++edm3rx5pnnz5iYoKMisXbvWGGPML7/8Yt544w0jybz00ktm/fr1Zvv27YX2Wb9+fRMTE1Pk6953331GkklLSzPGGDNx4kQjydx///0mNTXVZGZmFrpsYmKiCQ8PL3R6cnKykWQmTJjgbpswYYJ55ZVXzJdffmlSU1PNrFmzTHx8vGnbtq17nl9++cX06tXLSHJvE+vXrzfnzp0zxhjzyCOPGIfDYUaNGmWSk5PNnDlzzLXXXmtiYmLM4cOHi1zf0sjMzDQNGzY0FStWNDNnzjTLli0zf/7zn03t2rWNJJOUlOSe97PPPjN//etfzcKFC83q1avN3Llzze23326qVq3q3laPHj1qXnrpJSPJvPHGG+71Onr0qDHGmI8++sjYbDbTo0cPs2DBAvPPf/7TdOvWzQQEBJjly5e7X6uoz2ZOTo7HOkgydevWNTfddJOZP3++WbJkiWnTpo0JDAw0u3btMsYU/zmsU6eOiY2NNVdddZV5//33zapVq8y3335rvvjiCyPJpKSkeLzml19+aSSZL7/80mfvBayLoAK/ydsZFvUoLqh069bNXH/99UW+ztSpU40ks3v3bo/2HTt2GElmyJAhHu3ffPONkWSeffZZY4wx6enpJjg42PTu3dtjvvXr1xtJBQaV1q1bF7v+OTk5Jjs727Rv39707NnT3Z4XVBISEkxubq67fcaMGUaSufPOOz36eeKJJ4wk906/ILm5uSYuLs40adLEo8/Tp0+batWqmVatWuVbh88++6zYdQgJCTG33HJLkfM89dRTRpL55ptvjDHGnDt3zvTo0cP9HgcEBJgbbrjBPPfcc+4/qHmKCyp57+Hjjz9e4HSXy2Wys7PN6tWrjSSzdetW97ShQ4eagv6tlve+Tps2zaN9//79JjQ01IwZM6bI9S2Nt956y0gyX3zxhUf7I488ki+oXCwnJ8ecOXPGhIeHm1dffdXd/tlnnxlJHuHXGGPOnj1rqlSpki/U5ubmmoSEBHPTTTe524r6bAYEBHgsL8nExMSYjIwMd9vhw4eN3W43kyZNcrcV9jk05vfPdUBAgNm5c2e+2q666ipz1113ebR37tzZ1KtXz7hcrkLHB1cOTv3A7z788ENt3Lgx3yPvFERRbrrpJm3dulVDhgzRsmXLPO4YKc6qVaskKd9dRDfddJMaNmzoPhy+YcMGOZ1O3XfffR7z3XLLLfnuSspzzz33FNg+a9YsNWvWTCEhIQoMDJTD4dCKFSu0Y8eOfPN26dJFdvt/P6INGzaUpHyHu/Pa9+3bV8iaSjt37tShQ4f00EMPefRZoUIF3XPPPdqwYYMyMzMLXf5SmP9/2D7vlE5wcLAWLlyon376Sa+88or69OmjY8eO6cUXX1TDhg21c+fOUvd9oV9//VV9+/ZV9erVFRAQIIfD4b7WpqBxvtjixYtls9n04IMPKicnx/2oXr26EhISir3j5cJlcnJyCqwxz6pVqxQREaE777zTo71v37755j1z5oyeeuopXX311QoMDFRgYKAqVKigs2fPlmi91q1bp/T0dCUmJnrU53K51KlTJ23cuNHj9JhU8Gfzm2++ydd327ZtPa43iomJUbVq1TxOSxWnadOm+S7attvtGjZsmBYvXuzevnft2qXk5GQNGTKkxKcJcXm7PK7uwxWtYcOGBV6wV7FiRe3fv7/IZZ955hmFh4fr448/1qxZsxQQEKDWrVtrypQpxV4EeOLECUlSbGxsvmlxcXHunWzefDExMfnmK6itsD6nT5+uUaNGafDgwZowYYKio6MVEBCg559/vsA/NFWqVPF4HhQUVGT7uXPnCqzlwnUobF1dLpd+++03hYWFFdpHQWrXrq3du3cXOU/edR+1atXyaG/YsKE7ZBljNGPGDI0cOVLPP/+85s+fX6LXz3uP4uLiJP3+x/y2225TSEiIJk6cqPr16yssLEz79+/X3XffraysrGL7PHLkiIwxhb63V111VaHL7tmzR/Hx8R5tq1atKvQ6nxMnThT4OtWrV8/X1rdvX61YsULPP/+8brzxRkVGRspms6lLly4lXi/p94uWC5Oenu5xrVFhn82LRUVF5WsLDg4uUV15Cto2JWngwIH661//qlmzZumll17SG2+8odDQUA0cOLDEfePyRlDBZS0wMFAjR47UyJEjdfLkSS1fvlzPPvus/ud//kf79+8v8g9v3s41LS1NNWvW9Jh26NAhRUdHe8yXt6O/0OHDhws8qlLQv/Q+/vhjtWnTxn3rbp7Tp08XvZI+cOG6XuzQoUOy2+2qXLlyqfvt0KGD3njjDW3YsCHfnTmSlJmZqZSUFDVu3LjAP755bDabnnzySb3wwgv68ccfS/z6ixYtkiR3EFi5cqUOHTqk1NRUjzuWSnNLbHR0tGw2m9auXavg4OB80wtqyxMXF6eNGzd6tDVo0KDQ+aOiovTtt9/ma7/4YtpTp05p8eLFGjt2rJ5++ml3u9PpzHc7d2HytueZM2cW+F5JhQfvP0JhR0cqVqyoxMREvfvuuxo9erSSkpLUt29fVapU6Y8tEH7DqR9cMSpVqqRevXpp6NChSk9Pd/9LPu8Py8X/umvXrp2k3wPEhTZu3KgdO3aoffv2kqSbb75ZwcHBmjdvnsd8GzZsKNWhbZvNlu+P3A8//OBx101ZadCggWrUqKE5c+Z4nIo4e/as/vGPf7jvBCqtJ598UqGhoRo+fHi+0waSNHr0aP3222/63//9X3dbQWFJ+j0wZWRkuI+OFCclJUXvvvuuWrVq5T5NeOHppQu9/fbb+ZYvbLvo1q2bjDE6ePCgWrRoke/RpEmTQmsKCgrKN/+Fp0Qu1rZtW50+fdoduPLMmTPH47nNZpMxJt96vfvuu8rNzS3Rev3pT39SpUqV9NNPPxW4Xi1atHAfnSsLhdVVEn/+8591/Phx9erVSydPntSwYcN8XR4sjCMquKx1797d/V0PVatW1d69ezVjxgzVqVNH11xzjSS5/7C8+uqrSkxMlMPhUIMGDdSgQQM9+uijmjlzpux2uzp37qw9e/bo+eefV61atfTkk09K+v1Uy8iRIzVp0iRVrlxZPXv21IEDBzR+/HjFxsZ6XPNRlG7dumnChAkaO3asbr/9du3cuVMvvPCC4uPjlZOTUzYD9P/Z7Xa9/PLLeuCBB9StWzc99thjcjqdmjp1qk6ePKnJkyd71W+9evX00Ucf6YEHHtCNN96okSNHqkGDBjpy5Ijef/99LV26VKNHj1bv3r3dyzz66KM6efKk7rnnHjVu3FgBAQH697//rVdeeUV2u11PPfWUx2u4XC7396Q4nU7t27dPS5cu1fz589WwYUOP00StWrVS5cqVNXjwYI0dO1YOh0OffPKJtm7dmq/2vO1iypQp6ty5swICAtS0aVP96U9/0qOPPqoBAwZo06ZNat26tcLDw5WWlqavvvpKTZo00eOPP+7VeF2sX79+euWVV9SvXz+9+OKLuuaaa7RkyRItW7bMY77IyEi1bt1aU6dOVXR0tOrWravVq1frvffey3dkoXHjxpKkv/3tb4qIiFBISIji4+MVFRWlmTNnKjExUenp6erVq5eqVaumY8eOaevWrTp27Fi+o30//vhjgdtmvXr1VLVq1VKta2Gfw6KCXJ769eurU6dOWrp0qW699VYlJCSU6rVxmfPfdbwo7/LuLNi4cWOB07t27VrsXT/Tpk0zrVq1MtHR0SYoKMjUrl3bDBo0yOzZs8djuWeeecbExcUZu93ucUdEbm6umTJliqlfv75xOBwmOjraPPjgg2b//v0ey7tcLjNx4kRTs2ZNExQUZJo2bWoWL15sEhISPO7YKeqOGafTaUaPHm1q1KhhQkJCTLNmzcznn39uEhMTPdYz766fqVOneixfWN/FjeOFPv/8c3PzzTebkJAQEx4ebtq3b2++/vrrEr1OUbZv324SExNNzZo1jcPhMFWqVDGdOnUq8PbRZcuWmYEDB5rrrrvOVKxY0QQGBprY2Fhz9913m/Xr13vMm5iY6HHHSWhoqKldu7bp3r27ef/9943T6czX/7p160zLli1NWFiYqVq1qnn44YfNd999l+8uGqfTaR5++GFTtWpVY7PZ8t2R8v7775ubb77ZhIeHm9DQUFOvXj3Tr18/s2nTphKPS0kcOHDA3HPPPaZChQomIiLC3HPPPWbdunX56s2br3LlyiYiIsJ06tTJ/Pjjj/k+E8b8fodYfHy8CQgIyNfP6tWrTdeuXU2VKlWMw+EwNWrUMF27dvV4v4u7I++dd95xzyvJDB06NN96FVRXYZ/DOnXqmK5duxY5TrNnzzaSzNy5c4seUFxxbMYUcUk6gELt3r1b1157rcaOHatnn33W3+UAV7S8u9P27Nkjh8Ph73LwB+LUD1ACW7du1aeffqpWrVopMjJSO3fu1Msvv6zIyEgNGjTI3+UBVySn06nvvvtO3377rRYuXKjp06cTUsohggpQAuHh4dq0aZPee+89nTx5UhUrVlSbNm304osv+vVOCeBKlpaW5v7HwWOPPabhw4f7uyT4Aad+AACAZXF7MgAAsCyCCgAAsCyCCgAAsKzL+mJal8ulQ4cOKSIigh+nAgDgMmGM0enTpxUXF1fsl2Ze1kHl0KFD+X7oDAAAXB7279+f77fWLnZZB5W8r17ev3+/IiMjS718dna2/vWvf6ljx47cm3+JGEvfYSx9i/H0HcbSd8r7WGZkZKhWrVol+gmFyzqo5J3uiYyM9DqohIWFKTIyslxuKL7EWPoOY+lbjKfvMJa+w1j+riSXbXAxLQAAsCyCCgAAsCyCCgAAsCyCCgAAsCyCCgAAsCyCCgAAsCyCCgAAsCyCCgAAsCyCCgAAsCyCCgAAsCy/BpVx48bJZrN5PKpXr+7PkgAAgIX4/bd+GjVqpOXLl7ufBwQE+LEaAABgJX4PKoGBgRxFAQAABfL7NSo///yz4uLiFB8frz59+ujXX3/1d0kAAMAi/HpE5eabb9aHH36o+vXr68iRI5o4caJatWql7du3KyoqKt/8TqdTTqfT/TwjI0PS7z+XnZ2dXerXz1umsGUPHDigEydOlLrf4kRFRalmzZo+79efihtLlBxj6VuMp+8wlr5T3seyNOttM8aYMqylVM6ePat69eppzJgxGjlyZL7p48aN0/jx4/O1z5kzR2FhYX9EiQAA4BJlZmaqb9++OnXqlCIjI4uc11JBRZI6dOigq6++Wm+99Va+aQUdUalVq5aOHz9e7IoWJDs7WykpKerQoYMcDofHtK1bt6p169aq0mm4HFVqlH5FCnvN9INKT56pNWvWKCEhwWf9+ltRY4nSYSx9i/H0HcbSd8r7WGZkZCg6OrpEQcXvF9NeyOl0aseOHbrtttsKnB4cHKzg4OB87Q6H45Le6IKWt9vtysrKUm5knAKj63nd98Vyc4yysrJkt9uvyI3zUt8L/Bdj6VuMp+8wlr5TXseyNOvs14tpR48erdWrV2v37t365ptv1KtXL2VkZCgxMdGfZQEAAIvw6xGVAwcO6P7779fx48dVtWpV3XLLLdqwYYPq1Knjz7IAAIBF+DWozJ07158vDwAALM7v36MCAABQGIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLIIKAACwLMsElUmTJslms+mJJ57wdykAAMAiLBFUNm7cqL/97W9q2rSpv0sBAAAW4vegcubMGT3wwAN65513VLlyZX+XAwAALCTQ3wUMHTpUXbt21R133KGJEycWOa/T6ZTT6XQ/z8jIkCRlZ2crOzu71K+dt0xBy7pcLoWGhiok0KagAFPqvgtjC7QpNDRULpfLq5qtqqixROkwlr7FeBbvwIEDOnHiRLHzuVwuSdKWLVtkt5fs37lRUVGqWbPmJdV3JSrv22Vp1ttmjPHdX+FSmjt3rl588UVt3LhRISEhatOmja6//nrNmDGjwPnHjRun8ePH52ufM2eOwsLCyrhaAADgC5mZmerbt69OnTqlyMjIIuf1W1DZv3+/WrRooX/9619KSEiQpGKDSkFHVGrVqqXjx48Xu6IFyc7OVkpKijp06CCHw+ExbevWrWrdurVi+k5WUMxVpe67MOeP/Kojc57WmjVr3Ot9JShqLFE6jKVvMZ5Fy9vXVek0XI4qNYqcNzjQpimda+uppfvkzCn+T0d2+kGlJ8+84vZ3vlDet8uMjAxFR0eXKKj47dTP5s2bdfToUTVv3tzdlpubqzVr1uj111+X0+lUQECAxzLBwcEKDg7O15fD4bikN7qg5e12u7KysnQux8jk2rzu+2LOHKOsrCzZ7fYrcuO81PcC/8VY+hbjWbC8fV1uZJwCo+sVOa8JMJJyZaLiS7RfzL3C93e+UF63y9Kss9+CSvv27bVt2zaPtgEDBujaa6/VU089lS+kAACA8sdvQSUiIkKNGzf2aAsPD1dUVFS+dgAAUD75/fZkAACAwvj99uQLpaam+rsEAABgIRxRAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAlkVQAQAAluVVUNm9e7ev6wAAAMjHq6By9dVXq23btvr444917tw5X9cEAAAgycugsnXrVt1www0aNWqUqlevrscee0zffvttqft566231LRpU0VGRioyMlItW7bU0qVLvSkJAABcgbwKKo0bN9b06dN18OBBJSUl6fDhw7r11lvVqFEjTZ8+XceOHStRPzVr1tTkyZO1adMmbdq0Se3atdNdd92l7du3e1MWAAC4wlzSxbSBgYHq2bOn5s+frylTpmjXrl0aPXq0atasqX79+iktLa3I5bt3764uXbqofv36ql+/vl588UVVqFBBGzZsuJSyAADAFeKSgsqmTZs0ZMgQxcbGavr06Ro9erR27dqllStX6uDBg7rrrrtK3Fdubq7mzp2rs2fPqmXLlpdSFgAAuEIEerPQ9OnTlZSUpJ07d6pLly768MMP1aVLF9ntv+ee+Ph4vf3227r22muL7Wvbtm1q2bKlzp07pwoVKmjhwoW67rrrCpzX6XTK6XS6n2dkZEiSsrOzlZ2dXer1yFumoGVdLpdCQ0MVEmhTUIApdd+FsQXaFBoaKpfL5VXNVlXUWKJ0GEvfYjyLVpp9XbDdePy3OGW5vztw4IBOnDjh0z7zREVFqWbNmmXSd57yvl2WZr1txphS/xW+5pprNHDgQA0YMEDVq1cvcJ7z58/r008/VWJiYpF9nT9/Xvv27dPJkyf1j3/8Q++++65Wr15dYFgZN26cxo8fn699zpw5CgsLK+1qAAAAP8jMzFTfvn116tQpRUZGFjmvV0GlLN1xxx2qV6+e3n777XzTCjqiUqtWLR0/frzYFS1Idna2UlJS1KFDBzkcDo9pW7duVevWrRXTd7KCYq4q/YoU4vyRX3VkztNas2aNEhISfNavvxU1ligdxtK3GM+ilWZfF2w3mtDCpec32eV02Yrtu6z2d3k1V+k0XI4qNXzWryRlpx9UevLMMt9Hl/ftMiMjQ9HR0SUKKl6d+klKSlKFChV07733erR/9tlnyszMLPYoSlGMMR5h5ELBwcEKDg7O1+5wOC7pjS5oebvdrqysLJ3LMTK5xX8gS8qZY5SVlSW73X5FbpyX+l7gvxhL32I8C+bNvs7psslZgnnLan+XV3NuZJwCo+v5rF9Jyv2D99HldbsszTp7dTHt5MmTFR0dna+9WrVqeumll0rcz7PPPqu1a9dqz5492rZtm5577jmlpqbqgQce8KYsAABwhfHqiMrevXsVHx+fr71OnTrat29fifs5cuSIHnroIaWlpalixYpq2rSpkpOT1aFDB2/KAgAAVxivgkq1atX0ww8/qG7duh7tW7duVVRUVIn7ee+997x5eQAAUE54deqnT58++vOf/6xVq1YpNzdXubm5WrlypUaMGKE+ffr4ukYAAFBOeXVEZeLEidq7d6/at2+vwMDfu3C5XOrXr1+prlEBAAAoildBJSgoSPPmzdOECRO0detWhYaGqkmTJqpTp46v6wMAAOWYV0ElT95v9AAAAJQFr4JKbm6uZs+erRUrVujo0aNyuVwe01euXOmT4gAAQPnmVVAZMWKEZs+era5du6px48ay2Xz3hWgAAAB5vAoqc+fO1fz589WlSxdf1wMAAODm1e3JQUFBuvrqq31dCwAAgAevgsqoUaP06quvymK/ZwgAAK4wXp36+eqrr7Rq1SotXbpUjRo1yvfjQgsWLPBJcQAAoHzzKqhUqlRJPXv29HUtAAAAHrwKKklJSb6uAwAAIB+vrlGRpJycHC1fvlxvv/22Tp8+LUk6dOiQzpw547PiAABA+ebVEZW9e/eqU6dO2rdvn5xOpzp06KCIiAi9/PLLOnfunGbNmuXrOgEAQDnk1RGVESNGqEWLFvrtt98UGhrqbu/Zs6dWrFjhs+IAAED55vVdP19//bWCgoI82uvUqaODBw/6pDAAAACvjqi4XC7l5ubmaz9w4IAiIiIuuSgAAADJy6DSoUMHzZgxw/3cZrPpzJkzGjt2LF+rDwAAfMarUz+vvPKK2rZtq+uuu07nzp1T37599fPPPys6Olqffvqpr2sEAADllFdBJS4uTt9//70+/fRTfffdd3K5XBo0aJAeeOABj4trAQAALoVXQUWSQkNDNXDgQA0cONCX9QAAALh5FVQ+/PDDIqf369fPq2IAAAAu5FVQGTFihMfz7OxsZWZmKigoSGFhYQQVAADgE17d9fPbb795PM6cOaOdO3fq1ltv5WJaAADgM17/1s/FrrnmGk2ePDnf0RYAAABv+SyoSFJAQIAOHTrkyy4BAEA55tU1KosWLfJ4boxRWlqaXn/9df3pT3/ySWEAAABeBZUePXp4PLfZbKpataratWunadOm+aIuAAAA74KKy+XydR0AAAD5+PQaFQAAAF/y6ojKyJEjSzzv9OnTvXkJAAAA74LKli1b9N133yknJ0cNGjSQJP3nP/9RQECAmjVr5p7PZrP5pkoAAFAueRVUunfvroiICH3wwQeqXLmypN+/BG7AgAG67bbbNGrUKJ8WCQAAyievrlGZNm2aJk2a5A4pklS5cmVNnDiRu34AAIDPeBVUMjIydOTIkXztR48e1enTpy+5KAAAAMnLoNKzZ08NGDBAf//733XgwAEdOHBAf//73zVo0CDdfffdvq4RAACUU15dozJr1iyNHj1aDz74oLKzs3/vKDBQgwYN0tSpU31aIAAAKL+8CiphYWF68803NXXqVO3atUvGGF199dUKDw/3dX0AAKAcu6QvfEtLS1NaWprq16+v8PBwGWN8VRcAAIB3QeXEiRNq37696tevry5duigtLU2S9PDDD3NrMgAA8BmvgsqTTz4ph8Ohffv2KSwszN3eu3dvJScn+6w4AABQvnl1jcq//vUvLVu2TDVr1vRov+aaa7R3716fFAYAAODVEZWzZ896HEnJc/z4cQUHB19yUQAAAJKXQaV169b68MMP3c9tNptcLpemTp2qtm3b+qw4AABQvnl16mfq1Klq06aNNm3apPPnz2vMmDHavn270tPT9fXXX/u6RgAAUE55dUTluuuu0w8//KCbbrpJHTp00NmzZ3X33Xdry5Ytqlevnq9rBAAA5VSpj6hkZ2erY8eOevvttzV+/PiyqAkAAECSF0dUHA6HfvzxR9lstrKoBwAAwM2rUz/9+vXTe++95+taAAAAPHh1Me358+f17rvvKiUlRS1atMj3Gz/Tp0/3SXEAAKB8K1VQ+fXXX1W3bl39+OOPatasmSTpP//5j8c8nBICAAC+Uqqgcs011ygtLU2rVq2S9PtX5r/22muKiYkpk+IAAED5VqprVC7+deSlS5fq7NmzPi0IAAAgj1cX0+a5OLgAAAD4UqmCis1my3cNCtekAACAslKqa1SMMerfv7/7hwfPnTunwYMH57vrZ8GCBb6rEAAAlFulCiqJiYkezx988EGfFgMAAHChUgWVpKSksqoDAAAgn0u6mBYAAKAsEVQAAIBl+TWoTJo0STfeeKMiIiJUrVo19ejRQzt37vRnSQAAwEL8GlRWr16toUOHasOGDUpJSVFOTo46duzIl8gBAABJXv4ooa8kJyd7PE9KSlK1atW0efNmtW7d2k9VAQAAq/BrULnYqVOnJElVqlQpcLrT6ZTT6XQ/z8jIkCRlZ2crOzu71K+Xt0xBy7pcLoWGhiok0KagAN99A68t0KbQ0FC5XC6vai7OgQMHdOLECZ/363Q63d+fUxCXyyVJ2rJli+z20h2oi4qKUs2aNS+pvitJUdslSo/xLFpp9nXBduPx3+KU1f6urPbPUtnvo/OU9+2yNOttMxb5HnxjjO666y799ttvWrt2bYHzjBs3TuPHj8/XPmfOHIWFhZV1iQAAwAcyMzPVt29fnTp1SpGRkUXOa5mgMnToUH355Zf66quvCv3XdUFHVGrVqqXjx48Xu6IFyc7OVkpKijp06CCHw+ExbevWrWrdurVi+k5WUMxVpe67MOeP/Kojc57WmjVrlJCQ4LN+pf/WXKXTcDmq1PBZv1l7tihj3bwi+w0OtGlK59p6auk+OXNKvkllpx9UevLMMhmPy1VR2yVKj/EsWmn2dcF2owktXHp+k11OV/E/n1JW+7uy2j9LZbuPvlB53y4zMjIUHR1doqBiiVM/w4cP16JFi7RmzZoiTwEEBwcXePrB4XBc0htd0PJ2u11ZWVk6l2Nkcn33e0bOHKOsrCzZ7Xafb5x5NedGxikwup7P+s05sq/Yfk2AkZQrExVfqvHKLcPxuNxd6nYNT4xnwbzZ1zldNjlLMG9Z7e/Kav8sle0+uiDldbsszTr7NagYYzR8+HAtXLhQqampio+P92c5AADAYvwaVIYOHao5c+boiy++UEREhA4fPixJqlixokJDQ/1ZGgAAsAC/fo/KW2+9pVOnTqlNmzaKjY11P+bNm+fPsgAAgEX4/dQPAABAYfitHwAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFkEFQAAYFl+DSpr1qxR9+7dFRcXJ5vNps8//9yf5QAAAIvxa1A5e/asEhIS9Prrr/uzDAAAYFGB/nzxzp07q3Pnzv4sAQAAWJhfg0ppOZ1OOZ1O9/OMjAxJUnZ2trKzs0vdX94yBS3rcrkUGhqqkECbggKMlxXnZwu0KTQ0VDt27JDL5fJZv5K0c+fOMqk5xxFQbL/BduPx35Iqy/GIiopSzZo1fdrnH6Go7RKl90eP54EDB3TixIky6bsstunS7OtK+zkvq893We3rpLLdJ0m//x0LDg52971lyxbZ7Zd+ciOv37JQFttdaT6PNmOMb99lL9lsNi1cuFA9evQodJ5x48Zp/Pjx+drnzJmjsLCwMqwOAAD4SmZmpvr27atTp04pMjKyyHkvq6BS0BGVWrVq6fjx48WuaEGys7OVkpKiDh06yOFweEzbunWrWrdurZi+kxUUc1Wp+y7M2R1rlZ48U1U6DZejSg2f9StJWXu2KGPdvDKruah+g+1GE1q49Pwmu5wuW6n79vV4ZKcfVHryTK1Zs0YJCQk+6/ePUNR2idL7I8czb79RFp/vstqmS7OvK+3nvKw+32W1r5P+mH10lU7DVaFaTU3pXFtPLd0nZ86l/Rm+sN/LZbvLyMhQdHR0iYLKZXXqJzg4uMBDWw6H45J2QAUtb7fblZWVpXM5Ria35H94i3MuO1dZWVnKjYxTYHQ9n/UrSTlH9pVpzSXp1+myyVmK1y6r8cjNMcrKypLdbr9s/9hf6nYNT3/EeObtN8ri811W27Q3+7qSfs7L6vNdVvs66Y/ZR+dGxslExUvKlYmKv+R1uLDfy2W7K01ffI8KAACwLL8eUTlz5ox++eUX9/Pdu3fr+++/V5UqVVS7dm0/VgYAAKzAr0Fl06ZNatu2rfv5yJEjJUmJiYmaPXu2n6oCAABW4deg0qZNG1nkWl4AAGBBXKMCAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsi6ACAAAsy+9B5c0331R8fLxCQkLUvHlzrV271t8lAQAAi/BrUJk3b56eeOIJPffcc9qyZYtuu+02de7cWfv27fNnWQAAwCL8GlSmT5+uQYMG6eGHH1bDhg01Y8YM1apVS2+99ZY/ywIAABbht6By/vx5bd68WR07dvRo79ixo9atW+enqgAAgJUE+uuFjx8/rtzcXMXExHi0x8TE6PDhwwUu43Q65XQ63c9PnTolSUpPT1d2dnapa8jOzlZmZqZOnDghh8PhMS0jI0MhISGyndgt43IW0kPp2U+nlUm/Zdl3Sfp1BUqZmbXkStsvk+Pbvr1h++2QQkJCtHnzZmVkZPis3zx2u10ul8vn/ebJzMzU2rVrZbf77t8SZVlzWfXti35dLleB41kWNf/8889l9vkuq226NDWX9nPuz32S1ft22Z1e7TOL67estruMjAydOHHCZ/2ePn1akmSMKX5m4ycHDx40ksy6des82idOnGgaNGhQ4DJjx441knjw4MGDBw8eV8Bj//79xeYFvx1RiY6OVkBAQL6jJ0ePHs13lCXPM888o5EjR7qfu1wupaenKyoqSjabrdQ1ZGRkqFatWtq/f78iIyNLvTz+i7H0HcbStxhP32Esfae8j6UxRqdPn1ZcXFyx8/otqAQFBal58+ZKSUlRz5493e0pKSm66667ClwmODhYwcHBHm2VKlW65FoiIyPL5YZSFhhL32EsfYvx9B3G0nfK81hWrFixRPP5LahI0siRI/XQQw+pRYsWatmypf72t79p3759Gjx4sD/LAgAAFuHXoNK7d2+dOHFCL7zwgtLS0tS4cWMtWbJEderU8WdZAADAIvwaVCRpyJAhGjJkiF9eOzg4WGPHjs13Ogmlx1j6DmPpW4yn7zCWvsNYlpzNmJLcGwQAAPDH8/tv/QAAABSGoAIAACyLoAIAACyLoAIAACyr3AaVN998U/Hx8QoJCVHz5s21du1af5dkeePGjZPNZvN4VK9e3T3dGKNx48YpLi5OoaGhatOmjbZv3+7Hiq1lzZo16t69u+Li4mSz2fT55597TC/J+DmdTg0fPlzR0dEKDw/XnXfeqQMHDvyBa2ENxY1l//79822rt9xyi8c8jOXvJk2apBtvvFERERGqVq2aevTooZ07d3rMw7ZZMiUZS7bN0iuXQWXevHl64okn9Nxzz2nLli267bbb1LlzZ+3bt8/fpVleo0aNlJaW5n5s27bNPe3ll1/W9OnT9frrr2vjxo2qXr26OnTo4P7xqfLu7NmzSkhI0Ouvv17g9JKM3xNPPKGFCxdq7ty5+uqrr3TmzBl169ZNubm5f9RqWEJxYylJnTp18thWlyxZ4jGdsfzd6tWrNXToUG3YsEEpKSnKyclRx44ddfbsWfc8bJslU5KxlNg2S+0Sf1vwsnTTTTeZwYMHe7Rde+215umnn/ZTRZeHsWPHmoSEhAKnuVwuU716dTN58mR327lz50zFihXNrFmz/qAKLx+SzMKFC93PSzJ+J0+eNA6Hw8ydO9c9z8GDB43dbjfJycl/WO1Wc/FYGmNMYmKiueuuuwpdhrEs3NGjR40ks3r1amMM2+aluHgsjWHb9Ea5O6Jy/vx5bd68WR07dvRo79ixo9atW+enqi4fP//8s+Li4hQfH68+ffro119/lSTt3r1bhw8f9hjX4OBg3X777YxrCZRk/DZv3qzs7GyPeeLi4tS4cWPGuACpqamqVq2a6tevr0ceeURHjx51T2MsC3fq1ClJUpUqVSSxbV6Ki8cyD9tm6ZS7oHL8+HHl5ubm+4XmmJiYfL/kDE8333yzPvzwQy1btkzvvPOODh8+rFatWunEiRPusWNcvVOS8Tt8+LCCgoJUuXLlQufB7zp37qxPPvlEK1eu1LRp07Rx40a1a9dOTqdTEmNZGGOMRo4cqVtvvVWNGzeWxLbprYLGUmLb9Ibfv0LfX2w2m8dzY0y+Nnjq3Lmz+/+bNGmili1bql69evrggw/cF4MxrpfGm/FjjPPr3bu3+/8bN26sFi1aqE6dOvryyy919913F7pceR/LYcOG6YcfftBXX32VbxrbZukUNpZsm6VX7o6oREdHKyAgIF8yPXr0aL5/MaBo4eHhatKkiX7++Wf33T+Mq3dKMn7Vq1fX+fPn9dtvvxU6DwoWGxurOnXq6Oeff5bEWBZk+PDhWrRokVatWqWaNWu629k2S6+wsSwI22bxyl1QCQoKUvPmzZWSkuLRnpKSolatWvmpqsuT0+nUjh07FBsbq/j4eFWvXt1jXM+fP6/Vq1czriVQkvFr3ry5HA6HxzxpaWn68ccfGeNinDhxQvv371dsbKwkxvJCxhgNGzZMCxYs0MqVKxUfH+8xnW2z5Ioby4KwbZaAf67h9a+5c+cah8Nh3nvvPfPTTz+ZJ554woSHh5s9e/b4uzRLGzVqlElNTTW//vqr2bBhg+nWrZuJiIhwj9vkyZNNxYoVzYIFC8y2bdvM/fffb2JjY01GRoafK7eG06dPmy1btpgtW7YYSWb69Olmy5YtZu/evcaYko3f4MGDTc2aNc3y5cvNd999Z9q1a2cSEhJMTk6Ov1bLL4oay9OnT5tRo0aZdevWmd27d5tVq1aZli1bmho1ajCWBXj88cdNxYoVTWpqqklLS3M/MjMz3fOwbZZMcWPJtumdchlUjDHmjTfeMHXq1DFBQUGmWbNmHrePoWC9e/c2sbGxxuFwmLi4OHP33Xeb7du3u6e7XC4zduxYU716dRMcHGxat25ttm3b5seKrWXVqlVGUr5HYmKiMaZk45eVlWWGDRtmqlSpYkJDQ023bt3Mvn37/LA2/lXUWGZmZpqOHTuaqlWrGofDYWrXrm0SExPzjRNj+buCxlGSSUpKcs/DtlkyxY0l26Z3bMYY88cdvwEAACi5cneNCgAAuHwQVAAAgGURVAAAgGURVAAAgGURVAAAgGURVAAAgGURVAAAgGURVAD4Rf/+/dWjRw/38zZt2uiJJ55wP8/MzNQ999yjyMhI2Ww2nTx5ssA2AFc2ggpwhejfv79sNptsNpscDodiYmLUoUMHvf/++3K5XP4ur1gLFizQhAkT3M8/+OADrV27VuvWrVNaWpoqVqxYYBuAKxtBBbiCdOrUSWlpadqzZ4+WLl2qtm3basSIEerWrZtycnL8XV6RqlSpooiICPfzXbt2qWHDhmrcuLGqV68um81WYFtp5ebmXhbBDcDvCCrAFSQ4OFjVq1dXjRo11KxZMz377LP64osvtHTpUs2ePds93/Tp09WkSROFh4erVq1aGjJkiM6cOSNJOnv2rCIjI/X3v//do+9//vOfCg8P1+nTp3X+/HkNGzZMsbGxCgkJUd26dTVp0qRC68rNzdXIkSNVqVIlRUVFacyYMbr41zsuPPXTpk0bTZs2TWvWrJHNZlObNm0KbJN+/yXfMWPGqEaNGgoPD9fNN9+s1NRUd7+zZ89WpUqVtHjxYl133XUKDg7W3r17S7zcsmXL1LBhQ1WoUMEdBC/0/vvvq1GjRgoODlZsbKyGDRvmnnbq1Ck9+uijqlatmiIjI9WuXTtt3bq1uLcRwAUIKsAVrl27dkpISNCCBQvcbXa7Xa+99pp+/PFHffDBB1q5cqXGjBkjSQoPD1efPn2UlJTk0U9SUpJ69eqliIgIvfbaa1q0aJHmz5+vnTt36uOPP1bdunULrWHatGl6//339d577+mrr75Senq6Fi5cWOj8CxYs0COPPKKWLVsqLS1NCxYsKLBNkgYMGKCvv/5ac+fO1Q8//KB7771XnTp10s8//+zuLzMzU5MmTdK7776r7du3q1q1aiVe7v/+7//00Ucfac2aNdq3b59Gjx7tnv7WW29p6NChevTRR7Vt2zYtWrRIV199tSTJGKOuXbvq8OHDWrJkiTZv3qxmzZqpffv2Sk9PL8E7B0CSyu2vJwNXmsTERHPXXXcVOK13796mYcOGhS47f/58ExUV5X7+zTffmICAAHPw4EFjjDHHjh0zDofDpKamGmOMGT58uGnXrp1xuVwlqi02NtZMnjzZ/Tw7O9vUrFnTo97bb7/djBgxwv18xIgR5vbbb/fo5+K2X375xdhsNnededq3b2+eeeYZY4wxSUlJRpL5/vvvvVrul19+cU9/4403TExMjPt5XFycee655wpc5xUrVpjIyEhz7tw5j/Z69eqZt99+u8BlAOQX6O+gBKDsGWM8rudYtWqVXnrpJf3000/KyMhQTk6Ozp07p7Nnzyo8PFw33XSTGjVqpA8//FBPP/20PvroI9WuXVutW7eW9PuFux06dFCDBg3UqVMndevWTR07dizwtU+dOqW0tDS1bNnS3RYYGKgWLVrkO/1TWt99952MMapfv75Hu9PpVFRUlPt5UFCQmjZtWurlwsLCVK9ePffz2NhYHT16VJJ09OhRHTp0SO3bty+wts2bN+vMmTMe/UlSVlaWdu3aVco1BcovggpQDuzYsUPx8fGSpL1796pLly4aPHiwJkyYoCpVquirr77SoEGDlJ2d7V7m4Ycf1uuvv66nn35aSUlJGjBggDvsNGvWTLt379bSpUu1fPly3XfffbrjjjvyXddS1lwulwICArR582YFBAR4TKtQoYL7/0NDQz2CWkmXczgcHtNsNps7XIWGhhZbW2xsrMd1L3kqVapU5LIA/ougAlzhVq5cqW3btunJJ5+UJG3atEk5OTmaNm2a7PbfL1ObP39+vuUefPBBjRkzRq+99pq2b9+uxMREj+mRkZHq3bu3evfurV69eqlTp05KT09XlSpVPOarWLGiYmNjtWHDBvcRmZycHPc1G5fihhtuUG5uro4eParbbrutzJe7UEREhOrWrasVK1aobdu2+aY3a9ZMhw8fVmBgYJHX7wAoGkEFuII4nU4dPnxYubm5OnLkiJKTkzVp0iR169ZN/fr1kyTVq1dPOTk5mjlzprp3766vv/5as2bNytdX5cqVdffdd+svf/mLOnbsqJo1a7qnvfLKK4qNjdX1118vu92uzz77TNWrVy/0SMGIESM0efJkXXPNNWrYsKGmT5/uky9rq1+/vh544AH169dP06ZN0w033KDjx49r5cqVatKkibp06eLT5S42btw4DR48WNWqVVPnzp11+vRpff311xo+fLjuuOMOtWzZUj169NCUKVPUoEEDHTp0SEuWLFGPHj3UokWLS15/oDzgrh/gCpKcnKzY2FjVrVtXnTp10qpVq/Taa6/piy++cJ/iuP766zV9+nRNmTJFjRs31ieffFLorcWDBg3S+fPnNXDgQI/2ChUqaMqUKWrRooVuvPFG7dmzR0uWLHEfobnYqFGj1K9fP/Xv318tW7ZURESEevbs6ZN1TkpKUr9+/TRq1Cg1aNBAd955p7755hvVqlWrTJa7UGJiombMmKE333xTjRo1Urdu3dx3DdlsNi1ZskStW7fWwIEDVb9+ffXp00d79uxRTEzMJa0zUJ7YzKVezQbgivXJJ59oxIgROnTokIKCgvxdDoByiFM/APLJzMzU7t27NWnSJD322GOEFAB+w6kfAPm8/PLLuv766xUTE6NnnnnG3+UAKMc49QMAACyLIyoAAMCyCCoAAMCyCCoAAMCyCCoAAMCyCCoAAMCyCCoAAMCyCCoAAMCyCCoAAMCyCCoAAMCy/h8+BcJqXi7TVwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['OSDate - dateEntry'][df['OSDate - dateEntry'] > 0], bins=20, edgecolor='k')  # You can adjust the number of bins as needed\n",
    "plt.xlabel('Days difference')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of OSDate - dateEntry')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     244.000000\n",
       "mean       44.913934\n",
       "std       157.208666\n",
       "min     -1620.000000\n",
       "25%        43.000000\n",
       "50%        63.000000\n",
       "75%        91.000000\n",
       "max       313.000000\n",
       "Name: HDMFDate - dateEntry, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['HDMFDate - dateEntry'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['HDMFDate - dateEntry'], bins=20, edgecolor='k')  # You can adjust the number of bins as needed\n",
    "plt.xlabel('Days difference')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of HDMFDate - dateEntry')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    230.000000\n",
       "mean      75.717391\n",
       "std       41.672388\n",
       "min       21.000000\n",
       "25%       46.000000\n",
       "50%       65.000000\n",
       "75%       92.000000\n",
       "max      313.000000\n",
       "Name: HDMFDate - dateEntry, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['HDMFDate - dateEntry'][df['HDMFDate - dateEntry'] > 0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['HDMFDate - dateEntry'][df['HDMFDate - dateEntry'] > 0], bins=20, edgecolor='k')  # You can adjust the number of bins as needed\n",
    "plt.xlabel('Days difference')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of HDMFDate - dateEntry')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df.copy().loc[:, ['userId', 'lastFirstName', 'basicMonthlySalary', 'home_ownership_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>lastFirstName</th>\n",
       "      <th>basicMonthlySalary</th>\n",
       "      <th>home_ownership_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>370</td>\n",
       "      <td>IBALI, HOWARD</td>\n",
       "      <td>19000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1025</td>\n",
       "      <td>PATALINGHOG, KIMBERLY</td>\n",
       "      <td>15000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1105</td>\n",
       "      <td>TEMILLOSO, DENNIS</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1719</td>\n",
       "      <td>OSCARES, ELMER</td>\n",
       "      <td>21200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2081</td>\n",
       "      <td>LEGASPI, MANNY</td>\n",
       "      <td>19800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>12850</td>\n",
       "      <td>LIPANGO, ARVIN</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>12852</td>\n",
       "      <td>TEJADA, JERIC</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>12853</td>\n",
       "      <td>RAMOS, RHEALYN</td>\n",
       "      <td>13962</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>12854</td>\n",
       "      <td>BURGOS, LIEZEL</td>\n",
       "      <td>51000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>12857</td>\n",
       "      <td>SEÑERES, JENNY MAE</td>\n",
       "      <td>98800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1070 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId          lastFirstName  basicMonthlySalary  home_ownership_class\n",
       "0        370          IBALI, HOWARD               19000                     1\n",
       "1       1025  PATALINGHOG, KIMBERLY               15000                     0\n",
       "2       1105      TEMILLOSO, DENNIS               20000                     0\n",
       "3       1719         OSCARES, ELMER               21200                     1\n",
       "4       2081         LEGASPI, MANNY               19800                     0\n",
       "...      ...                    ...                 ...                   ...\n",
       "1065   12850         LIPANGO, ARVIN               19000                     0\n",
       "1066   12852          TEJADA, JERIC                  20                     0\n",
       "1067   12853         RAMOS, RHEALYN               13962                     0\n",
       "1068   12854         BURGOS, LIEZEL               51000                     0\n",
       "1069   12857     SEÑERES, JENNY MAE               98800                     0\n",
       "\n",
       "[1070 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x226d62b8c70>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(\n",
    "    x='home_ownership_class',\n",
    "    y='basicMonthlySalary',\n",
    "    data=df_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "columns = [\n",
    "    'age', 'basicMonthlySalary', 'preferredNetDisposableIncomeId',\n",
    "    'workingFamilyCount', 'residentsCount', 'monthlyFamilyIncome', 'food',\n",
    "    'hygiene', 'houseCleaning', 'fare', 'parking', 'gasoline', 'tuition',\n",
    "    'allowance', 'uniform', 'otherEducation', 'emergency', 'medicine',\n",
    "    'water', 'electricity', 'rent', 'repair', 'cinema', 'dineOut',\n",
    "    'leisure', 'personalCare', 'clothing', 'mobileLoad', 'internet',\n",
    "    'vehicleLoan', 'informalLenders', 'companyLoan', 'privateLoans',\n",
    "    'governmentLoans', 'smoking', 'alcohol', 'gambling', 'smallLottery',\n",
    "    'otherVices', 'savings', 'loanOthers', 'payInsurance', 'loanSSS',\n",
    "    'payFamilySupport', 'loanPagIbig', 'loanGSIS', 'loanPersonal',\n",
    "    'houseHasPensioner', 'houseHasPrivateEmployee', 'houseHasBusiness',\n",
    "    'houseHasFreelancer', 'houseHasGovtEmployee', 'houseHasOFW',\n",
    "    'houseOnlyFamily', 'houseExtendedFamily',\n",
    "    'monthlyUtilityBills', 'monthlyVices', 'monthlyExpenses',\n",
    "    'monthlySoloNetIncome', 'positiveMonthlySoloNetIncome',\n",
    "    'monthlyFamilyNetIncome', 'positiveMonthlyFamilyNetIncome',\n",
    "    'monthlySoloNetIncomeWithSavings',\n",
    "    'positiveMonthlySoloNetIncomeWithSavings',\n",
    "    'monthlyFamilyNetIncomeWithSavings',\n",
    "    'positiveMonthlyFamilyNetIncomeWithSavings',\n",
    "    'monthlyFamilyIncome - basicMonthlySalary',\n",
    "    'positive monthlyFamilyIncome - basicMonthlySalary',\n",
    "    'basicMonthlySalary - monthlyExpenses',\n",
    "    'positive basicMonthlySalary - monthlyExpenses',\n",
    "    'monthlyFamilyIncome - monthlyExpenses',\n",
    "    'positive monthlyFamilyIncome - monthlyExpenses',\n",
    "    'basicMonthlySalary / monthlyFamilyIncome',\n",
    "    'monthlyExpenses / monthlyFamilyIncome',\n",
    "    'monthlyVices / monthlyFamilyIncome'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22517611 0.19630974 0.025      0.26182082 0.23276093 0.22573166\n",
      " 0.17743827 0.         0.26182082 0.23290937 0.22310612 0.17743827\n",
      " 0.         0.27194076 0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22528242 0.19630974 0.025      0.26182082 0.23140749 0.22528242\n",
      " 0.17743827 0.         0.26182082 0.23196304 0.22310612 0.17743827\n",
      " 0.         0.27194076 0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22528242 0.19630974 0.02       0.26182082 0.23196304 0.22557659\n",
      " 0.17743827 0.         0.269853   0.23196304 0.22310612 0.17743827\n",
      " 0.         0.27194076 0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22528242 0.19630974 0.02       0.26182082 0.23196304 0.22597693\n",
      " 0.2060097  0.         0.271331   0.23104251 0.22310612 0.2060097\n",
      " 0.         0.27194076 0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.21976117 0.20059584 0.06428571 0.2640333  0.23196304 0.22310612\n",
      " 0.20075568 0.         0.271331   0.23516006 0.22310612 0.20075568\n",
      " 0.         0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.21976117 0.19540925 0.13095238 0.26495575 0.23516006 0.22310612\n",
      " 0.19540925 0.13928571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13928571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.14095238 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.14095238 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.14095238 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.13428571 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.13428571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13428571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.13428571 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.13428571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13428571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.13428571 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.13428571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13428571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.13428571 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.13428571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13428571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.13428571 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.13428571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13428571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.13428571 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.13428571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13428571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006]\n",
      "One or more of the train scores are non-finite: [0.25290316 0.19977741 0.021875   0.25625752 0.25174875 0.2530831\n",
      " 0.17296195 0.         0.25625752 0.25138793 0.25606611 0.17296195\n",
      " 0.         0.26232921 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25306213 0.19977741 0.021875   0.25625752 0.25124635 0.25306213\n",
      " 0.17296195 0.         0.25625752 0.25142628 0.25606611 0.17296195\n",
      " 0.         0.26232921 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25306213 0.19977741 0.0275     0.25625752 0.25142628 0.25585681\n",
      " 0.17296195 0.         0.25983608 0.25142628 0.25606611 0.17296195\n",
      " 0.         0.26232921 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25306213 0.20001976 0.0275     0.25625752 0.25142628 0.25605975\n",
      " 0.19609628 0.         0.26192757 0.25384962 0.25606611 0.19609628\n",
      " 0.         0.26232921 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25602463 0.20002023 0.06305263 0.26020752 0.25142628 0.25606611\n",
      " 0.2003313  0.01666667 0.26192757 0.25426513 0.25606611 0.2003313\n",
      " 0.01666667 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25602463 0.2004652  0.09853207 0.26177347 0.25426513 0.25606611\n",
      " 0.2004652  0.09881136 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.09881136 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11541015 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11541015 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11541015 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11365185 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11365185 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11365185 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11365185 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11365185 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11365185 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2719407594857509\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.5336658354114713\n",
      "GridSearchCV Runtime: 6.2308313846588135 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2711864406779661\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2605042016806723\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2127659574468085\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.25806451612903225\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.265625\n",
      "Ave Test Precision: 0.25362922318689585\n",
      "Stdev Test Precision: 0.023392831097881244\n",
      "Ave Test Accuracy: 0.5208955223880597\n",
      "Stdev Test Accuracy: 0.03989223062956446\n",
      "Ave Test Specificity: 0.5336633663366336\n",
      "Ave Test Recall: 0.4818181818181818\n",
      "Ave Test NPV: 0.7580573795725198\n",
      "Ave Test F1-Score: 0.33203477274722176\n",
      "Ave Test G-mean: 0.5067362124311666\n",
      "Ave Runtime: 0.006259298324584961\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with age. 86 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.3198298  0.40019254 0.33417039 0.3198298  0.28051706 0.3198298\n",
      " 0.05       0.         0.3198298  0.28051706 0.34898253 0.05\n",
      " 0.         0.35279991 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.32043888 0.40019254 0.33417039 0.32043888 0.28051706 0.32151889\n",
      " 0.05       0.         0.34730131 0.28590403 0.34898253 0.05\n",
      " 0.         0.35279991 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.32133631 0.41792755 0.33090241 0.32133631 0.28112614 0.34321436\n",
      " 0.08333333 0.         0.35279991 0.30006485 0.34898253 0.08333333\n",
      " 0.         0.35279991 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.32930214 0.31101301 0.32574689 0.32690583 0.28467052 0.34479093\n",
      " 0.18116654 0.2        0.35279991 0.30006485 0.35017301 0.18116654\n",
      " 0.2        0.35279991 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34602388 0.31807018 0.3325518  0.34820177 0.29164388 0.34599214\n",
      " 0.29626437 0.31166667 0.35370221 0.30006485 0.35017301 0.29626437\n",
      " 0.31166667 0.35370221 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.35212335 0.29626437 0.32992968 0.35010734 0.29780076 0.34482547\n",
      " 0.29626437 0.31432151 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.31432151 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35094031 0.2976455  0.34482547\n",
      " 0.29626437 0.32516777 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32516777 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35370221 0.2976455  0.34482547\n",
      " 0.29626437 0.32863098 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32863098 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35370221 0.2976455  0.34482547\n",
      " 0.29626437 0.32863098 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32863098 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35370221 0.2976455  0.34482547\n",
      " 0.29626437 0.32863098 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32863098 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35370221 0.2976455  0.34482547\n",
      " 0.29626437 0.32863098 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32863098 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35370221 0.2976455  0.34482547\n",
      " 0.29626437 0.32863098 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32863098 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35370221 0.2976455  0.34482547\n",
      " 0.29626437 0.32863098 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32863098 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455 ]\n",
      "One or more of the train scores are non-finite: [0.31899892 0.34347137 0.33292962 0.31899892 0.27516701 0.31899892\n",
      " 0.04903047 0.         0.31899892 0.27516701 0.3369827  0.04903047\n",
      " 0.         0.3378266  0.30521314        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.31971635 0.32807183 0.33292962 0.31971635 0.27516701 0.32941341\n",
      " 0.04903047 0.         0.3349485  0.28717644 0.3369827  0.04903047\n",
      " 0.         0.3378266  0.30521314        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.3205307  0.32285482 0.33207943 0.3205307  0.2755598  0.33826158\n",
      " 0.19314812 0.         0.3378266  0.30537151 0.3369827  0.19314812\n",
      " 0.         0.3378266  0.30521314        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.32799459 0.31354538 0.3315562  0.32903233 0.27933125 0.33602989\n",
      " 0.35519003 0.50217391 0.33776536 0.30554522 0.33774473 0.35519003\n",
      " 0.50217391 0.33776536 0.30554522        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33723794 0.35911284 0.32048266 0.33829892 0.2968398  0.33557773\n",
      " 0.37120765 0.33476722 0.33803269 0.30519213 0.33760229 0.37120765\n",
      " 0.33476722 0.33803269 0.30528526        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33793385 0.37146532 0.31794624 0.33768384 0.30231787 0.33586879\n",
      " 0.37185748 0.31943517 0.33803269 0.30496637 0.33789334 0.37185748\n",
      " 0.31943517 0.33803269 0.30496637        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33798374 0.30614854 0.33586879\n",
      " 0.37185748 0.31817972 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31817972 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33803269 0.30512572 0.33586879\n",
      " 0.37185748 0.31810269 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31810269 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33803269 0.30508579 0.33586879\n",
      " 0.37185748 0.31810269 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31810269 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33803269 0.30508579 0.33586879\n",
      " 0.37185748 0.31810269 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31810269 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33803269 0.30508579 0.33586879\n",
      " 0.37185748 0.31810269 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31810269 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33803269 0.30508579 0.33586879\n",
      " 0.37185748 0.31810269 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31810269 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33803269 0.30508579 0.33586879\n",
      " 0.37185748 0.31810269 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31810269 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.0001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.41792755045602437\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.0001, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6670822942643392\n",
      "GridSearchCV Runtime: 3.9486300945281982 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.42424242424242425\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3063063063063063\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.32947976878612717\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.26842105263157895\n",
      "Ave Test Precision: 0.332356577059954\n",
      "Stdev Test Precision: 0.057486100235996\n",
      "Ave Test Accuracy: 0.5813432835820895\n",
      "Stdev Test Accuracy: 0.109786493619808\n",
      "Ave Test Specificity: 0.5722772277227722\n",
      "Ave Test Recall: 0.6090909090909091\n",
      "Ave Test NPV: 0.8242029995309803\n",
      "Ave Test F1-Score: 0.41475705396397194\n",
      "Ave Test G-mean: 0.5638672192060966\n",
      "Ave Runtime: 0.006036138534545899\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with basicMonthlySalary. 85 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.33040034 0.31535089 0.         0.33040034 0.33040034 0.33040034\n",
      " 0.07469136 0.         0.33040034 0.33040034 0.33040034 0.07469136\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.         0.33040034 0.33040034 0.33040034\n",
      " 0.07469136 0.         0.33040034 0.33040034 0.33040034 0.07469136\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.         0.33040034 0.33040034 0.33040034\n",
      " 0.07469136 0.         0.33040034 0.33040034 0.33040034 0.07469136\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.         0.33040034 0.33040034 0.33040034\n",
      " 0.17734287 0.         0.33040034 0.33040034 0.33040034 0.17734287\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.         0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034]\n",
      "One or more of the train scores are non-finite: [0.33330429 0.31136017 0.         0.33330429 0.33330429 0.33330429\n",
      " 0.07357971 0.         0.33330429 0.33330429 0.33330429 0.07357971\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.         0.33330429 0.33330429 0.33330429\n",
      " 0.07357971 0.         0.33330429 0.33330429 0.33330429 0.07357971\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.         0.33330429 0.33330429 0.33330429\n",
      " 0.07357971 0.         0.33330429 0.33330429 0.33330429 0.07357971\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.         0.33330429 0.33330429 0.33330429\n",
      " 0.1731696  0.         0.33330429 0.33330429 0.33330429 0.1731696\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.         0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.33040033960292586\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6384039900249376\n",
      "GridSearchCV Runtime: 4.090520858764648 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4044943820224719\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.32978723404255317\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.39325842696629215\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3645833333333333\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.34831460674157305\n",
      "Ave Test Precision: 0.3680875966212447\n",
      "Stdev Test Precision: 0.03094023871571617\n",
      "Ave Test Accuracy: 0.6634328358208956\n",
      "Stdev Test Accuracy: 0.022910569302998132\n",
      "Ave Test Specificity: 0.7138613861386138\n",
      "Ave Test Recall: 0.509090909090909\n",
      "Ave Test NPV: 0.8164610334822197\n",
      "Ave Test F1-Score: 0.4271455595380327\n",
      "Ave Test G-mean: 0.6026404897216738\n",
      "Ave Runtime: 0.005557060241699219\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with preferredNetDisposableIncomeId. 84 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27245955 0.18305297 0.14844396 0.22709238 0.25021041 0.27245955\n",
      " 0.17219136 0.14719136 0.22709238 0.25021041 0.27245955 0.14719136\n",
      " 0.14719136 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.25315313 0.18305297 0.14844396 0.22069131 0.25021041 0.27245955\n",
      " 0.14719136 0.14719136 0.22069131 0.25021041 0.27245955 0.14719136\n",
      " 0.14719136 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.18305297 0.14844396 0.22463868 0.23315737 0.27245955\n",
      " 0.14719136 0.14719136 0.22069131 0.23315737 0.27245955 0.14719136\n",
      " 0.14719136 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.19943073 0.14844396 0.22069131 0.23315737 0.27245955\n",
      " 0.14248121 0.14719136 0.22069131 0.23315737 0.27245955 0.14248121\n",
      " 0.14719136 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.24727715 0.14770163 0.22069131 0.23315737 0.27245955\n",
      " 0.19843073 0.14984477 0.22069131 0.23315737 0.27245955 0.19843073\n",
      " 0.14984477 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.17133863 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.17306672 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.17306672 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737]\n",
      "One or more of the train scores are non-finite: [0.27465602 0.23174272 0.14586824 0.27110458 0.25684778 0.27465602\n",
      " 0.17523747 0.14740242 0.27110458 0.25684778 0.27465602 0.14740242\n",
      " 0.14740242 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27005669 0.23174272 0.14586824 0.26677571 0.25684778 0.27465602\n",
      " 0.14740242 0.14740242 0.26677571 0.25526413 0.27465602 0.14740242\n",
      " 0.14740242 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.23174272 0.14586824 0.26791354 0.2471273  0.27465602\n",
      " 0.14740242 0.14740242 0.26677571 0.2471273  0.27465602 0.14740242\n",
      " 0.14740242 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25650871 0.14586824 0.26677571 0.2471273  0.27465602\n",
      " 0.14740642 0.14740242 0.26677571 0.2471273  0.27465602 0.14740642\n",
      " 0.14740242 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.24937137 0.14537891 0.26677571 0.2471273  0.27465602\n",
      " 0.25355629 0.14699652 0.26677571 0.2471273  0.27465602 0.25355629\n",
      " 0.14699652 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.17296206 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.17397301 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.17397301 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273 ]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2724595467673271\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6234413965087282\n",
      "GridSearchCV Runtime: 4.1496312618255615 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.32786885245901637\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.28169014084507044\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.32051282051282054\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2911392405063291\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.28378378378378377\n",
      "Ave Test Precision: 0.30099896762140405\n",
      "Stdev Test Precision: 0.021617057678368582\n",
      "Ave Test Accuracy: 0.6455223880597015\n",
      "Stdev Test Accuracy: 0.018088357676926597\n",
      "Ave Test Specificity: 0.7485148514851485\n",
      "Ave Test Recall: 0.3303030303030303\n",
      "Ave Test NPV: 0.7738027551245351\n",
      "Ave Test F1-Score: 0.314279006874707\n",
      "Ave Test G-mean: 0.49646187714329437\n",
      "Ave Runtime: 0.012451171875\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with workingFamilyCount. 83 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27001717 0.24332925 0.09102079 0.24286099 0.2444425  0.27133808\n",
      " 0.07344136 0.02375    0.240807   0.2444425  0.25757038 0.07344136\n",
      " 0.02375    0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.24332925 0.09102079 0.240807   0.2444425  0.25757038\n",
      " 0.07344136 0.02375    0.240807   0.23668641 0.25757038 0.07344136\n",
      " 0.02375    0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.24332925 0.09102079 0.240807   0.22497664 0.25757038\n",
      " 0.07344136 0.02375    0.240807   0.22928698 0.25757038 0.07344136\n",
      " 0.02375    0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.24332925 0.09102079 0.240807   0.22928698 0.25757038\n",
      " 0.18285312 0.02375    0.240807   0.22928698 0.25757038 0.18285312\n",
      " 0.02375    0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.24332925 0.09102079 0.240807   0.22928698 0.25757038\n",
      " 0.23351648 0.06982323 0.240807   0.22928698 0.25757038 0.23351648\n",
      " 0.06982323 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09102079 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698]\n",
      "One or more of the train scores are non-finite: [0.27403194 0.25945632 0.11437926 0.27202179 0.23198163 0.25988291\n",
      " 0.07371821 0.02465374 0.25645463 0.23198163 0.25770653 0.07371821\n",
      " 0.02465374 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25945632 0.11437926 0.25645463 0.23198163 0.25770653\n",
      " 0.07371821 0.02465374 0.25645463 0.22408459 0.25770653 0.07371821\n",
      " 0.02465374 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25945632 0.11437926 0.25645463 0.22356627 0.25770653\n",
      " 0.07371821 0.02465374 0.25645463 0.22314433 0.25770653 0.07371821\n",
      " 0.02465374 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25945632 0.11437926 0.25645463 0.22314433 0.25770653\n",
      " 0.20093741 0.02465374 0.25645463 0.22314433 0.25770653 0.20093741\n",
      " 0.02465374 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25945632 0.11437926 0.25645463 0.22314433 0.25770653\n",
      " 0.25005062 0.09483707 0.25645463 0.22314433 0.25770653 0.25005062\n",
      " 0.09483707 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11437926 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.10876482 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.10876482 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.27133808202670107\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.4800498753117207\n",
      "GridSearchCV Runtime: 4.369429349899292 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.22580645161290322\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.26666666666666666\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.19607843137254902\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.27906976744186046\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25308641975308643\n",
      "Ave Test Precision: 0.24414154736941315\n",
      "Stdev Test Precision: 0.03336769078942016\n",
      "Ave Test Accuracy: 0.5037313432835822\n",
      "Stdev Test Accuracy: 0.06900463436159303\n",
      "Ave Test Specificity: 0.5099009900990099\n",
      "Ave Test Recall: 0.48484848484848475\n",
      "Ave Test NPV: 0.7523196025310736\n",
      "Ave Test F1-Score: 0.3201292572809601\n",
      "Ave Test G-mean: 0.48295248368187044\n",
      "Ave Runtime: 0.0069316387176513675\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with residentsCount. 82 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.21493977 0.21814642 0.29943445 0.21493977 0.25294743 0.21493977\n",
      " 0.12344136 0.         0.21493977 0.25294743 0.23730719 0.12344136\n",
      " 0.         0.2391029  0.2538565         nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.21493977 0.21427035 0.29943445 0.21493977 0.25294743 0.23730719\n",
      " 0.12375781 0.         0.2391029  0.26428882 0.23730719 0.12375781\n",
      " 0.         0.2391029  0.2538565         nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.21493977 0.22505434 0.29943445 0.21493977 0.25294743 0.23730719\n",
      " 0.14169872 0.         0.2391029  0.25663428 0.23730719 0.14169872\n",
      " 0.         0.2391029  0.2538565         nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.21100484 0.23558609 0.29794627 0.21103655 0.25063776 0.23762448\n",
      " 0.15253521 0.025      0.2391029  0.25504227 0.23762448 0.15253521\n",
      " 0.025      0.2391029  0.25504227        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.22657022 0.17551338 0.29534817 0.22600159 0.25448271 0.23762448\n",
      " 0.15934556 0.35333333 0.2391029  0.25504227 0.23762448 0.15934556\n",
      " 0.35333333 0.2391029  0.25504227        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23229967 0.18024685 0.29175558 0.23689853 0.26371167 0.23272938\n",
      " 0.18096114 0.29461272 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.29461272 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18024685 0.31766056 0.2391029  0.26121929 0.23272938\n",
      " 0.18096114 0.31766056 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18096114 0.31766056 0.2391029  0.25883834 0.23272938\n",
      " 0.18096114 0.31766056 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834]\n",
      "One or more of the train scores are non-finite: [0.22420288 0.2236422  0.3020995  0.22420288 0.25180495 0.22420288\n",
      " 0.12281688 0.         0.22420288 0.25180495 0.24120377 0.12281688\n",
      " 0.         0.24176348 0.26256588        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.22420288 0.22338995 0.3020995  0.22420288 0.25180495 0.23993746\n",
      " 0.12291482 0.         0.24028389 0.25373772 0.24120377 0.12291482\n",
      " 0.         0.24176348 0.26256588        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.22408338 0.22335157 0.3020995  0.22402326 0.25180495 0.24106638\n",
      " 0.14454219 0.         0.24126054 0.26289177 0.24106638 0.14454219\n",
      " 0.         0.24126054 0.26256156        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.22249445 0.23277745 0.29942311 0.22254189 0.25121477 0.24060268\n",
      " 0.16309034 0.20733954 0.24129152 0.26217372 0.24060268 0.16309034\n",
      " 0.20733954 0.24129152 0.26217372        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.23584198 0.18657272 0.31450171 0.23702248 0.25956427 0.2406455\n",
      " 0.20163046 0.33806063 0.24129152 0.26215121 0.2406455  0.20168153\n",
      " 0.33806063 0.24129152 0.26215121        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24015815 0.19674384 0.31642852 0.24096069 0.26172847 0.24087133\n",
      " 0.19644567 0.32131888 0.24117372 0.26179596 0.24087133 0.19644567\n",
      " 0.32131888 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24082079 0.19633765 0.30504612 0.24121567 0.26179006 0.24087133\n",
      " 0.19644567 0.30447141 0.24117372 0.26179596 0.24087133 0.19644567\n",
      " 0.30447141 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24091444 0.19670904 0.30504612 0.24121567 0.26179596 0.24087133\n",
      " 0.19670904 0.30504612 0.24117372 0.26179596 0.24087133 0.19670904\n",
      " 0.30504612 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24087133 0.19670904 0.30356562 0.24121567 0.26179596 0.24087133\n",
      " 0.19670904 0.30356562 0.24117372 0.26179596 0.24087133 0.19670904\n",
      " 0.30504612 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24087133 0.19670904 0.30356562 0.24121567 0.26179596 0.24087133\n",
      " 0.19670904 0.30356562 0.24117372 0.26179596 0.24087133 0.19670904\n",
      " 0.30504612 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24087133 0.19670904 0.30356562 0.24121567 0.26179596 0.24087133\n",
      " 0.19670904 0.30356562 0.24117372 0.26179596 0.24087133 0.19670904\n",
      " 0.30504612 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24087133 0.19670904 0.30356562 0.24121567 0.26179596 0.24087133\n",
      " 0.19670904 0.30356562 0.24117372 0.26179596 0.24087133 0.19670904\n",
      " 0.30504612 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24087133 0.19670904 0.30356562 0.24121567 0.26179596 0.24087133\n",
      " 0.19670904 0.30356562 0.24117372 0.26179596 0.24087133 0.19670904\n",
      " 0.30504612 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.35333333333333333\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.729426433915212\n",
      "GridSearchCV Runtime: 4.404484987258911 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.391304347826087\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.5\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.4\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4\n",
      "Ave Test Precision: 0.3982608695652174\n",
      "Stdev Test Precision: 0.07081753224552474\n",
      "Ave Test Accuracy: 0.7402985074626864\n",
      "Stdev Test Accuracy: 0.007737478116913322\n",
      "Ave Test Specificity: 0.9485148514851485\n",
      "Ave Test Recall: 0.10303030303030303\n",
      "Ave Test NPV: 0.7640751422326578\n",
      "Ave Test F1-Score: 0.15902587978970467\n",
      "Ave Test G-mean: 0.30432592506686246\n",
      "Ave Runtime: 0.007053804397583008\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyIncome. 81 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24609164 0.2834696  0.27184211 0.26540728 0.26540728 0.25036514\n",
      " 0.1225     0.0475     0.26540728 0.26540728 0.2520287  0.1225\n",
      " 0.0475     0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.25036514 0.2834696  0.27184211 0.26540728 0.26540728 0.25036514\n",
      " 0.1225     0.0475     0.26540728 0.26540728 0.2520287  0.1225\n",
      " 0.0475     0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.25036514 0.29180293 0.27184211 0.26540728 0.26540728 0.25112145\n",
      " 0.12102941 0.0475     0.26977934 0.26616359 0.2520287  0.12102941\n",
      " 0.0475     0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.25036514 0.28528179 0.27184211 0.26540728 0.26540728 0.2520287\n",
      " 0.19293665 0.0975     0.2706866  0.27275081 0.2520287  0.19293665\n",
      " 0.0975     0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2512724  0.33922783 0.29221725 0.26631454 0.26631454 0.2520287\n",
      " 0.33139046 0.20738801 0.2706866  0.27275081 0.2520287  0.33139046\n",
      " 0.20738801 0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23953868 0.26984626 0.26647338 0.2520287\n",
      " 0.34123467 0.23391928 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23391928 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.26931594 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.26931594 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224]\n",
      "One or more of the train scores are non-finite: [0.25734902 0.27740934 0.30806605 0.2659201  0.2659201  0.25937202\n",
      " 0.12285319 0.04930748 0.2659201  0.2659201  0.26147392 0.12285319\n",
      " 0.04930748 0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.25937202 0.27740934 0.30806605 0.2659201  0.2659201  0.25937202\n",
      " 0.12285319 0.04930748 0.2659201  0.2659201  0.26147392 0.12285319\n",
      " 0.04930748 0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.25937202 0.28129598 0.30806605 0.2659201  0.2659201  0.2613433\n",
      " 0.1222614  0.04930748 0.27091955 0.26687707 0.26147392 0.1222614\n",
      " 0.04930748 0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.25937202 0.28218502 0.30806605 0.2659201  0.2659201  0.26147392\n",
      " 0.29681453 0.23680748 0.27100077 0.2716977  0.26147392 0.29681453\n",
      " 0.23680748 0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.25983315 0.28244277 0.30599652 0.2666605  0.2666605  0.26147392\n",
      " 0.28332143 0.3381712  0.27100077 0.27249068 0.26147392 0.28332143\n",
      " 0.3381712  0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26053995 0.27746423 0.31372612 0.27030576 0.26917304 0.26138586\n",
      " 0.28455405 0.31330474 0.27030576 0.27196809 0.26147392 0.28455405\n",
      " 0.31330474 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27126128 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27126128 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.34123467464930873\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.32917705735660846\n",
      "GridSearchCV Runtime: 4.322758913040161 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2555066079295154\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2838709677419355\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2828282828282828\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.255\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23387096774193547\n",
      "Ave Test Precision: 0.26221536524833383\n",
      "Stdev Test Precision: 0.021179773847642008\n",
      "Ave Test Accuracy: 0.41716417910447756\n",
      "Stdev Test Accuracy: 0.1320125840346531\n",
      "Ave Test Specificity: 0.31683168316831684\n",
      "Ave Test Recall: 0.7242424242424242\n",
      "Ave Test NPV: 0.7529494953987454\n",
      "Ave Test F1-Score: 0.3772747639423549\n",
      "Ave Test G-mean: 0.4260411353913523\n",
      "Ave Runtime: 0.0075547218322753905\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with food. 80 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22070697 0.23867697 0.22114254 0.26786214 0.26786214 0.22494547\n",
      " 0.07344136 0.17344136 0.26786214 0.26786214 0.22186576 0.07344136\n",
      " 0.17344136 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22494547 0.23867697 0.22114254 0.26786214 0.26786214 0.22494547\n",
      " 0.07344136 0.17344136 0.26786214 0.26786214 0.22186576 0.07344136\n",
      " 0.17344136 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22494547 0.23867697 0.21958004 0.26786214 0.26786214 0.22494547\n",
      " 0.07344136 0.17344136 0.26478243 0.26786214 0.22186576 0.07344136\n",
      " 0.17344136 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22494547 0.23867697 0.22218861 0.26786214 0.26786214 0.22186576\n",
      " 0.31035767 0.17344136 0.26478243 0.26478243 0.22186576 0.31035767\n",
      " 0.17344136 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22494547 0.24596665 0.22427235 0.26478243 0.26786214 0.22186576\n",
      " 0.24867945 0.3627843  0.26478243 0.26478243 0.22186576 0.24867945\n",
      " 0.3627843  0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32480475 0.26478243 0.26478243 0.22186576\n",
      " 0.25495678 0.32650081 0.26478243 0.26478243 0.22186576 0.25495678\n",
      " 0.32650081 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243]\n",
      "One or more of the train scores are non-finite: [0.27293166 0.27203762 0.2289863  0.27405126 0.27405126 0.2708357\n",
      " 0.07371821 0.17177915 0.27405126 0.27405126 0.26731933 0.07371821\n",
      " 0.17177915 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26934312 0.27203762 0.2289863  0.27405126 0.27405126 0.2708357\n",
      " 0.07371821 0.17177915 0.27405126 0.27405126 0.26731933 0.07371821\n",
      " 0.17177915 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26934312 0.27203762 0.22996695 0.27405126 0.27405126 0.26746309\n",
      " 0.50571821 0.17177915 0.27154894 0.27405126 0.26731933 0.50571821\n",
      " 0.17177915 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26934312 0.27091939 0.23069112 0.27405126 0.27405126 0.26716148\n",
      " 0.25347926 0.37156967 0.27123273 0.27123273 0.26731933 0.25347926\n",
      " 0.37156967 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26903524 0.27238778 0.23070979 0.27344451 0.27405126 0.26731933\n",
      " 0.27415619 0.2363395  0.27123273 0.27123273 0.26731933 0.27415619\n",
      " 0.2363395  0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26716148 0.27380146 0.22906336 0.27123273 0.27155733 0.26731933\n",
      " 0.27519122 0.22908004 0.27123273 0.27123273 0.26731933 0.27519122\n",
      " 0.22908004 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273]\n",
      "invalid value encountered in scalar divide\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.36278429903429904\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.2456359102244389\n",
      "GridSearchCV Runtime: 4.206760406494141 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2608695652173913\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.14142764438676184\n",
      "Stdev Test Precision: 0.13104606857292384\n",
      "Ave Test Accuracy: 0.6395522388059701\n",
      "Stdev Test Accuracy: 0.22052207237899468\n",
      "Ave Test Specificity: 0.7752475247524753\n",
      "Ave Test Recall: 0.22424242424242422\n",
      "Ave Test NPV: 0.7536256779699038\n",
      "Ave Test F1-Score: 0.11653452409196982\n",
      "Ave Test G-mean: 0.09182818792766535\n",
      "Ave Runtime: 0.007882928848266602\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with hygiene. 79 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25379797 0.26464849 0.23497413 0.26018894 0.29240658 0.25379797\n",
      " 0.12094136 0.09938272 0.25379797 0.28320685 0.24902536 0.12094136\n",
      " 0.09938272 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.25379797 0.26464849 0.23497413 0.25379797 0.27681587 0.25379797\n",
      " 0.12094136 0.09938272 0.25379797 0.27681587 0.24902536 0.12094136\n",
      " 0.09938272 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.25379797 0.27828486 0.23497413 0.25379797 0.27681587 0.24902536\n",
      " 0.17094136 0.09938272 0.24829006 0.27071291 0.24902536 0.17094136\n",
      " 0.09938272 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.25379797 0.26099089 0.20738189 0.25379797 0.27681587 0.24902536\n",
      " 0.17122839 0.09905033 0.24902536 0.26985869 0.24902536 0.17122839\n",
      " 0.09905033 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24828953 0.22122226 0.171297   0.24828953 0.27246151 0.24902536\n",
      " 0.21264968 0.13648078 0.24902536 0.26985869 0.24902536 0.21264968\n",
      " 0.13648078 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.17021005 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.17177278 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.17177278 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.1686768  0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.1686768  0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.16945247 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.16945247 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.16945247 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.16945247 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.16945247 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.16945247 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869]\n",
      "One or more of the train scores are non-finite: [0.27184806 0.26902998 0.22825881 0.27334539 0.29048509 0.27184806\n",
      " 0.12302569 0.09812895 0.27184806 0.27794305 0.26632203 0.12302569\n",
      " 0.09812895 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.27184806 0.26902998 0.22825881 0.27184806 0.27644573 0.27184806\n",
      " 0.12302569 0.09812895 0.27184806 0.27644573 0.26632203 0.12302569\n",
      " 0.09812895 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.27184806 0.26641394 0.2292207  0.27184806 0.27644573 0.26632203\n",
      " 0.15635902 0.09812895 0.26595859 0.27429497 0.26632203 0.15635902\n",
      " 0.09812895 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.27184806 0.26305439 0.20069737 0.27184806 0.27644573 0.26632203\n",
      " 0.20419283 0.13792092 0.26632203 0.27002058 0.26632203 0.20419283\n",
      " 0.13792092 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26618094 0.23041002 0.17319579 0.26618094 0.27279994 0.26632203\n",
      " 0.22972563 0.14676467 0.26632203 0.27002058 0.26632203 0.22972563\n",
      " 0.14676467 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17310655 0.26632203 0.27000176 0.26632203\n",
      " 0.23015013 0.17107996 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17107996 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17305364 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17305364 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17307091 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17307091 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17307091 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17307091 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17307091 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17307091 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2924065776970142\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.5224438902743143\n",
      "GridSearchCV Runtime: 4.267999172210693 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.23484848484848486\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2222222222222222\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.296875\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.24242424242424243\n",
      "Ave Test Precision: 0.2392739898989899\n",
      "Stdev Test Precision: 0.03598817380081265\n",
      "Ave Test Accuracy: 0.5798507462686567\n",
      "Stdev Test Accuracy: 0.08053134416876992\n",
      "Ave Test Specificity: 0.6653465346534654\n",
      "Ave Test Recall: 0.31818181818181823\n",
      "Ave Test NPV: 0.747781710648281\n",
      "Ave Test F1-Score: 0.26426961926961934\n",
      "Ave Test G-mean: 0.442630007440229\n",
      "Ave Runtime: 0.005855178833007813\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseCleaning. 78 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25883009 0.19466016 0.17420361 0.25883009 0.25362049 0.25883009\n",
      " 0.09594136 0.07375    0.25883009 0.25362049 0.26032097 0.09594136\n",
      " 0.07375    0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.25883009 0.19466016 0.17420361 0.25883009 0.25362049 0.25883009\n",
      " 0.09594136 0.07375    0.25883009 0.25362049 0.26032097 0.09594136\n",
      " 0.07375    0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.25883009 0.19466016 0.1752381  0.25883009 0.25362049 0.26032097\n",
      " 0.09594136 0.07375    0.2615525  0.256974   0.26032097 0.09594136\n",
      " 0.07375    0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.25682815 0.19089105 0.16857143 0.25729624 0.25362049 0.26032097\n",
      " 0.16219136 0.07375    0.2615525  0.25387231 0.26032097 0.16219136\n",
      " 0.07375    0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.25656745 0.19598753 0.15715949 0.25939285 0.24018907 0.26032097\n",
      " 0.20517875 0.1675     0.2615525  0.25387231 0.26032097 0.20517875\n",
      " 0.1675     0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26085328 0.21355541 0.19587271 0.25873995 0.25452839 0.26032097\n",
      " 0.21355541 0.19920604 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19920604 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25430709 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231]\n",
      "One or more of the train scores are non-finite: [0.25688869 0.20314955 0.20350229 0.25688869 0.25188511 0.25688869\n",
      " 0.09851046 0.07368421 0.25688869 0.25278503 0.26671905 0.09851046\n",
      " 0.07368421 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.25688869 0.20314955 0.20350229 0.25688869 0.25278503 0.25681867\n",
      " 0.09851046 0.07368421 0.25681867 0.25278503 0.26671905 0.09851046\n",
      " 0.07368421 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.25667978 0.20343702 0.20330657 0.25667978 0.25278503 0.26671905\n",
      " 0.14851046 0.07368421 0.26723955 0.25615272 0.26671905 0.14851046\n",
      " 0.07368421 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.25804506 0.20605259 0.20888507 0.2575013  0.252715   0.26671905\n",
      " 0.21903108 0.07368421 0.26723955 0.26084006 0.26671905 0.21903108\n",
      " 0.07368421 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26056118 0.23688466 0.21439426 0.26133571 0.25424269 0.26671905\n",
      " 0.23771286 0.21041631 0.26723955 0.26084006 0.26671905 0.23771286\n",
      " 0.21041631 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.2654713  0.26550612 0.21328813 0.26611452 0.25900083 0.26671905\n",
      " 0.2654851  0.21134196 0.26712216 0.26084006 0.26671905 0.2654851\n",
      " 0.21134196 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26638066 0.26310415 0.20992275 0.26712216 0.26087086 0.26671905\n",
      " 0.26310415 0.21054331 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.21054331 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26671905 0.26310415 0.20992275 0.26712216 0.26088814 0.26671905\n",
      " 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.20992275 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905\n",
      " 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.20992275 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905\n",
      " 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.20992275 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905\n",
      " 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.20992275 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905\n",
      " 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.20992275 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905\n",
      " 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.20992275 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.26155249978779394\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.6047381546134664\n",
      "GridSearchCV Runtime: 4.25722336769104 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2549019607843137\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.27835051546391754\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.24175824175824176\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.23404255319148937\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3\n",
      "Ave Test Precision: 0.2618106542395925\n",
      "Stdev Test Precision: 0.027170249659907404\n",
      "Ave Test Accuracy: 0.5850746268656716\n",
      "Stdev Test Accuracy: 0.021980114141825602\n",
      "Ave Test Specificity: 0.6534653465346535\n",
      "Ave Test Recall: 0.37575757575757573\n",
      "Ave Test NPV: 0.7620807423408703\n",
      "Ave Test F1-Score: 0.30844415526120034\n",
      "Ave Test G-mean: 0.4949908535914968\n",
      "Ave Runtime: 0.00545797348022461\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with fare. 77 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.23426025 0.13545073 0.24588183 0.23426025 0.23887563 0.23988796\n",
      " 0.15166667 0.15479958 0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.15479958 0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.23426025 0.14261905 0.2289691  0.23426025 0.23887563 0.25592692\n",
      " 0.16119048 0.2038421  0.25088023 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.23151925 0.15140693 0.24822077 0.23151925 0.24001783 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.24455751 0.1747619  0.24576979 0.24455751 0.24272041 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.27105212 0.16190476 0.23064374 0.27105212 0.25353436 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.27441813 0.15666667 0.19967543 0.28118581 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513]\n",
      "One or more of the train scores are non-finite: [0.24205756 0.11975397 0.21464037 0.24205756 0.24220463 0.25516393\n",
      " 0.16954082 0.28793253 0.27680635 0.26334607 0.30185304 0.17676565\n",
      " 0.28793253 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.24205756 0.11716382 0.21505814 0.24205756 0.24220463 0.26266551\n",
      " 0.16021774 0.28371666 0.26231363 0.26334607 0.30185304 0.17676565\n",
      " 0.28371666 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.23991823 0.12664958 0.22896122 0.24085828 0.24242935 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27069079 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.25576426 0.15165947 0.25870406 0.25576426 0.24091985 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.28018316 0.16976304 0.28057304 0.28022261 0.2595521  0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.29912007 0.17681912 0.28097012 0.29935584 0.28038393 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30042424 0.17676565 0.28371666 0.30180954 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30185304 0.17676565 0.28371666 0.30255705 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.28951914098972925\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.7069825436408977\n",
      "GridSearchCV Runtime: 4.4770941734313965 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.15151515151515152\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.21739130434782608\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.24166666666666667\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2384937238493724\n",
      "Ave Test Precision: 0.21981336927580336\n",
      "Stdev Test Precision: 0.040025828773816614\n",
      "Ave Test Accuracy: 0.4388059701492537\n",
      "Stdev Test Accuracy: 0.21445650037872302\n",
      "Ave Test Specificity: 0.403960396039604\n",
      "Ave Test Recall: 0.5454545454545455\n",
      "Ave Test NPV: 0.695539950389547\n",
      "Ave Test F1-Score: 0.2760765286508996\n",
      "Ave Test G-mean: 0.29785410950910735\n",
      "Ave Runtime: 0.006041860580444336\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with parking. 76 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.2443672  0.27124112 0.22447406 0.2443672  0.2443672  0.2443672\n",
      " 0.08897707 0.09969136 0.2443672  0.2443672  0.27204502 0.09007597\n",
      " 0.09969136 0.26547744 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.2443672  0.27124112 0.22447406 0.2443672  0.2443672  0.24494345\n",
      " 0.08897707 0.09969136 0.24494345 0.24494345 0.27204502 0.09007597\n",
      " 0.09969136 0.26547744 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.2443672  0.2689684  0.22447406 0.2443672  0.2443672  0.27204502\n",
      " 0.19007597 0.09969136 0.26547744 0.26633093 0.27204502 0.19007597\n",
      " 0.09969136 0.26547744 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.24415743 0.26909486 0.23302469 0.24415743 0.24524982 0.27204502\n",
      " 0.24007597 0.09969136 0.26886206 0.26633093 0.27204502 0.24007597\n",
      " 0.09969136 0.26886206 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.24478859 0.30933718 0.28257974 0.25952073 0.25198024 0.27204502\n",
      " 0.30589724 0.34103335 0.26728311 0.26633093 0.27204502 0.30589724\n",
      " 0.34103335 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.2712353  0.30262768 0.35056739 0.27204502 0.26634559 0.27204502\n",
      " 0.30262768 0.34045431 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34045431 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.2638985  0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093]\n",
      "One or more of the train scores are non-finite: [0.24769496 0.26716439 0.20248286 0.24769496 0.24769496 0.24769496\n",
      " 0.09934148 0.09809494 0.24769496 0.24769496 0.2660345  0.10161709\n",
      " 0.09809494 0.26698393 0.2669056         nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.24769496 0.26716439 0.20248286 0.24769496 0.24769496 0.24760582\n",
      " 0.09934148 0.09809494 0.24760582 0.24760582 0.2660345  0.10161709\n",
      " 0.09809494 0.26698393 0.2669056         nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.24769496 0.26732322 0.20248286 0.24769496 0.24769496 0.2660345\n",
      " 0.46828376 0.09809494 0.26698393 0.2669056  0.2660345  0.46828376\n",
      " 0.09809494 0.26698393 0.2669056         nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.24695665 0.27144004 0.2085943  0.24695665 0.24763882 0.2660345\n",
      " 0.38292233 0.59791907 0.26658529 0.2669056  0.2660345  0.38292233\n",
      " 0.59791907 0.26658529 0.2669056         nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26150885 0.28259651 0.36911933 0.2626147  0.25081288 0.26599082\n",
      " 0.29078213 0.43506092 0.26668165 0.26687193 0.26599082 0.29078213\n",
      " 0.43506092 0.26668165 0.26687193        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26615734 0.28200875 0.3050342  0.26599082 0.26664882 0.26599082\n",
      " 0.28183421 0.31725231 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31725231 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26704662 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.35056738714628083\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7518703241895262\n",
      "GridSearchCV Runtime: 4.268959999084473 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.375\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.29545454545454547\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.5416666666666666\n",
      "Ave Test Precision: 0.24242424242424238\n",
      "Stdev Test Precision: 0.23847096240039925\n",
      "Ave Test Accuracy: 0.7358208955223882\n",
      "Stdev Test Accuracy: 0.029663592409480286\n",
      "Ave Test Specificity: 0.9475247524752476\n",
      "Ave Test Recall: 0.08787878787878788\n",
      "Ave Test NPV: 0.7611389992836168\n",
      "Ave Test F1-Score: 0.12126672126672126\n",
      "Ave Test G-mean: 0.21008904625998212\n",
      "Ave Runtime: 0.006798458099365234\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with gasoline. 75 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25827039 0.02321429 0.07635802 0.27394437 0.26626412 0.25827039\n",
      " 0.02321429 0.02469136 0.27220826 0.24248909 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23244166        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.25746621 0.02280702 0.07400508 0.27522247 0.26452271 0.25690596\n",
      " 0.02321429 0.02469136 0.26926702 0.24336987 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.25654416 0.02333333 0.08251699 0.27654416 0.26366249 0.25719773\n",
      " 0.02321429 0.02469136 0.25934311 0.24639281 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2532803  0.02419355 0.08897707 0.27248115 0.26399277 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.25030223 0.02424242 0.02469136 0.24954538 0.2623776  0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2510964  0.02424242 0.02469136 0.24419432 0.23509497 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23461361 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189]\n",
      "One or more of the train scores are non-finite: [0.25855778 0.02613636 0.09073148 0.25151968 0.24709273 0.25855778\n",
      " 0.02613636 0.25048393 0.25042606 0.24920974 0.25060776 0.0246875\n",
      " 0.25048393 0.24595344 0.24256093        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25757408 0.02592593 0.08969351 0.25047028 0.24744062 0.25789611\n",
      " 0.02613636 0.27625115 0.25087363 0.24966169 0.25060776 0.0246875\n",
      " 0.27625115 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25662588 0.02591241 0.09304259 0.24926598 0.24748724 0.25704059\n",
      " 0.02613636 0.26791781 0.25284674 0.25015454 0.25060776 0.0246875\n",
      " 0.26791781 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25343654 0.02508772 0.12538412 0.24566496 0.24531312 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25113646 0.02468553 0.23446405 0.24775836 0.24405847 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25113419 0.0246875  0.27908997 0.24646313 0.24403673 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27908997 0.24595344 0.24321057 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.0001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2765441574615398\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.0001, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.385286783042394\n",
      "GridSearchCV Runtime: 4.523259878158569 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.26066350710900477\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.19298245614035087\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.24861878453038674\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.24644549763033174\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.24878048780487805\n",
      "Ave Test Precision: 0.2394981466429904\n",
      "Stdev Test Precision: 0.02659556130848562\n",
      "Ave Test Accuracy: 0.4276119402985075\n",
      "Stdev Test Accuracy: 0.11149163819094386\n",
      "Ave Test Specificity: 0.35544554455445543\n",
      "Ave Test Recall: 0.6484848484848484\n",
      "Ave Test NPV: 0.7642530906445719\n",
      "Ave Test F1-Score: 0.33843623979729176\n",
      "Ave Test G-mean: 0.4208835336155188\n",
      "Ave Runtime: 0.005028438568115234\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with tuition. 74 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.23183073 0.09739055 0.14429487 0.23183073 0.22010912 0.23183073\n",
      " 0.04844136 0.0725     0.23183073 0.21998938 0.26451764 0.04844136\n",
      " 0.0725     0.26169099 0.25057988        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.23183073 0.09739055 0.14429487 0.23183073 0.22010912 0.23183073\n",
      " 0.04844136 0.0725     0.23520911 0.2219539  0.26451764 0.04844136\n",
      " 0.0725     0.26169099 0.25057988        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.23183073 0.09739055 0.14429487 0.23183073 0.22010912 0.26451764\n",
      " 0.04844136 0.0725     0.26169099 0.24248589 0.26451764 0.04844136\n",
      " 0.0725     0.26169099 0.25057988        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.23599151 0.10570682 0.14958333 0.23599151 0.22110196 0.26451764\n",
      " 0.15875    0.0725     0.26169099 0.25057988 0.26451764 0.15875\n",
      " 0.0725     0.26169099 0.25057988        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.24893497 0.17075376 0.43617063 0.25067789 0.21166582 0.26451764\n",
      " 0.22499267 0.37122294 0.26169099 0.25402191 0.26451764 0.22499267\n",
      " 0.37122294 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.34640386 0.26451764 0.25402191 0.26451764\n",
      " 0.24219697 0.34559368 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.34559368 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.34222551 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.34222551 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.34222551 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.32876397 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.33068705 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.33068705 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.31487508 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.31487508 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.31487508 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.31487508 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.31487508 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191]\n",
      "One or more of the train scores are non-finite: [0.23310758 0.09407757 0.23851104 0.23310758 0.23721101 0.23310758\n",
      " 0.04920298 0.07382271 0.23310758 0.23755242 0.26173823 0.04920298\n",
      " 0.07382271 0.26135432 0.26004765        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.23310758 0.09407757 0.23851104 0.23310758 0.23721101 0.23310758\n",
      " 0.04920298 0.07382271 0.23595968 0.23809542 0.26173823 0.04920298\n",
      " 0.07382271 0.26135432 0.26004765        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.23310758 0.09413918 0.26909712 0.23310758 0.23721101 0.26173823\n",
      " 0.04920298 0.07382271 0.26135432 0.25306727 0.26173823 0.04920298\n",
      " 0.07382271 0.26135432 0.26004765        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.23661837 0.13077268 0.28614416 0.23638037 0.23738106 0.26173823\n",
      " 0.13088497 0.07382271 0.26135432 0.26004765 0.26173823 0.13088497\n",
      " 0.07382271 0.26135432 0.26004765        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.24831876 0.2132835  0.32156428 0.24977083 0.237303   0.26182795\n",
      " 0.24671515 0.34375413 0.26145237 0.25978877 0.26182795 0.24671515\n",
      " 0.34375413 0.26145237 0.25978877        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.31650873 0.26182795 0.25910186 0.26182795\n",
      " 0.25921109 0.31846559 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.31846559 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.31177353 0.26145237 0.26000083 0.26182795\n",
      " 0.25921109 0.31177353 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.31177353 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.31036387 0.26145237 0.25990279 0.26182795\n",
      " 0.25921109 0.31088695 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.31088695 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "invalid value encountered in scalar divide\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.4361706349206349\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.743142144638404\n",
      "GridSearchCV Runtime: 4.321817398071289 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.13043478260869565\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.5\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.17534068786502272\n",
      "Stdev Test Precision: 0.2085303932824841\n",
      "Ave Test Accuracy: 0.6395522388059701\n",
      "Stdev Test Accuracy: 0.22156137040428409\n",
      "Ave Test Specificity: 0.7792079207920792\n",
      "Ave Test Recall: 0.2121212121212121\n",
      "Ave Test NPV: 0.751489731792167\n",
      "Ave Test F1-Score: 0.09840741517625687\n",
      "Ave Test G-mean: 0.06503152266274008\n",
      "Ave Runtime: 0.007324934005737305\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with allowance. 73 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.23915161 0.01111111 0.02469136 0.20563948 0.11380361 0.23915161\n",
      " 0.         0.02469136 0.20563948 0.24060616 0.22085467 0.\n",
      " 0.02469136 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.23915161 0.01111111 0.02469136 0.2315654  0.24060616 0.23915161\n",
      " 0.         0.02469136 0.2315654  0.24060616 0.22085467 0.\n",
      " 0.02469136 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.23425809 0.01111111 0.02469136 0.22509756 0.24231098 0.22085467\n",
      " 0.         0.02469136 0.21711914 0.23293405 0.22085467 0.\n",
      " 0.02469136 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.2350749  0.03119497 0.07469136 0.22178745 0.23579564 0.22085467\n",
      " 0.02407407 0.02469136 0.20449655 0.23293405 0.22085467 0.02407407\n",
      " 0.02469136 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22800567 0.06156009 0.12597403 0.20907094 0.23300876 0.22085467\n",
      " 0.06156009 0.12597403 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.12597403 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22261223 0.06156009 0.13264069 0.20369077 0.23177406 0.22085467\n",
      " 0.06156009 0.18264069 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.18264069 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405]\n",
      "One or more of the train scores are non-finite: [0.25301195 0.02176166 0.02458333 0.22168977 0.12709259 0.25301195\n",
      " 0.         0.02454924 0.22168977 0.2587137  0.24999426 0.\n",
      " 0.02454924 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.25301195 0.02176166 0.02458333 0.2479524  0.2587137  0.25301195\n",
      " 0.         0.02454924 0.2479524  0.2587137  0.24999426 0.\n",
      " 0.02454924 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.25071278 0.02176166 0.02458333 0.24681352 0.25747437 0.24999426\n",
      " 0.         0.02454924 0.24605186 0.25376518 0.24999426 0.\n",
      " 0.02454924 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.25141409 0.04908392 0.07681945 0.2468003  0.25671394 0.24999426\n",
      " 0.05130561 0.07458333 0.24644833 0.25376518 0.24999426 0.05130561\n",
      " 0.07458333 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.25091183 0.07905117 0.08474701 0.24718401 0.25416455 0.24999426\n",
      " 0.07980699 0.22641368 0.24644833 0.25376518 0.24999426 0.07980699\n",
      " 0.22641368 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.25031403 0.08070205 0.23884558 0.24679834 0.25412574 0.24999426\n",
      " 0.08070205 0.23733043 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.23733043 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.0001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.24231097870381357\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.0001, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.5798004987531172\n",
      "GridSearchCV Runtime: 4.324409246444702 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.17073170731707318\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.26424870466321243\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2529411764705882\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.26424870466321243\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2236842105263158\n",
      "Ave Test Precision: 0.2351709007280804\n",
      "Stdev Test Precision: 0.039658459887510863\n",
      "Ave Test Accuracy: 0.48358208955223886\n",
      "Stdev Test Accuracy: 0.08527568543362812\n",
      "Ave Test Specificity: 0.4673267326732673\n",
      "Ave Test Recall: 0.5333333333333334\n",
      "Ave Test NPV: 0.7661055793285055\n",
      "Ave Test F1-Score: 0.3161354752426607\n",
      "Ave Test G-mean: 0.45042779345007755\n",
      "Ave Runtime: 0.005036544799804687\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with uniform. 72 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.19749313 0.09319998 0.01666667 0.19169179 0.19179782 0.19749313\n",
      " 0.07125    0.         0.19086843 0.22394068 0.18113248 0.07125\n",
      " 0.         0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.19749313 0.09319998 0.01666667 0.19169179 0.22394068 0.1905567\n",
      " 0.07125    0.         0.19104604 0.21847576 0.18113248 0.07125\n",
      " 0.         0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.19110597 0.07785334 0.01818182 0.18140676 0.22063858 0.18113248\n",
      " 0.11028481 0.         0.20347054 0.20982682 0.18113248 0.10320148\n",
      " 0.         0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18818315 0.06467836 0.01666667 0.1820372  0.21221764 0.18113248\n",
      " 0.08588751 0.13333333 0.2091815  0.20982682 0.18113248 0.08588751\n",
      " 0.13333333 0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18058744 0.06407379 0.11666667 0.19589869 0.20464434 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20314657 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682]\n",
      "One or more of the train scores are non-finite: [0.23472969 0.11958355 0.05220779 0.19275756 0.21407182 0.23472969\n",
      " 0.07396122 0.         0.19198469 0.24042997 0.23895782 0.07396122\n",
      " 0.         0.20791805 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23472969 0.11958355 0.05220779 0.19275756 0.24042997 0.24168888\n",
      " 0.10729455 0.         0.20183193 0.24001078 0.23895782 0.10729455\n",
      " 0.         0.20791805 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.2312664  0.12279631 0.05441927 0.18707663 0.23951925 0.23895782\n",
      " 0.13933474 0.         0.20462025 0.2400415  0.23895782 0.13349288\n",
      " 0.         0.20791805 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23137596 0.13039311 0.07308612 0.18689335 0.23790163 0.23895782\n",
      " 0.13246526 0.40196078 0.20633856 0.2400415  0.23895782 0.13246526\n",
      " 0.40196078 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23433137 0.13122553 0.23507403 0.19647388 0.23793872 0.23895782\n",
      " 0.1334222  0.41834784 0.20480207 0.2400415  0.23895782 0.1334222\n",
      " 0.41834784 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20727682 0.23856999 0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415 ]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.35\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.7094763092269327\n",
      "GridSearchCV Runtime: 4.344894886016846 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2702702702702703\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.37037037037037035\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25396825396825395\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.0\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23868312757201646\n",
      "Ave Test Precision: 0.2266584044361822\n",
      "Stdev Test Precision: 0.13676583833665623\n",
      "Ave Test Accuracy: 0.6164179104477612\n",
      "Stdev Test Accuracy: 0.19263274275123088\n",
      "Ave Test Specificity: 0.7247524752475247\n",
      "Ave Test Recall: 0.28484848484848485\n",
      "Ave Test NPV: 0.7426375745142544\n",
      "Ave Test F1-Score: 0.20653901339412525\n",
      "Ave Test G-mean: 0.28761252002017845\n",
      "Ave Runtime: 0.0092132568359375\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with otherEducation. 71 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22961435 0.24108772 0.15644835 0.23396037 0.26509081 0.23314023\n",
      " 0.1475     0.07313272 0.23597832 0.25939727 0.22670954 0.1475\n",
      " 0.07313272 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.23314023 0.24108772 0.15644835 0.23597832 0.26455243 0.23314023\n",
      " 0.1475     0.07313272 0.23597832 0.25939727 0.22670954 0.1475\n",
      " 0.07313272 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.23314023 0.240535   0.15644835 0.23597832 0.26455243 0.23314023\n",
      " 0.1475     0.07313272 0.23597832 0.26455243 0.22670954 0.1475\n",
      " 0.07313272 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.23314023 0.23773166 0.15644835 0.23597832 0.26455243 0.22670954\n",
      " 0.18193223 0.07313272 0.22621429 0.25942117 0.22670954 0.18193223\n",
      " 0.07313272 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22705328 0.24042865 0.20619343 0.22989136 0.25846547 0.22670954\n",
      " 0.17952207 0.1431782  0.22621429 0.25942117 0.22670954 0.17952207\n",
      " 0.1431782  0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21952676 0.22655803 0.25942117 0.22670954\n",
      " 0.2409337  0.19407222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.19407222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117]\n",
      "One or more of the train scores are non-finite: [0.2497371  0.24929083 0.2176507  0.24931631 0.24881983 0.25169437\n",
      " 0.14736842 0.07375221 0.25136004 0.25073864 0.25225756 0.14736842\n",
      " 0.07375221 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25169437 0.24929083 0.2176507  0.25136004 0.25065451 0.25169437\n",
      " 0.14736842 0.07375221 0.25136004 0.25073864 0.25225756 0.14736842\n",
      " 0.07375221 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25169437 0.24924073 0.2176507  0.25136004 0.25065451 0.25145629\n",
      " 0.17464115 0.07375221 0.25136004 0.25065451 0.25225756 0.17464115\n",
      " 0.07375221 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25169437 0.24962085 0.21654493 0.25136004 0.25065451 0.25225756\n",
      " 0.20564076 0.07375221 0.25210893 0.2516254  0.25225756 0.20564076\n",
      " 0.07375221 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25280079 0.25055943 0.21038245 0.25270454 0.25199902 0.25225756\n",
      " 0.22309179 0.18535531 0.25210893 0.25157928 0.25225756 0.22309179\n",
      " 0.18535531 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.2521729  0.25161031 0.2107624  0.25210893 0.25235006 0.25225756\n",
      " 0.2515829  0.20399491 0.25210893 0.25157928 0.25225756 0.2515829\n",
      " 0.20399491 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25121951 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2650908137498526\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6172069825436409\n",
      "GridSearchCV Runtime: 4.515530586242676 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.3013698630136986\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2876712328767123\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.22388059701492538\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2569444444444444\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3235294117647059\n",
      "Ave Test Precision: 0.2786791098228973\n",
      "Stdev Test Precision: 0.038995354937258205\n",
      "Ave Test Accuracy: 0.6111940298507463\n",
      "Stdev Test Accuracy: 0.06856946110162625\n",
      "Ave Test Specificity: 0.695049504950495\n",
      "Ave Test Recall: 0.35454545454545455\n",
      "Ave Test NPV: 0.7671974865128453\n",
      "Ave Test F1-Score: 0.30500162141636705\n",
      "Ave Test G-mean: 0.48338157660770403\n",
      "Ave Runtime: 0.004467916488647461\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with emergency. 70 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24362231 0.3048146  0.14832308 0.24362231 0.24362231 0.24362231\n",
      " 0.09813272 0.07344136 0.24362231 0.24362231 0.24823343 0.09813272\n",
      " 0.07344136 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24362231 0.3048146  0.14832308 0.24362231 0.24362231 0.24362231\n",
      " 0.09813272 0.07344136 0.24362231 0.24362231 0.24823343 0.09813272\n",
      " 0.07344136 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24362231 0.3048146  0.14832308 0.24362231 0.24362231 0.24823343\n",
      " 0.24813272 0.07344136 0.24823343 0.24525275 0.24823343 0.24813272\n",
      " 0.07344136 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24362231 0.3048146  0.14832308 0.24362231 0.24362231 0.24823343\n",
      " 0.23146605 0.07344136 0.24823343 0.24641941 0.24823343 0.23146605\n",
      " 0.07344136 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24551362 0.25596009 0.18116547 0.24551362 0.24551362 0.24823343\n",
      " 0.290051   0.12852023 0.24823343 0.24641941 0.24823343 0.290051\n",
      " 0.12852023 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17582481 0.24752779 0.24641941 0.24823343\n",
      " 0.25596009 0.17682481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17682481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17582481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17582481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17057481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17057481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17057481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17057481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17057481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17057481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941]\n",
      "One or more of the train scores are non-finite: [0.24314208 0.23942567 0.15549678 0.24314208 0.24314208 0.24314208\n",
      " 0.09826745 0.07371821 0.24314208 0.24314208 0.24768942 0.09826745\n",
      " 0.07371821 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24314208 0.23942567 0.15549678 0.24314208 0.24314208 0.24314208\n",
      " 0.09826745 0.07371821 0.24314208 0.24314208 0.24768942 0.09826745\n",
      " 0.07371821 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24314208 0.23942567 0.15549678 0.24314208 0.24314208 0.24748505\n",
      " 0.17857435 0.07371821 0.24747103 0.24498825 0.24768942 0.17857435\n",
      " 0.07371821 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24314208 0.23969779 0.15549678 0.24314208 0.24314208 0.24768942\n",
      " 0.24148784 0.07375221 0.24768093 0.24692449 0.24768942 0.24148784\n",
      " 0.07375221 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24506451 0.24205973 0.2331001  0.24506451 0.24400422 0.24768942\n",
      " 0.24518544 0.15589581 0.24768093 0.24692449 0.24768942 0.24518544\n",
      " 0.15589581 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24737061 0.241635   0.22517502 0.24712915 0.24628106 0.24768942\n",
      " 0.24140575 0.22509755 0.24768093 0.24659988 0.24768942 0.24140575\n",
      " 0.22509755 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22812942 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22812942 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22797893 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22797893 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22797893 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22797893 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22797893 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22797893 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22797893 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22797893 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22797893 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22797893 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22797893 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22797893 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.30481460434358987\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.5972568578553616\n",
      "GridSearchCV Runtime: 4.193583250045776 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2388888888888889\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.26582278481012656\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.22404371584699453\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.19575107790920201\n",
      "Stdev Test Precision: 0.11049074424610782\n",
      "Ave Test Accuracy: 0.5567164179104478\n",
      "Stdev Test Accuracy: 0.16147277584832945\n",
      "Ave Test Specificity: 0.6188118811881188\n",
      "Ave Test Recall: 0.3666666666666667\n",
      "Ave Test NPV: 0.7430113565100396\n",
      "Ave Test F1-Score: 0.24294395671578073\n",
      "Ave Test G-mean: 0.35871829660766386\n",
      "Ave Runtime: 0.007434844970703125\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with medicine. 69 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27703768 0.26408678 0.27255194 0.27703768 0.27703768 0.27703768\n",
      " 0.20219136 0.09844136 0.27703768 0.27703768 0.28411582 0.20219136\n",
      " 0.09844136 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.27703768 0.26408678 0.27549312 0.27703768 0.27703768 0.27703768\n",
      " 0.20219136 0.09844136 0.27703768 0.27703768 0.28411582 0.20219136\n",
      " 0.09844136 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.27703768 0.26408678 0.27549312 0.27703768 0.27703768 0.28411582\n",
      " 0.20219136 0.09844136 0.28150191 0.27986796 0.28411582 0.20219136\n",
      " 0.09844136 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.27472039 0.27231329 0.27500332 0.27472039 0.27703768 0.28411582\n",
      " 0.22069302 0.09620578 0.28150191 0.28150191 0.28411582 0.22069302\n",
      " 0.09620578 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.27370858 0.2751261  0.27588681 0.27437146 0.2727562  0.28065255\n",
      " 0.2761052  0.30370767 0.28150191 0.28150191 0.28065255 0.2761052\n",
      " 0.30370767 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.27875908 0.27758097 0.26768379 0.28150191 0.27937656 0.28065255\n",
      " 0.27758097 0.26980981 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26980981 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28109898 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121]\n",
      "One or more of the train scores are non-finite: [0.2779338  0.25848598 0.25436941 0.2779338  0.2779338  0.2779338\n",
      " 0.19949241 0.09823345 0.2779338  0.2779338  0.28170213 0.19928608\n",
      " 0.09823345 0.28433758 0.28422363        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.2779338  0.25848598 0.25436941 0.2779338  0.2779338  0.2779338\n",
      " 0.19925099 0.09823345 0.2779338  0.2779338  0.28170213 0.19928608\n",
      " 0.09823345 0.28433758 0.28422363        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.2779338  0.25847031 0.25436941 0.2779338  0.2779338  0.28170213\n",
      " 0.23564972 0.19823345 0.28433758 0.28231264 0.28170213 0.23564972\n",
      " 0.19823345 0.28433758 0.28422363        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27598508 0.2620282  0.25441456 0.27574367 0.2779338  0.28170213\n",
      " 0.26950983 0.4820651  0.28411485 0.28422363 0.28170213 0.26950983\n",
      " 0.4820651  0.28411485 0.28422363        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.2754909  0.26077026 0.36560638 0.27588176 0.27477941 0.27959101\n",
      " 0.26127643 0.36417613 0.28388674 0.28411268 0.27959101 0.26127643\n",
      " 0.36417613 0.28388674 0.28411268        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27734264 0.2608411  0.31662053 0.28377159 0.28234059 0.27959101\n",
      " 0.26093962 0.29965528 0.28377159 0.28366105 0.27959101 0.26093962\n",
      " 0.29965528 0.28377159 0.28366105        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.2787257  0.26093962 0.29912548 0.28377159 0.28343416 0.27959101\n",
      " 0.26093962 0.29974555 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29974555 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101\n",
      " 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29912548 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101\n",
      " 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29912548 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101\n",
      " 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29912548 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101\n",
      " 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29912548 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101\n",
      " 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29912548 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101\n",
      " 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29912548 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931]\n",
      "invalid value encountered in scalar divide\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "invalid value encountered in scalar divide\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3037076689150119\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.2456359102244389\n",
      "GridSearchCV Runtime: 4.3672285079956055 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.5\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2317596566523605\n",
      "Ave Test Precision: 0.24485939401703924\n",
      "Stdev Test Precision: 0.1769381437264725\n",
      "Ave Test Accuracy: 0.45746268656716416\n",
      "Stdev Test Accuracy: 0.2709736329145468\n",
      "Ave Test Specificity: 0.42079207920792083\n",
      "Ave Test Recall: 0.5696969696969697\n",
      "Ave Test NPV: 0.7228166526673988\n",
      "Ave Test F1-Score: 0.24175320643948606\n",
      "Ave Test G-mean: 0.09568677045764229\n",
      "Ave Runtime: 0.007575035095214844\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with water. 68 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.30084728 0.26722619 0.31690021 0.30084728 0.30084728 0.30084728\n",
      " 0.24563272 0.02469136 0.30084728 0.30084728 0.30270794 0.24563272\n",
      " 0.02469136 0.30161138 0.30862137        nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.30084728 0.26722619 0.31690021 0.30084728 0.30084728 0.30084728\n",
      " 0.24563272 0.02469136 0.30084728 0.30084728 0.30270794 0.24563272\n",
      " 0.02469136 0.30161138 0.30862137        nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.30084728 0.26722619 0.32244716 0.30084728 0.30084728 0.30270794\n",
      " 0.24563272 0.02469136 0.30161138 0.30084728 0.30270794 0.24563272\n",
      " 0.02469136 0.30161138 0.30862137        nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.30084728 0.26890075 0.32231729 0.30084728 0.30084728 0.30270794\n",
      " 0.25789579 0.02469136 0.30161138 0.30862137 0.30270794 0.25789579\n",
      " 0.02469136 0.30161138 0.30862137        nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28017711 0.30968742 0.3019967  0.30084728 0.30270794\n",
      " 0.28110355 0.2766358  0.30161138 0.3066983  0.30270794 0.28110355\n",
      " 0.2766358  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30391978 0.30425113 0.3019967\n",
      " 0.28927676 0.38760422 0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.38760422 0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983 ]\n",
      "One or more of the train scores are non-finite: [0.2993445  0.26921295 0.31304315 0.2993445  0.2993445  0.2993445\n",
      " 0.24563587 0.02454924 0.2993445  0.2993445  0.30092026 0.24563587\n",
      " 0.02454924 0.30209065 0.30197741        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.2993445  0.26927297 0.31304315 0.2993445  0.2993445  0.2993445\n",
      " 0.24563587 0.02454924 0.2993445  0.2993445  0.30092026 0.24563587\n",
      " 0.02454924 0.30209065 0.30197741        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.2993445  0.26950675 0.31337255 0.2993445  0.2993445  0.30092026\n",
      " 0.24563587 0.02454924 0.30209065 0.29957592 0.30092026 0.24563587\n",
      " 0.02454924 0.30209065 0.30197741        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.2993445  0.27088958 0.30847678 0.2993445  0.2993445  0.30092026\n",
      " 0.25929405 0.12454924 0.30209065 0.30197741 0.30092026 0.25929405\n",
      " 0.12454924 0.30209065 0.30197741        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28136321 0.31505016 0.30036814 0.29967903 0.30092026\n",
      " 0.28173623 0.35459817 0.30209065 0.30156868 0.30092026 0.28173623\n",
      " 0.35459817 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28635351 0.31498771 0.30077686 0.30129698 0.30036814\n",
      " 0.28646615 0.32299389 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.32299389 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3996855953375717\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.6770573566084788\n",
      "GridSearchCV Runtime: 4.312411546707153 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.21739130434782608\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.16666666666666666\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3018867924528302\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.5\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2553191489361702\n",
      "Ave Test Precision: 0.2882527824806986\n",
      "Stdev Test Precision: 0.12836659146313203\n",
      "Ave Test Accuracy: 0.6985074626865672\n",
      "Stdev Test Accuracy: 0.04441709295508351\n",
      "Ave Test Specificity: 0.8831683168316831\n",
      "Ave Test Recall: 0.13333333333333336\n",
      "Ave Test NPV: 0.7572640010459165\n",
      "Ave Test F1-Score: 0.1638450194547622\n",
      "Ave Test G-mean: 0.31578677779821496\n",
      "Ave Runtime: 0.006726264953613281\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with electricity. 67 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.13456819 0.06832298 0.10737292 0.12812031 0.20795319 0.24354221\n",
      " 0.0725     0.09844136 0.2212469  0.24285977 0.22830941 0.0725\n",
      " 0.09844136 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.24330171 0.06776794 0.1068202  0.25616763 0.26854679 0.2445783\n",
      " 0.0725     0.09844136 0.25548535 0.27085052 0.22830941 0.0725\n",
      " 0.09844136 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.2435127  0.06743668 0.1068202  0.25448665 0.26756998 0.23069036\n",
      " 0.07280063 0.09844136 0.24799519 0.26290011 0.22830941 0.07280063\n",
      " 0.09844136 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.24287845 0.06949424 0.10345012 0.25340943 0.26756998 0.22830941\n",
      " 0.06702151 0.09844136 0.25005408 0.26645323 0.22830941 0.06702151\n",
      " 0.09844136 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.23507194 0.06834248 0.13968689 0.2524277  0.26763699 0.22830941\n",
      " 0.0655042  0.13768355 0.25005408 0.26645323 0.22830941 0.0655042\n",
      " 0.13768355 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22873458 0.11760655 0.18919476 0.25069511 0.27156738 0.22830941\n",
      " 0.11807563 0.18606777 0.25069511 0.26645323 0.22830941 0.11807563\n",
      " 0.18606777 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26755617 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323]\n",
      "One or more of the train scores are non-finite: [0.15474158 0.07597141 0.0980444  0.12377978 0.1952009  0.2580131\n",
      " 0.07382271 0.09823345 0.22870244 0.22097938 0.24632226 0.07382271\n",
      " 0.09823345 0.24377289 0.23971829        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.25570475 0.07582219 0.09804562 0.25274009 0.24600411 0.25529638\n",
      " 0.07385691 0.09823345 0.25265878 0.24642723 0.24632226 0.07385691\n",
      " 0.09823345 0.24377289 0.23971829        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.25508241 0.07582219 0.09804562 0.25144994 0.24522893 0.24449084\n",
      " 0.07378135 0.09833564 0.24796166 0.24118465 0.24632226 0.07378135\n",
      " 0.09833564 0.24377289 0.23971829        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.25456412 0.0754023  0.09671405 0.2507434  0.24522893 0.24632226\n",
      " 0.07234392 0.09822825 0.24377289 0.23976884 0.24632226 0.07234392\n",
      " 0.09822825 0.24377289 0.23976884        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.25124798 0.07285838 0.1459082  0.24777424 0.24413689 0.24643316\n",
      " 0.07195599 0.29679458 0.24377289 0.23986972 0.24643316 0.07195599\n",
      " 0.29679458 0.24377289 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24654888 0.12295523 0.23062    0.24398949 0.24165306 0.24643316\n",
      " 0.12320939 0.27139703 0.24382803 0.23986972 0.24643316 0.12320939\n",
      " 0.27139703 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.24055666 0.24643316\n",
      " 0.12285289 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316\n",
      " 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316\n",
      " 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316\n",
      " 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316\n",
      " 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316\n",
      " 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316\n",
      " 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.27156738349006126\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.5498753117206983\n",
      "GridSearchCV Runtime: 4.375663757324219 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.216\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.22330097087378642\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25210084033613445\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.22772277227722773\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.24561403508771928\n",
      "Ave Test Precision: 0.23294772371497358\n",
      "Stdev Test Precision: 0.01528776158500623\n",
      "Ave Test Accuracy: 0.5298507462686567\n",
      "Stdev Test Accuracy: 0.023599087016181915\n",
      "Ave Test Specificity: 0.5732673267326733\n",
      "Ave Test Recall: 0.396969696969697\n",
      "Ave Test NPV: 0.7441635303436533\n",
      "Ave Test F1-Score: 0.2931592798866548\n",
      "Ave Test G-mean: 0.47561255982375117\n",
      "Ave Runtime: 0.005628728866577148\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with rent. 66 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.21453101 0.12824409 0.11690572 0.21453101 0.18640601 0.21453101\n",
      " 0.         0.09969136 0.21453101 0.18640601 0.22769151 0.\n",
      " 0.09969136 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.21453101 0.12583029 0.11690572 0.21453101 0.20932268 0.21162008\n",
      " 0.04       0.09969136 0.24075663 0.21050787 0.22769151 0.04\n",
      " 0.09969136 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.21152025 0.12755081 0.11690572 0.21152025 0.20896803 0.22769151\n",
      " 0.17714286 0.09969136 0.22769151 0.23222058 0.22769151 0.17714286\n",
      " 0.09969136 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.21345188 0.12878119 0.11592533 0.21345188 0.20795737 0.22769151\n",
      " 0.15285548 0.14905063 0.22769151 0.23801047 0.22769151 0.15285548\n",
      " 0.14905063 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.21843661 0.16392831 0.1885759  0.23071164 0.21719924 0.22769151\n",
      " 0.16812158 0.1949561  0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.1949561  0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16713344 0.22828943 0.22769151 0.23226817 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16713344 0.26162276 0.22769151 0.23324856 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047]\n",
      "One or more of the train scores are non-finite: [0.21597925 0.16611704 0.20401966 0.21597925 0.19533705 0.21597925\n",
      " 0.         0.09809494 0.21597925 0.19533705 0.22650063 0.\n",
      " 0.09809494 0.22928213 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.21597925 0.16611704 0.20401966 0.21597925 0.22152753 0.22384554\n",
      " 0.126      0.09809494 0.22395148 0.22177182 0.22650063 0.126\n",
      " 0.09809494 0.22928213 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.21547441 0.16639596 0.18735299 0.21547441 0.2241393  0.22650063\n",
      " 0.20018544 0.09809494 0.22928213 0.23154032 0.22650063 0.20018544\n",
      " 0.09809494 0.22928213 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.21734971 0.16551967 0.19838389 0.21734971 0.22251917 0.22650063\n",
      " 0.26206217 0.29620715 0.22791524 0.23275055 0.22650063 0.26206217\n",
      " 0.29620715 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22410146 0.19490465 0.31705466 0.22377456 0.22431032 0.22650063\n",
      " 0.24681458 0.31294728 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31294728 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22503123 0.24546136 0.30425484 0.22503123 0.22903937 0.22650063\n",
      " 0.24681458 0.30253674 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.30253674 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24546136 0.31182406 0.22791524 0.23182747 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.26162276200250884\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.7418952618453866\n",
      "GridSearchCV Runtime: 4.3937859535217285 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.46153846153846156\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.42857142857142855\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3076923076923077\n",
      "Ave Test Precision: 0.3633699633699634\n",
      "Stdev Test Precision: 0.07733189809993642\n",
      "Ave Test Accuracy: 0.7447761194029849\n",
      "Stdev Test Accuracy: 0.006243731541299092\n",
      "Ave Test Specificity: 0.9693069306930692\n",
      "Ave Test Recall: 0.05757575757575757\n",
      "Ave Test NPV: 0.7589708816345524\n",
      "Ave Test F1-Score: 0.09738891695126947\n",
      "Ave Test G-mean: 0.22534962951358578\n",
      "Ave Runtime: 0.0070880413055419925\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with repair. 65 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.26786339 0.26786339 0.27696409 0.26786339 0.26786339 0.30469697\n",
      " 0.32722222 0.26898009 0.30469697 0.2980303  0.30469697 0.32722222\n",
      " 0.26898009 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.26786339 0.26786339 0.28982123 0.26786339 0.26786339 0.30469697\n",
      " 0.32722222 0.26909973 0.30469697 0.2980303  0.30469697 0.32722222\n",
      " 0.26909973 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.28660173 0.28660173 0.2790853  0.28660173 0.26786339 0.30469697\n",
      " 0.32722222 0.26687751 0.30469697 0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.29772727 0.29772727 0.26034071 0.29772727 0.2812987  0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.30469697 0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.29772727 0.29530303 0.26088485 0.29772727 0.29772727 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.31136364 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697]\n",
      "One or more of the train scores are non-finite: [0.27100136 0.27100136 0.2586463  0.27100136 0.27100136 0.28755412\n",
      " 0.29923365 0.34632661 0.28755412 0.28644528 0.28755412 0.29923365\n",
      " 0.34632661 0.28755412 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.27100136 0.27100136 0.26086892 0.27100136 0.27100136 0.28740222\n",
      " 0.29868538 0.34089897 0.28755412 0.28644528 0.28740222 0.29868538\n",
      " 0.34089897 0.28755412 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28428323 0.28428323 0.26725007 0.28428323 0.27100136 0.28740222\n",
      " 0.29868538 0.3384327  0.28740222 0.28644528 0.28740222 0.29868538\n",
      " 0.3384327  0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28589184 0.28589184 0.2983294  0.28589184 0.27757888 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28740222 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28589184 0.28936177 0.32869123 0.28589184 0.28589184 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29955861 0.33956495 0.28716172 0.28608023 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.32722222222222225\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7069825436408977\n",
      "GridSearchCV Runtime: 4.731696844100952 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2727272727272727\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.36363636363636365\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.21428571428571427\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.15384615384615385\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3076923076923077\n",
      "Ave Test Precision: 0.26243756243756244\n",
      "Stdev Test Precision: 0.08140295137508442\n",
      "Ave Test Accuracy: 0.7052238805970149\n",
      "Stdev Test Accuracy: 0.018279774199874463\n",
      "Ave Test Specificity: 0.9009900990099011\n",
      "Ave Test Recall: 0.10606060606060605\n",
      "Ave Test NPV: 0.7551613364384039\n",
      "Ave Test F1-Score: 0.15043310066436802\n",
      "Ave Test G-mean: 0.30636184996106863\n",
      "Ave Runtime: 0.010785865783691406\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with cinema. 64 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27054528 0.26776102 0.22644098 0.27054528 0.27054528 0.27054528\n",
      " 0.0725     0.09469136 0.27054528 0.27054528 0.28673775 0.0725\n",
      " 0.09135802 0.30285286 0.28472961        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27054528 0.26776102 0.22644098 0.27054528 0.27054528 0.27170358\n",
      " 0.0725     0.09469136 0.27054528 0.27054528 0.28673775 0.0725\n",
      " 0.09135802 0.30285286 0.28472961        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27054528 0.26776102 0.22644098 0.27054528 0.27054528 0.28673775\n",
      " 0.26582317 0.09135802 0.30285286 0.28472961 0.28673775 0.26582317\n",
      " 0.09135802 0.30285286 0.28472961        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.26780132 0.27475768 0.19310765 0.26780132 0.27054528 0.27909069\n",
      " 0.27255934 0.12885802 0.29232918 0.28472961 0.27909069 0.27255934\n",
      " 0.12885802 0.29232918 0.28472961        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27739796 0.29563318 0.25977431 0.27761867 0.27743786 0.27909069\n",
      " 0.30374901 0.20538101 0.28673775 0.28034364 0.27909069 0.30374901\n",
      " 0.20538101 0.28673775 0.28034364        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.2750677  0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194]\n",
      "One or more of the train scores are non-finite: [0.27343909 0.27916496 0.27162953 0.27343909 0.27343909 0.27343909\n",
      " 0.07382271 0.10215114 0.27343909 0.27343909 0.28128833 0.07382271\n",
      " 0.10467727 0.29252973 0.29444698        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.27343909 0.27916496 0.27162953 0.27343909 0.27343909 0.27430715\n",
      " 0.17382271 0.10215114 0.27343909 0.27343909 0.28128833 0.17382271\n",
      " 0.10467727 0.29252973 0.29444698        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.27343909 0.27916496 0.27162953 0.27343909 0.27343909 0.28128833\n",
      " 0.49374892 0.20467727 0.29252973 0.29444698 0.28128833 0.49374892\n",
      " 0.20467727 0.29252973 0.29444698        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.27614715 0.28342558 0.27879074 0.27614715 0.27343909 0.28009999\n",
      " 0.31048781 0.53922272 0.28991991 0.29444698 0.28009999 0.31048781\n",
      " 0.53922272 0.28991991 0.29444698        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.27923676 0.29016958 0.26279997 0.27889477 0.27925172 0.28009999\n",
      " 0.30132882 0.38630113 0.28307956 0.29136289 0.28009999 0.30132882\n",
      " 0.38630113 0.28307956 0.29136289        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.27992724 0.29332931 0.26429987 0.28172253 0.28236627 0.28009999\n",
      " 0.29370965 0.36467168 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36467168 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "invalid value encountered in scalar divide\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "invalid value encountered in scalar divide\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.31140247007894073\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.6670822942643392\n",
      "GridSearchCV Runtime: 4.43420934677124 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.375\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.30597014925373134\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.46153846153846156\n",
      "Ave Test Precision: 0.32700918484500574\n",
      "Stdev Test Precision: 0.09203525331426364\n",
      "Ave Test Accuracy: 0.5022388059701492\n",
      "Stdev Test Accuracy: 0.24416218056155595\n",
      "Ave Test Specificity: 0.47128712871287126\n",
      "Ave Test Recall: 0.5969696969696969\n",
      "Ave Test NPV: 0.7866522999973395\n",
      "Ave Test F1-Score: 0.3336214739076145\n",
      "Ave Test G-mean: 0.27142169529888394\n",
      "Ave Runtime: 0.008639717102050781\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with dineOut. 63 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25212008 0.19656579 0.09843335 0.25212008 0.25212008 0.25836629\n",
      " 0.14625    0.09938272 0.2567063  0.25934126 0.26852693 0.14625\n",
      " 0.09938272 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.25212008 0.19656579 0.09843335 0.25212008 0.25212008 0.25445933\n",
      " 0.14625    0.09938272 0.26852693 0.26033841 0.26852693 0.14625\n",
      " 0.09938272 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.25685241 0.19673447 0.09843335 0.25685241 0.26021731 0.26852693\n",
      " 0.15075    0.09938272 0.26852693 0.26852693 0.26852693 0.15075\n",
      " 0.09938272 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.25616404 0.29324002 0.09905804 0.25616404 0.25811024 0.26852693\n",
      " 0.21483999 0.09938272 0.26852693 0.26852693 0.26852693 0.21483999\n",
      " 0.09938272 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26933098 0.24875073 0.18124312 0.27029872 0.26798947 0.26852693\n",
      " 0.25253755 0.17560005 0.26852693 0.26852693 0.26852693 0.25253755\n",
      " 0.17560005 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.3193323  0.34372609 0.26852693 0.26864003 0.26852693\n",
      " 0.32025184 0.39339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.39339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.39339337 0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.39339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.39339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.39339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.39339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.3902565  0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.3902565  0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.3902565  0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.3902565  0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.3902565  0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693]\n",
      "One or more of the train scores are non-finite: [0.2528304  0.20023111 0.09727893 0.2528304  0.2528304  0.25824666\n",
      " 0.14750693 0.09812895 0.25871207 0.25798377 0.26819894 0.14750693\n",
      " 0.09812895 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.2528304  0.20023111 0.09727893 0.2528304  0.2528304  0.26607426\n",
      " 0.14750693 0.09812895 0.26819894 0.25838446 0.26819894 0.14750693\n",
      " 0.09812895 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.25679526 0.20446646 0.09727893 0.25679526 0.26035095 0.26819894\n",
      " 0.15174111 0.09812895 0.26819894 0.26819894 0.26819894 0.15174111\n",
      " 0.09812895 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.25686927 0.24041688 0.09718618 0.25686927 0.25835329 0.26819894\n",
      " 0.22726003 0.09812895 0.26819894 0.26819894 0.26819894 0.22726003\n",
      " 0.09812895 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.27009128 0.32646413 0.19096961 0.27030177 0.26716475 0.26819894\n",
      " 0.32725985 0.24032938 0.26819894 0.26819894 0.26819894 0.32725985\n",
      " 0.24032938 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30387564 0.32747447 0.26819894 0.2696211  0.26819894\n",
      " 0.30399233 0.30934013 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30934013 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30335722 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30335722 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30335722 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30335722 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30335722 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30316783 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30316783 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30316783 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30316783 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30316783 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3933933663709828\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.25561097256857856\n",
      "GridSearchCV Runtime: 4.4185075759887695 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.24609375\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.30612244897959184\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.4117647058823529\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2222222222222222\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.35\n",
      "Ave Test Precision: 0.30724062541683345\n",
      "Stdev Test Precision: 0.07700968621779661\n",
      "Ave Test Accuracy: 0.6238805970149254\n",
      "Stdev Test Accuracy: 0.199712284696006\n",
      "Ave Test Specificity: 0.7257425742574257\n",
      "Ave Test Recall: 0.31212121212121213\n",
      "Ave Test NPV: 0.7611986702552921\n",
      "Ave Test F1-Score: 0.23604753009459611\n",
      "Ave Test G-mean: 0.32517984198485406\n",
      "Ave Runtime: 0.008108282089233398\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with leisure. 62 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24590355 0.22560591 0.16421673 0.24085076 0.24724523 0.24590355\n",
      " 0.12319851 0.         0.24221374 0.24724523 0.27214045 0.12088272\n",
      " 0.         0.25246785 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.24590355 0.22560591 0.16421673 0.24221374 0.24464648 0.24590355\n",
      " 0.12319851 0.         0.24221374 0.24464648 0.27214045 0.12088272\n",
      " 0.         0.25246785 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.24590355 0.22560591 0.16421673 0.24221374 0.24464648 0.26090496\n",
      " 0.12088272 0.         0.25238875 0.24290045 0.27214045 0.12088272\n",
      " 0.         0.25246785 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.25029594 0.23057667 0.16878816 0.24172451 0.24464648 0.27214045\n",
      " 0.29528748 0.1        0.25246785 0.26204622 0.27214045 0.29528748\n",
      " 0.1        0.25246785 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.26084919 0.22011784 0.16878816 0.24852151 0.25867546 0.26423569\n",
      " 0.21581542 0.21607143 0.25403307 0.26204622 0.26423569 0.21581542\n",
      " 0.21607143 0.25403307 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.26259113 0.22214213 0.20255291 0.25082755 0.25872266 0.2638219\n",
      " 0.22214213 0.14255291 0.253326   0.26027811 0.2638219  0.22214213\n",
      " 0.14255291 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.24999266 0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811]\n",
      "One or more of the train scores are non-finite: [0.24803383 0.22690808 0.20685484 0.24805236 0.24736634 0.24803383\n",
      " 0.12304779 0.         0.24792011 0.24736634 0.26365807 0.1250453\n",
      " 0.         0.26469351 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.24803383 0.22690808 0.20685484 0.24792011 0.24766315 0.24803383\n",
      " 0.22304779 0.         0.24792011 0.24766315 0.26365807 0.2250453\n",
      " 0.         0.26469351 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.24803383 0.22690808 0.20685484 0.24792011 0.24766315 0.26027013\n",
      " 0.34587863 0.         0.26410437 0.25170813 0.26365807 0.34587863\n",
      " 0.         0.26469351 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.25209562 0.22823696 0.20671549 0.25213915 0.24766315 0.26365807\n",
      " 0.24253461 0.475      0.26469351 0.26341731 0.26365807 0.24253461\n",
      " 0.475      0.26469351 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26102205 0.23183243 0.2103165  0.26026459 0.2532693  0.26378117\n",
      " 0.24372824 0.20457418 0.26425056 0.26341731 0.26378117 0.24372824\n",
      " 0.20457418 0.26425056 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26339015 0.23363069 0.19851198 0.26211968 0.26104941 0.26350056\n",
      " 0.23373943 0.40362839 0.2635985  0.26295811 0.26350056 0.23373943\n",
      " 0.40362839 0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.39763052 0.26358225 0.26295811 0.26350056\n",
      " 0.23356884 0.49763052 0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.49763052 0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.4974116  0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.4974116  0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.4974116  0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.4974116  0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.4974116  0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.4974116  0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811]\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2952874779541447\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.2456359102244389\n",
      "GridSearchCV Runtime: 4.4820451736450195 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.24436090225563908\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.21428571428571427\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2462686567164179\n",
      "Ave Test Precision: 0.23949051733812143\n",
      "Stdev Test Precision: 0.014114108778543692\n",
      "Ave Test Accuracy: 0.3358208955223881\n",
      "Stdev Test Accuracy: 0.20024489350744387\n",
      "Ave Test Specificity: 0.1792079207920792\n",
      "Ave Test Recall: 0.8151515151515152\n",
      "Ave Test NPV: 0.625\n",
      "Ave Test F1-Score: 0.34097091640865923\n",
      "Ave Test G-mean: 0.0708887824545379\n",
      "Ave Runtime: 0.007739591598510742\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with personalCare. 61 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25432036 0.27085759 0.24127941 0.25432036 0.25432036 0.25432036\n",
      " 0.07375    0.12344136 0.25432036 0.25432036 0.26883361 0.07375\n",
      " 0.12344136 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.25432036 0.27085759 0.24127941 0.25432036 0.25432036 0.25432036\n",
      " 0.07375    0.12344136 0.25432036 0.25195293 0.26883361 0.07375\n",
      " 0.12344136 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.25432036 0.27085759 0.24127941 0.25432036 0.25432036 0.26883361\n",
      " 0.12375    0.12344136 0.26883361 0.26161245 0.26883361 0.12375\n",
      " 0.12344136 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.25482456 0.26936545 0.28127941 0.25500331 0.25432036 0.26883361\n",
      " 0.32508127 0.29010802 0.26883361 0.26883361 0.26883361 0.32508127\n",
      " 0.29010802 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26746641 0.26685534 0.28127941 0.26746641 0.26480578 0.26883361\n",
      " 0.33858161 0.27844136 0.26883361 0.26883361 0.26883361 0.33858161\n",
      " 0.27844136 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.27398236 0.25880978 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.25880978 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.25880978 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.25838138 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.25838138 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.25838138 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.25730446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.25730446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.25730446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361]\n",
      "One or more of the train scores are non-finite: [0.25514949 0.27869038 0.2984899  0.25514949 0.25514949 0.25514949\n",
      " 0.07368421 0.12274868 0.25514949 0.25514949 0.26923137 0.07368421\n",
      " 0.12274868 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.25514949 0.27869038 0.2984899  0.25514949 0.25514949 0.25514949\n",
      " 0.07368421 0.12274868 0.25514949 0.25559018 0.26923137 0.07368421\n",
      " 0.12274868 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.25514949 0.27869038 0.2984899  0.25514949 0.25514949 0.26923137\n",
      " 0.16315789 0.12274868 0.26923137 0.26689283 0.26923137 0.16315789\n",
      " 0.12274868 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.25553089 0.28219012 0.29416659 0.25660959 0.25514949 0.26923137\n",
      " 0.37018754 0.25322487 0.26923137 0.26923137 0.26923137 0.37018754\n",
      " 0.25322487 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26821545 0.30384638 0.29376118 0.26821545 0.26645015 0.26923137\n",
      " 0.31714727 0.30904954 0.26923137 0.26923137 0.26923137 0.31714727\n",
      " 0.30904954 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26911115 0.3100908  0.28855114 0.26923137 0.26901826 0.26923137\n",
      " 0.3114073  0.28973827 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28973827 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28734225 0.26923137 0.26912917 0.26923137\n",
      " 0.3114073  0.28734225 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28734225 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28734225 0.26923137 0.26923137 0.26923137\n",
      " 0.3114073  0.28734225 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28734225 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.33858160792371317\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.5548628428927681\n",
      "GridSearchCV Runtime: 4.506165027618408 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2830188679245283\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.4186046511627907\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.22699386503067484\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2222222222222222\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23893805309734514\n",
      "Ave Test Precision: 0.27795553188751226\n",
      "Stdev Test Precision: 0.08220928942484357\n",
      "Ave Test Accuracy: 0.5552238805970149\n",
      "Stdev Test Accuracy: 0.11257283971491158\n",
      "Ave Test Specificity: 0.6\n",
      "Ave Test Recall: 0.41818181818181815\n",
      "Ave Test NPV: 0.7543480805553012\n",
      "Ave Test F1-Score: 0.31761710531634585\n",
      "Ave Test G-mean: 0.4861078075843257\n",
      "Ave Runtime: 0.007764387130737305\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with clothing. 60 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24750769 0.24857716 0.24267477 0.24750769 0.2581215  0.24750769\n",
      " 0.09813272 0.1        0.24750769 0.2581215  0.22356655 0.09813272\n",
      " 0.1        0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.24750769 0.24857716 0.24267477 0.24750769 0.2581215  0.24750769\n",
      " 0.09813272 0.1        0.24750769 0.2581215  0.22356655 0.09813272\n",
      " 0.1        0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.24750769 0.24857716 0.24267477 0.24750769 0.2581215  0.23099014\n",
      " 0.19813272 0.1        0.2243602  0.26206887 0.22356655 0.19813272\n",
      " 0.1        0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.24546669 0.24941049 0.24393424 0.24546669 0.26010241 0.22356655\n",
      " 0.34035494 0.09807692 0.2243602  0.24029313 0.22356655 0.34035494\n",
      " 0.09807692 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22695    0.23749933 0.25288568 0.22893679 0.24875605 0.22356655\n",
      " 0.23307319 0.15095238 0.2243602  0.23467599 0.22356655 0.23307319\n",
      " 0.15095238 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.22356655 0.22947057 0.22356655\n",
      " 0.23596803 0.31775264 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31775264 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599]\n",
      "One or more of the train scores are non-finite: [0.25001205 0.27203125 0.16498253 0.25001205 0.24779599 0.25001205\n",
      " 0.09826745 0.09806094 0.25001205 0.24779599 0.22463815 0.09826745\n",
      " 0.09806094 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.25001205 0.27203125 0.16498253 0.25001205 0.24779599 0.25001205\n",
      " 0.19826745 0.09806094 0.25001205 0.24779599 0.22463815 0.19826745\n",
      " 0.09806094 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.25001205 0.27203125 0.16492787 0.25001205 0.24779599 0.22834081\n",
      " 0.19124991 0.09806094 0.22463907 0.24441942 0.22463815 0.19124991\n",
      " 0.09806094 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.24776137 0.27262963 0.16552741 0.24776137 0.24934436 0.22463815\n",
      " 0.38402634 0.19747533 0.22463907 0.22896095 0.22463815 0.38402634\n",
      " 0.19747533 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.23240157 0.26831405 0.2643078  0.23098767 0.23915618 0.22455054\n",
      " 0.27717073 0.44529656 0.22463907 0.22658    0.22455054 0.27717073\n",
      " 0.44529656 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26205685 0.40579101 0.22455054 0.22822454 0.22455054\n",
      " 0.26866379 0.40655713 0.22463907 0.22658    0.22455054 0.26866379\n",
      " 0.40655713 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.40557996 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658   ]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.34035493827160496\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.743142144638404\n",
      "GridSearchCV Runtime: 4.444406270980835 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.29411764705882354\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.17391304347826086\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2727272727272727\n",
      "Ave Test Precision: 0.14815159265287142\n",
      "Stdev Test Precision: 0.14264171047123156\n",
      "Ave Test Accuracy: 0.7223880597014926\n",
      "Stdev Test Accuracy: 0.04649978381992352\n",
      "Ave Test Specificity: 0.9425742574257425\n",
      "Ave Test Recall: 0.048484848484848485\n",
      "Ave Test NPV: 0.7516074700178519\n",
      "Ave Test F1-Score: 0.06825222969801284\n",
      "Ave Test G-mean: 0.15791600065457903\n",
      "Ave Runtime: 0.008803701400756836\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with mobileLoad. 59 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.28265159 0.26110281 0.17860161 0.28265159 0.28265159 0.28265159\n",
      " 0.12219136 0.14719136 0.28265159 0.28265159 0.30665091 0.12219136\n",
      " 0.14719136 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.28265159 0.26110281 0.17860161 0.28265159 0.28265159 0.28265159\n",
      " 0.12219136 0.14719136 0.28265159 0.28326135 0.30665091 0.12219136\n",
      " 0.14719136 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.28265159 0.26295511 0.17860161 0.28265159 0.28265159 0.30665091\n",
      " 0.12219136 0.14719136 0.30797366 0.30253024 0.30665091 0.12219136\n",
      " 0.14719136 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.28533517 0.26295205 0.19556798 0.28533517 0.28405403 0.30665091\n",
      " 0.23697937 0.14719136 0.30797366 0.30253024 0.30665091 0.23697937\n",
      " 0.14719136 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30426997 0.26993437 0.33052469 0.30426997 0.28827635 0.30665091\n",
      " 0.27774586 0.17860161 0.30797366 0.30253024 0.30665091 0.27774586\n",
      " 0.17860161 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28616215 0.24755316 0.30665091 0.30253024 0.30665091\n",
      " 0.28299549 0.25014806 0.30797366 0.30253024 0.30665091 0.28299549\n",
      " 0.25014806 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024]\n",
      "One or more of the train scores are non-finite: [0.28307638 0.27839136 0.20152228 0.28307638 0.28307638 0.28307638\n",
      " 0.12288719 0.14740242 0.28307638 0.28307638 0.30604687 0.12288719\n",
      " 0.14740242 0.30649813 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.28307638 0.27839136 0.20152228 0.28307638 0.28307638 0.28307638\n",
      " 0.12288719 0.14740242 0.28307638 0.28349189 0.30604687 0.12288719\n",
      " 0.14740242 0.30649813 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.28307638 0.27850622 0.20152228 0.28307638 0.28307638 0.30604687\n",
      " 0.12288719 0.14740242 0.30649813 0.30651448 0.30604687 0.12288719\n",
      " 0.14740242 0.30649813 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.28540879 0.28055705 0.26326814 0.28557756 0.28434539 0.30604687\n",
      " 0.25366757 0.14740242 0.30649813 0.30651448 0.30604687 0.25366757\n",
      " 0.14740242 0.30649813 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30408467 0.28616186 0.37898701 0.30408467 0.28923159 0.30604687\n",
      " 0.28898453 0.29152744 0.30627022 0.30651448 0.30604687 0.28898453\n",
      " 0.29152744 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30582361 0.2938237  0.27864938 0.30604968 0.30541997 0.30604687\n",
      " 0.29443981 0.27860324 0.30627022 0.30651448 0.30604687 0.29443981\n",
      " 0.27860324 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26581211 0.30627022 0.30651448 0.30604687\n",
      " 0.29504535 0.26533539 0.30627022 0.30651448 0.30604687 0.29504535\n",
      " 0.26533539 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539\n",
      " 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539 0.29504535\n",
      " 0.26573892 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539\n",
      " 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539 0.29504535\n",
      " 0.26573892 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539\n",
      " 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539 0.29504535\n",
      " 0.26573892 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539\n",
      " 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539 0.29504535\n",
      " 0.26573892 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539\n",
      " 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539 0.29504535\n",
      " 0.26573892 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539\n",
      " 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539 0.29504535\n",
      " 0.26573892 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3305246913580247\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.756857855361596\n",
      "GridSearchCV Runtime: 4.517886638641357 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.5\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.18994413407821228\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.328125\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.20361382681564244\n",
      "Stdev Test Precision: 0.21590034584454101\n",
      "Ave Test Accuracy: 0.6537313432835822\n",
      "Stdev Test Accuracy: 0.17909281678458255\n",
      "Ave Test Specificity: 0.8118811881188119\n",
      "Ave Test Recall: 0.16969696969696968\n",
      "Ave Test NPV: 0.7364081265400235\n",
      "Ave Test F1-Score: 0.12600794163819373\n",
      "Ave Test G-mean: 0.20090065766814585\n",
      "Ave Runtime: 0.008978319168090821\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with internet. 58 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25345238 0.25345238 0.28576279 0.25345238 0.25345238 0.25345238\n",
      " 0.28694444 0.28540866 0.23845238 0.25595238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.25345238 0.26392857 0.30441358 0.25345238 0.25345238 0.27916667\n",
      " 0.25345238 0.28540866 0.26456349 0.2552381  0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.25916667 0.26686508 0.27024691 0.25916667 0.25583333 0.25345238\n",
      " 0.27527778 0.28540866 0.25345238 0.26734127 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.25916667 0.29912698 0.29897707 0.25916667 0.25916667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.26416667 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.33916667 0.33635802 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762]\n",
      "One or more of the train scores are non-finite: [0.2735763  0.2735763  0.27064891 0.2735763  0.2735763  0.2735763\n",
      " 0.2978488  0.36147026 0.28123396 0.2818759  0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.2735763  0.28756448 0.27957894 0.2735763  0.2735763  0.28795097\n",
      " 0.2735763  0.36147026 0.27961885 0.29435407 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27667154 0.30089298 0.28462036 0.27667154 0.2753079  0.2735763\n",
      " 0.28435781 0.36147026 0.28008912 0.28174701 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27667154 0.31357225 0.29400494 0.27667154 0.27667154 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.2809265  0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32428708 0.33760254 0.27708034 0.27648635 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32718931 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32804297 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32877467 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32877467 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32877467 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32877467 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32877467 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32877467 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538 ]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.33916666666666667\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7331670822942643\n",
      "GridSearchCV Runtime: 4.650293350219727 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.5454545454545454\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.5\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.4482758620689655\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2571428571428571\n",
      "Ave Test Precision: 0.40731751007613076\n",
      "Stdev Test Precision: 0.12912130114588072\n",
      "Ave Test Accuracy: 0.7328358208955225\n",
      "Stdev Test Accuracy: 0.02787290954773594\n",
      "Ave Test Specificity: 0.9346534653465346\n",
      "Ave Test Recall: 0.11515151515151514\n",
      "Ave Test NPV: 0.7637535862906284\n",
      "Ave Test F1-Score: 0.17075706614870328\n",
      "Ave Test G-mean: 0.32043096007558375\n",
      "Ave Runtime: 0.008403205871582031\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with vehicleLoan. 57 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.3025     0.3025     0.28552469 0.3025     0.3025     0.42916667\n",
      " 0.45       0.35874199 0.42916667 0.42916667 0.42916667 0.45\n",
      " 0.35874199 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.3025     0.3025     0.29385802 0.3025     0.3025     0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.32416667 0.3375     0.32885802 0.32416667 0.3025     0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.42916667 0.32885802 0.42916667 0.3575     0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.42916667 0.30969136 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.42916667 0.30969136 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30874199 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667]\n",
      "One or more of the train scores are non-finite: [0.33312914 0.33312914 0.29945734 0.33312914 0.33312914 0.40663038\n",
      " 0.45368924 0.4667004  0.40663038 0.40663038 0.40663038 0.45368924\n",
      " 0.4667004  0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.33312914 0.33312914 0.3187877  0.33312914 0.33312914 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.36340997 0.3735933  0.34407513 0.36340997 0.33312914 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40164901 0.40663038 0.35171862 0.40440569 0.37618542 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.42557596 0.41072652 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.43057596 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.45\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7468827930174564\n",
      "GridSearchCV Runtime: 4.818542003631592 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.23076923076923078\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.5454545454545454\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4166666666666667\n",
      "Ave Test Precision: 0.35524475524475524\n",
      "Stdev Test Precision: 0.1294248243233685\n",
      "Ave Test Accuracy: 0.7432835820895523\n",
      "Stdev Test Accuracy: 0.011005881495337775\n",
      "Ave Test Specificity: 0.9673267326732674\n",
      "Ave Test Recall: 0.05757575757575758\n",
      "Ave Test NPV: 0.7585594540478023\n",
      "Ave Test F1-Score: 0.09822683645468455\n",
      "Ave Test G-mean: 0.22789511905128673\n",
      "Ave Runtime: 0.01023087501525879\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with informalLenders. 56 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22869546 0.09083333 0.09859726 0.20028711 0.22553133 0.22944792\n",
      " 0.07083333 0.09767448 0.20028711 0.22553133 0.2399706  0.03333333\n",
      " 0.09767448 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.22869546 0.08690476 0.09756085 0.20028711 0.22553133 0.22944792\n",
      " 0.07083333 0.09798079 0.20028711 0.22553133 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.23005036 0.08690476 0.09831331 0.21940476 0.23684628 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25053133 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.23584683 0.05       0.09798765 0.24047021 0.23865516 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09797903 0.24523212 0.23865516 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09766433 0.24459398 0.25532183 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733 ]\n",
      "One or more of the train scores are non-finite: [0.24067471 0.09506111 0.09822167 0.23787841 0.24045567 0.24034427\n",
      " 0.09425466 0.49831583 0.23787841 0.24045567 0.25401405 0.10640437\n",
      " 0.49831583 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.24067471 0.10014517 0.09800951 0.23787841 0.24045567 0.24034427\n",
      " 0.09789743 0.49842221 0.24058372 0.24391417 0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.24851699 0.10401775 0.09806637 0.24996559 0.2468563  0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2419725  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25201832 0.10164069 0.09833626 0.25493519 0.25095694 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25339708 0.10303324 0.28145805 0.25693553 0.251888   0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10362659 0.4984225  0.25715111 0.25204925 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2646733036185091\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.7231920199501247\n",
      "GridSearchCV Runtime: 4.530595064163208 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.15\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2388663967611336\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25301204819277107\n",
      "Ave Test Precision: 0.21837568899078094\n",
      "Stdev Test Precision: 0.04368221698058891\n",
      "Ave Test Accuracy: 0.4544776119402985\n",
      "Stdev Test Accuracy: 0.23423505605187847\n",
      "Ave Test Specificity: 0.4138613861386139\n",
      "Ave Test Recall: 0.5787878787878789\n",
      "Ave Test NPV: 0.7636455628105071\n",
      "Ave Test F1-Score: 0.26341294722814\n",
      "Ave Test G-mean: 0.2365069405488125\n",
      "Ave Runtime: 0.006511163711547851\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with companyLoan. 55 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.29119048 0.30119048 0.31619048 0.29119048 0.29119048 0.27833333\n",
      " 0.34       0.23       0.29119048 0.30587302 0.38190476 0.38166667\n",
      " 0.23       0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.29119048 0.3359127  0.33234127 0.29119048 0.29119048 0.30119048\n",
      " 0.37611111 0.21857143 0.28301587 0.285      0.38190476 0.38166667\n",
      " 0.21857143 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.3409127  0.37638889 0.37162698 0.3359127  0.31710317 0.30785714\n",
      " 0.27888889 0.21222222 0.26833333 0.37420635 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.35138889 0.35055556 0.37555556 0.35138889 0.35138889 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.36940476 0.37333333 0.31222222 0.38190476 0.35138889 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.37333333 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476]\n",
      "One or more of the train scores are non-finite: [0.29168357 0.29423541 0.29404047 0.29168357 0.29168357 0.30881349\n",
      " 0.34430847 0.38039425 0.29168357 0.30593111 0.3596441  0.36830088\n",
      " 0.38039425 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.29168357 0.32071662 0.31354822 0.29168357 0.29168357 0.29841056\n",
      " 0.34949227 0.37617788 0.31412438 0.31579334 0.3596441  0.36830088\n",
      " 0.37617788 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.32287019 0.3353991  0.33266286 0.32407989 0.3219805  0.30548771\n",
      " 0.33485959 0.3763184  0.30149126 0.3313945  0.3596441  0.36830088\n",
      " 0.3763184  0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.33358092 0.34801606 0.34120557 0.33358092 0.33430556 0.30548771\n",
      " 0.32349595 0.3763184  0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.3763184  0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.36039931 0.37066854 0.34477697 0.36193696 0.33358092 0.30548771\n",
      " 0.32349595 0.3763184  0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.3763184  0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3605099  0.37917536 0.3597902  0.35880542 0.36128761 0.30548771\n",
      " 0.32349595 0.37693305 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37693305 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37693305 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37693305 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37545672 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37545672 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37545672 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37545672 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37545672 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37545672 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37545672 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37545672 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37545672 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37545672 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37545672 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37545672 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.38190476190476186\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.7381546134663342\n",
      "GridSearchCV Runtime: 4.794448375701904 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.1\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.1111111111111111\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.23715415019762845\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.26666666666666666\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.42857142857142855\n",
      "Ave Test Precision: 0.22870067130936697\n",
      "Stdev Test Precision: 0.13402645744441252\n",
      "Ave Test Accuracy: 0.6313432835820896\n",
      "Stdev Test Accuracy: 0.20961051300967495\n",
      "Ave Test Specificity: 0.7653465346534654\n",
      "Ave Test Recall: 0.2212121212121212\n",
      "Ave Test NPV: 0.7221564509050763\n",
      "Ave Test F1-Score: 0.1397751635561678\n",
      "Ave Test G-mean: 0.20469644730637687\n",
      "Ave Runtime: 0.007073593139648437\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with privateLoans. 54 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24422637 0.         0.12827272 0.22008844 0.17567255 0.24422637\n",
      " 0.         0.12188272 0.24422637 0.23067255 0.23380233 0.\n",
      " 0.12188272 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.24297187 0.         0.12859018 0.24297187 0.25108071 0.24679677\n",
      " 0.         0.12188272 0.24442809 0.23873867 0.23380233 0.\n",
      " 0.12188272 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.24437091 0.         0.12699102 0.24437091 0.24784642 0.23380233\n",
      " 0.         0.12154262 0.23351695 0.22918826 0.23380233 0.\n",
      " 0.12154262 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.24166242 0.         0.12406315 0.24166242 0.24911086 0.23380233\n",
      " 0.         0.12245962 0.23351695 0.22906215 0.23380233 0.\n",
      " 0.12245962 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23111161 0.         0.12238865 0.23337248 0.23637776 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22906215 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23409558 0.         0.12205597 0.23351695 0.22647169 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22782347 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925]\n",
      "One or more of the train scores are non-finite: [0.25941592 0.         0.13154794 0.23207779 0.18362803 0.25941592\n",
      " 0.         0.12292119 0.25941592 0.23746098 0.24889402 0.\n",
      " 0.12292119 0.24745294 0.25004889        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.25698887 0.         0.13190373 0.25708196 0.26516051 0.24931848\n",
      " 0.         0.12292119 0.2531278  0.25712209 0.24889402 0.\n",
      " 0.12292119 0.24745294 0.25004889        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.25633817 0.         0.13188792 0.25633817 0.26163532 0.24898491\n",
      " 0.         0.12318429 0.24770359 0.251564   0.24898491 0.\n",
      " 0.12318429 0.24770359 0.25004889        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.25586401 0.         0.12711313 0.25586401 0.26193029 0.24907515\n",
      " 0.         0.32229279 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.32229279 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.25164977 0.         0.22218018 0.25157422 0.25708223 0.24907515\n",
      " 0.         0.35550195 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.35550195 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24872129 0.         0.25578404 0.24827076 0.2491633  0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30571395 0.24757271 0.25041518 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30560829 0.24770359 0.25009048 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30560829 0.24770359 0.25009048 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30560829 0.24770359 0.25009048 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30560829 0.24770359 0.25009048 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30560829 0.24770359 0.25009048 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30560829 0.24770359 0.25009048 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-05, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2510807142769716\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-05, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.48129675810473815\n",
      "GridSearchCV Runtime: 4.600949287414551 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2804878048780488\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2621951219512195\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.211864406779661\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.23\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.20175438596491227\n",
      "Ave Test Precision: 0.23726034391476833\n",
      "Stdev Test Precision: 0.03334927211996916\n",
      "Ave Test Accuracy: 0.5\n",
      "Stdev Test Accuracy: 0.032954331590775564\n",
      "Ave Test Specificity: 0.504950495049505\n",
      "Ave Test Recall: 0.484848484848485\n",
      "Ave Test NPV: 0.7556063936063936\n",
      "Ave Test F1-Score: 0.3156632326407077\n",
      "Ave Test G-mean: 0.48071283471666815\n",
      "Ave Runtime: 0.006579113006591797\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with governmentLoans. 53 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.30319597 0.25319597 0.27113683 0.30319597 0.30319597 0.32001082\n",
      " 0.19809524 0.18238367 0.33212454 0.31299756 0.32759019 0.19809524\n",
      " 0.18238367 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.30319597 0.25319597 0.27311402 0.30319597 0.30319597 0.32001082\n",
      " 0.19809524 0.31905033 0.30823565 0.33264042 0.32759019 0.19809524\n",
      " 0.31905033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.3196176  0.27755411 0.27697713 0.3196176  0.30319597 0.32425685\n",
      " 0.19809524 0.32405033 0.31279915 0.31803724 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.34147908 0.28314574 0.32925235 0.34147908 0.32199856 0.32981241\n",
      " 0.19809524 0.32405033 0.32930708 0.32279915 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.34223665 0.26107143 0.32440748 0.32223665 0.34223665 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.20809524 0.32405033 0.3266811  0.3266811  0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.20809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019]\n",
      "One or more of the train scores are non-finite: [0.29805773 0.27067678 0.27872465 0.29805773 0.29805773 0.3114852\n",
      " 0.31340409 0.3167918  0.31106446 0.31180764 0.32446574 0.31340409\n",
      " 0.3167918  0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.29805773 0.27081113 0.27885333 0.29805773 0.29805773 0.3114852\n",
      " 0.31219479 0.2937685  0.31460312 0.31100367 0.32446574 0.31219479\n",
      " 0.2937685  0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.29779328 0.27300515 0.28991115 0.29779328 0.29805773 0.31945792\n",
      " 0.31219479 0.29645595 0.30895261 0.31987676 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.31868533 0.29384367 0.2944938  0.31868533 0.30138353 0.31595652\n",
      " 0.31219479 0.29645595 0.30584218 0.3164527  0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32482938 0.30078667 0.29806777 0.32555122 0.32392088 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.30915112 0.29645595 0.32489187 0.32504554 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.30996413 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.3422366522366523\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7206982543640897\n",
      "GridSearchCV Runtime: 4.6834447383880615 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2222222222222222\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.4\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3548387096774194\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.36363636363636365\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25\n",
      "Ave Test Precision: 0.31813945910720104\n",
      "Stdev Test Precision: 0.07739679206608645\n",
      "Ave Test Accuracy: 0.7238805970149254\n",
      "Stdev Test Accuracy: 0.008750775671312369\n",
      "Ave Test Specificity: 0.9227722772277229\n",
      "Ave Test Recall: 0.11515151515151516\n",
      "Ave Test NPV: 0.7615479429790593\n",
      "Ave Test F1-Score: 0.1679839380630558\n",
      "Ave Test G-mean: 0.32004536454318994\n",
      "Ave Runtime: 0.006627798080444336\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with smoking. 52 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22418914 0.22418914 0.23405033 0.22418914 0.22418914 0.22589147\n",
      " 0.24126984 0.09874199 0.23452242 0.23801893 0.23617077 0.24126984\n",
      " 0.09874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.22418914 0.22418914 0.24041144 0.22418914 0.22418914 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.2195177  0.21979548 0.265296   0.2195177  0.22418914 0.22589147\n",
      " 0.24126984 0.24874199 0.23005814 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.2268759  0.23342352 0.34934889 0.2268759  0.22043651 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.24770924 0.24274892 0.09874199 0.24770924 0.24770924 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.14874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242]\n",
      "One or more of the train scores are non-finite: [0.22035238 0.22035238 0.22594855 0.22035238 0.22035238 0.22355581\n",
      " 0.25010772 0.69749364 0.22306956 0.22317126 0.22214523 0.25061138\n",
      " 0.69749364 0.2231764  0.22390563        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22035238 0.22035238 0.23015995 0.22035238 0.22035238 0.22339164\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22214523 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.21496167 0.21187651 0.24415089 0.21496167 0.22035238 0.22329763\n",
      " 0.24232978 0.62666031 0.22365779 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.21930218 0.22291601 0.57712229 0.22000361 0.20998117 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22477394 0.23201537 0.6643698  0.22580948 0.22355329 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22204354 0.24232978 0.62666031 0.22328434 0.22500576 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22429177 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22363311 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22363311 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22363311 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22363311 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22363311 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22363311 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.34934889227126675\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.756857855361596\n",
      "GridSearchCV Runtime: 4.741650104522705 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.5\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2727272727272727\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.5\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2413793103448276\n",
      "Ave Test Precision: 0.3028213166144201\n",
      "Stdev Test Precision: 0.20865065766212348\n",
      "Ave Test Accuracy: 0.7313432835820896\n",
      "Stdev Test Accuracy: 0.030656113293199595\n",
      "Ave Test Specificity: 0.9514851485148516\n",
      "Ave Test Recall: 0.05757575757575758\n",
      "Ave Test NPV: 0.7555062164856292\n",
      "Ave Test F1-Score: 0.08314824494391057\n",
      "Ave Test G-mean: 0.19001360003003093\n",
      "Ave Runtime: 0.0070838451385498045\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with alcohol. 51 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.32       0.32       0.28635802 0.32       0.32       0.42333333\n",
      " 0.22333333 0.09874199 0.42333333 0.40666667 0.42333333 0.22333333\n",
      " 0.09874199 0.42333333 0.42333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.32       0.32       0.28635802 0.32       0.32       0.42333333\n",
      " 0.32333333 0.24874199 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24874199 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.32       0.32       0.27302469 0.32       0.32       0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.32333333 0.32333333 0.20302469 0.32333333 0.33666667 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.32333333 0.32333333 0.19969136 0.32333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333]\n",
      "One or more of the train scores are non-finite: [0.29761626 0.29761626 0.27800812 0.29761626 0.29761626 0.34589372\n",
      " 0.33707946 0.69809494 0.33886758 0.33690679 0.34589372 0.33707946\n",
      " 0.69809494 0.34033816 0.33704254        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.29761626 0.29761626 0.27378314 0.29761626 0.29761626 0.33577295\n",
      " 0.33329556 0.50357113 0.34033816 0.3208971  0.33577295 0.33329556\n",
      " 0.50357113 0.34033816 0.3208971         nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.29761626 0.29280073 0.29142828 0.29761626 0.29761626 0.32658103\n",
      " 0.32825354 0.45238066 0.33478261 0.31799855 0.32658103 0.32825354\n",
      " 0.45238066 0.33478261 0.31799855        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.28644791 0.29747213 0.31289205 0.28644791 0.29512757 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31496824 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31496824        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.30681124 0.33518165 0.42476161 0.30873432 0.29282082 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.30836102 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.42333333333333334\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7418952618453866\n",
      "GridSearchCV Runtime: 4.965595483779907 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.1111111111111111\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.16666666666666666\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.4166666666666667\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.14285714285714285\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.16666666666666666\n",
      "Ave Test Precision: 0.2007936507936508\n",
      "Stdev Test Precision: 0.12281088912338513\n",
      "Ave Test Accuracy: 0.7373134328358208\n",
      "Stdev Test Accuracy: 0.0067783216882779706\n",
      "Ave Test Specificity: 0.9693069306930692\n",
      "Ave Test Recall: 0.02727272727272727\n",
      "Ave Test NPV: 0.7531056294668017\n",
      "Ave Test F1-Score: 0.04756492214026461\n",
      "Ave Test G-mean: 0.1510851941747013\n",
      "Ave Runtime: 0.006829690933227539\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with gambling. 50 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.29880952 0.22380952 0.29016755 0.29880952 0.29880952 0.32\n",
      " 0.13333333 0.09874199 0.30333333 0.29880952 0.345      0.13333333\n",
      " 0.09874199 0.32       0.345             nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.29880952 0.22380952 0.28064374 0.29880952 0.29880952 0.32333333\n",
      " 0.23333333 0.24874199 0.32333333 0.29       0.34       0.23333333\n",
      " 0.24874199 0.34       0.29              nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.29880952 0.22714286 0.33635802 0.29880952 0.29880952 0.32\n",
      " 0.225      0.19874199 0.34       0.28666667 0.33666667 0.225\n",
      " 0.19874199 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.38666667 0.29166667 0.33135802 0.38666667 0.31547619 0.32\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.38666667 0.245      0.23207532 0.38666667 0.38666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667]\n",
      "One or more of the train scores are non-finite: [0.29554617 0.27054617 0.27592044 0.29554617 0.29554617 0.32879277\n",
      " 0.31731075 0.69809494 0.32874927 0.30156246 0.33137897 0.31731075\n",
      " 0.69809494 0.32864765 0.32619948        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.29554617 0.27054617 0.27852304 0.29554617 0.29554617 0.32998004\n",
      " 0.29445105 0.44738066 0.3278848  0.33088176 0.33271175 0.29445105\n",
      " 0.44738066 0.32778318 0.33088176        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.29554617 0.27356131 0.28739506 0.29554617 0.29554617 0.32752117\n",
      " 0.2926978  0.34990254 0.32778318 0.32312377 0.33025287 0.2926978\n",
      " 0.34990254 0.32778318 0.32312377        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.30394319 0.29376326 0.29005634 0.30394319 0.29531014 0.32752117\n",
      " 0.2926978  0.33339905 0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.33339905 0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.32304772 0.29724326 0.32603666 0.32482747 0.30782041 0.33025287\n",
      " 0.2926978  0.33932762 0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.33932762 0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.29566223 0.33883097 0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.33883097 0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.33883097 0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.33883097 0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.3361643  0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.3361643  0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.3361643  0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.3361643  0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.3361643  0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.3361643  0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.38666666666666666\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7381546134663342\n",
      "GridSearchCV Runtime: 5.33472466468811 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.125\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.23577235772357724\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.14285714285714285\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.150725900116144\n",
      "Stdev Test Precision: 0.10066067144462562\n",
      "Ave Test Accuracy: 0.6402985074626865\n",
      "Stdev Test Accuracy: 0.20779595801964662\n",
      "Ave Test Specificity: 0.7881188118811882\n",
      "Ave Test Recall: 0.18787878787878787\n",
      "Ave Test NPV: 0.7276588162795059\n",
      "Ave Test F1-Score: 0.0960546426299851\n",
      "Ave Test G-mean: 0.13209079637177706\n",
      "Ave Runtime: 0.008598423004150391\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with smallLottery. 49 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.30166667 0.30166667 0.26802469 0.30166667 0.30166667 0.3\n",
      " 0.21190476 0.09874199 0.35       0.26833333 0.3        0.21190476\n",
      " 0.09874199 0.3        0.3               nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.30166667 0.30166667 0.29969136 0.30166667 0.30166667 0.3\n",
      " 0.31190476 0.09874199 0.3        0.25       0.3        0.31190476\n",
      " 0.09874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.30166667 0.385      0.21159612 0.30166667 0.30166667 0.3\n",
      " 0.31190476 0.09874199 0.3        0.23333333 0.3        0.31190476\n",
      " 0.09874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.35       0.4        0.14969136 0.35       0.36833333 0.3\n",
      " 0.31190476 0.19874199 0.3        0.23333333 0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.35       0.3        0.09969136 0.3        0.35       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.09874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.09874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.19874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.19874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.19874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.19874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.19874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.19874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25      ]\n",
      "One or more of the train scores are non-finite: [0.25041557 0.25041557 0.24824634 0.25041557 0.25041557 0.25110421\n",
      " 0.3051079  0.69809494 0.25379999 0.2517117  0.25110421 0.3051079\n",
      " 0.69809494 0.25903899 0.25412651        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25041557 0.25041557 0.24756076 0.25041557 0.25041557 0.25110421\n",
      " 0.27997322 0.56476161 0.2527864  0.25426912 0.25110421 0.27997322\n",
      " 0.56476161 0.25445307 0.25426912        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25041557 0.23570301 0.26684786 0.25041557 0.25041557 0.25110421\n",
      " 0.2763847  0.54799025 0.2527864  0.25142424 0.25110421 0.2763847\n",
      " 0.54799025 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.23395066 0.25037161 0.36416637 0.23395066 0.24187976 0.25110421\n",
      " 0.2763847  0.54465692 0.25445307 0.25142424 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.24932213 0.269425   0.54476161 0.25041247 0.24005181 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25169945 0.2763847  0.54465692 0.25445307 0.24982718 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.4\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7369077306733167\n",
      "GridSearchCV Runtime: 4.9686079025268555 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2222222222222222\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.14285714285714285\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.14285714285714285\n",
      "Ave Test Precision: 0.10158730158730159\n",
      "Stdev Test Precision: 0.0982333234715289\n",
      "Ave Test Accuracy: 0.7388059701492538\n",
      "Stdev Test Accuracy: 0.008343537229476828\n",
      "Ave Test Specificity: 0.9762376237623762\n",
      "Ave Test Recall: 0.012121212121212121\n",
      "Ave Test NPV: 0.7515184469298781\n",
      "Ave Test F1-Score: 0.02162557077625571\n",
      "Ave Test G-mean: 0.08270682080933188\n",
      "Ave Runtime: 0.007114934921264649\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with otherVices. 48 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.26099611 0.26682245 0.2329219  0.26099611 0.26099611 0.26099611\n",
      " 0.0975     0.09969136 0.26099611 0.26099611 0.32645677 0.0975\n",
      " 0.09969136 0.3284154  0.33647913        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.26099611 0.26682245 0.2329219  0.26099611 0.26099611 0.30889344\n",
      " 0.0975     0.09969136 0.31545244 0.26099611 0.32645677 0.0975\n",
      " 0.09969136 0.3284154  0.33647913        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.26099611 0.27275835 0.23571879 0.26099611 0.26099611 0.32645677\n",
      " 0.28083333 0.09969136 0.3284154  0.33647913 0.32645677 0.28083333\n",
      " 0.09969136 0.3284154  0.33647913        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.27104097 0.3178464  0.22157149 0.27104097 0.26560071 0.32645677\n",
      " 0.40372909 0.18302469 0.32388324 0.33647913 0.32645677 0.40372909\n",
      " 0.18302469 0.32388324 0.33647913        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.31910679 0.39158509 0.22534071 0.31620842 0.27731516 0.32645677\n",
      " 0.40151942 0.21782789 0.32626419 0.33533067 0.32645677 0.40151942\n",
      " 0.21782789 0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32883772 0.41818609 0.22505766 0.32626419 0.3316155  0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32883772 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067]\n",
      "One or more of the train scores are non-finite: [0.2577556  0.25997953 0.2457579  0.2577556  0.2577556  0.2577556\n",
      " 0.09833795 0.09809494 0.2577556  0.2577556  0.32336281 0.09833795\n",
      " 0.09809494 0.32292046 0.32667809        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.2577556  0.25997953 0.2457579  0.2577556  0.2577556  0.31291582\n",
      " 0.14833795 0.09809494 0.31858088 0.2577556  0.32336281 0.14833795\n",
      " 0.09809494 0.32292046 0.32667809        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.2577556  0.26130795 0.24485335 0.2577556  0.2577556  0.32336281\n",
      " 0.30067006 0.25476161 0.32292046 0.32667809 0.32336281 0.30067006\n",
      " 0.25476161 0.32292046 0.32667809        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.26619824 0.28780795 0.24875714 0.26619824 0.25781535 0.32291979\n",
      " 0.28248515 0.31320483 0.31744442 0.32667809 0.32291979 0.28248515\n",
      " 0.31320483 0.31744442 0.32667809        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.30041964 0.28680107 0.32214868 0.30061341 0.27384967 0.32074527\n",
      " 0.28754376 0.30043999 0.31555701 0.32313527 0.32074527 0.28754376\n",
      " 0.30043999 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.31862311 0.28928592 0.30798899 0.31534671 0.31881519 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.31885785 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "invalid value encountered in scalar divide\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "invalid value encountered in scalar divide\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.41818609022556397\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.2456359102244389\n",
      "GridSearchCV Runtime: 4.479140996932983 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3076923076923077\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2543859649122807\n",
      "Ave Test Precision: 0.2775897838741515\n",
      "Stdev Test Precision: 0.040354517943496794\n",
      "Ave Test Accuracy: 0.49850746268656715\n",
      "Stdev Test Accuracy: 0.2421868880643179\n",
      "Ave Test Specificity: 0.4900990099009901\n",
      "Ave Test Recall: 0.5242424242424242\n",
      "Ave Test NPV: 0.7592944460842189\n",
      "Ave Test F1-Score: 0.27782370598824646\n",
      "Ave Test G-mean: 0.2156041540326256\n",
      "Ave Runtime: 0.008820486068725587\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with savings. 47 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "910 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "208 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "598 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.28297619 0.                nan 0.24011905 0.19011905 0.28297619\n",
      " 0.                nan 0.24011905 0.19011905 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.24672619 0.                nan 0.20386905 0.21380326 0.28297619\n",
      " 0.                nan 0.24011905 0.21380326 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107]\n",
      "One or more of the train scores are non-finite: [0.29082237 0.                nan 0.26373904 0.23628806 0.29082237\n",
      " 0.                nan 0.26373904 0.23628806 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.28947611 0.                nan 0.26239278 0.26058016 0.29082237\n",
      " 0.                nan 0.26373904 0.26058016 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2829761904761905\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7256857855361596\n",
      "GridSearchCV Runtime: 4.534821510314941 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2777777777777778\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.0\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.1126984126984127\n",
      "Stdev Test Precision: 0.15434416555597125\n",
      "Ave Test Accuracy: 0.7455223880597015\n",
      "Stdev Test Accuracy: 0.013033021788487286\n",
      "Ave Test Specificity: 0.9821782178217822\n",
      "Ave Test Recall: 0.021212121212121213\n",
      "Ave Test NPV: 0.7543966603762795\n",
      "Ave Test F1-Score: 0.03476842791911285\n",
      "Ave Test G-mean: 0.0876293264991058\n",
      "Ave Runtime: 0.006800413131713867\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanOthers. 46 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "910 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "208 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "598 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.23936508 0.03222222        nan 0.33936508 0.08079365 0.23936508\n",
      " 0.                nan 0.33936508 0.08079365 0.25936508 0.\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.05222222        nan 0.33936508 0.12545119 0.25936508\n",
      " 0.                nan 0.33936508 0.10079365 0.25936508 0.\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.05222222        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.                nan 0.33936508 0.16707281 0.25936508 0.\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.05222222        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.                nan 0.33936508 0.16707281 0.25936508 0.\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281]\n",
      "One or more of the train scores are non-finite: [0.26838504 0.09575013        nan 0.29226564 0.18650841 0.26838504\n",
      " 0.                nan 0.29226564 0.18650841 0.29296519 0.\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.12639529        nan 0.29226564 0.23513727 0.29296519\n",
      " 0.                nan 0.29226564 0.21108856 0.29296519 0.\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.12639529        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.                nan 0.29226564 0.28410767 0.29296519 0.\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.12639529        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.                nan 0.29226564 0.28410767 0.29296519 0.\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.33936507936507937\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7169576059850374\n",
      "GridSearchCV Runtime: 4.663201808929443 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4166666666666667\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.35714285714285715\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.34615384615384615\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.35714285714285715\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4166666666666667\n",
      "Ave Test Precision: 0.3787545787545788\n",
      "Stdev Test Precision: 0.03489840113765008\n",
      "Ave Test Accuracy: 0.7298507462686568\n",
      "Stdev Test Accuracy: 0.00817496354485325\n",
      "Ave Test Specificity: 0.9198019801980198\n",
      "Ave Test Recall: 0.1484848484848485\n",
      "Ave Test NPV: 0.7677559499616132\n",
      "Ave Test F1-Score: 0.21312570665022096\n",
      "Ave Test G-mean: 0.36948706056345093\n",
      "Ave Runtime: 0.005794095993041992\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with payInsurance. 45 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.15726284 0.         0.         0.08054711 0.         0.26114724\n",
      " 0.         0.         0.26114724 0.1806671  0.26114724 0.\n",
      " 0.         0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.         0.         0.26114724 0.26114724 0.26114724\n",
      " 0.         0.         0.26114724 0.26114724 0.26114724 0.\n",
      " 0.         0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.         0.         0.26114724 0.26114724 0.26114724\n",
      " 0.         0.         0.26114724 0.26114724 0.26114724 0.\n",
      " 0.         0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.         0.         0.26114724 0.26114724 0.26114724\n",
      " 0.         0.         0.26114724 0.26114724 0.26114724 0.\n",
      " 0.         0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.11374646 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.11374646 0.21433873 0.26114724 0.26114724 0.26114724 0.11374646\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724]\n",
      "One or more of the train scores are non-finite: [0.15638915 0.         0.         0.07790897 0.         0.26070874\n",
      " 0.         0.         0.26070874 0.18272748 0.26070874 0.\n",
      " 0.         0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.         0.         0.26070874 0.26070874 0.26070874\n",
      " 0.         0.         0.26070874 0.26070874 0.26070874 0.\n",
      " 0.         0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.         0.         0.26070874 0.26070874 0.26070874\n",
      " 0.         0.         0.26070874 0.26070874 0.26070874 0.\n",
      " 0.         0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.         0.         0.26070874 0.26070874 0.26070874\n",
      " 0.         0.         0.26070874 0.26070874 0.26070874 0.\n",
      " 0.         0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.13224755 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.13224755 0.20803208 0.26070874 0.26070874 0.26070874 0.13224755\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2611472392843984\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.4476309226932668\n",
      "GridSearchCV Runtime: 4.882431268692017 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.26666666666666666\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.26737967914438504\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23125\n",
      "Ave Test Precision: 0.20305926916221032\n",
      "Stdev Test Precision: 0.11447077407247937\n",
      "Ave Test Accuracy: 0.4977611940298508\n",
      "Stdev Test Accuracy: 0.1431494241335313\n",
      "Ave Test Specificity: 0.48415841584158414\n",
      "Ave Test Recall: 0.5393939393939394\n",
      "Ave Test NPV: 0.7687106345377489\n",
      "Ave Test F1-Score: 0.29485579711375637\n",
      "Ave Test G-mean: 0.3898449514301894\n",
      "Ave Runtime: 0.008230209350585938\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanSSS. 44 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "780 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "624 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22341625 0.025             nan 0.21008292 0.13882623 0.24495471\n",
      " 0.025             nan 0.21008292 0.13882623 0.24495471 0.025\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.025             nan 0.21008292 0.18422306 0.24495471\n",
      " 0.025             nan 0.21008292 0.18422306 0.24495471 0.025\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.025             nan 0.21008292 0.18422306 0.24495471\n",
      " 0.025             nan 0.21008292 0.18422306 0.24495471 0.025\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.025             nan 0.21008292 0.18422306 0.24495471\n",
      " 0.025             nan 0.21008292 0.18422306 0.24495471 0.025\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.06184211        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.04078947        nan 0.21008292 0.18422306 0.24495471 0.04078947\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.13099795        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.13099795        nan 0.21008292 0.18422306 0.24495471 0.13099795\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306]\n",
      "One or more of the train scores are non-finite: [0.22706693 0.02451524        nan 0.22828644 0.15234286 0.25179811\n",
      " 0.02451524        nan 0.22828644 0.15234286 0.25179811 0.02451524\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.02451524        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.02451524        nan 0.22828644 0.20153897 0.25179811 0.02451524\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.02451524        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.02451524        nan 0.22828644 0.20153897 0.25179811 0.02451524\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.02451524        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.02451524        nan 0.22828644 0.20153897 0.25179811 0.02451524\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.07639024        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.05076524        nan 0.22828644 0.20153897 0.25179811 0.05076524\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15247972        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15247972        nan 0.22828644 0.20153897 0.25179811 0.15247972\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.24495471161105836\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.3566084788029925\n",
      "GridSearchCV Runtime: 4.519453287124634 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.23880597014925373\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.20634920634920634\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.18571428571428572\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25\n",
      "Ave Test Precision: 0.23331674958540632\n",
      "Stdev Test Precision: 0.038881895317645854\n",
      "Ave Test Accuracy: 0.5694029850746269\n",
      "Stdev Test Accuracy: 0.11818996719198696\n",
      "Ave Test Specificity: 0.6475247524752475\n",
      "Ave Test Recall: 0.3303030303030303\n",
      "Ave Test NPV: 0.7451967178152107\n",
      "Ave Test F1-Score: 0.25701570614166597\n",
      "Ave Test G-mean: 0.4160454402909529\n",
      "Ave Runtime: 0.006975126266479492\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with payFamilySupport. 43 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "780 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "624 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.07807018 0.                nan 0.1061176  0.         0.26579155\n",
      " 0.                nan 0.26579155 0.24079155 0.26579155 0.\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.                nan 0.26579155 0.26579155 0.26579155\n",
      " 0.                nan 0.26579155 0.26579155 0.26579155 0.\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.                nan 0.26579155 0.26579155 0.26579155\n",
      " 0.                nan 0.26579155 0.26579155 0.26579155 0.\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.                nan 0.26579155 0.26579155 0.26579155\n",
      " 0.                nan 0.26579155 0.26579155 0.26579155 0.\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155]\n",
      "One or more of the train scores are non-finite: [0.0800134  0.                nan 0.10641855 0.         0.26605232\n",
      " 0.                nan 0.26605232 0.23927747 0.26605232 0.\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.                nan 0.26605232 0.26605232 0.26605232\n",
      " 0.                nan 0.26605232 0.26605232 0.26605232 0.\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.                nan 0.26605232 0.26605232 0.26605232\n",
      " 0.                nan 0.26605232 0.26605232 0.26605232 0.\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.                nan 0.26605232 0.26605232 0.26605232\n",
      " 0.                nan 0.26605232 0.26605232 0.26605232 0.\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2657915475780755\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.43640897755610975\n",
      "GridSearchCV Runtime: 4.436023473739624 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25925925925925924\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.288135593220339\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2617801047120419\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2541436464088398\n",
      "Ave Test Precision: 0.26266372072009603\n",
      "Stdev Test Precision: 0.014951161430659526\n",
      "Ave Test Accuracy: 0.4298507462686567\n",
      "Stdev Test Accuracy: 0.025086173602942703\n",
      "Ave Test Specificity: 0.3326732673267327\n",
      "Ave Test Recall: 0.7272727272727273\n",
      "Ave Test NPV: 0.7886334523402065\n",
      "Ave Test F1-Score: 0.3858555739079204\n",
      "Ave Test G-mean: 0.4912617142364824\n",
      "Ave Runtime: 0.00722036361694336\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanPagIbig. 42 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.03333333 0.03333333        nan 0.03333333 0.09333333 0.03333333\n",
      " 0.                nan 0.03333333 0.09333333 0.07907692 0.\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.11949359 0.03333333        nan 0.08069136 0.09333333 0.056\n",
      " 0.                nan 0.056      0.09333333 0.07907692 0.\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.                nan 0.056      0.11641026 0.07907692 0.\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.                nan 0.056      0.11641026 0.07907692 0.\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026]\n",
      "One or more of the train scores are non-finite: [0.23215995 0.05631579        nan 0.20358852 0.159807   0.23215995\n",
      " 0.                nan 0.20358852 0.159807   0.28148467 0.\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.27028166 0.05631579        nan 0.2228186  0.159807   0.2568408\n",
      " 0.                nan 0.22826937 0.159807   0.28148467 0.\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.05631579        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.                nan 0.22826937 0.18445088 0.28148467 0.\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.05631579        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.                nan 0.22826937 0.18445088 0.28148467 0.\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.14631579        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.08631579        nan 0.22826937 0.18445088 0.28148467 0.08631579\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-05, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.11949358974358974\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-05, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.2581047381546135\n",
      "GridSearchCV Runtime: 5.5879809856414795 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.14285714285714285\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.24806201550387597\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.23846153846153847\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2471042471042471\n",
      "Ave Test Precision: 0.22529698878536086\n",
      "Stdev Test Precision: 0.04629707141061084\n",
      "Ave Test Accuracy: 0.3567164179104477\n",
      "Stdev Test Accuracy: 0.2117122222449494\n",
      "Ave Test Specificity: 0.21980198019801983\n",
      "Ave Test Recall: 0.7757575757575758\n",
      "Ave Test NPV: 0.7407471264367815\n",
      "Ave Test F1-Score: 0.3190892493619334\n",
      "Ave Test G-mean: 0.16433163933532752\n",
      "Ave Runtime: 0.01231369972229004\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanGSIS. 41 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.                nan 0.34202381 0.34202381 0.34202381 0.\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.32291667 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.                nan 0.34202381 0.34202381 0.34202381 0.\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.                nan 0.34202381 0.34202381 0.34202381 0.\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.                nan 0.34202381 0.34202381 0.34202381 0.\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381]\n",
      "One or more of the train scores are non-finite: [0.32278992 0.09406699        nan 0.32278992 0.32278992 0.32278992\n",
      " 0.                nan 0.32278992 0.32278992 0.32278992 0.\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.31653457 0.09406699        nan 0.32278992 0.32278992 0.32278992\n",
      " 0.                nan 0.32278992 0.32278992 0.32278992 0.\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.09406699        nan 0.32278992 0.32278992 0.32278992\n",
      " 0.                nan 0.32278992 0.32278992 0.32278992 0.\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.09406699        nan 0.32278992 0.32278992 0.32278992\n",
      " 0.                nan 0.32278992 0.32278992 0.32278992 0.\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.09406699        nan 0.32278992 0.32278992 0.32278992\n",
      " 0.09406699        nan 0.32278992 0.32278992 0.32278992 0.09406699\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.3420238095238095\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.726932668329177\n",
      "GridSearchCV Runtime: 5.627745628356934 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.35\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2727272727272727\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4\n",
      "Ave Test Precision: 0.3283549783549783\n",
      "Stdev Test Precision: 0.051329743836055444\n",
      "Ave Test Accuracy: 0.7276119402985074\n",
      "Stdev Test Accuracy: 0.008750775671312359\n",
      "Ave Test Specificity: 0.9316831683168317\n",
      "Ave Test Recall: 0.10303030303030303\n",
      "Ave Test NPV: 0.760743992229473\n",
      "Ave Test F1-Score: 0.15606447568489804\n",
      "Ave Test G-mean: 0.30746008147610804\n",
      "Ave Runtime: 0.009988212585449218\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanPersonal. 40 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.35       0.                nan 0.35       0.3        0.35\n",
      " 0.                nan 0.35       0.3        0.35       0.\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.                nan 0.34969136 0.3        0.35\n",
      " 0.                nan 0.35       0.3        0.35       0.\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.                nan 0.35       0.3        0.35\n",
      " 0.                nan 0.35       0.32368421 0.35       0.\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.03333333        nan 0.35       0.32368421 0.35\n",
      " 0.                nan 0.35       0.32368421 0.35       0.\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.18333333        nan 0.35       0.32368421 0.35\n",
      " 0.18333333        nan 0.35       0.32368421 0.35       0.18333333\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.3               nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.3               nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.3               nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.3               nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421]\n",
      "One or more of the train scores are non-finite: [0.34779107 0.                nan 0.34779107 0.31621212 0.34779107\n",
      " 0.                nan 0.34779107 0.31621212 0.34779107 0.\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.                nan 0.32715857 0.31621212 0.34779107\n",
      " 0.                nan 0.34779107 0.31621212 0.34779107 0.\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.                nan 0.34779107 0.31621212 0.34779107\n",
      " 0.                nan 0.34779107 0.34053645 0.34779107 0.\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.035             nan 0.34779107 0.34053645 0.34779107\n",
      " 0.                nan 0.34779107 0.34053645 0.34779107 0.\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.10015152        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.10015152        nan 0.34779107 0.34053645 0.34779107 0.10015152\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.31621212        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.31621212        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.31621212        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.31621212        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.35\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7456359102244389\n",
      "GridSearchCV Runtime: 5.414266109466553 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.18181818181818182\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.6\n",
      "Ave Test Precision: 0.32303030303030306\n",
      "Stdev Test Precision: 0.16764999177716466\n",
      "Ave Test Accuracy: 0.7425373134328359\n",
      "Stdev Test Accuracy: 0.01087864159486063\n",
      "Ave Test Specificity: 0.9742574257425742\n",
      "Ave Test Recall: 0.03333333333333334\n",
      "Ave Test NPV: 0.7551639783529711\n",
      "Ave Test F1-Score: 0.059825406452537665\n",
      "Ave Test G-mean: 0.1771968772153508\n",
      "Ave Runtime: 0.009093809127807616\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasPensioner. 39 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "975 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "130 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "260 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "585 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.0684771  0.02469136        nan 0.07441789 0.0521978  0.1618558\n",
      " 0.02469136        nan 0.16822178 0.0521978  0.22254674 0.02469136\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.02469136        nan 0.21913087 0.14636352 0.18725263\n",
      " 0.02469136        nan 0.16822178 0.15239527 0.22254674 0.02469136\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.02469136        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.02469136        nan 0.21913087 0.19959269 0.22254674 0.02469136\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.02469136        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.02469136        nan 0.21913087 0.19959269 0.22254674 0.02469136\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.0265625         nan 0.21913087 0.19959269 0.18725263\n",
      " 0.0265625         nan 0.21913087 0.19959269 0.22254674 0.0265625\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269]\n",
      "One or more of the train scores are non-finite: [0.09917277 0.02454924        nan 0.09954993 0.04797748 0.19846212\n",
      " 0.02454924        nan 0.19880329 0.04797748 0.24599383 0.02454924\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.02454924        nan 0.24800822 0.14718676 0.22306389\n",
      " 0.02454924        nan 0.19880329 0.14652584 0.24599383 0.02454924\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.02454924        nan 0.24800822 0.19612667 0.22306389\n",
      " 0.02454924        nan 0.24800822 0.19612667 0.24599383 0.02454924\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.02454924        nan 0.24800822 0.19612667 0.22306389\n",
      " 0.02454924        nan 0.24800822 0.19612667 0.24599383 0.02454924\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.02446809        nan 0.24800822 0.19612667 0.22306389\n",
      " 0.02446809        nan 0.24800822 0.19612667 0.24599383 0.02446809\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2225467444686368\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.35785536159601\n",
      "GridSearchCV Runtime: 5.130429983139038 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2350230414746544\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.16666666666666666\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2727272727272727\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.24193548387096775\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.21311475409836064\n",
      "Ave Test Precision: 0.22589344376758444\n",
      "Stdev Test Precision: 0.0393865782196834\n",
      "Ave Test Accuracy: 0.5656716417910448\n",
      "Stdev Test Accuracy: 0.13547910907339883\n",
      "Ave Test Specificity: 0.6445544554455445\n",
      "Ave Test Recall: 0.3242424242424242\n",
      "Ave Test NPV: 0.7390832716914387\n",
      "Ave Test F1-Score: 0.24619617383496034\n",
      "Ave Test G-mean: 0.39397032977849544\n",
      "Ave Runtime: 0.009513092041015626\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasPrivateEmployee. 38 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.02727273 0.12219136 0.17219136 0.         0.02991453 0.12442131\n",
      " 0.12219136 0.17219136 0.22178444 0.02991453 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.20543651 0.12219136 0.17219136 0.22178444 0.17887471 0.19986833\n",
      " 0.12219136 0.17219136 0.22178444 0.20281837 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12219136 0.17219136 0.22178444 0.20281837 0.19986833\n",
      " 0.12219136 0.17219136 0.22178444 0.20281837 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12219136 0.17219136 0.22178444 0.20281837 0.19986833\n",
      " 0.12219136 0.17219136 0.22178444 0.20281837 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12219136 0.17219136 0.22178444 0.20281837 0.19986833\n",
      " 0.12219136 0.17219136 0.22178444 0.20281837 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.10889882 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.10889882 0.17688219 0.22178444 0.20281837 0.22294525 0.10889882\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837]\n",
      "One or more of the train scores are non-finite: [0.04736842 0.12288719 0.17191766 0.         0.0474116  0.14704803\n",
      " 0.12288719 0.17191766 0.22392204 0.0474116  0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22302655 0.12288719 0.17191766 0.22392204 0.19657363 0.22153071\n",
      " 0.12288719 0.17191766 0.22392204 0.22153363 0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.12288719 0.17191766 0.22392204 0.22153363 0.22153071\n",
      " 0.12288719 0.17191766 0.22392204 0.22153363 0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.12288719 0.17191766 0.22392204 0.22153363 0.22153071\n",
      " 0.12288719 0.17191766 0.22392204 0.22153363 0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.12288719 0.17191766 0.22392204 0.22153363 0.22153071\n",
      " 0.12288719 0.17191766 0.22392204 0.22153363 0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11895355 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11895355 0.17368407 0.22392204 0.22153363 0.24411135 0.11895355\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.22947674419304326\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.3179551122194514\n",
      "GridSearchCV Runtime: 5.604830741882324 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2145748987854251\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3448275862068966\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.21982758620689655\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2647058823529412\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23529411764705882\n",
      "Ave Test Precision: 0.25584601423984366\n",
      "Stdev Test Precision: 0.053430398639459815\n",
      "Ave Test Accuracy: 0.5194029850746269\n",
      "Stdev Test Accuracy: 0.24837128176702294\n",
      "Ave Test Specificity: 0.5594059405940593\n",
      "Ave Test Recall: 0.39696969696969686\n",
      "Ave Test NPV: 0.6477046198803521\n",
      "Ave Test F1-Score: 0.24629326838979212\n",
      "Ave Test G-mean: 0.3005806224694716\n",
      "Ave Runtime: 0.008933591842651366\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasBusiness. 37 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.01666667 0.         0.025      0.         0.01666667 0.03903509\n",
      " 0.         0.025      0.         0.01666667 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.24521431 0.         0.025      0.24420077 0.06634323 0.18843962\n",
      " 0.         0.025      0.17102509 0.06559536 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.         0.025      0.19670077 0.21408065 0.18843962\n",
      " 0.         0.025      0.19670077 0.21408065 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.         0.025      0.19670077 0.21408065 0.18843962\n",
      " 0.         0.025      0.19670077 0.21408065 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.         0.025      0.19670077 0.21408065 0.18843962\n",
      " 0.         0.025      0.19670077 0.21408065 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.         0.025      0.19670077 0.21408065 0.18843962\n",
      " 0.         0.025      0.19670077 0.21408065 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065]\n",
      "One or more of the train scores are non-finite: [0.06805556 0.         0.02451524 0.04849806 0.04513889 0.09309259\n",
      " 0.         0.02451524 0.04849806 0.04513889 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.24727836 0.         0.02451524 0.24760596 0.09466014 0.2416157\n",
      " 0.         0.02451524 0.22212889 0.09473082 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.         0.02451524 0.24679654 0.24336471 0.2416157\n",
      " 0.         0.02451524 0.24679654 0.24336471 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.         0.02451524 0.24679654 0.24336471 0.2416157\n",
      " 0.         0.02451524 0.24679654 0.24336471 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.         0.02451524 0.24679654 0.24336471 0.2416157\n",
      " 0.         0.02451524 0.24679654 0.24336471 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.0255814  0.02451524 0.24679654 0.24336471 0.2416157\n",
      " 0.0255814  0.02451524 0.24679654 0.24336471 0.2416157  0.0255814\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-05, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.24521431392706322\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-05, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.2817955112219451\n",
      "GridSearchCV Runtime: 5.5226826667785645 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.23293172690763053\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.236\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.29411764705882354\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.29411764705882354\n",
      "Ave Test Precision: 0.2685762613479127\n",
      "Stdev Test Precision: 0.03134558296682887\n",
      "Ave Test Accuracy: 0.5388059701492537\n",
      "Stdev Test Accuracy: 0.2551485880038287\n",
      "Ave Test Specificity: 0.5831683168316831\n",
      "Ave Test Recall: 0.403030303030303\n",
      "Ave Test NPV: 0.6922175445765297\n",
      "Ave Test F1-Score: 0.22411331593548017\n",
      "Ave Test G-mean: 0.25267476404609096\n",
      "Ave Runtime: 0.009825658798217774\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasFreelancer. 36 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.         0.04875           nan 0.         0.         0.09709085\n",
      " 0.04875           nan 0.09512141 0.         0.22453706 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.07243421        nan 0.22453706 0.17209926 0.1984501\n",
      " 0.07243421        nan 0.22453706 0.16904371 0.22453706 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.07243421        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.07243421        nan 0.22453706 0.24919459 0.22453706 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.07243421        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.07243421        nan 0.22453706 0.24919459 0.22453706 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.07243421        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.07243421        nan 0.22453706 0.24919459 0.22453706 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.09417334        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.09417334        nan 0.22453706 0.24919459 0.22453706 0.09417334\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11070112        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11070112        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459]\n",
      "One or more of the train scores are non-finite: [0.         0.04524197        nan 0.         0.         0.09999804\n",
      " 0.04524197        nan 0.10022808 0.         0.22435173 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.07031843        nan 0.22435173 0.17476926 0.19954084\n",
      " 0.07031843        nan 0.22435173 0.17511243 0.22435173 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.07031843        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.07031843        nan 0.22435173 0.24931368 0.22435173 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.07031843        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.07031843        nan 0.22435173 0.24931368 0.22435173 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.07031843        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.07031843        nan 0.22435173 0.24931368 0.22435173 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.09558318        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.09558318        nan 0.22435173 0.24931368 0.22435173 0.09558318\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.11721755        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.11721755        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.24919459136576155\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.29800498753117205\n",
      "GridSearchCV Runtime: 5.398139476776123 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25311203319502074\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.26141078838174275\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25210084033613445\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.24481327800829875\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2625\n",
      "Ave Test Precision: 0.25478738798423933\n",
      "Stdev Test Precision: 0.007294879003087672\n",
      "Ave Test Accuracy: 0.31417910447761194\n",
      "Stdev Test Accuracy: 0.013297410209854815\n",
      "Ave Test Specificity: 0.11386138613861385\n",
      "Ave Test Recall: 0.9272727272727274\n",
      "Ave Test NPV: 0.8274603174603176\n",
      "Ave Test F1-Score: 0.39973679168221377\n",
      "Ave Test G-mean: 0.324780082136426\n",
      "Ave Runtime: 0.012639665603637695\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasGovtEmployee. 35 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22270722 0.08865947        nan 0.26361631 0.23634359 0.22270722\n",
      " 0.                nan 0.26361631 0.23634359 0.2399486  0.\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.08865947        nan 0.26361631 0.23634359 0.2399486\n",
      " 0.                nan 0.26361631 0.26098127 0.2399486  0.\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.08865947        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.                nan 0.26361631 0.26098127 0.2399486  0.\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.14032614        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.                nan 0.26361631 0.26098127 0.2399486  0.\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127]\n",
      "One or more of the train scores are non-finite: [0.24356555 0.10942141        nan 0.26821344 0.24141605 0.24356555\n",
      " 0.                nan 0.26821344 0.24141605 0.26822072 0.\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.10942141        nan 0.26821344 0.24141605 0.26822072\n",
      " 0.                nan 0.26821344 0.26531763 0.26822072 0.\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.10942141        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.                nan 0.26821344 0.26531763 0.26822072 0.\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.16335038        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.                nan 0.26821344 0.26531763 0.26822072 0.\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2636163131673967\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6596009975062345\n",
      "GridSearchCV Runtime: 5.556886911392212 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.36538461538461536\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.27586206896551724\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3275862068965517\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3103448275862069\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2903225806451613\n",
      "Ave Test Precision: 0.3139000598956105\n",
      "Stdev Test Precision: 0.03483315960478191\n",
      "Ave Test Accuracy: 0.673134328358209\n",
      "Stdev Test Accuracy: 0.01858193969849065\n",
      "Ave Test Specificity: 0.803960396039604\n",
      "Ave Test Recall: 0.2727272727272727\n",
      "Ave Test NPV: 0.7717843016386705\n",
      "Ave Test F1-Score: 0.2916245215965008\n",
      "Ave Test G-mean: 0.46809626526532977\n",
      "Ave Runtime: 0.0073432445526123045\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasOFW. 34 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.29961993 0.14510802 0.         0.29961993 0.29961993 0.29961993\n",
      " 0.09844136 0.         0.29961993 0.29961993 0.29961993 0.09844136\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.14510802 0.         0.29961993 0.29961993 0.29961993\n",
      " 0.09844136 0.         0.29961993 0.29961993 0.29961993 0.09844136\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.14510802 0.         0.29961993 0.29961993 0.29961993\n",
      " 0.09844136 0.         0.29961993 0.29961993 0.29961993 0.09844136\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.14510802 0.         0.29961993 0.29961993 0.29961993\n",
      " 0.09844136 0.         0.29961993 0.29961993 0.29961993 0.09844136\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.         0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.         0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993]\n",
      "One or more of the train scores are non-finite: [0.30323976 0.16032457 0.         0.30323976 0.30323976 0.30323976\n",
      " 0.09823345 0.         0.30323976 0.30323976 0.30323976 0.09823345\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.16032457 0.         0.30323976 0.30323976 0.30323976\n",
      " 0.09823345 0.         0.30323976 0.30323976 0.30323976 0.09823345\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.16032457 0.         0.30323976 0.30323976 0.30323976\n",
      " 0.09823345 0.         0.30323976 0.30323976 0.30323976 0.09823345\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.16032457 0.         0.30323976 0.30323976 0.30323976\n",
      " 0.09823345 0.         0.30323976 0.30323976 0.30323976 0.09823345\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.         0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.         0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2996199340112383\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6346633416458853\n",
      "GridSearchCV Runtime: 5.472182750701904 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.20512820512820512\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.38666666666666666\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2328767123287671\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2558139534883721\n",
      "Ave Test Precision: 0.26609710752240223\n",
      "Stdev Test Precision: 0.07021873047074419\n",
      "Ave Test Accuracy: 0.6134328358208955\n",
      "Stdev Test Accuracy: 0.04403933619050918\n",
      "Ave Test Specificity: 0.7089108910891089\n",
      "Ave Test Recall: 0.32121212121212117\n",
      "Ave Test NPV: 0.7615295046437669\n",
      "Ave Test F1-Score: 0.29067240528485866\n",
      "Ave Test G-mean: 0.47496608666078843\n",
      "Ave Runtime: 0.009041213989257812\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseOnlyFamily. 33 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.31037195 0.14936077        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.                nan 0.31037195 0.31037195 0.31037195 0.\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.14936077        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.                nan 0.31037195 0.31037195 0.31037195 0.\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.14936077        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.                nan 0.31037195 0.31037195 0.31037195 0.\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.14936077        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.                nan 0.31037195 0.31037195 0.31037195 0.\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.14936077        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.14936077        nan 0.31037195 0.31037195 0.31037195 0.14936077\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.14936077        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.14936077        nan 0.31037195 0.31037195 0.31037195 0.14936077\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195]\n",
      "One or more of the train scores are non-finite: [0.31918216 0.12503367        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.                nan 0.31918216 0.31918216 0.31918216 0.\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.12503367        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.                nan 0.31918216 0.31918216 0.31918216 0.\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.12503367        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.                nan 0.31918216 0.31918216 0.31918216 0.\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.12503367        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.                nan 0.31918216 0.31918216 0.31918216 0.\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.12503367        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.12503367        nan 0.31918216 0.31918216 0.31918216 0.12503367\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.12503367        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.12503367        nan 0.31918216 0.31918216 0.31918216 0.12503367\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.3103719517496607\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6795511221945137\n",
      "GridSearchCV Runtime: 5.091847658157349 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.24193548387096775\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2698412698412698\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.4230769230769231\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2978723404255319\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25396825396825395\n",
      "Ave Test Precision: 0.2973388542365893\n",
      "Stdev Test Precision: 0.07334338096633872\n",
      "Ave Test Accuracy: 0.6649253731343284\n",
      "Stdev Test Accuracy: 0.03818023057398948\n",
      "Ave Test Specificity: 0.7990099009900989\n",
      "Ave Test Recall: 0.2545454545454545\n",
      "Ave Test NPV: 0.7661005067693928\n",
      "Ave Test F1-Score: 0.27333437470568334\n",
      "Ave Test G-mean: 0.4497861963040686\n",
      "Ave Runtime: 0.006584072113037109\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseExtendedFamily. 32 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.28341881 0.30843881 0.22833333 0.28341881 0.2621076  0.28341881\n",
      " 0.04969136 0.         0.28341881 0.26152789 0.29806797 0.04969136\n",
      " 0.         0.29892314 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.28397153 0.30843881 0.22833333 0.28397153 0.26086501 0.28397153\n",
      " 0.04969136 0.         0.28397153 0.26086501 0.29806797 0.04969136\n",
      " 0.         0.29892314 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.28465181 0.31389007 0.22833333 0.28465181 0.26086501 0.28957046\n",
      " 0.04969136 0.         0.29892314 0.2661736  0.29806797 0.04969136\n",
      " 0.         0.29892314 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.28875481 0.34712071 0.35833333 0.2896083  0.26235391 0.29806797\n",
      " 0.24135802 0.         0.29816683 0.27554335 0.29806797 0.24135802\n",
      " 0.         0.29816683 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.28947671 0.42689708 0.43807692 0.28998094 0.26387487 0.29806797\n",
      " 0.32367242 0.3        0.29816683 0.27554335 0.29806797 0.32367242\n",
      " 0.3        0.29816683 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29440482 0.41445744 0.34813897 0.29129142 0.2730491  0.29544649\n",
      " 0.41029576 0.34261822 0.30113126 0.27554335 0.29667801 0.41029576\n",
      " 0.34261822 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35744222 0.29838401 0.27554335 0.29544649\n",
      " 0.40960895 0.35839424 0.29838401 0.27554335 0.29667801 0.40960895\n",
      " 0.35839424 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35592707 0.29838401 0.27554335 0.29544649\n",
      " 0.41491198 0.35592707 0.29838401 0.27554335 0.29667801 0.40960895\n",
      " 0.35592707 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35592707 0.29838401 0.27554335 0.29544649\n",
      " 0.41491198 0.35592707 0.29838401 0.27554335 0.29667801 0.40960895\n",
      " 0.35592707 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35592707 0.29838401 0.27554335 0.29544649\n",
      " 0.41491198 0.35592707 0.29838401 0.27554335 0.29667801 0.41491198\n",
      " 0.35592707 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35592707 0.29838401 0.27554335 0.29544649\n",
      " 0.41491198 0.35592707 0.29838401 0.27554335 0.29667801 0.41491198\n",
      " 0.35592707 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35592707 0.29838401 0.27554335 0.29544649\n",
      " 0.41491198 0.35592707 0.29838401 0.27554335 0.29667801 0.41491198\n",
      " 0.35592707 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35592707 0.29838401 0.27554335 0.29544649\n",
      " 0.41491198 0.35592707 0.29838401 0.27554335 0.29667801 0.41491198\n",
      " 0.35592707 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335]\n",
      "One or more of the train scores are non-finite: [0.2856378  0.2933594  0.34086814 0.2856378  0.25202424 0.2856378\n",
      " 0.04906447 0.         0.2856378  0.252463   0.29533539 0.04906447\n",
      " 0.         0.29778371 0.26544085        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.28580001 0.2933594  0.34441699 0.28580001 0.25250385 0.28580001\n",
      " 0.04906447 0.         0.28580001 0.25250385 0.29533539 0.04906447\n",
      " 0.         0.29778371 0.26544085        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.2863845  0.29306972 0.34441699 0.2863845  0.2524486  0.29332285\n",
      " 0.04906447 0.         0.2981182  0.25522607 0.29533539 0.04906447\n",
      " 0.         0.29778371 0.26544085        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29001797 0.2955035  0.32130187 0.29103067 0.25320988 0.29559315\n",
      " 0.31316811 0.         0.2982698  0.26515107 0.29559315 0.31316811\n",
      " 0.         0.2982698  0.26515107        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.28735149 0.30015886 0.33957052 0.28806455 0.26142481 0.29544123\n",
      " 0.30131265 0.31071078 0.29900407 0.26515107 0.29544123 0.30276379\n",
      " 0.31071078 0.29900407 0.26515107        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29393078 0.30698773 0.33156927 0.2974003  0.26237361 0.29520762\n",
      " 0.31067495 0.33565592 0.29870723 0.26442213 0.29536663 0.30845721\n",
      " 0.33565592 0.29870723 0.26437569        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29445094 0.31039453 0.32183095 0.29826983 0.26451487 0.29520762\n",
      " 0.31184991 0.32383637 0.29860028 0.26442213 0.29536663 0.31142982\n",
      " 0.32383637 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29520762 0.31039453 0.32131088 0.29860028 0.26442213 0.29520762\n",
      " 0.31039453 0.32232494 0.29860028 0.26442213 0.29536663 0.31144303\n",
      " 0.32232494 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29520762 0.31039453 0.32131088 0.29860028 0.26442213 0.29520762\n",
      " 0.31039453 0.32131088 0.29860028 0.26442213 0.29536663 0.31144303\n",
      " 0.32131088 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29520762 0.31039453 0.32131088 0.29860028 0.26442213 0.29520762\n",
      " 0.31039453 0.32131088 0.29860028 0.26442213 0.29536663 0.31102109\n",
      " 0.32131088 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29520762 0.31039453 0.32131088 0.29860028 0.26442213 0.29520762\n",
      " 0.31039453 0.32131088 0.29860028 0.26442213 0.29536663 0.31102109\n",
      " 0.32131088 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29520762 0.31039453 0.32131088 0.29860028 0.26442213 0.29520762\n",
      " 0.31039453 0.32131088 0.29860028 0.26442213 0.29536663 0.31102109\n",
      " 0.32131088 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29520762 0.31039453 0.32131088 0.29860028 0.26442213 0.29520762\n",
      " 0.31039453 0.32131088 0.29860028 0.26442213 0.29536663 0.31102109\n",
      " 0.32131088 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.43807692307692303\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7394014962593516\n",
      "GridSearchCV Runtime: 5.12712287902832 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.23076923076923078\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.5\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.5\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.14285714285714285\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2727272727272727\n",
      "Ave Test Precision: 0.32927072927072926\n",
      "Stdev Test Precision: 0.16274727390343235\n",
      "Ave Test Accuracy: 0.741044776119403\n",
      "Stdev Test Accuracy: 0.011975228758120283\n",
      "Ave Test Specificity: 0.9742574257425742\n",
      "Ave Test Recall: 0.02727272727272727\n",
      "Ave Test NPV: 0.7540082079190025\n",
      "Ave Test F1-Score: 0.048018446939284566\n",
      "Ave Test G-mean: 0.15672304849874294\n",
      "Ave Runtime: 0.012569999694824219\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyUtilityBills. 31 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24188518 0.21063518 0.2535305  0.24188518 0.24188518 0.2559535\n",
      " 0.26767399 0.32306798 0.2517402  0.2690517  0.2649534  0.26767399\n",
      " 0.32306798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.24188518 0.21682616 0.25344155 0.24188518 0.24188518 0.2517402\n",
      " 0.25690476 0.28806798 0.25884546 0.2716158  0.2649534  0.26767399\n",
      " 0.28806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.24262702 0.20896304 0.26746362 0.23854409 0.24340034 0.26352926\n",
      " 0.26767399 0.27806798 0.26352926 0.2690517  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.23753447 0.22209955 0.27719136 0.23414265 0.23428799 0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.26315    0.25357143 0.27905033 0.24266585 0.24210507 0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.27005495 0.27806798 0.25239289 0.26419397 0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405]\n",
      "One or more of the train scores are non-finite: [0.24056735 0.21732792 0.24289783 0.24056735 0.24056735 0.2582042\n",
      " 0.32344983 0.34163165 0.25915094 0.26076208 0.26522162 0.32344983\n",
      " 0.34163165 0.2676002  0.26734693        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.24056735 0.22122198 0.24250732 0.24056735 0.24056735 0.25843653\n",
      " 0.32073301 0.30848347 0.25830426 0.25742391 0.26522162 0.32307946\n",
      " 0.30848347 0.2676002  0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.24504329 0.2161901  0.25047042 0.24504618 0.24311069 0.25999065\n",
      " 0.32307946 0.30848347 0.26112692 0.26027291 0.26522162 0.32307946\n",
      " 0.30848347 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.24076939 0.2396737  0.29723856 0.24104709 0.2398313  0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.2603177  0.32579065 0.31011153 0.25073555 0.24466804 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32323322 0.30718292 0.25765249 0.25850629 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26162012 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26095249 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26095249 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26095249 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26095249 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26095249 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26095249 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.32306798140131476\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7481296758104738\n",
      "GridSearchCV Runtime: 5.610814809799194 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.42857142857142855\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.36363636363636365\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.36363636363636365\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4444444444444444\n",
      "Ave Test Precision: 0.38672438672438675\n",
      "Stdev Test Precision: 0.04743284246433938\n",
      "Ave Test Accuracy: 0.7470149253731343\n",
      "Stdev Test Accuracy: 0.004087481772426625\n",
      "Ave Test Specificity: 0.9752475247524753\n",
      "Ave Test Recall: 0.048484848484848485\n",
      "Ave Test NPV: 0.7582930318062935\n",
      "Ave Test F1-Score: 0.08512723250543382\n",
      "Ave Test G-mean: 0.2120873200015206\n",
      "Ave Runtime: 0.012636041641235352\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyVices. 30 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27559308 0.26685048 0.29864253 0.27559308 0.27559308 0.27559308\n",
      " 0.14563272 0.         0.27559308 0.27559308 0.27902625 0.14563272\n",
      " 0.         0.28040213 0.27753464        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.27559308 0.26685048 0.29944899 0.27559308 0.27559308 0.27559308\n",
      " 0.24563272 0.         0.27559308 0.27559308 0.27902625 0.24563272\n",
      " 0.         0.28040213 0.27753464        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.27018457 0.26747012 0.30197587 0.27113154 0.27629821 0.27902625\n",
      " 0.23433121 0.         0.28040213 0.26985997 0.27902625 0.23175768\n",
      " 0.         0.28040213 0.27753464        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.26721534 0.25836767 0.32592671 0.26721534 0.26725493 0.27902625\n",
      " 0.27161781 0.05       0.28210343 0.27753464 0.27902625 0.27161781\n",
      " 0.05       0.28210343 0.27753464        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.27903616 0.27133802 0.34142485 0.276148   0.27536741 0.27784107\n",
      " 0.28351995 0.37452381 0.28169809 0.27607186 0.27784107 0.28351995\n",
      " 0.37452381 0.28169809 0.27607186        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28497493 0.28372634 0.35722222 0.28560985 0.27801488 0.28308099\n",
      " 0.28141536 0.38029762 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.38029762 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28081476 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.37009921 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.37009921 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.36340803 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.36340803 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.36340803 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.36340803 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.36340803 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.36340803 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262 ]\n",
      "One or more of the train scores are non-finite: [0.26930146 0.26762252 0.3089438  0.26930146 0.26930146 0.26930146\n",
      " 0.14757493 0.         0.26930146 0.26930146 0.27898251 0.14757493\n",
      " 0.         0.27847205 0.28029106        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.26937379 0.26762252 0.30865565 0.26937379 0.26930146 0.26937379\n",
      " 0.2792416  0.         0.26937379 0.26930146 0.27898251 0.2792416\n",
      " 0.         0.27847205 0.28029106        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.268036   0.26820301 0.31026667 0.268036   0.26845701 0.27909351\n",
      " 0.29406433 0.         0.27859526 0.27456115 0.27898251 0.29435159\n",
      " 0.         0.27847205 0.28029106        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.26531896 0.26769462 0.31037838 0.26569284 0.2652553  0.27875237\n",
      " 0.27557168 0.4452381  0.27831342 0.28015878 0.27875237 0.27557168\n",
      " 0.4452381  0.27831342 0.28015878        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27537047 0.27233797 0.33821835 0.27450209 0.27123192 0.2790534\n",
      " 0.27624563 0.42440367 0.27928318 0.28077078 0.2790534  0.27624563\n",
      " 0.42440367 0.27928318 0.28077078        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.28008782 0.27602334 0.34323621 0.27908157 0.27867439 0.27863227\n",
      " 0.27739138 0.35313799 0.27908564 0.28019789 0.27863227 0.27739138\n",
      " 0.35313799 0.27908564 0.28019789        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27943248 0.2768604  0.34449406 0.27904163 0.27977057 0.27911561\n",
      " 0.27744748 0.34405652 0.27896561 0.28034557 0.27851643 0.27744748\n",
      " 0.34405652 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27911561 0.2770838  0.34570448 0.27926877 0.28020991 0.27911561\n",
      " 0.27730812 0.34506968 0.27896561 0.28034557 0.27851643 0.27730812\n",
      " 0.34374078 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27911561 0.27730812 0.34570448 0.27896561 0.28020991 0.27911561\n",
      " 0.27730812 0.34570448 0.27896561 0.28034557 0.27851643 0.27730812\n",
      " 0.34437558 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27911561 0.27730812 0.34570448 0.27896561 0.28034557 0.27911561\n",
      " 0.27730812 0.34570448 0.27896561 0.28034557 0.27851643 0.27730812\n",
      " 0.34437558 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27911561 0.27730812 0.34570448 0.27896561 0.28034557 0.27911561\n",
      " 0.27730812 0.34570448 0.27896561 0.28034557 0.27851643 0.27730812\n",
      " 0.34437558 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27911561 0.27730812 0.34570448 0.27896561 0.28034557 0.27911561\n",
      " 0.27730812 0.34570448 0.27896561 0.28034557 0.27851643 0.27730812\n",
      " 0.34437558 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27911561 0.27730812 0.34570448 0.27896561 0.28034557 0.27911561\n",
      " 0.27730812 0.34570448 0.27896561 0.28034557 0.27851643 0.27730812\n",
      " 0.34437558 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.380297619047619\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7119700748129676\n",
      "GridSearchCV Runtime: 5.064406394958496 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.3225806451612903\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.5714285714285714\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.5\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3157894736842105\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2857142857142857\n",
      "Ave Test Precision: 0.39910259519767155\n",
      "Stdev Test Precision: 0.12799425979410642\n",
      "Ave Test Accuracy: 0.7283582089552239\n",
      "Stdev Test Accuracy: 0.030473905953873746\n",
      "Ave Test Specificity: 0.9287128712871286\n",
      "Ave Test Recall: 0.11515151515151514\n",
      "Ave Test NPV: 0.7626080945065177\n",
      "Ave Test F1-Score: 0.1653454233927248\n",
      "Ave Test G-mean: 0.3139468889712652\n",
      "Ave Runtime: 0.014818572998046875\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyExpenses. 29 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.206957   0.2070859  0.36390693 0.206957   0.27026184 0.206957\n",
      " 0.10944878 0.         0.20645866 0.2697635  0.20121743 0.11718034\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20645866 0.20486816 0.36390693 0.20645866 0.2697635  0.20577725\n",
      " 0.11843335 0.         0.20694236 0.26944344 0.20121743 0.11748097\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20787322 0.21053384 0.36525974 0.20787322 0.27117806 0.20746609\n",
      " 0.18497553 0.         0.20632572 0.26860801 0.20121743 0.18607443\n",
      " 0.         0.19714027 0.27106964        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20912052 0.2129899  0.3552381  0.20872158 0.27294574 0.20317587\n",
      " 0.22124032 0.         0.20258582 0.27071881 0.20121743 0.22124032\n",
      " 0.         0.19760485 0.27071881        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20154865 0.25548036 0.37285714 0.20119575 0.26782754 0.20317587\n",
      " 0.23358949 0.36666667 0.19997585 0.27000482 0.20121743 0.23358949\n",
      " 0.36666667 0.19760485 0.27000482        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20169311 0.25746124 0.38333333 0.1998966  0.26986083 0.20317587\n",
      " 0.25898576 0.4        0.19997585 0.27063283 0.20121743 0.25898576\n",
      " 0.4        0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.38333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.38333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283]\n",
      "One or more of the train scores are non-finite: [0.21406456 0.21753168 0.34579767 0.21433556 0.25980589 0.21485609\n",
      " 0.11555348 0.         0.21524006 0.2605446  0.21190105 0.11858785\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21524006 0.21779676 0.3463148  0.21524006 0.26046086 0.21693595\n",
      " 0.18597355 0.         0.21607005 0.26201811 0.21190105 0.1852493\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21499148 0.21400501 0.34650148 0.21507196 0.26053989 0.21511944\n",
      " 0.2913795  0.         0.21487681 0.26369047 0.21176171 0.29039832\n",
      " 0.         0.21053761 0.26395912        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2174422  0.22071031 0.35897384 0.2172269  0.26169073 0.21319033\n",
      " 0.28575602 0.16666667 0.21490766 0.26352371 0.21179116 0.28575602\n",
      " 0.16666667 0.2106657  0.26352371        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21301237 0.24906234 0.38825152 0.21245925 0.25972252 0.21316659\n",
      " 0.26871203 0.47803207 0.21228805 0.26231435 0.21176742 0.26930026\n",
      " 0.47803207 0.21081978 0.26231435        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21130223 0.26264816 0.41770404 0.21152949 0.26028815 0.21310005\n",
      " 0.26431071 0.42410912 0.21241002 0.2625036  0.2117339  0.26431071\n",
      " 0.42410912 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21178291 0.26243359 0.41615745 0.21108015 0.26219953 0.21310005\n",
      " 0.26431071 0.41599638 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41599638 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26210887 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.39999999999999997\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7518703241895262\n",
      "GridSearchCV Runtime: 5.0297582149505615 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3870967741935484\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2727272727272727\n",
      "Ave Test Precision: 0.32910766652702134\n",
      "Stdev Test Precision: 0.059785972659199466\n",
      "Ave Test Accuracy: 0.7358208955223879\n",
      "Stdev Test Accuracy: 0.007177382112563941\n",
      "Ave Test Specificity: 0.9504950495049505\n",
      "Ave Test Recall: 0.0787878787878788\n",
      "Ave Test NPV: 0.759685019948753\n",
      "Ave Test F1-Score: 0.12191105693004774\n",
      "Ave Test G-mean: 0.2613548711958696\n",
      "Ave Runtime: 0.0016713619232177734\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlySoloNetIncome. 28 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1105 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "182 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "364 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "559 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.07694209 0.                nan 0.07421482 0.         0.25989423\n",
      " 0.                nan 0.25989423 0.1774121  0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.                nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.                nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.                nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.07478773        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.07478773        nan 0.25989423 0.25989423 0.25989423 0.07478773\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.25989423        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.25989423        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423]\n",
      "One or more of the train scores are non-finite: [0.07770881 0.                nan 0.07804872 0.         0.25887731\n",
      " 0.                nan 0.25887731 0.18165174 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.                nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.                nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.                nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.07799125        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.07799125        nan 0.25887731 0.25887731 0.25887731 0.07799125\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.25887731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.25887731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.259894230383722\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.415211970074813\n",
      "GridSearchCV Runtime: 4.621138572692871 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2736842105263158\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25263157894736843\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.22941176470588234\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.28888888888888886\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2512820512820513\n",
      "Ave Test Precision: 0.25917969887010134\n",
      "Stdev Test Precision: 0.022827600872137695\n",
      "Ave Test Accuracy: 0.42164179104477606\n",
      "Stdev Test Accuracy: 0.03099486516014207\n",
      "Ave Test Specificity: 0.32178217821782173\n",
      "Ave Test Recall: 0.7272727272727272\n",
      "Ave Test NPV: 0.7844531528484562\n",
      "Ave Test F1-Score: 0.38200032588432253\n",
      "Ave Test G-mean: 0.48221180258827284\n",
      "Ave Runtime: 0.010414648056030273\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positiveMonthlySoloNetIncome. 27 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22669875 0.23653443 0.23010684 0.22669875 0.23660771 0.22669875\n",
      " 0.20939724 0.         0.22669875 0.23660771 0.24001419 0.2147074\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.22669875 0.23602161 0.23010684 0.22669875 0.23660771 0.2404398\n",
      " 0.21500803 0.         0.24285524 0.24691285 0.24001419 0.21500803\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.22956664 0.23467117 0.23189255 0.22956664 0.23660771 0.24096413\n",
      " 0.24062621 0.         0.23992435 0.2461551  0.23949354 0.24062621\n",
      " 0.         0.23992435 0.24888676        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.24003546 0.24115818 0.25315129 0.24003546 0.24378851 0.23748021\n",
      " 0.24270697 0.         0.23375239 0.24922126 0.23748021 0.24270697\n",
      " 0.         0.23375239 0.24922126        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23049107 0.24537943 0.24583333 0.23296233 0.23747562 0.23663853\n",
      " 0.24188832 0.42333333 0.23169206 0.24742561 0.23663853 0.24188832\n",
      " 0.42333333 0.23169206 0.24742561        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23783161 0.23921981 0.24345238 0.23200984 0.24163056 0.23663853\n",
      " 0.2421235  0.25119048 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.25119048 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24526455 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957]\n",
      "One or more of the train scores are non-finite: [0.23477526 0.23770697 0.29560901 0.23477526 0.24626193 0.23477526\n",
      " 0.219768   0.         0.23477526 0.24647661 0.2455207  0.22102263\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.23464593 0.23797083 0.29560901 0.23485901 0.24641156 0.24556157\n",
      " 0.26114871 0.         0.24533465 0.25108246 0.2455207  0.26114871\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.23676889 0.23825403 0.29807227 0.23697656 0.24683148 0.24417298\n",
      " 0.24111749 0.         0.24446346 0.2501251  0.24476588 0.24111749\n",
      " 0.         0.24446346 0.25065147        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24353191 0.23967955 0.30121849 0.24338633 0.24618009 0.24286218\n",
      " 0.24167568 0.095      0.24396914 0.2503354  0.24286218 0.24167568\n",
      " 0.095      0.24396914 0.2503354         nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.23543138 0.24041165 0.32269832 0.23569409 0.24570293 0.24142589\n",
      " 0.2394545  0.31088804 0.24285622 0.24972114 0.24152145 0.2394545\n",
      " 0.31088804 0.24290218 0.24972114        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24052581 0.23980302 0.33679572 0.24101142 0.24929717 0.24107228\n",
      " 0.23959051 0.34914591 0.24289462 0.24985954 0.24116783 0.23955247\n",
      " 0.34914591 0.24289462 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24089904 0.23929204 0.3369035  0.24249328 0.24991287 0.24111965\n",
      " 0.23935736 0.33823487 0.24289462 0.24985954 0.2412152  0.23935736\n",
      " 0.33823487 0.24289462 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24126215 0.23941048 0.3369035  0.24249328 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24990407        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.42333333333333334\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7468827930174564\n",
      "GridSearchCV Runtime: 4.775115966796875 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.38461538461538464\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.4117647058823529\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2916666666666667\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.4444444444444444\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3333333333333333\n",
      "Ave Test Precision: 0.3731649069884364\n",
      "Stdev Test Precision: 0.061094901194505016\n",
      "Ave Test Accuracy: 0.7365671641791045\n",
      "Stdev Test Accuracy: 0.013086326803544922\n",
      "Ave Test Specificity: 0.9485148514851485\n",
      "Ave Test Recall: 0.08787878787878789\n",
      "Ave Test NPV: 0.7609078069397102\n",
      "Ave Test F1-Score: 0.1400672684711117\n",
      "Ave Test G-mean: 0.2868883094391115\n",
      "Ave Runtime: 0.008251619338989259\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyNetIncome. 26 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.09629953 0.                nan 0.15781469 0.14781469 0.09629953\n",
      " 0.                nan 0.1813441  0.14781469 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.                nan 0.19118519 0.22013531 0.19128534\n",
      " 0.                nan 0.1813441  0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.                nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.                nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.03068182        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531]\n",
      "One or more of the train scores are non-finite: [0.12870734 0.                nan 0.1505025  0.1512757  0.12870734\n",
      " 0.                nan 0.17506669 0.1512757  0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.                nan 0.19943274 0.22476988 0.22687699\n",
      " 0.                nan 0.17506669 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.                nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.                nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.05240275        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.22195442571788887\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.6907730673316709\n",
      "GridSearchCV Runtime: 4.9040961265563965 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.21621621621621623\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24669603524229075\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2409090909090909\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2565217391304348\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2297872340425532\n",
      "Ave Test Precision: 0.2380260631081172\n",
      "Stdev Test Precision: 0.015565330252285143\n",
      "Ave Test Accuracy: 0.3888059701492537\n",
      "Stdev Test Accuracy: 0.16168819361247286\n",
      "Ave Test Specificity: 0.2881188118811881\n",
      "Ave Test Recall: 0.696969696969697\n",
      "Ave Test NPV: 0.7372670173215744\n",
      "Ave Test F1-Score: 0.33313487430823757\n",
      "Ave Test G-mean: 0.34361955132076955\n",
      "Ave Runtime: 0.003034830093383789\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positiveMonthlyFamilyNetIncome. 25 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.21850877 0.22681418 0.33063007 0.25359649 0.26797559 0.21850877\n",
      " 0.19594136 0.         0.25359649 0.26797559 0.23118435 0.19594136\n",
      " 0.         0.25455179 0.28232675        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.21904017 0.22810789 0.33063007 0.25412789 0.26797559 0.22675879\n",
      " 0.19499199 0.         0.25729216 0.26937657 0.23118435 0.19499199\n",
      " 0.         0.25455179 0.28232675        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.21912339 0.21761647 0.34063007 0.25627872 0.268507   0.23253476\n",
      " 0.19046581 0.         0.25068354 0.27891113 0.23118435 0.19046581\n",
      " 0.         0.25455179 0.28232675        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22787433 0.23231653 0.36298319 0.25882005 0.26885671 0.22942997\n",
      " 0.20539879 0.05       0.25455179 0.28232675 0.22942997 0.20539879\n",
      " 0.05       0.25455179 0.28232675        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22181008 0.20984022 0.40857143 0.257009   0.27549028 0.2261919\n",
      " 0.21858358 0.41785714 0.25618208 0.28162192 0.22785102 0.21858358\n",
      " 0.41785714 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.23187888 0.21679889 0.41829365 0.25276443 0.2777616  0.2261919\n",
      " 0.21919334 0.41412698 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.41412698 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22823505 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.43365079 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.43365079 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.42698413 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.42698413 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.42698413 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.42698413 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.42698413 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.42698413 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192]\n",
      "One or more of the train scores are non-finite: [0.25930101 0.23055323 0.3362354  0.27536749 0.26290519 0.25924638\n",
      " 0.1965714  0.         0.27531286 0.26284902 0.26901286 0.1965714\n",
      " 0.         0.28356117 0.2757061         nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.25935288 0.23058684 0.3362354  0.27563527 0.2627396  0.26223427\n",
      " 0.19663719 0.         0.27672691 0.26514506 0.26901286 0.19663719\n",
      " 0.         0.28356117 0.2757061         nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.2595868  0.23380997 0.33181331 0.2762616  0.2633811  0.26775278\n",
      " 0.23869682 0.         0.28338591 0.27040037 0.26901286 0.22917301\n",
      " 0.         0.28356117 0.2757061         nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.26202197 0.24687007 0.3382123  0.27949116 0.26644983 0.26880455\n",
      " 0.28174169 0.3952381  0.28332307 0.27572434 0.2689841  0.28174169\n",
      " 0.3952381  0.28318943 0.27572434        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.26693919 0.26489394 0.33605412 0.28484823 0.27270896 0.26744138\n",
      " 0.28100176 0.39687637 0.28288093 0.27665697 0.26992009 0.28100176\n",
      " 0.39687637 0.28317719 0.27665697        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27007994 0.27862952 0.35468991 0.2820084  0.27540482 0.26806877\n",
      " 0.27911858 0.37929045 0.28244533 0.27691334 0.27040912 0.27911858\n",
      " 0.37929045 0.28317719 0.27691334        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27037414 0.28012028 0.3706446  0.28267927 0.27584733 0.26806803\n",
      " 0.27911858 0.36928415 0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.36928415 0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27040912 0.27911858 0.369257   0.28317719 0.27680581 0.26806803\n",
      " 0.27911858 0.369257   0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.369257   0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27040912 0.27911858 0.369257   0.28317719 0.27680581 0.26806803\n",
      " 0.27911858 0.369257   0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.369257   0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27040912 0.27911858 0.369257   0.28317719 0.27680581 0.26806803\n",
      " 0.27911858 0.369257   0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.369257   0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27040912 0.27911858 0.369257   0.28317719 0.27680581 0.26806803\n",
      " 0.27911858 0.369257   0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.369257   0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27040912 0.27911858 0.369257   0.28317719 0.27680581 0.26806803\n",
      " 0.27911858 0.369257   0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.369257   0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27040912 0.27911858 0.369257   0.28317719 0.27680581 0.26806803\n",
      " 0.27911858 0.369257   0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.369257   0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.4336507936507936\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7344139650872819\n",
      "GridSearchCV Runtime: 4.6366143226623535 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4375\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2903225806451613\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.47619047619047616\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2631578947368421\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3103448275862069\n",
      "Ave Test Precision: 0.35550315583173725\n",
      "Stdev Test Precision: 0.09500571165355737\n",
      "Ave Test Accuracy: 0.726865671641791\n",
      "Stdev Test Accuracy: 0.02016306878523329\n",
      "Ave Test Specificity: 0.9247524752475247\n",
      "Ave Test Recall: 0.12121212121212119\n",
      "Ave Test NPV: 0.7630344790915451\n",
      "Ave Test F1-Score: 0.17866090362633433\n",
      "Ave Test G-mean: 0.33229385584329857\n",
      "Ave Runtime: 0.0034006595611572265\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlySoloNetIncomeWithSavings. 24 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "910 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "208 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "598 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.15952403 0.                nan 0.12931614 0.06       0.15985116\n",
      " 0.                nan 0.2274904  0.06       0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20463497 0.                nan 0.2274904  0.22905116 0.20721634\n",
      " 0.                nan 0.2274904  0.26063011 0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.                nan 0.2274904  0.26063011 0.20721634\n",
      " 0.                nan 0.2274904  0.26063011 0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.                nan 0.2274904  0.26063011 0.20721634\n",
      " 0.                nan 0.2274904  0.26063011 0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.00869565        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.                nan 0.2274904  0.26063011 0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.03488613        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.03488613        nan 0.2274904  0.26063011 0.20721634 0.03488613\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.0805437         nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.0805437         nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011]\n",
      "One or more of the train scores are non-finite: [0.19705997 0.                nan 0.14920796 0.0462963  0.19705746\n",
      " 0.                nan 0.24849319 0.0462963  0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.24793666 0.                nan 0.24849319 0.22035867 0.2468814\n",
      " 0.                nan 0.24849319 0.24447839 0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.                nan 0.24849319 0.24447839 0.2468814\n",
      " 0.                nan 0.24849319 0.24447839 0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.                nan 0.24849319 0.24447839 0.2468814\n",
      " 0.                nan 0.24849319 0.24447839 0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.02597403        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.                nan 0.24849319 0.24447839 0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.07657224        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.07657224        nan 0.24849319 0.24447839 0.2468814  0.07657224\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.12661665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.12661665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2606301088307362\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.6384039900249376\n",
      "GridSearchCV Runtime: 4.614759683609009 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.16393442622950818\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24390243902439024\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.22613065326633167\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2660098522167488\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.24537037037037038\n",
      "Ave Test Precision: 0.22906954822146983\n",
      "Stdev Test Precision: 0.039056173430170205\n",
      "Ave Test Accuracy: 0.41044776119402987\n",
      "Stdev Test Accuracy: 0.10865835655642565\n",
      "Ave Test Specificity: 0.3346534653465346\n",
      "Ave Test Recall: 0.6424242424242423\n",
      "Ave Test NPV: 0.7473074268726443\n",
      "Ave Test F1-Score: 0.32869603203544856\n",
      "Ave Test G-mean: 0.40319526001692435\n",
      "Ave Runtime: 0.006252861022949219\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positiveMonthlySoloNetIncomeWithSavings. 23 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22986345 0.23214016 0.2847619  0.22986345 0.24767587 0.22986345\n",
      " 0.22520965 0.         0.22986345 0.24767587 0.24458784 0.22013491\n",
      " 0.         0.24621365 0.23911882        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.22986345 0.23214016 0.2847619  0.22986345 0.24767587 0.24268308\n",
      " 0.22013491 0.         0.24346855 0.25298827 0.24458784 0.22013491\n",
      " 0.         0.24621365 0.23911882        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.2286578  0.22870567 0.28642857 0.2286578  0.24716708 0.24499511\n",
      " 0.21452606 0.         0.24750399 0.25731386 0.24499511 0.21452606\n",
      " 0.         0.24750399 0.23911882        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.22722573 0.24322594 0.29386447 0.22812343 0.24654304 0.24520716\n",
      " 0.24629621 0.         0.24192381 0.23881744 0.24520716 0.24629621\n",
      " 0.         0.24192381 0.23881744        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24622302 0.2514528  0.37138889 0.24526385 0.24543963 0.24419163\n",
      " 0.25044663 0.3        0.24108347 0.23565463 0.24419163 0.25044663\n",
      " 0.3        0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24044137 0.24719686 0.39166667 0.2398269  0.23938392 0.24459076\n",
      " 0.24659763 0.385      0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.385      0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.2398269  0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463]\n",
      "One or more of the train scores are non-finite: [0.23006765 0.23143067 0.32636769 0.23006765 0.24486765 0.23006765\n",
      " 0.2187965  0.         0.23006765 0.2449588  0.24093331 0.22052129\n",
      " 0.         0.24101197 0.25910633        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.22956441 0.23188958 0.32636769 0.22968991 0.2448964  0.2396491\n",
      " 0.22078445 0.         0.24009199 0.24594458 0.24093331 0.22078445\n",
      " 0.         0.24101197 0.25910633        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.22922844 0.22963666 0.32728336 0.22895784 0.24538274 0.24113362\n",
      " 0.26638747 0.         0.24086041 0.25312776 0.24102277 0.26638747\n",
      " 0.         0.24086041 0.25904913        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.2290762  0.23442361 0.33684931 0.2298772  0.24653106 0.24224361\n",
      " 0.24339603 0.04166667 0.24193358 0.25837921 0.24224361 0.24339603\n",
      " 0.04166667 0.24198152 0.25837921        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24716679 0.24428605 0.35021016 0.2435348  0.25192142 0.24125951\n",
      " 0.24210305 0.38818761 0.24191817 0.25850841 0.24125951 0.24210305\n",
      " 0.38818761 0.24191817 0.25850841        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24149578 0.24354476 0.35728609 0.24252058 0.26069963 0.24130573\n",
      " 0.24163862 0.37093914 0.24191817 0.25944955 0.24135079 0.24159978\n",
      " 0.37093914 0.24196504 0.25944955        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24129341 0.24249379 0.35854353 0.242004   0.25952269 0.24135079\n",
      " 0.24210365 0.36207813 0.24196504 0.25934383 0.24135079 0.24206394\n",
      " 0.36207813 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24135079 0.24252417 0.35854353 0.24201209 0.25967176 0.24135079\n",
      " 0.24252417 0.35926817 0.24196504 0.25934383 0.24135079 0.24248446\n",
      " 0.36030984 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24135079 0.24252417 0.35854353 0.24201209 0.25938768 0.24135079\n",
      " 0.24252417 0.35854353 0.24196504 0.25934383 0.24135079 0.24248446\n",
      " 0.35926817 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24135079 0.24252417 0.35854353 0.24201209 0.25938768 0.24135079\n",
      " 0.24252417 0.35854353 0.24196504 0.25934383 0.24135079 0.24248446\n",
      " 0.35926817 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24135079 0.24252417 0.35854353 0.24201209 0.25938768 0.24135079\n",
      " 0.24252417 0.35854353 0.24196504 0.25934383 0.24135079 0.24248446\n",
      " 0.35926817 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24135079 0.24252417 0.35854353 0.24201209 0.25938768 0.24135079\n",
      " 0.24252417 0.35854353 0.24196504 0.25934383 0.24135079 0.24248446\n",
      " 0.35926817 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24135079 0.24252417 0.35854353 0.24201209 0.25938768 0.24135079\n",
      " 0.24252417 0.35854353 0.24196504 0.25934383 0.24135079 0.24248446\n",
      " 0.35926817 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3916666666666667\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.7468827930174564\n",
      "GridSearchCV Runtime: 4.783532381057739 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.3888888888888889\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.48148148148148145\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3181818181818182\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2972972972972973\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.24242424242424243\n",
      "Ave Test Precision: 0.3456547456547457\n",
      "Stdev Test Precision: 0.09228737001797055\n",
      "Ave Test Accuracy: 0.714179104477612\n",
      "Stdev Test Accuracy: 0.0279975096750807\n",
      "Ave Test Specificity: 0.895049504950495\n",
      "Ave Test Recall: 0.16060606060606059\n",
      "Ave Test NPV: 0.7654072763350948\n",
      "Ave Test F1-Score: 0.21519808166222196\n",
      "Ave Test G-mean: 0.37535935900961476\n",
      "Ave Runtime: 0.006836366653442383\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyNetIncomeWithSavings. 22 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.08428571 0.                nan 0.18731602 0.13969697 0.08428571\n",
      " 0.                nan 0.18731602 0.13969697 0.16555448 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.16305448 0.                nan 0.19301046 0.16330808 0.13222115\n",
      " 0.                nan 0.21092713 0.1853669  0.16555448 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.                nan 0.21092713 0.1853669  0.13222115\n",
      " 0.                nan 0.21092713 0.1853669  0.16555448 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.                nan 0.21092713 0.1853669  0.13222115\n",
      " 0.                nan 0.21092713 0.1853669  0.16555448 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.03095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.                nan 0.21092713 0.1853669  0.16555448 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669 ]\n",
      "One or more of the train scores are non-finite: [0.15684432 0.                nan 0.22872903 0.15206002 0.15684432\n",
      " 0.                nan 0.22872903 0.15206002 0.22973629 0.\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.2027584  0.                nan 0.25403095 0.17664018 0.20592677\n",
      " 0.                nan 0.25330918 0.20137462 0.22973629 0.\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.                nan 0.25330918 0.20137462 0.20592677\n",
      " 0.                nan 0.25330918 0.20137462 0.22973629 0.\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.                nan 0.25330918 0.20137462 0.20592677\n",
      " 0.                nan 0.25330918 0.20137462 0.22973629 0.\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.05255754        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.                nan 0.25330918 0.20137462 0.22973629 0.\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.21092712842712844\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.7082294264339152\n",
      "GridSearchCV Runtime: 4.994728326797485 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.15384615384615385\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24583333333333332\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.23809523809523808\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2489451476793249\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.22950819672131148\n",
      "Ave Test Precision: 0.22324561393507233\n",
      "Stdev Test Precision: 0.039517994141190345\n",
      "Ave Test Accuracy: 0.3716417910447761\n",
      "Stdev Test Accuracy: 0.17704427659305888\n",
      "Ave Test Specificity: 0.26237623762376244\n",
      "Ave Test Recall: 0.7060606060606062\n",
      "Ave Test NPV: 0.7108062474631389\n",
      "Ave Test F1-Score: 0.318735414723443\n",
      "Ave Test G-mean: 0.2866295334225509\n",
      "Ave Runtime: 0.00923933982849121\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positiveMonthlyFamilyNetIncomeWithSavings. 21 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25434951 0.25520923 0.225      0.25434951 0.27867813 0.25434951\n",
      " 0.19480898 0.         0.25434951 0.28028389 0.24786284 0.16320925\n",
      " 0.         0.24753768 0.26561708        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.25305081 0.25574687 0.225      0.25305081 0.28028389 0.24990829\n",
      " 0.1635257  0.         0.24958313 0.27489173 0.24786284 0.1635257\n",
      " 0.         0.24753768 0.26561708        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.25244597 0.25021517 0.225      0.25244597 0.27898519 0.24926181\n",
      " 0.24058748 0.         0.24668806 0.26560945 0.24926181 0.24058748\n",
      " 0.         0.24668806 0.26561708        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.25784982 0.24820251 0.22424242 0.25598117 0.28186634 0.24354307\n",
      " 0.22192713 0.02727273 0.24517347 0.26571534 0.24354307 0.22192713\n",
      " 0.02727273 0.24517347 0.26645758        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24923461 0.23492195 0.28524031 0.25074392 0.27714601 0.24533351\n",
      " 0.23843614 0.026      0.24936379 0.26738271 0.24533351 0.23843614\n",
      " 0.026      0.24936379 0.26738271        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24803107 0.23569464 0.18310767 0.25021694 0.26662391 0.24710207\n",
      " 0.2352906  0.20013793 0.24796617 0.26820939 0.24710207 0.2352906\n",
      " 0.20013793 0.24796617 0.26820939        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24611172 0.2352906  0.18286275 0.24882269 0.2679871  0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24611172 0.2352906  0.18286275 0.24882269 0.26709332 0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332]\n",
      "One or more of the train scores are non-finite: [0.25574251 0.25754297 0.27709265 0.25574251 0.24615161 0.25572399\n",
      " 0.19907031 0.         0.25572399 0.24614778 0.2517971  0.17338213\n",
      " 0.         0.25184939 0.24227824        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25544126 0.25681424 0.277169   0.2553694  0.24614778 0.2519456\n",
      " 0.25691782 0.         0.25200956 0.24592487 0.2517971  0.25691782\n",
      " 0.         0.25184939 0.24227824        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.2558974  0.25558272 0.27703558 0.2558974  0.24612235 0.25206091\n",
      " 0.22067608 0.05       0.25200038 0.24538622 0.25206091 0.22067608\n",
      " 0.05       0.25214658 0.24223482        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25644744 0.25389216 0.27080839 0.25653683 0.24612162 0.25159616\n",
      " 0.2736352  0.12520107 0.25130479 0.24332844 0.25159616 0.2736352\n",
      " 0.12520107 0.25124709 0.2426575         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25356324 0.25154653 0.23767531 0.25295123 0.24441498 0.25216639\n",
      " 0.2538636  0.36515508 0.25214024 0.24272142 0.25216639 0.2538636\n",
      " 0.36515508 0.25214024 0.24272142        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25183186 0.25112269 0.29970809 0.25128917 0.24257828 0.2520732\n",
      " 0.25121301 0.30680013 0.25219421 0.24288575 0.2520732  0.25121301\n",
      " 0.30680013 0.25219421 0.24288575        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25218116 0.25124171 0.29725365 0.25202742 0.24298121 0.25228095\n",
      " 0.24957504 0.29725365 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.29725365 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25228095 0.24957504 0.30433609 0.25202742 0.2429277  0.25228095\n",
      " 0.24957504 0.30433609 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.30433609 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25228095 0.24957504 0.30433609 0.25202742 0.2429277  0.25228095\n",
      " 0.24957504 0.30433609 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.30433609 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25228095 0.24957504 0.30433609 0.25202742 0.2429277  0.25228095\n",
      " 0.24957504 0.30433609 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.30433609 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25228095 0.24957504 0.30433609 0.25202742 0.2429277  0.25228095\n",
      " 0.24957504 0.30433609 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.30433609 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25228095 0.24957504 0.30433609 0.25202742 0.2429277  0.25228095\n",
      " 0.24957504 0.30433609 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.30433609 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25228095 0.24957504 0.30433609 0.25202742 0.2429277  0.25228095\n",
      " 0.24957504 0.30433609 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.30433609 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277 ]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.2852403100775194\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6159600997506235\n",
      "GridSearchCV Runtime: 4.725091218948364 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2289156626506024\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.375\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.0\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.12078313253012048\n",
      "Stdev Test Precision: 0.17326609238958235\n",
      "Ave Test Accuracy: 0.7134328358208955\n",
      "Stdev Test Accuracy: 0.07135553819687412\n",
      "Ave Test Specificity: 0.9247524752475247\n",
      "Ave Test Recall: 0.06666666666666668\n",
      "Ave Test NPV: 0.7516682096721817\n",
      "Ave Test F1-Score: 0.06722292762561219\n",
      "Ave Test G-mean: 0.1308040199019298\n",
      "Ave Runtime: 0.008063077926635742\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyIncome - basicMonthlySalary. 20 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.1524111  0.02469136        nan 0.12260069 0.23857484 0.17771413\n",
      " 0.02469136        nan 0.21316419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.23403989 0.02469136        nan 0.23816419 0.23857484 0.20271413\n",
      " 0.02469136        nan 0.21316419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.02469136        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.02469136        nan 0.23816419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.02469136        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.02469136        nan 0.23816419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.02469136        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.02469136        nan 0.23816419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.13472673        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.13472673        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484]\n",
      "One or more of the train scores are non-finite: [0.17406968 0.02454924        nan 0.12267077 0.24416029 0.19859948\n",
      " 0.02454924        nan 0.2219619  0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24716329 0.02454924        nan 0.24653004 0.24416029 0.22316762\n",
      " 0.02454924        nan 0.2219619  0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.02454924        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.02454924        nan 0.24653004 0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.02454924        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.02454924        nan 0.24653004 0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.02454924        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.02454924        nan 0.24653004 0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.17496007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.17496007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.23857484303136473\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6159600997506235\n",
      "GridSearchCV Runtime: 4.6933815479278564 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2345679012345679\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2289156626506024\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.0\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.09269671277703406\n",
      "Stdev Test Precision: 0.1269459315003977\n",
      "Ave Test Accuracy: 0.6880597014925373\n",
      "Stdev Test Accuracy: 0.08996329788014193\n",
      "Ave Test Specificity: 0.8752475247524754\n",
      "Ave Test Recall: 0.11515151515151516\n",
      "Ave Test NPV: 0.7511606154801941\n",
      "Ave Test F1-Score: 0.10270739168150482\n",
      "Ave Test G-mean: 0.17803022046464823\n",
      "Ave Runtime: 0.006714487075805664\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positive monthlyFamilyIncome - basicMonthlySalary. 19 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.206957   0.2070859  0.36390693 0.206957   0.27026184 0.206957\n",
      " 0.10944878 0.         0.20645866 0.2697635  0.20121743 0.11718034\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20645866 0.20486816 0.36390693 0.20645866 0.2697635  0.20577725\n",
      " 0.11843335 0.         0.20694236 0.26944344 0.20121743 0.11748097\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20787322 0.21053384 0.36525974 0.20787322 0.27117806 0.20746609\n",
      " 0.18497553 0.         0.20632572 0.26860801 0.20121743 0.18607443\n",
      " 0.         0.19714027 0.27106964        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20912052 0.2129899  0.3552381  0.20872158 0.27294574 0.20317587\n",
      " 0.22124032 0.         0.20258582 0.27071881 0.20121743 0.22124032\n",
      " 0.         0.19760485 0.27071881        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20154865 0.25548036 0.37285714 0.20119575 0.26782754 0.20317587\n",
      " 0.23358949 0.36666667 0.19997585 0.27000482 0.20121743 0.23358949\n",
      " 0.36666667 0.19760485 0.27000482        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20169311 0.25746124 0.38333333 0.1998966  0.26986083 0.20317587\n",
      " 0.25898576 0.4        0.19997585 0.27063283 0.20121743 0.25898576\n",
      " 0.4        0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.38333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.38333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283]\n",
      "One or more of the train scores are non-finite: [0.21406456 0.21753168 0.34579767 0.21433556 0.25980589 0.21485609\n",
      " 0.11555348 0.         0.21524006 0.2605446  0.21190105 0.11858785\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21524006 0.21779676 0.3463148  0.21524006 0.26046086 0.21693595\n",
      " 0.18597355 0.         0.21607005 0.26201811 0.21190105 0.1852493\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21499148 0.21400501 0.34650148 0.21507196 0.26053989 0.21511944\n",
      " 0.2913795  0.         0.21487681 0.26369047 0.21176171 0.29039832\n",
      " 0.         0.21053761 0.26395912        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2174422  0.22071031 0.35897384 0.2172269  0.26169073 0.21319033\n",
      " 0.28575602 0.16666667 0.21490766 0.26352371 0.21179116 0.28575602\n",
      " 0.16666667 0.2106657  0.26352371        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21301237 0.24906234 0.38825152 0.21245925 0.25972252 0.21316659\n",
      " 0.26871203 0.47803207 0.21228805 0.26231435 0.21176742 0.26930026\n",
      " 0.47803207 0.21081978 0.26231435        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21130223 0.26264816 0.41770404 0.21152949 0.26028815 0.21310005\n",
      " 0.26431071 0.42410912 0.21241002 0.2625036  0.2117339  0.26431071\n",
      " 0.42410912 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21178291 0.26243359 0.41615745 0.21108015 0.26219953 0.21310005\n",
      " 0.26431071 0.41599638 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41599638 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26210887 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.39999999999999997\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7518703241895262\n",
      "GridSearchCV Runtime: 4.878476858139038 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3870967741935484\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2727272727272727\n",
      "Ave Test Precision: 0.32910766652702134\n",
      "Stdev Test Precision: 0.059785972659199466\n",
      "Ave Test Accuracy: 0.7358208955223879\n",
      "Stdev Test Accuracy: 0.007177382112563941\n",
      "Ave Test Specificity: 0.9504950495049505\n",
      "Ave Test Recall: 0.0787878787878788\n",
      "Ave Test NPV: 0.759685019948753\n",
      "Ave Test F1-Score: 0.12191105693004774\n",
      "Ave Test G-mean: 0.2613548711958696\n",
      "Ave Runtime: 0.01204218864440918\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with basicMonthlySalary - monthlyExpenses. 18 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1105 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "182 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "364 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "559 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.07694209 0.                nan 0.07421482 0.         0.25989423\n",
      " 0.                nan 0.25989423 0.1774121  0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.                nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.                nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.                nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.07478773        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.07478773        nan 0.25989423 0.25989423 0.25989423 0.07478773\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.25989423        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.25989423        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423]\n",
      "One or more of the train scores are non-finite: [0.07770881 0.                nan 0.07804872 0.         0.25887731\n",
      " 0.                nan 0.25887731 0.18165174 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.                nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.                nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.                nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.07799125        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.07799125        nan 0.25887731 0.25887731 0.25887731 0.07799125\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.25887731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.25887731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.259894230383722\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.415211970074813\n",
      "GridSearchCV Runtime: 4.592023849487305 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2736842105263158\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25263157894736843\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.22941176470588234\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.28888888888888886\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2512820512820513\n",
      "Ave Test Precision: 0.25917969887010134\n",
      "Stdev Test Precision: 0.022827600872137695\n",
      "Ave Test Accuracy: 0.42164179104477606\n",
      "Stdev Test Accuracy: 0.03099486516014207\n",
      "Ave Test Specificity: 0.32178217821782173\n",
      "Ave Test Recall: 0.7272727272727272\n",
      "Ave Test NPV: 0.7844531528484562\n",
      "Ave Test F1-Score: 0.38200032588432253\n",
      "Ave Test G-mean: 0.48221180258827284\n",
      "Ave Runtime: 0.0026090145111083984\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positive basicMonthlySalary - monthlyExpenses. 17 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22669875 0.23653443 0.23010684 0.22669875 0.23660771 0.22669875\n",
      " 0.20939724 0.         0.22669875 0.23660771 0.24001419 0.2147074\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.22669875 0.23602161 0.23010684 0.22669875 0.23660771 0.2404398\n",
      " 0.21500803 0.         0.24285524 0.24691285 0.24001419 0.21500803\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.22956664 0.23467117 0.23189255 0.22956664 0.23660771 0.24096413\n",
      " 0.24062621 0.         0.23992435 0.2461551  0.23949354 0.24062621\n",
      " 0.         0.23992435 0.24888676        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.24003546 0.24115818 0.25315129 0.24003546 0.24378851 0.23748021\n",
      " 0.24270697 0.         0.23375239 0.24922126 0.23748021 0.24270697\n",
      " 0.         0.23375239 0.24922126        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23049107 0.24537943 0.24583333 0.23296233 0.23747562 0.23663853\n",
      " 0.24188832 0.42333333 0.23169206 0.24742561 0.23663853 0.24188832\n",
      " 0.42333333 0.23169206 0.24742561        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23783161 0.23921981 0.24345238 0.23200984 0.24163056 0.23663853\n",
      " 0.2421235  0.25119048 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.25119048 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24526455 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957]\n",
      "One or more of the train scores are non-finite: [0.23477526 0.23770697 0.29560901 0.23477526 0.24626193 0.23477526\n",
      " 0.219768   0.         0.23477526 0.24647661 0.2455207  0.22102263\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.23464593 0.23797083 0.29560901 0.23485901 0.24641156 0.24556157\n",
      " 0.26114871 0.         0.24533465 0.25108246 0.2455207  0.26114871\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.23676889 0.23825403 0.29807227 0.23697656 0.24683148 0.24417298\n",
      " 0.24111749 0.         0.24446346 0.2501251  0.24476588 0.24111749\n",
      " 0.         0.24446346 0.25065147        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24353191 0.23967955 0.30121849 0.24338633 0.24618009 0.24286218\n",
      " 0.24167568 0.095      0.24396914 0.2503354  0.24286218 0.24167568\n",
      " 0.095      0.24396914 0.2503354         nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.23543138 0.24041165 0.32269832 0.23569409 0.24570293 0.24142589\n",
      " 0.2394545  0.31088804 0.24285622 0.24972114 0.24152145 0.2394545\n",
      " 0.31088804 0.24290218 0.24972114        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24052581 0.23980302 0.33679572 0.24101142 0.24929717 0.24107228\n",
      " 0.23959051 0.34914591 0.24289462 0.24985954 0.24116783 0.23955247\n",
      " 0.34914591 0.24289462 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24089904 0.23929204 0.3369035  0.24249328 0.24991287 0.24111965\n",
      " 0.23935736 0.33823487 0.24289462 0.24985954 0.2412152  0.23935736\n",
      " 0.33823487 0.24289462 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24126215 0.23941048 0.3369035  0.24249328 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24990407        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.42333333333333334\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7468827930174564\n",
      "GridSearchCV Runtime: 4.762001037597656 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.38461538461538464\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.4117647058823529\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2916666666666667\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.4444444444444444\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3333333333333333\n",
      "Ave Test Precision: 0.3731649069884364\n",
      "Stdev Test Precision: 0.061094901194505016\n",
      "Ave Test Accuracy: 0.7365671641791045\n",
      "Stdev Test Accuracy: 0.013086326803544922\n",
      "Ave Test Specificity: 0.9485148514851485\n",
      "Ave Test Recall: 0.08787878787878789\n",
      "Ave Test NPV: 0.7609078069397102\n",
      "Ave Test F1-Score: 0.1400672684711117\n",
      "Ave Test G-mean: 0.2868883094391115\n",
      "Ave Runtime: 0.006245183944702149\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyIncome - monthlyExpenses. 16 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.09629953 0.                nan 0.15781469 0.14781469 0.09629953\n",
      " 0.                nan 0.1813441  0.14781469 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.                nan 0.19118519 0.22013531 0.19128534\n",
      " 0.                nan 0.1813441  0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.                nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.                nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.03068182        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531]\n",
      "One or more of the train scores are non-finite: [0.12870734 0.                nan 0.1505025  0.1512757  0.12870734\n",
      " 0.                nan 0.17506669 0.1512757  0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.                nan 0.19943274 0.22476988 0.22687699\n",
      " 0.                nan 0.17506669 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.                nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.                nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.05240275        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.22195442571788887\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.6907730673316709\n",
      "GridSearchCV Runtime: 4.815627336502075 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.21621621621621623\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24669603524229075\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2409090909090909\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2565217391304348\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2297872340425532\n",
      "Ave Test Precision: 0.2380260631081172\n",
      "Stdev Test Precision: 0.015565330252285143\n",
      "Ave Test Accuracy: 0.3888059701492537\n",
      "Stdev Test Accuracy: 0.16168819361247286\n",
      "Ave Test Specificity: 0.2881188118811881\n",
      "Ave Test Recall: 0.696969696969697\n",
      "Ave Test NPV: 0.7372670173215744\n",
      "Ave Test F1-Score: 0.33313487430823757\n",
      "Ave Test G-mean: 0.34361955132076955\n",
      "Ave Runtime: 0.007882165908813476\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positive monthlyFamilyIncome - monthlyExpenses. 15 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.21177073 0.17680636 0.13440412 0.2258301  0.24398084 0.21101315\n",
      " 0.16944224 0.02469136 0.2258301  0.24398084 0.219761   0.14563272\n",
      " 0.02469136 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.21067302 0.17680636 0.13440412 0.22548997 0.24398084 0.21067302\n",
      " 0.14563272 0.02469136 0.22548997 0.24398084 0.219761   0.14563272\n",
      " 0.02469136 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.21050879 0.17680636 0.13241473 0.22659379 0.24364071 0.21182931\n",
      " 0.14563272 0.02469136 0.21565835 0.24606506 0.219761   0.14563272\n",
      " 0.02469136 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.2157896  0.17705548 0.12916798 0.22429175 0.24407788 0.21854519\n",
      " 0.14271605 0.02647059 0.21641752 0.24126761 0.219761   0.14271605\n",
      " 0.02647059 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.22012154 0.18545195 0.17179843 0.2157354  0.24076848 0.219761\n",
      " 0.19466351 0.11190476 0.21817191 0.24386924 0.219761   0.19466351\n",
      " 0.11190476 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.21755999 0.18734212 0.18509512 0.21763354 0.24418627 0.219761\n",
      " 0.18689014 0.18608459 0.21817191 0.24386924 0.219761   0.18897347\n",
      " 0.18608459 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18856833 0.21817191 0.24261893 0.219761\n",
      " 0.18653926 0.18856833 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18856833 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924]\n",
      "One or more of the train scores are non-finite: [0.23960513 0.20150029 0.21154485 0.25572029 0.26284364 0.24018906\n",
      " 0.16838418 0.02454924 0.25612311 0.26284364 0.24204582 0.14757493\n",
      " 0.02454924 0.26007573 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24027096 0.20150029 0.21154485 0.25612434 0.26284364 0.24041757\n",
      " 0.14757493 0.02454924 0.25627095 0.26284364 0.24204582 0.14757493\n",
      " 0.02454924 0.26007573 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.23997661 0.20150029 0.2121425  0.25589505 0.26306458 0.23978231\n",
      " 0.14757493 0.02454924 0.257159   0.26322902 0.24204582 0.14757493\n",
      " 0.02454924 0.26007573 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24007467 0.20606076 0.21191338 0.25698938 0.26322278 0.24218319\n",
      " 0.15056781 0.05066038 0.2600003  0.2651445  0.24204582 0.15056781\n",
      " 0.05066038 0.26007573 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24157305 0.20100473 0.21479849 0.25808951 0.26370923 0.24208858\n",
      " 0.20104837 0.22259473 0.25948243 0.26491422 0.24208858 0.20104837\n",
      " 0.22259473 0.25999406 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24090943 0.20131456 0.21815866 0.25843224 0.26431975 0.24160947\n",
      " 0.20601454 0.21685128 0.25948243 0.26524178 0.24160947 0.20631732\n",
      " 0.21652203 0.25948243 0.26524178        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21815037 0.25905654 0.26484588 0.24166765\n",
      " 0.20318846 0.21810933 0.25893919 0.26516277 0.24166765 0.20344\n",
      " 0.21810933 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21879701 0.25893919 0.26516277 0.24166765 0.20318846\n",
      " 0.21879701 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.0001, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2460650573029935\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.0001, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.5211970074812967\n",
      "GridSearchCV Runtime: 4.871887445449829 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.21897810218978103\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2827586206896552\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.25296442687747034\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.27205882352941174\n",
      "Ave Test Precision: 0.25535199465726366\n",
      "Stdev Test Precision: 0.024431770614269228\n",
      "Ave Test Accuracy: 0.44253731343283587\n",
      "Stdev Test Accuracy: 0.09674906165521746\n",
      "Ave Test Specificity: 0.3722772277227723\n",
      "Ave Test Recall: 0.6575757575757575\n",
      "Ave Test NPV: 0.7860544281015329\n",
      "Ave Test F1-Score: 0.36352726111507294\n",
      "Ave Test G-mean: 0.45427003050056314\n",
      "Ave Runtime: 0.006943178176879883\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with basicMonthlySalary / monthlyFamilyIncome. 14 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3575 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "702 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 293, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 250, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 422, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 106, in fit_resample\n",
      "    X, y, binarize_y = self._check_X_y(X, y)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 161, in _check_X_y\n",
      "    X, y = self._validate_data(X, y, reset=True, accept_sparse=accept_sparse)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 622, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1146, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 957, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 122, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 171, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "SMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "702 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 293, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 250, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 422, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 106, in fit_resample\n",
      "    X, y, binarize_y = self._check_X_y(X, y)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 161, in _check_X_y\n",
      "    X, y = self._validate_data(X, y, reset=True, accept_sparse=accept_sparse)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 622, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1146, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 957, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 122, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 171, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "ADASYN does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "702 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 293, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 250, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 422, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 106, in fit_resample\n",
      "    X, y, binarize_y = self._check_X_y(X, y)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 161, in _check_X_y\n",
      "    X, y = self._validate_data(X, y, reset=True, accept_sparse=accept_sparse)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 622, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1146, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 957, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 122, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 171, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "EditedNearestNeighbours does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1404 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1208, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 622, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1146, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 957, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 122, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 171, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "65 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "One or more of the train scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Libraries\\Documents\\carlo1coder\\MSDS2023\\Capstone\\EDA deltaT.ipynb Cell 29\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, col \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(columns):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     df_2 \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcopy()\u001b[39m.\u001b[39mloc[:, [\u001b[39m'\u001b[39m\u001b[39muserId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlastFirstName\u001b[39m\u001b[39m'\u001b[39m, col, \u001b[39m'\u001b[39m\u001b[39mhome_ownership_class\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     t01_logreg_gs, t01_logreg_be, t01_logreg_model_info, t01_logreg_metrics_df \u001b[39m=\u001b[39m logreg_class2(df_2, \u001b[39m'\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                                                 \u001b[39m'\u001b[39;49m\u001b[39mhome_ownership_class\u001b[39;49m\u001b[39m'\u001b[39;49m, scaler, random_state\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     new_row \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mVariable\u001b[39m\u001b[39m'\u001b[39m: col,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mMean Test Precision\u001b[39m\u001b[39m'\u001b[39m: t01_logreg_model_info[\u001b[39m'\u001b[39m\u001b[39maverage_test_precision\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mStd Test Accuracy\u001b[39m\u001b[39m'\u001b[39m: t01_logreg_model_info[\u001b[39m'\u001b[39m\u001b[39mstdev_test_accuracy\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     }\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     res_df \u001b[39m=\u001b[39m res_df\u001b[39m.\u001b[39mappend(new_row, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32md:\\Libraries\\Documents\\carlo1coder\\MSDS2023\\Capstone\\EDA deltaT.ipynb Cell 29\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=397'>398</a>\u001b[0m start_time_cv \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=398'>399</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(pipeline, param_grid,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=399'>400</a>\u001b[0m                            cv\u001b[39m=\u001b[39mskf, scoring\u001b[39m=\u001b[39mscorer,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=400'>401</a>\u001b[0m                            n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=401'>402</a>\u001b[0m                            verbose\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, return_train_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=402'>403</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=404'>405</a>\u001b[0m best_params \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X33sZmlsZQ%3D%3D?line=405'>406</a>\u001b[0m best_score \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_score_\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:936\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    934\u001b[0m refit_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    935\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 936\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_estimator_\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    937\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    938\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py:293\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m    292\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 293\u001b[0m Xt, yt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[0;32m    294\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    295\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py:250\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    240\u001b[0m     X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    241\u001b[0m         cloned_transformer,\n\u001b[0;32m    242\u001b[0m         X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    248\u001b[0m     )\n\u001b[0;32m    249\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(cloned_transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_resample\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 250\u001b[0m     X, y, fitted_transformer \u001b[39m=\u001b[39m fit_resample_one_cached(\n\u001b[0;32m    251\u001b[0m         cloned_transformer,\n\u001b[0;32m    252\u001b[0m         X,\n\u001b[0;32m    253\u001b[0m         y,\n\u001b[0;32m    254\u001b[0m         message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    255\u001b[0m         message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[0;32m    256\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    258\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py:422\u001b[0m, in \u001b[0;36m_fit_resample_one\u001b[1;34m(sampler, X, y, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit_resample_one\u001b[39m(sampler, X, y, message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params):\n\u001b[0;32m    421\u001b[0m     \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m--> 422\u001b[0m         X_res, y_res \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39;49mfit_resample(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    424\u001b[0m         \u001b[39mreturn\u001b[39;00m X_res, y_res, sampler\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_resample(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py:106\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    104\u001b[0m check_classification_targets(y)\n\u001b[0;32m    105\u001b[0m arrays_transformer \u001b[39m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m--> 106\u001b[0m X, y, binarize_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X_y(X, y)\n\u001b[0;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy_ \u001b[39m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy, y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[0;32m    112\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_resample(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py:161\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    159\u001b[0m     accept_sparse \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    160\u001b[0m y, binarize_y \u001b[39m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 161\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, reset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse)\n\u001b[0;32m    162\u001b[0m \u001b[39mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1149\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1150\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1151\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1152\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1153\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1154\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1155\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1156\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1157\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1158\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[0;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m         )\n\u001b[0;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 957\u001b[0m         _assert_all_finite(\n\u001b[0;32m    958\u001b[0m             array,\n\u001b[0;32m    959\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    960\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    961\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    964\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    123\u001b[0m     X,\n\u001b[0;32m    124\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[0;32m    125\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[0;32m    126\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[0;32m    127\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    128\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    129\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    155\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "res_df = pd.DataFrame(columns=[\n",
    "    'Variable',\n",
    "    'Mean Test Precision', 'Std Test Precision', 'Mean Val Precision',\n",
    "    'Mean Train Precision', 'Mean Test Accuracy', 'Std Test Accuracy'\n",
    "])\n",
    "\n",
    "for index, col in enumerate(columns):\n",
    "    df_2 = df.copy().loc[:, ['userId', 'lastFirstName', col, 'home_ownership_class']]\n",
    "    t01_logreg_gs, t01_logreg_be, t01_logreg_model_info, t01_logreg_metrics_df = logreg_class2(df_2, 'precision',\n",
    "                                                                'home_ownership_class', scaler, random_state=0)\n",
    "    new_row = {\n",
    "        'Variable': col,\n",
    "        'Mean Test Precision': t01_logreg_model_info['average_test_precision'],\n",
    "        'Std Test Precision': t01_logreg_model_info['stdev_test_precision'],\n",
    "        'Val Precision of Best Model': t01_logreg_model_info['best_cv_score'],\n",
    "        'Train Precision of Best Model': t01_logreg_model_info['train_score'],\n",
    "        'Mean Test Accuracy': t01_logreg_model_info['average_test_accuracy'],\n",
    "        'Std Test Accuracy': t01_logreg_model_info['stdev_test_accuracy']\n",
    "    }\n",
    "    res_df = res_df.append(new_row, ignore_index=True)\n",
    "    print(f'\\n=====================\\n\\n\\nDone with {col}. {len(columns) - index + 1} columns left\\n\\n\\n=====================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Mean Test Precision</th>\n",
       "      <th>Std Test Precision</th>\n",
       "      <th>Val Precision of Best Model</th>\n",
       "      <th>Train Precision of Best Model</th>\n",
       "      <th>Mean Test Accuracy</th>\n",
       "      <th>Std Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>vehicleLoan</td>\n",
       "      <td>0.407318</td>\n",
       "      <td>0.129121</td>\n",
       "      <td>0.339167</td>\n",
       "      <td>0.733167</td>\n",
       "      <td>0.732836</td>\n",
       "      <td>0.027873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>monthlyExpenses</td>\n",
       "      <td>0.399103</td>\n",
       "      <td>0.127994</td>\n",
       "      <td>0.380298</td>\n",
       "      <td>0.711970</td>\n",
       "      <td>0.728358</td>\n",
       "      <td>0.030474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>monthlyFamilyIncome</td>\n",
       "      <td>0.398261</td>\n",
       "      <td>0.070818</td>\n",
       "      <td>0.353333</td>\n",
       "      <td>0.729426</td>\n",
       "      <td>0.740299</td>\n",
       "      <td>0.007737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>monthlyVices</td>\n",
       "      <td>0.386724</td>\n",
       "      <td>0.047433</td>\n",
       "      <td>0.323068</td>\n",
       "      <td>0.748130</td>\n",
       "      <td>0.747015</td>\n",
       "      <td>0.004087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>payInsurance</td>\n",
       "      <td>0.378755</td>\n",
       "      <td>0.034898</td>\n",
       "      <td>0.339365</td>\n",
       "      <td>0.716958</td>\n",
       "      <td>0.729851</td>\n",
       "      <td>0.008175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>monthlyFamilyNetIncome</td>\n",
       "      <td>0.373165</td>\n",
       "      <td>0.061095</td>\n",
       "      <td>0.423333</td>\n",
       "      <td>0.746883</td>\n",
       "      <td>0.736567</td>\n",
       "      <td>0.013086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>preferredNetDisposableIncomeId</td>\n",
       "      <td>0.368088</td>\n",
       "      <td>0.030940</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>0.638404</td>\n",
       "      <td>0.663433</td>\n",
       "      <td>0.022911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>repair</td>\n",
       "      <td>0.363370</td>\n",
       "      <td>0.077332</td>\n",
       "      <td>0.261623</td>\n",
       "      <td>0.741895</td>\n",
       "      <td>0.744776</td>\n",
       "      <td>0.006244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>monthlySoloNetIncomeWithSavings</td>\n",
       "      <td>0.355503</td>\n",
       "      <td>0.095006</td>\n",
       "      <td>0.433651</td>\n",
       "      <td>0.734414</td>\n",
       "      <td>0.726866</td>\n",
       "      <td>0.020163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>informalLenders</td>\n",
       "      <td>0.355245</td>\n",
       "      <td>0.129425</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.746883</td>\n",
       "      <td>0.743284</td>\n",
       "      <td>0.011006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Variable  Mean Test Precision  Std Test Precision  \\\n",
       "29                      vehicleLoan             0.407318            0.129121   \n",
       "57                  monthlyExpenses             0.399103            0.127994   \n",
       "5               monthlyFamilyIncome             0.398261            0.070818   \n",
       "56                     monthlyVices             0.386724            0.047433   \n",
       "41                     payInsurance             0.378755            0.034898   \n",
       "60           monthlyFamilyNetIncome             0.373165            0.061095   \n",
       "2    preferredNetDisposableIncomeId             0.368088            0.030940   \n",
       "21                           repair             0.363370            0.077332   \n",
       "62  monthlySoloNetIncomeWithSavings             0.355503            0.095006   \n",
       "30                  informalLenders             0.355245            0.129425   \n",
       "\n",
       "    Val Precision of Best Model  Train Precision of Best Model  \\\n",
       "29                     0.339167                       0.733167   \n",
       "57                     0.380298                       0.711970   \n",
       "5                      0.353333                       0.729426   \n",
       "56                     0.323068                       0.748130   \n",
       "41                     0.339365                       0.716958   \n",
       "60                     0.423333                       0.746883   \n",
       "2                      0.330400                       0.638404   \n",
       "21                     0.261623                       0.741895   \n",
       "62                     0.433651                       0.734414   \n",
       "30                     0.450000                       0.746883   \n",
       "\n",
       "    Mean Test Accuracy  Std Test Accuracy  \n",
       "29            0.732836           0.027873  \n",
       "57            0.728358           0.030474  \n",
       "5             0.740299           0.007737  \n",
       "56            0.747015           0.004087  \n",
       "41            0.729851           0.008175  \n",
       "60            0.736567           0.013086  \n",
       "2             0.663433           0.022911  \n",
       "21            0.744776           0.006244  \n",
       "62            0.726866           0.020163  \n",
       "30            0.743284           0.011006  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ren = {\n",
    "    'Mean Val Precision': 'Val Precision of Best Model',\n",
    "    'Mean Train Precision': 'Train Precision of Best Model'\n",
    "}\n",
    "sorted_res_df = res_df.copy().rename(columns=ren).drop(70)\n",
    "sorted_res_df = sorted_res_df.sort_values(by='Mean Test Precision', ascending=False)\n",
    "# sorted_res_df[sorted_res_df['Variable'] == 'basicMonthlySalary']\n",
    "sorted_res_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.21976117 0.1648537  0.13857143 0.2640333  0.23407452 0.21976117\n",
      " 0.14688272 0.         0.2640333  0.23407452 0.22310612 0.14688272\n",
      " 0.         0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.16516425 0.12793651 0.271331\n",
      " 0.23516006 0.22310612 0.16516425 0.12793651 0.271331   0.23516006\n",
      " 0.21976117 0.1648537  0.13857143 0.2640333  0.23407452 0.21976117\n",
      " 0.14688272 0.         0.2640333  0.23407452 0.22310612 0.14688272\n",
      " 0.         0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.16516425 0.12793651 0.271331\n",
      " 0.23516006 0.22310612 0.16516425 0.12793651 0.271331   0.23516006\n",
      " 0.21976117 0.1648537  0.13857143 0.2640333  0.23407452 0.21976117\n",
      " 0.14688272 0.         0.2640333  0.23407452 0.22310612 0.14688272\n",
      " 0.         0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.16516425 0.12793651 0.271331\n",
      " 0.23516006 0.22310612 0.16516425 0.12793651 0.271331   0.23516006\n",
      " 0.21976117 0.1648537  0.13857143 0.2640333  0.23407452 0.22310612\n",
      " 0.17188272 0.         0.271331   0.23516006 0.22310612 0.17188272\n",
      " 0.         0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.16516425 0.12793651 0.271331\n",
      " 0.23516006 0.22310612 0.16516425 0.12793651 0.271331   0.23516006\n",
      " 0.21976117 0.16516425 0.14095238 0.26495575 0.23516006 0.22310612\n",
      " 0.1690828  0.025      0.271331   0.23516006 0.22310612 0.1690828\n",
      " 0.025      0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.16516425 0.12793651 0.271331\n",
      " 0.23516006 0.22310612 0.16516425 0.12793651 0.271331   0.23516006\n",
      " 0.22310612 0.16516425 0.12793651 0.271331   0.23516006 0.22310612\n",
      " 0.1648537  0.13095238 0.271331   0.23516006 0.22310612 0.1648537\n",
      " 0.13095238 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.16516425 0.12793651 0.271331\n",
      " 0.23516006 0.22310612 0.16516425 0.12793651 0.271331   0.23516006\n",
      " 0.22310612 0.16516425 0.12793651 0.271331   0.23516006 0.22310612\n",
      " 0.16516425 0.13460317 0.271331   0.23516006 0.22310612 0.16516425\n",
      " 0.13460317 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.16516425 0.12793651 0.271331\n",
      " 0.23516006 0.22310612 0.16516425 0.12793651 0.271331   0.23516006\n",
      " 0.22310612 0.16516425 0.12793651 0.271331   0.23516006 0.22310612\n",
      " 0.16516425 0.12793651 0.271331   0.23516006 0.22310612 0.16516425\n",
      " 0.12793651 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.16516425 0.12793651 0.271331\n",
      " 0.23516006 0.22310612 0.16516425 0.12793651 0.271331   0.23516006\n",
      " 0.22310612 0.16516425 0.12793651 0.271331   0.23516006 0.22310612\n",
      " 0.16516425 0.12793651 0.271331   0.23516006 0.22310612 0.16516425\n",
      " 0.12793651 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.16516425 0.12793651 0.271331\n",
      " 0.23516006 0.22310612 0.16516425 0.12793651 0.271331   0.23516006\n",
      " 0.22310612 0.16516425 0.12793651 0.271331   0.23516006 0.22310612\n",
      " 0.16516425 0.12793651 0.271331   0.23516006 0.22310612 0.16516425\n",
      " 0.12793651 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.16516425 0.12793651 0.271331\n",
      " 0.23516006 0.22310612 0.16516425 0.12793651 0.271331   0.23516006\n",
      " 0.22310612 0.16516425 0.12793651 0.271331   0.23516006 0.22310612\n",
      " 0.16516425 0.12793651 0.271331   0.23516006 0.22310612 0.16516425\n",
      " 0.12793651 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.16516425 0.12793651 0.271331\n",
      " 0.23516006 0.22310612 0.16516425 0.12793651 0.271331   0.23516006\n",
      " 0.22310612 0.16516425 0.12793651 0.271331   0.23516006 0.22310612\n",
      " 0.16516425 0.12793651 0.271331   0.23516006 0.22310612 0.16516425\n",
      " 0.12793651 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.16516425 0.12793651 0.271331\n",
      " 0.23516006 0.22310612 0.16516425 0.12793651 0.271331   0.23516006\n",
      " 0.22310612 0.16516425 0.12793651 0.271331   0.23516006 0.22310612\n",
      " 0.16516425 0.12793651 0.271331   0.23516006 0.22310612 0.16516425\n",
      " 0.12793651 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.16516425 0.12793651 0.271331\n",
      " 0.23516006 0.22310612 0.16516425 0.12793651 0.271331   0.23516006]\n",
      "One or more of the train scores are non-finite: [0.25602463 0.17497222 0.14030964 0.26020752 0.25446799 0.25602463\n",
      " 0.14743642 0.         0.26020752 0.25446799 0.25606611 0.14743642\n",
      " 0.         0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.17510043 0.11107724 0.26192757\n",
      " 0.25426513 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513\n",
      " 0.25602463 0.17497222 0.14030964 0.26020752 0.25446799 0.25602463\n",
      " 0.14743642 0.         0.26020752 0.25446799 0.25606611 0.14743642\n",
      " 0.         0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.17510043 0.11107724 0.26192757\n",
      " 0.25426513 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513\n",
      " 0.25602463 0.17497222 0.14030964 0.26020752 0.25446799 0.25602463\n",
      " 0.14743642 0.         0.26020752 0.25446799 0.25606611 0.14743642\n",
      " 0.         0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.17510043 0.11107724 0.26192757\n",
      " 0.25426513 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513\n",
      " 0.25602463 0.17497222 0.11364298 0.26020752 0.25446799 0.25606611\n",
      " 0.17289425 0.         0.26192757 0.25426513 0.25606611 0.17289425\n",
      " 0.         0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.17510043 0.11107724 0.26192757\n",
      " 0.25426513 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513\n",
      " 0.25602463 0.17510043 0.11240361 0.26177347 0.25426513 0.25606611\n",
      " 0.17463584 0.05520833 0.26192757 0.25426513 0.25606611 0.17463584\n",
      " 0.05520833 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.17510043 0.11107724 0.26192757\n",
      " 0.25426513 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513\n",
      " 0.25606611 0.17510043 0.11064592 0.26192757 0.25426513 0.25606611\n",
      " 0.17497222 0.11337344 0.26192757 0.25426513 0.25606611 0.17497222\n",
      " 0.11337344 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.17510043 0.11107724 0.26192757\n",
      " 0.25426513 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513\n",
      " 0.25606611 0.17510043 0.11064592 0.26192757 0.25426513 0.25606611\n",
      " 0.17510043 0.11220995 0.26192757 0.25426513 0.25606611 0.17510043\n",
      " 0.11220995 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.17510043 0.11107724 0.26192757\n",
      " 0.25426513 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513\n",
      " 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513 0.25606611\n",
      " 0.17510043 0.11064592 0.26192757 0.25426513 0.25606611 0.17510043\n",
      " 0.11064592 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.17510043 0.11107724 0.26192757\n",
      " 0.25426513 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513\n",
      " 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513 0.25606611\n",
      " 0.17510043 0.11107724 0.26192757 0.25426513 0.25606611 0.17510043\n",
      " 0.11107724 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.17510043 0.11107724 0.26192757\n",
      " 0.25426513 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513\n",
      " 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513 0.25606611\n",
      " 0.17510043 0.11107724 0.26192757 0.25426513 0.25606611 0.17510043\n",
      " 0.11107724 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.17510043 0.11107724 0.26192757\n",
      " 0.25426513 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513\n",
      " 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513 0.25606611\n",
      " 0.17510043 0.11107724 0.26192757 0.25426513 0.25606611 0.17510043\n",
      " 0.11107724 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.17510043 0.11107724 0.26192757\n",
      " 0.25426513 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513\n",
      " 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513 0.25606611\n",
      " 0.17510043 0.11107724 0.26192757 0.25426513 0.25606611 0.17510043\n",
      " 0.11107724 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.17510043 0.11107724 0.26192757\n",
      " 0.25426513 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513\n",
      " 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513 0.25606611\n",
      " 0.17510043 0.11107724 0.26192757 0.25426513 0.25606611 0.17510043\n",
      " 0.11107724 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.17510043 0.11107724 0.26192757\n",
      " 0.25426513 0.25606611 0.17510043 0.11107724 0.26192757 0.25426513]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.27133100338819\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.5336658354114713\n",
      "GridSearchCV Runtime: 6.277412176132202 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2711864406779661\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2605042016806723\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2127659574468085\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.25806451612903225\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.265625\n",
      "Ave Test Precision: 0.25362922318689585\n",
      "Stdev Test Precision: 0.023392831097881244\n",
      "Ave Test Accuracy: 0.5208955223880597\n",
      "Stdev Test Accuracy: 0.03989223062956446\n",
      "Ave Test Specificity: 0.5336633663366336\n",
      "Ave Test Recall: 0.4818181818181818\n",
      "Ave Test NPV: 0.7580573795725198\n",
      "Ave Test F1-Score: 0.33203477274722176\n",
      "Ave Test G-mean: 0.5067362124311666\n",
      "Ave Runtime: 0.0056572437286376955\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with age. 86 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.34888224 0.20969136 0.125      0.34888224 0.30128264 0.34888224\n",
      " 0.07469136 0.         0.34888224 0.30128264 0.34898253 0.07469136\n",
      " 0.         0.35279991 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.35150634 0.26619382 0.32148812 0.35370221\n",
      " 0.2976455  0.34900634 0.26619382 0.32327383 0.35370221 0.2976455\n",
      " 0.34888224 0.20969136 0.125      0.34888224 0.30128264 0.34888224\n",
      " 0.07469136 0.         0.34888224 0.30128264 0.34898253 0.07469136\n",
      " 0.         0.35279991 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.35150634 0.26619382 0.32148812 0.35370221\n",
      " 0.2976455  0.34900634 0.26619382 0.32327383 0.35370221 0.2976455\n",
      " 0.34888224 0.20969136 0.125      0.34696654 0.30220856 0.34888224\n",
      " 0.07469136 0.         0.34489757 0.30220856 0.34898253 0.07469136\n",
      " 0.         0.35279991 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.35150634 0.26619382 0.32148812 0.35370221\n",
      " 0.2976455  0.34900634 0.26619382 0.32327383 0.35370221 0.2976455\n",
      " 0.34696654 0.20969136 0.125      0.34748829 0.30308336 0.34826524\n",
      " 0.10802469 0.         0.34520468 0.30752086 0.34898253 0.10802469\n",
      " 0.         0.35279991 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.35150634 0.26619382 0.32148812 0.35370221\n",
      " 0.2976455  0.34900634 0.26619382 0.32327383 0.35370221 0.2976455\n",
      " 0.35017301 0.21428906 0.22       0.35486888 0.29810939 0.35017301\n",
      " 0.20969136 0.03333333 0.35486888 0.30006485 0.34755396 0.20969136\n",
      " 0.03333333 0.35486888 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.35150634 0.26619382 0.32148812 0.35370221\n",
      " 0.2976455  0.34900634 0.26619382 0.32327383 0.35370221 0.2976455\n",
      " 0.35017301 0.26579141 0.38060606 0.35370221 0.29937507 0.35267301\n",
      " 0.26579141 0.35393939 0.35486888 0.30006485 0.35017301 0.26501965\n",
      " 0.35393939 0.35486888 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.35150634 0.26619382 0.32148812 0.35370221\n",
      " 0.2976455  0.34900634 0.26619382 0.32327383 0.35370221 0.2976455\n",
      " 0.34900634 0.26619382 0.31656628 0.35370221 0.30006485 0.35150634\n",
      " 0.26619382 0.31656628 0.35370221 0.30006485 0.34900634 0.26579141\n",
      " 0.31656628 0.35370221 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.35150634 0.26619382 0.32148812 0.35370221\n",
      " 0.2976455  0.34900634 0.26619382 0.32327383 0.35370221 0.2976455\n",
      " 0.34900634 0.26619382 0.31907803 0.35370221 0.2976455  0.35150634\n",
      " 0.26619382 0.31907803 0.35370221 0.2976455  0.34900634 0.26619382\n",
      " 0.31907803 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.35150634 0.26619382 0.32148812 0.35370221\n",
      " 0.2976455  0.34900634 0.26619382 0.32327383 0.35370221 0.2976455\n",
      " 0.34900634 0.26619382 0.32327383 0.35370221 0.2976455  0.35150634\n",
      " 0.26619382 0.32327383 0.35370221 0.2976455  0.34900634 0.26619382\n",
      " 0.32327383 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.35150634 0.26619382 0.32148812 0.35370221\n",
      " 0.2976455  0.34900634 0.26619382 0.32327383 0.35370221 0.2976455\n",
      " 0.34900634 0.26619382 0.32148812 0.35370221 0.2976455  0.35150634\n",
      " 0.26619382 0.32148812 0.35370221 0.2976455  0.34900634 0.26619382\n",
      " 0.32327383 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.35150634 0.26619382 0.32148812 0.35370221\n",
      " 0.2976455  0.34900634 0.26619382 0.32327383 0.35370221 0.2976455\n",
      " 0.34900634 0.26619382 0.32148812 0.35370221 0.2976455  0.35150634\n",
      " 0.26619382 0.32148812 0.35370221 0.2976455  0.34900634 0.26619382\n",
      " 0.32327383 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.35150634 0.26619382 0.32148812 0.35370221\n",
      " 0.2976455  0.34900634 0.26619382 0.32327383 0.35370221 0.2976455\n",
      " 0.34900634 0.26619382 0.32148812 0.35370221 0.2976455  0.35150634\n",
      " 0.26619382 0.32148812 0.35370221 0.2976455  0.34900634 0.26619382\n",
      " 0.32327383 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.35150634 0.26619382 0.32148812 0.35370221\n",
      " 0.2976455  0.34900634 0.26619382 0.32327383 0.35370221 0.2976455\n",
      " 0.34900634 0.26619382 0.32148812 0.35370221 0.2976455  0.35150634\n",
      " 0.26619382 0.32148812 0.35370221 0.2976455  0.34900634 0.26619382\n",
      " 0.32327383 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.35150634 0.26619382 0.32148812 0.35370221\n",
      " 0.2976455  0.34900634 0.26619382 0.32327383 0.35370221 0.2976455 ]\n",
      "One or more of the train scores are non-finite: [0.33640426 0.35752745 0.23278689 0.33640426 0.29739021 0.33640426\n",
      " 0.07357971 0.         0.33640426 0.29739021 0.3369827  0.07357971\n",
      " 0.         0.3378266  0.30521314        nan        nan        nan\n",
      "        nan        nan 0.33788101 0.41046212 0.3181312  0.33819292\n",
      " 0.30508579 0.33805654 0.41046212 0.31812343 0.33803269 0.30508579\n",
      " 0.33640426 0.35752745 0.23278689 0.33640426 0.29739021 0.33640426\n",
      " 0.07357971 0.         0.33640426 0.29739021 0.3369827  0.07357971\n",
      " 0.         0.3378266  0.30521314        nan        nan        nan\n",
      "        nan        nan 0.33788101 0.41046212 0.3181312  0.33819292\n",
      " 0.30508579 0.33805654 0.41046212 0.31812343 0.33803269 0.30508579\n",
      " 0.33640426 0.35752745 0.26612022 0.33708516 0.29742787 0.33654084\n",
      " 0.14024637 0.         0.33765265 0.29757266 0.3369827  0.14024637\n",
      " 0.         0.3378266  0.30521314        nan        nan        nan\n",
      "        nan        nan 0.33788101 0.41046212 0.3181312  0.33819292\n",
      " 0.30508579 0.33805654 0.41046212 0.31812343 0.33803269 0.30508579\n",
      " 0.33756746 0.36307897 0.26612022 0.33726774 0.29843156 0.33800767\n",
      " 0.10561854 0.         0.33824145 0.30239816 0.3369827  0.10561854\n",
      " 0.         0.33738592 0.30521314        nan        nan        nan\n",
      "        nan        nan 0.33788101 0.41046212 0.3181312  0.33819292\n",
      " 0.30508579 0.33805654 0.41046212 0.31812343 0.33803269 0.30508579\n",
      " 0.33761071 0.41211885 0.46825397 0.33757844 0.30118939 0.33728971\n",
      " 0.34109772 0.21449275 0.33800371 0.30541468 0.33743351 0.34109772\n",
      " 0.21449275 0.33800371 0.3053761         nan        nan        nan\n",
      "        nan        nan 0.33788101 0.41046212 0.3181312  0.33819292\n",
      " 0.30508579 0.33805654 0.41046212 0.31812343 0.33803269 0.30508579\n",
      " 0.33760094 0.41042465 0.42566476 0.33803269 0.30493409 0.33758995\n",
      " 0.41397162 0.4477105  0.33773867 0.30516355 0.33760229 0.41408612\n",
      " 0.4477105  0.33757844 0.30516355        nan        nan        nan\n",
      "        nan        nan 0.33788101 0.41046212 0.3181312  0.33819292\n",
      " 0.30508579 0.33805654 0.41046212 0.31812343 0.33803269 0.30508579\n",
      " 0.33819028 0.41046212 0.31843471 0.33803269 0.30552562 0.3380442\n",
      " 0.41046212 0.32059615 0.33819292 0.30576118 0.33805654 0.41042465\n",
      " 0.32059615 0.33803269 0.3054857         nan        nan        nan\n",
      "        nan        nan 0.33788101 0.41046212 0.3181312  0.33819292\n",
      " 0.30508579 0.33805654 0.41046212 0.31812343 0.33803269 0.30508579\n",
      " 0.33819028 0.41046212 0.31967941 0.33803269 0.30508579 0.33788101\n",
      " 0.41046212 0.31967941 0.33819292 0.30508579 0.33805654 0.41046212\n",
      " 0.31967941 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33788101 0.41046212 0.3181312  0.33819292\n",
      " 0.30508579 0.33805654 0.41046212 0.31812343 0.33803269 0.30508579\n",
      " 0.33819028 0.41046212 0.31812343 0.33803269 0.30508579 0.33788101\n",
      " 0.41046212 0.31812343 0.33819292 0.30508579 0.33805654 0.41046212\n",
      " 0.31812343 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33788101 0.41046212 0.3181312  0.33819292\n",
      " 0.30508579 0.33805654 0.41046212 0.31812343 0.33803269 0.30508579\n",
      " 0.33819028 0.41046212 0.3181312  0.33803269 0.30508579 0.33788101\n",
      " 0.41046212 0.3181312  0.33819292 0.30508579 0.33805654 0.41046212\n",
      " 0.31812343 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33788101 0.41046212 0.3181312  0.33819292\n",
      " 0.30508579 0.33805654 0.41046212 0.31812343 0.33803269 0.30508579\n",
      " 0.33819028 0.41046212 0.3181312  0.33803269 0.30508579 0.33788101\n",
      " 0.41046212 0.3181312  0.33819292 0.30508579 0.33805654 0.41046212\n",
      " 0.31812343 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33788101 0.41046212 0.3181312  0.33819292\n",
      " 0.30508579 0.33805654 0.41046212 0.31812343 0.33803269 0.30508579\n",
      " 0.33819028 0.41046212 0.3181312  0.33803269 0.30508579 0.33788101\n",
      " 0.41046212 0.3181312  0.33819292 0.30508579 0.33805654 0.41046212\n",
      " 0.31812343 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33788101 0.41046212 0.3181312  0.33819292\n",
      " 0.30508579 0.33805654 0.41046212 0.31812343 0.33803269 0.30508579\n",
      " 0.33819028 0.41046212 0.3181312  0.33803269 0.30508579 0.33788101\n",
      " 0.41046212 0.3181312  0.33819292 0.30508579 0.33805654 0.41046212\n",
      " 0.31812343 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33788101 0.41046212 0.3181312  0.33819292\n",
      " 0.30508579 0.33805654 0.41046212 0.31812343 0.33803269 0.30508579]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.38060606060606056\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7144638403990025\n",
      "GridSearchCV Runtime: 4.124464511871338 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.5357142857142857\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3181818181818182\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.38461538461538464\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.38372093023255816\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.40816326530612246\n",
      "Ave Test Precision: 0.40607913681003377\n",
      "Stdev Test Precision: 0.07983754772142024\n",
      "Ave Test Accuracy: 0.7253731343283583\n",
      "Stdev Test Accuracy: 0.030610663137304008\n",
      "Ave Test Specificity: 0.8831683168316833\n",
      "Ave Test Recall: 0.24242424242424243\n",
      "Ave Test NPV: 0.7834165143864736\n",
      "Ave Test F1-Score: 0.27737174740288917\n",
      "Ave Test G-mean: 0.43219034281924334\n",
      "Ave Runtime: 0.0067825794219970705\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with basicMonthlySalary. 85 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.07469136 0.         0.33040034 0.33040034 0.33040034 0.07469136\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.07469136 0.         0.33040034 0.33040034 0.33040034 0.07469136\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.07469136 0.         0.33040034 0.33040034 0.33040034 0.07469136\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.25550241 0.         0.33040034 0.33040034 0.33040034 0.25550241\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034]\n",
      "One or more of the train scores are non-finite: [0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.07357971 0.         0.33330429 0.33330429 0.33330429 0.07357971\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.07357971 0.         0.33330429 0.33330429 0.33330429 0.07357971\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.07357971 0.         0.33330429 0.33330429 0.33330429 0.07357971\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.24230745 0.         0.33330429 0.33330429 0.33330429 0.24230745\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.33040033960292586\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6384039900249376\n",
      "GridSearchCV Runtime: 4.204370737075806 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4044943820224719\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.32978723404255317\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.39325842696629215\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3645833333333333\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.34831460674157305\n",
      "Ave Test Precision: 0.3680875966212447\n",
      "Stdev Test Precision: 0.03094023871571617\n",
      "Ave Test Accuracy: 0.6634328358208956\n",
      "Stdev Test Accuracy: 0.022910569302998132\n",
      "Ave Test Specificity: 0.7138613861386138\n",
      "Ave Test Recall: 0.509090909090909\n",
      "Ave Test NPV: 0.8164610334822197\n",
      "Ave Test F1-Score: 0.4271455595380327\n",
      "Ave Test G-mean: 0.6026404897216738\n",
      "Ave Runtime: 0.006032609939575195\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with preferredNetDisposableIncomeId. 84 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27245955 0.23621075 0.17016817 0.22069131 0.23315737 0.27245955\n",
      " 0.17119136 0.14719136 0.22069131 0.23315737 0.27245955 0.14719136\n",
      " 0.14719136 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23621075 0.17016817 0.22069131 0.23315737 0.27245955\n",
      " 0.14719136 0.14719136 0.22069131 0.23315737 0.27245955 0.14719136\n",
      " 0.14719136 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23621075 0.17016817 0.22069131 0.23315737 0.27245955\n",
      " 0.14719136 0.14719136 0.22069131 0.23315737 0.27245955 0.14719136\n",
      " 0.14719136 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23621075 0.17016817 0.22069131 0.23315737 0.27245955\n",
      " 0.14219136 0.14719136 0.22069131 0.23315737 0.27245955 0.14219136\n",
      " 0.14719136 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.17016817 0.22069131 0.23315737 0.27245955\n",
      " 0.24727715 0.14878776 0.22069131 0.23315737 0.27245955 0.24727715\n",
      " 0.14878776 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19356085 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737]\n",
      "One or more of the train scores are non-finite: [0.27465602 0.24924294 0.17262537 0.26677571 0.2471273  0.27465602\n",
      " 0.17228505 0.14740242 0.26677571 0.2471273  0.27465602 0.14740242\n",
      " 0.14740242 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.24924294 0.17262537 0.26677571 0.2471273  0.27465602\n",
      " 0.14740242 0.14740242 0.26677571 0.2471273  0.27465602 0.14740242\n",
      " 0.14740242 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.24924294 0.17262537 0.26677571 0.2471273  0.27465602\n",
      " 0.14740242 0.14740242 0.26677571 0.2471273  0.27465602 0.14740242\n",
      " 0.14740242 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.24924294 0.17262537 0.26677571 0.2471273  0.27465602\n",
      " 0.17295529 0.14747091 0.26677571 0.2471273  0.27465602 0.17295529\n",
      " 0.14747091 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.17262537 0.26677571 0.2471273  0.27465602\n",
      " 0.24937137 0.14697849 0.26677571 0.2471273  0.27465602 0.24937137\n",
      " 0.14697849 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20096206 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273 ]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2724595467673271\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6234413965087282\n",
      "GridSearchCV Runtime: 3.8908352851867676 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.32786885245901637\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.28169014084507044\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.24210526315789474\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2911392405063291\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23469387755102042\n",
      "Ave Test Precision: 0.2754994749038662\n",
      "Stdev Test Precision: 0.038097731139356536\n",
      "Ave Test Accuracy: 0.5395522388059701\n",
      "Stdev Test Accuracy: 0.14968687150516055\n",
      "Ave Test Specificity: 0.5623762376237623\n",
      "Ave Test Recall: 0.4696969696969697\n",
      "Ave Test NPV: 0.75651479560109\n",
      "Ave Test F1-Score: 0.32693857006385035\n",
      "Ave Test G-mean: 0.46803840312657136\n",
      "Ave Runtime: 0.005141019821166992\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with workingFamilyCount. 83 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25757038 0.24351648 0.09102079 0.240807   0.22522689 0.25757038\n",
      " 0.07344136 0.02375    0.240807   0.22522689 0.25757038 0.07344136\n",
      " 0.02375    0.240807   0.25214413        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.24351648 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.24351648 0.09181362 0.240807   0.25214413\n",
      " 0.25757038 0.24351648 0.09102079 0.240807   0.22522689 0.25757038\n",
      " 0.07344136 0.02375    0.240807   0.22522689 0.25757038 0.07344136\n",
      " 0.02375    0.240807   0.25214413        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.24351648 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.24351648 0.09181362 0.240807   0.25214413\n",
      " 0.25757038 0.24351648 0.09102079 0.240807   0.22522689 0.25757038\n",
      " 0.07344136 0.02375    0.240807   0.22522689 0.25757038 0.07344136\n",
      " 0.02375    0.240807   0.25214413        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.24351648 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.24351648 0.09181362 0.240807   0.25214413\n",
      " 0.25757038 0.24351648 0.09102079 0.240807   0.22522689 0.25757038\n",
      " 0.20010802 0.02375    0.240807   0.22497664 0.25757038 0.20010802\n",
      " 0.02375    0.240807   0.25214413        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.24351648 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.24351648 0.09181362 0.240807   0.25214413\n",
      " 0.25757038 0.24351648 0.09102079 0.240807   0.22522689 0.25757038\n",
      " 0.23351648 0.06982323 0.240807   0.22928698 0.25757038 0.23351648\n",
      " 0.06982323 0.240807   0.25214413        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.24351648 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.24351648 0.09181362 0.240807   0.25214413\n",
      " 0.25757038 0.24351648 0.09324301 0.240807   0.22928698 0.25757038\n",
      " 0.24351648 0.09181362 0.240807   0.22928698 0.25757038 0.24351648\n",
      " 0.09181362 0.240807   0.25214413        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.24351648 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.24351648 0.09181362 0.240807   0.25214413\n",
      " 0.25757038 0.24351648 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.24351648 0.09181362 0.240807   0.22928698 0.25757038 0.24351648\n",
      " 0.09181362 0.240807   0.25214413        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.24351648 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.24351648 0.09181362 0.240807   0.25214413\n",
      " 0.25757038 0.24351648 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.24351648 0.09181362 0.240807   0.22928698 0.25757038 0.24351648\n",
      " 0.09181362 0.240807   0.25214413        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.24351648 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.24351648 0.09181362 0.240807   0.25214413\n",
      " 0.25757038 0.24351648 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.24351648 0.09181362 0.240807   0.22928698 0.25757038 0.24351648\n",
      " 0.09181362 0.240807   0.25214413        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.24351648 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.24351648 0.09181362 0.240807   0.25214413\n",
      " 0.25757038 0.24351648 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.24351648 0.09181362 0.240807   0.22928698 0.25757038 0.24351648\n",
      " 0.09181362 0.240807   0.25214413        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.24351648 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.24351648 0.09181362 0.240807   0.25214413\n",
      " 0.25757038 0.24351648 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.24351648 0.09181362 0.240807   0.22928698 0.25757038 0.24351648\n",
      " 0.09181362 0.240807   0.25214413        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.24351648 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.24351648 0.09181362 0.240807   0.25214413\n",
      " 0.25757038 0.24351648 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.24351648 0.09181362 0.240807   0.22928698 0.25757038 0.24351648\n",
      " 0.09181362 0.240807   0.25214413        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.24351648 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.24351648 0.09181362 0.240807   0.25214413\n",
      " 0.25757038 0.24351648 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.24351648 0.09181362 0.240807   0.22928698 0.25757038 0.24351648\n",
      " 0.09181362 0.240807   0.25214413        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.24351648 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.24351648 0.09181362 0.240807   0.25214413]\n",
      "One or more of the train scores are non-finite: [0.25770653 0.25130062 0.09130233 0.25645463 0.2233833  0.25770653\n",
      " 0.07371821 0.02465374 0.25645463 0.2233833  0.25770653 0.07371821\n",
      " 0.02465374 0.25645463 0.24614753        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25130062 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25130062 0.11605227 0.25645463 0.24614753\n",
      " 0.25770653 0.25130062 0.09130233 0.25645463 0.2233833  0.25770653\n",
      " 0.07371821 0.02465374 0.25645463 0.2233833  0.25770653 0.07371821\n",
      " 0.02465374 0.25645463 0.24614753        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25130062 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25130062 0.11605227 0.25645463 0.24614753\n",
      " 0.25770653 0.25130062 0.09130233 0.25645463 0.2233833  0.25770653\n",
      " 0.07371821 0.02465374 0.25645463 0.2233833  0.25770653 0.07371821\n",
      " 0.02465374 0.25645463 0.24614753        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25130062 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25130062 0.11605227 0.25645463 0.24614753\n",
      " 0.25770653 0.25130062 0.09130233 0.25645463 0.2233833  0.25770653\n",
      " 0.18451801 0.02465374 0.25645463 0.22356627 0.25770653 0.18451801\n",
      " 0.02465374 0.25645463 0.24614753        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25130062 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25130062 0.11605227 0.25645463 0.24614753\n",
      " 0.25770653 0.25130062 0.10709181 0.25645463 0.2233833  0.25770653\n",
      " 0.25005062 0.08848413 0.25645463 0.22314433 0.25770653 0.25005062\n",
      " 0.08848413 0.25645463 0.24614753        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25130062 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25130062 0.11605227 0.25645463 0.24614753\n",
      " 0.25770653 0.25130062 0.11414138 0.25645463 0.22314433 0.25770653\n",
      " 0.25130062 0.10876482 0.25645463 0.22314433 0.25770653 0.25130062\n",
      " 0.10876482 0.25645463 0.24614753        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25130062 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25130062 0.11605227 0.25645463 0.24614753\n",
      " 0.25770653 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653 0.25130062\n",
      " 0.11605227 0.25645463 0.24614753        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25130062 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25130062 0.11605227 0.25645463 0.24614753\n",
      " 0.25770653 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653 0.25130062\n",
      " 0.11605227 0.25645463 0.24614753        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25130062 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25130062 0.11605227 0.25645463 0.24614753\n",
      " 0.25770653 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653 0.25130062\n",
      " 0.11605227 0.25645463 0.24614753        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25130062 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25130062 0.11605227 0.25645463 0.24614753\n",
      " 0.25770653 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653 0.25130062\n",
      " 0.11605227 0.25645463 0.24614753        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25130062 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25130062 0.11605227 0.25645463 0.24614753\n",
      " 0.25770653 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653 0.25130062\n",
      " 0.11605227 0.25645463 0.24614753        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25130062 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25130062 0.11605227 0.25645463 0.24614753\n",
      " 0.25770653 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653 0.25130062\n",
      " 0.11605227 0.25645463 0.24614753        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25130062 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25130062 0.11605227 0.25645463 0.24614753\n",
      " 0.25770653 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25130062 0.11605227 0.25645463 0.22314433 0.25770653 0.25130062\n",
      " 0.11605227 0.25645463 0.24614753        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25130062 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25130062 0.11605227 0.25645463 0.24614753]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2575703750454944\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.4800498753117207\n",
      "GridSearchCV Runtime: 3.864006996154785 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.22580645161290322\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.26666666666666666\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.19607843137254902\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.26143790849673204\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25308641975308643\n",
      "Ave Test Precision: 0.24061517558038745\n",
      "Stdev Test Precision: 0.029455976538380783\n",
      "Ave Test Accuracy: 0.4776119402985075\n",
      "Stdev Test Accuracy: 0.03326969123158023\n",
      "Ave Test Specificity: 0.45940594059405937\n",
      "Ave Test Recall: 0.5333333333333333\n",
      "Ave Test NPV: 0.7532560573805719\n",
      "Ave Test F1-Score: 0.33003072327471156\n",
      "Ave Test G-mean: 0.4858975132150499\n",
      "Ave Runtime: 0.006184625625610352\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with residentsCount. 82 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.23883324 0.17734609 0.04       0.23883324 0.26808052 0.23883324\n",
      " 0.12344136 0.         0.23883324 0.26808052 0.23730719 0.12344136\n",
      " 0.         0.2391029  0.2538565         nan        nan        nan\n",
      "        nan        nan 0.23272938 0.17027318 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834\n",
      " 0.23883324 0.17734609 0.04       0.23883324 0.26808052 0.23883324\n",
      " 0.12344136 0.         0.23883324 0.26808052 0.23730719 0.12344136\n",
      " 0.         0.2391029  0.2538565         nan        nan        nan\n",
      "        nan        nan 0.23272938 0.17027318 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834\n",
      " 0.23959082 0.16067942 0.04       0.23959082 0.26808052 0.23959082\n",
      " 0.12344136 0.         0.23959082 0.26415112 0.23730719 0.12344136\n",
      " 0.         0.2391029  0.2538565         nan        nan        nan\n",
      "        nan        nan 0.23272938 0.17027318 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834\n",
      " 0.24004032 0.19015443 0.03333333 0.24004032 0.26808052 0.2391358\n",
      " 0.12375781 0.         0.24004032 0.26464443 0.23730719 0.12375781\n",
      " 0.         0.2391029  0.2538565         nan        nan        nan\n",
      "        nan        nan 0.23272938 0.17027318 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834\n",
      " 0.23730719 0.18482437 0.04444444 0.2391029  0.27208688 0.23730719\n",
      " 0.22555359 0.025      0.2391029  0.2538565  0.23730719 0.22555359\n",
      " 0.025      0.2391029  0.2538565         nan        nan        nan\n",
      "        nan        nan 0.23272938 0.17027318 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834\n",
      " 0.23762448 0.17312879 0.10119048 0.2391029  0.25691232 0.23762448\n",
      " 0.17006668 0.11666667 0.2391029  0.25504227 0.23762448 0.17006668\n",
      " 0.11666667 0.2391029  0.25504227        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.17027318 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834\n",
      " 0.23762448 0.17027318 0.325      0.2391029  0.25504227 0.23272938\n",
      " 0.17027318 0.30833333 0.2391029  0.25504227 0.23272938 0.17027318\n",
      " 0.29166667 0.2391029  0.25504227        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.17027318 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834\n",
      " 0.23272938 0.17027318 0.29175558 0.2391029  0.25883834 0.23272938\n",
      " 0.17027318 0.29175558 0.2391029  0.25883834 0.23272938 0.17027318\n",
      " 0.29175558 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.17027318 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834\n",
      " 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.17027318 0.32099389 0.2391029  0.25883834 0.23272938 0.17027318\n",
      " 0.32099389 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.17027318 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834\n",
      " 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.17027318 0.32099389 0.2391029  0.25883834 0.23272938 0.17027318\n",
      " 0.32099389 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.17027318 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834\n",
      " 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.17027318 0.32099389 0.2391029  0.25883834 0.23272938 0.17027318\n",
      " 0.32099389 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.17027318 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834\n",
      " 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.17027318 0.32099389 0.2391029  0.25883834 0.23272938 0.17027318\n",
      " 0.32099389 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.17027318 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834\n",
      " 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.17027318 0.32099389 0.2391029  0.25883834 0.23272938 0.17027318\n",
      " 0.32099389 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.17027318 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.17027318 0.32099389 0.2391029  0.25883834]\n",
      "One or more of the train scores are non-finite: [0.24308207 0.15049906 0.05       0.24308207 0.25660681 0.24308207\n",
      " 0.12274868 0.         0.24308207 0.25660681 0.24120377 0.12274868\n",
      " 0.         0.24176348 0.26256588        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.18466824 0.30199079 0.24121567\n",
      " 0.26179596 0.24087133 0.18422406 0.30199079 0.24117372 0.26179596\n",
      " 0.24308207 0.15049906 0.05       0.24308207 0.25660681 0.24308207\n",
      " 0.12274868 0.         0.24308207 0.25625853 0.24120377 0.12274868\n",
      " 0.         0.24176348 0.26256588        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.18466824 0.30199079 0.24121567\n",
      " 0.26179596 0.24087133 0.18422406 0.30199079 0.24117372 0.26179596\n",
      " 0.24361482 0.1447881  0.05       0.24361482 0.25642856 0.24361482\n",
      " 0.12288526 0.         0.24361482 0.25726183 0.24120377 0.12288526\n",
      " 0.         0.24176348 0.26256588        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.18466824 0.30199079 0.24121567\n",
      " 0.26179596 0.24087133 0.18422406 0.30199079 0.24117372 0.26179596\n",
      " 0.24382729 0.17742059 0.04848485 0.24382729 0.25574441 0.24298293\n",
      " 0.12301777 0.         0.24407221 0.2633714  0.24120377 0.12301777\n",
      " 0.         0.24176348 0.26256588        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.18466824 0.30199079 0.24121567\n",
      " 0.26179596 0.24087133 0.18422406 0.30199079 0.24117372 0.26179596\n",
      " 0.24162394 0.18318486 0.0601626  0.24170065 0.2622546  0.24106638\n",
      " 0.15608629 0.05666667 0.24130313 0.26251991 0.24106638 0.15608629\n",
      " 0.05666667 0.24130313 0.26260418        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.18466824 0.30199079 0.24121567\n",
      " 0.26179596 0.24087133 0.18422406 0.30199079 0.24117372 0.26179596\n",
      " 0.24074004 0.18204718 0.22943723 0.24113098 0.26090395 0.24060268\n",
      " 0.18324847 0.21529221 0.24117372 0.26206867 0.24060268 0.18324847\n",
      " 0.21529221 0.24117372 0.26206867        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.18466824 0.30199079 0.24121567\n",
      " 0.26179596 0.24087133 0.18422406 0.30199079 0.24117372 0.26179596\n",
      " 0.2406455  0.18422406 0.31481523 0.24117372 0.26215121 0.24087133\n",
      " 0.18396069 0.31777965 0.24117372 0.26215121 0.24087133 0.18407587\n",
      " 0.32044423 0.24117372 0.26215121        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.18466824 0.30199079 0.24121567\n",
      " 0.26179596 0.24087133 0.18422406 0.30199079 0.24117372 0.26179596\n",
      " 0.24087133 0.18466824 0.31623395 0.24121567 0.26179596 0.24087133\n",
      " 0.18466824 0.31491592 0.24121567 0.26179596 0.24087133 0.18422406\n",
      " 0.31491592 0.24117372 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.18466824 0.30199079 0.24121567\n",
      " 0.26179596 0.24087133 0.18422406 0.30199079 0.24117372 0.26179596\n",
      " 0.24087133 0.18466824 0.30238749 0.24121567 0.26179596 0.24087133\n",
      " 0.18466824 0.30238749 0.24121567 0.26179596 0.24087133 0.18422406\n",
      " 0.30238749 0.24117372 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.18466824 0.30199079 0.24121567\n",
      " 0.26179596 0.24087133 0.18422406 0.30199079 0.24117372 0.26179596\n",
      " 0.24087133 0.18466824 0.30199079 0.24121567 0.26179596 0.24087133\n",
      " 0.18466824 0.30199079 0.24121567 0.26179596 0.24087133 0.18422406\n",
      " 0.30199079 0.24117372 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.18466824 0.30199079 0.24121567\n",
      " 0.26179596 0.24087133 0.18422406 0.30199079 0.24117372 0.26179596\n",
      " 0.24087133 0.18466824 0.30199079 0.24121567 0.26179596 0.24087133\n",
      " 0.18466824 0.30199079 0.24121567 0.26179596 0.24087133 0.18422406\n",
      " 0.30199079 0.24117372 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.18466824 0.30199079 0.24121567\n",
      " 0.26179596 0.24087133 0.18422406 0.30199079 0.24117372 0.26179596\n",
      " 0.24087133 0.18466824 0.30199079 0.24121567 0.26179596 0.24087133\n",
      " 0.18466824 0.30199079 0.24121567 0.26179596 0.24087133 0.18422406\n",
      " 0.30199079 0.24117372 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.18466824 0.30199079 0.24121567\n",
      " 0.26179596 0.24087133 0.18422406 0.30199079 0.24117372 0.26179596\n",
      " 0.24087133 0.18466824 0.30199079 0.24121567 0.26179596 0.24087133\n",
      " 0.18466824 0.30199079 0.24121567 0.26179596 0.24087133 0.18422406\n",
      " 0.30199079 0.24117372 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.18466824 0.30199079 0.24121567\n",
      " 0.26179596 0.24087133 0.18422406 0.30199079 0.24117372 0.26179596]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.325\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7256857855361596\n",
      "GridSearchCV Runtime: 3.898428201675415 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.39285714285714285\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.42857142857142855\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.36363636363636365\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.32558139534883723\n",
      "Ave Test Precision: 0.3687959327494211\n",
      "Stdev Test Precision: 0.042731034056466353\n",
      "Ave Test Accuracy: 0.7231343283582089\n",
      "Stdev Test Accuracy: 0.01928061295934899\n",
      "Ave Test Specificity: 0.9099009900990099\n",
      "Ave Test Recall: 0.15151515151515152\n",
      "Ave Test NPV: 0.7666523120375565\n",
      "Ave Test F1-Score: 0.2047745287431874\n",
      "Ave Test G-mean: 0.36022064511802654\n",
      "Ave Runtime: 0.00537109375\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyIncome. 81 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.2512724  0.32917268 0.21930401 0.26631454 0.26631454 0.2512724\n",
      " 0.1225     0.0475     0.26631454 0.26631454 0.2520287  0.1225\n",
      " 0.0475     0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34036887 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224\n",
      " 0.2512724  0.32917268 0.21930401 0.26631454 0.26631454 0.2512724\n",
      " 0.1225     0.0475     0.26631454 0.26631454 0.2520287  0.1225\n",
      " 0.0475     0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34036887 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224\n",
      " 0.2512724  0.32917268 0.21930401 0.26631454 0.26631454 0.2512724\n",
      " 0.1225     0.0475     0.26631454 0.26631454 0.2520287  0.1225\n",
      " 0.0475     0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34036887 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.32811634 0.20956376 0.26707084 0.26631454 0.2520287\n",
      " 0.16232496 0.0475     0.2706866  0.2656877  0.2520287  0.16232496\n",
      " 0.0475     0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34036887 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34145539 0.20757495 0.26984626 0.26560366 0.2520287\n",
      " 0.36940078 0.1741829  0.2706866  0.27275081 0.2520287  0.36940078\n",
      " 0.1741829  0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34036887 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34036887 0.23644802 0.26984626 0.27007224 0.2520287\n",
      " 0.34036887 0.24657622 0.26984626 0.27007224 0.2520287  0.34036887\n",
      " 0.24657622 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34036887 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34036887 0.23376945 0.26984626 0.27007224 0.2520287  0.34036887\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34036887 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34036887 0.23376945 0.26984626 0.27007224 0.2520287  0.34036887\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34036887 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34036887 0.23376945 0.26984626 0.27007224 0.2520287  0.34036887\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34036887 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34036887 0.23376945 0.26984626 0.27007224 0.2520287  0.34036887\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34036887 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34036887 0.23376945 0.26984626 0.27007224 0.2520287  0.34036887\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34036887 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34036887 0.23376945 0.26984626 0.27007224 0.2520287  0.34036887\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34036887 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34036887 0.23376945 0.26984626 0.27007224 0.2520287  0.34036887\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34036887 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34036887 0.23376945 0.26984626 0.27007224]\n",
      "One or more of the train scores are non-finite: [0.25983315 0.28570073 0.35475287 0.2666605  0.2666605  0.25983315\n",
      " 0.12285319 0.04930748 0.2666605  0.2666605  0.26147392 0.12285319\n",
      " 0.04930748 0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.28710289 0.31292808 0.27030576\n",
      " 0.27196809 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809\n",
      " 0.25983315 0.28570073 0.35475287 0.2666605  0.2666605  0.25983315\n",
      " 0.12285319 0.04930748 0.2666605  0.2666605  0.26147392 0.12285319\n",
      " 0.04930748 0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.28710289 0.31292808 0.27030576\n",
      " 0.27196809 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809\n",
      " 0.25983315 0.28570073 0.35475287 0.2666605  0.2666605  0.25983315\n",
      " 0.12285319 0.04930748 0.2666605  0.2666605  0.26147392 0.12285319\n",
      " 0.04930748 0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.28710289 0.31292808 0.27030576\n",
      " 0.27196809 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809\n",
      " 0.26053995 0.28577219 0.33050496 0.26736731 0.2666605  0.26147392\n",
      " 0.20130934 0.04930748 0.27100077 0.26917579 0.26147392 0.20130934\n",
      " 0.04930748 0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.28710289 0.31292808 0.27030576\n",
      " 0.27196809 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809\n",
      " 0.26138586 0.28992815 0.33793803 0.27030576 0.26909793 0.26147392\n",
      " 0.31059309 0.27565062 0.27100077 0.2716977  0.26147392 0.31059309\n",
      " 0.27565062 0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.28710289 0.31292808 0.27030576\n",
      " 0.27196809 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809\n",
      " 0.26138586 0.28710289 0.31452345 0.27030576 0.27196809 0.26147392\n",
      " 0.28926235 0.3126167  0.27030576 0.27196809 0.26147392 0.28926235\n",
      " 0.3126167  0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.28710289 0.31292808 0.27030576\n",
      " 0.27196809 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809\n",
      " 0.26138586 0.28710289 0.31301339 0.27030576 0.27196809 0.26138586\n",
      " 0.28710289 0.31496145 0.27030576 0.27196809 0.26138586 0.28710289\n",
      " 0.31496145 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.28710289 0.31292808 0.27030576\n",
      " 0.27196809 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809\n",
      " 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809 0.26138586\n",
      " 0.28710289 0.31292808 0.27030576 0.27196809 0.26138586 0.28710289\n",
      " 0.31292808 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.28710289 0.31292808 0.27030576\n",
      " 0.27196809 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809\n",
      " 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809 0.26138586\n",
      " 0.28710289 0.31292808 0.27030576 0.27196809 0.26138586 0.28710289\n",
      " 0.31292808 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.28710289 0.31292808 0.27030576\n",
      " 0.27196809 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809\n",
      " 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809 0.26138586\n",
      " 0.28710289 0.31292808 0.27030576 0.27196809 0.26138586 0.28710289\n",
      " 0.31292808 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.28710289 0.31292808 0.27030576\n",
      " 0.27196809 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809\n",
      " 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809 0.26138586\n",
      " 0.28710289 0.31292808 0.27030576 0.27196809 0.26138586 0.28710289\n",
      " 0.31292808 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.28710289 0.31292808 0.27030576\n",
      " 0.27196809 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809\n",
      " 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809 0.26138586\n",
      " 0.28710289 0.31292808 0.27030576 0.27196809 0.26138586 0.28710289\n",
      " 0.31292808 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.28710289 0.31292808 0.27030576\n",
      " 0.27196809 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809\n",
      " 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809 0.26138586\n",
      " 0.28710289 0.31292808 0.27030576 0.27196809 0.26138586 0.28710289\n",
      " 0.31292808 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.28710289 0.31292808 0.27030576\n",
      " 0.27196809 0.26138586 0.28710289 0.31292808 0.27030576 0.27196809]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.3694007844007844\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.2830423940149626\n",
      "GridSearchCV Runtime: 3.982741594314575 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25301204819277107\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.26262626262626265\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.32142857142857145\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2463768115942029\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23107569721115537\n",
      "Ave Test Precision: 0.2629038782105927\n",
      "Stdev Test Precision: 0.0346743497575953\n",
      "Ave Test Accuracy: 0.3977611940298508\n",
      "Stdev Test Accuracy: 0.16800577725058893\n",
      "Ave Test Specificity: 0.28811881188118815\n",
      "Ave Test Recall: 0.7333333333333334\n",
      "Ave Test NPV: 0.7398400588359784\n",
      "Ave Test F1-Score: 0.3657156668295935\n",
      "Ave Test G-mean: 0.3660476121896702\n",
      "Ave Runtime: 0.006265974044799805\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with food. 80 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24186576 0.26970238 0.32324495 0.26478243 0.26478243 0.24186576\n",
      " 0.07344136 0.17344136 0.26478243 0.26478243 0.24186576 0.07344136\n",
      " 0.17344136 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.24186576 0.25176587 0.32376927 0.26478243\n",
      " 0.26478243 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243\n",
      " 0.24186576 0.26970238 0.32324495 0.26478243 0.26478243 0.24186576\n",
      " 0.07344136 0.17344136 0.26478243 0.26478243 0.24186576 0.07344136\n",
      " 0.17344136 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.24186576 0.25176587 0.32376927 0.26478243\n",
      " 0.26478243 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243\n",
      " 0.24186576 0.26932009 0.32324495 0.26478243 0.26478243 0.24186576\n",
      " 0.07344136 0.17344136 0.26478243 0.26478243 0.24186576 0.07344136\n",
      " 0.17344136 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.24186576 0.25176587 0.32376927 0.26478243\n",
      " 0.26478243 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243\n",
      " 0.24186576 0.26932009 0.32324495 0.26478243 0.26478243 0.24186576\n",
      " 0.28004224 0.17344136 0.26478243 0.26478243 0.24186576 0.28004224\n",
      " 0.17344136 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.24186576 0.25176587 0.32376927 0.26478243\n",
      " 0.26478243 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243\n",
      " 0.24186576 0.26676587 0.3253378  0.26478243 0.26478243 0.24186576\n",
      " 0.2565366  0.24580128 0.26478243 0.26478243 0.24186576 0.2565366\n",
      " 0.24580128 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.24186576 0.25176587 0.32376927 0.26478243\n",
      " 0.26478243 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243\n",
      " 0.24186576 0.25176587 0.32785289 0.26478243 0.26478243 0.24186576\n",
      " 0.26638536 0.32755715 0.26478243 0.26478243 0.24186576 0.26638536\n",
      " 0.32755715 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.24186576 0.25176587 0.32376927 0.26478243\n",
      " 0.26478243 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243\n",
      " 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243 0.24186576\n",
      " 0.25138536 0.32376927 0.26478243 0.26478243 0.24186576 0.25138536\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.24186576 0.25176587 0.32376927 0.26478243\n",
      " 0.26478243 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243\n",
      " 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243 0.24186576\n",
      " 0.25176587 0.32376927 0.26478243 0.26478243 0.24186576 0.25176587\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.24186576 0.25176587 0.32376927 0.26478243\n",
      " 0.26478243 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243\n",
      " 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243 0.24186576\n",
      " 0.25176587 0.32376927 0.26478243 0.26478243 0.24186576 0.25176587\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.24186576 0.25176587 0.32376927 0.26478243\n",
      " 0.26478243 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243\n",
      " 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243 0.24186576\n",
      " 0.25176587 0.32376927 0.26478243 0.26478243 0.24186576 0.25176587\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.24186576 0.25176587 0.32376927 0.26478243\n",
      " 0.26478243 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243\n",
      " 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243 0.24186576\n",
      " 0.25176587 0.32376927 0.26478243 0.26478243 0.24186576 0.25176587\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.24186576 0.25176587 0.32376927 0.26478243\n",
      " 0.26478243 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243\n",
      " 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243 0.24186576\n",
      " 0.25176587 0.32376927 0.26478243 0.26478243 0.24186576 0.25176587\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.24186576 0.25176587 0.32376927 0.26478243\n",
      " 0.26478243 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243\n",
      " 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243 0.24186576\n",
      " 0.25176587 0.32376927 0.26478243 0.26478243 0.24186576 0.25176587\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.24186576 0.25176587 0.32376927 0.26478243\n",
      " 0.26478243 0.24186576 0.25176587 0.32376927 0.26478243 0.26478243]\n",
      "One or more of the train scores are non-finite: [0.26976256 0.27934852 0.22747979 0.27123273 0.27123273 0.26976256\n",
      " 0.07371821 0.17177915 0.27123273 0.27123273 0.26960942 0.07371821\n",
      " 0.17177915 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26960942 0.27351999 0.22890689 0.27123273\n",
      " 0.27123273 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273\n",
      " 0.26976256 0.27934852 0.22747979 0.27123273 0.27123273 0.26976256\n",
      " 0.07371821 0.17177915 0.27123273 0.27123273 0.26960942 0.07371821\n",
      " 0.17177915 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26960942 0.27351999 0.22890689 0.27123273\n",
      " 0.27123273 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273\n",
      " 0.26976256 0.27906791 0.22747979 0.27123273 0.27123273 0.26976256\n",
      " 0.57371821 0.17177915 0.27123273 0.27123273 0.26960942 0.57371821\n",
      " 0.17177915 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26960942 0.27351999 0.22890689 0.27123273\n",
      " 0.27123273 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273\n",
      " 0.26976256 0.27906791 0.22747979 0.27123273 0.27123273 0.26976256\n",
      " 0.33743439 0.27156967 0.27123273 0.27123273 0.26960942 0.33743439\n",
      " 0.27156967 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26960942 0.27351999 0.22890689 0.27123273\n",
      " 0.27123273 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273\n",
      " 0.26960942 0.28113501 0.22760927 0.27123273 0.27123273 0.26960942\n",
      " 0.28203103 0.23285536 0.27123273 0.27123273 0.26960942 0.28203103\n",
      " 0.23285536 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26960942 0.27351999 0.22890689 0.27123273\n",
      " 0.27123273 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273\n",
      " 0.26960942 0.27351999 0.2292232  0.27123273 0.27123273 0.26960942\n",
      " 0.28146896 0.22578975 0.27123273 0.27123273 0.26960942 0.28146896\n",
      " 0.22578975 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26960942 0.27351999 0.22890689 0.27123273\n",
      " 0.27123273 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273\n",
      " 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273 0.26960942\n",
      " 0.27385394 0.22890689 0.27123273 0.27123273 0.26960942 0.27385394\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26960942 0.27351999 0.22890689 0.27123273\n",
      " 0.27123273 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273\n",
      " 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273 0.26960942\n",
      " 0.27351999 0.22890689 0.27123273 0.27123273 0.26960942 0.27351999\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26960942 0.27351999 0.22890689 0.27123273\n",
      " 0.27123273 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273\n",
      " 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273 0.26960942\n",
      " 0.27351999 0.22890689 0.27123273 0.27123273 0.26960942 0.27351999\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26960942 0.27351999 0.22890689 0.27123273\n",
      " 0.27123273 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273\n",
      " 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273 0.26960942\n",
      " 0.27351999 0.22890689 0.27123273 0.27123273 0.26960942 0.27351999\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26960942 0.27351999 0.22890689 0.27123273\n",
      " 0.27123273 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273\n",
      " 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273 0.26960942\n",
      " 0.27351999 0.22890689 0.27123273 0.27123273 0.26960942 0.27351999\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26960942 0.27351999 0.22890689 0.27123273\n",
      " 0.27123273 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273\n",
      " 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273 0.26960942\n",
      " 0.27351999 0.22890689 0.27123273 0.27123273 0.26960942 0.27351999\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26960942 0.27351999 0.22890689 0.27123273\n",
      " 0.27123273 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273\n",
      " 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273 0.26960942\n",
      " 0.27351999 0.22890689 0.27123273 0.27123273 0.26960942 0.27351999\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26960942 0.27351999 0.22890689 0.27123273\n",
      " 0.27123273 0.26960942 0.27351999 0.22890689 0.27123273 0.27123273]\n",
      "invalid value encountered in scalar divide\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3278528863091375\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.2456359102244389\n",
      "GridSearchCV Runtime: 4.113641262054443 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.23076923076923078\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.15384615384615385\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.27586206896551724\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.18134922205946397\n",
      "Stdev Test Precision: 0.11094312460677516\n",
      "Ave Test Accuracy: 0.6238805970149254\n",
      "Stdev Test Accuracy: 0.212578527154492\n",
      "Ave Test Specificity: 0.7475247524752475\n",
      "Ave Test Recall: 0.24545454545454543\n",
      "Ave Test NPV: 0.7519490870947375\n",
      "Ave Test F1-Score: 0.14530730445952805\n",
      "Ave Test G-mean: 0.15396174602904167\n",
      "Ave Runtime: 0.007212114334106445\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with hygiene. 79 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24902536 0.22048476 0.17275899 0.24902536 0.26985869 0.24902536\n",
      " 0.12094136 0.09938272 0.24902536 0.26985869 0.24902536 0.12094136\n",
      " 0.09938272 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.22048476 0.16586395 0.24902536\n",
      " 0.26985869 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869\n",
      " 0.24902536 0.22048476 0.17275899 0.24902536 0.26985869 0.24902536\n",
      " 0.12094136 0.09938272 0.24902536 0.26985869 0.24902536 0.12094136\n",
      " 0.09938272 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.22048476 0.16586395 0.24902536\n",
      " 0.26985869 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869\n",
      " 0.24902536 0.22048476 0.17275899 0.24902536 0.26985869 0.24902536\n",
      " 0.12094136 0.09938272 0.24902536 0.26985869 0.24902536 0.12094136\n",
      " 0.09938272 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.22048476 0.16586395 0.24902536\n",
      " 0.26985869 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869\n",
      " 0.24902536 0.22048476 0.17275899 0.24902536 0.26985869 0.24902536\n",
      " 0.1393749  0.09938272 0.24902536 0.26985869 0.24902536 0.1393749\n",
      " 0.09938272 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.22048476 0.16586395 0.24902536\n",
      " 0.26985869 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869\n",
      " 0.24902536 0.22048476 0.17167203 0.24902536 0.26985869 0.24902536\n",
      " 0.22813989 0.13918752 0.24902536 0.26985869 0.24902536 0.22813989\n",
      " 0.13918752 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.22048476 0.16586395 0.24902536\n",
      " 0.26985869 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869\n",
      " 0.24902536 0.22048476 0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.22019219 0.17177278 0.24902536 0.26985869 0.24902536 0.22019219\n",
      " 0.17177278 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.22048476 0.16586395 0.24902536\n",
      " 0.26985869 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869\n",
      " 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869 0.24902536\n",
      " 0.22048476 0.16508829 0.24902536 0.26985869 0.24902536 0.22048476\n",
      " 0.16508829 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.22048476 0.16586395 0.24902536\n",
      " 0.26985869 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869\n",
      " 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869 0.24902536\n",
      " 0.22048476 0.16508829 0.24902536 0.26985869 0.24902536 0.22048476\n",
      " 0.16508829 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.22048476 0.16586395 0.24902536\n",
      " 0.26985869 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869\n",
      " 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869 0.24902536\n",
      " 0.22048476 0.16586395 0.24902536 0.26985869 0.24902536 0.22048476\n",
      " 0.16586395 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.22048476 0.16586395 0.24902536\n",
      " 0.26985869 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869\n",
      " 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869 0.24902536\n",
      " 0.22048476 0.16586395 0.24902536 0.26985869 0.24902536 0.22048476\n",
      " 0.16586395 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.22048476 0.16586395 0.24902536\n",
      " 0.26985869 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869\n",
      " 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869 0.24902536\n",
      " 0.22048476 0.16586395 0.24902536 0.26985869 0.24902536 0.22048476\n",
      " 0.16586395 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.22048476 0.16586395 0.24902536\n",
      " 0.26985869 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869\n",
      " 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869 0.24902536\n",
      " 0.22048476 0.16586395 0.24902536 0.26985869 0.24902536 0.22048476\n",
      " 0.16586395 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.22048476 0.16586395 0.24902536\n",
      " 0.26985869 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869\n",
      " 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869 0.24902536\n",
      " 0.22048476 0.16586395 0.24902536 0.26985869 0.24902536 0.22048476\n",
      " 0.16586395 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.22048476 0.16586395 0.24902536\n",
      " 0.26985869 0.24902536 0.22048476 0.16586395 0.24902536 0.26985869]\n",
      "One or more of the train scores are non-finite: [0.26632203 0.23274927 0.17163901 0.26632203 0.27002058 0.26632203\n",
      " 0.12302569 0.09812895 0.26632203 0.27002058 0.26632203 0.12302569\n",
      " 0.09812895 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23292134 0.17428399 0.26632203\n",
      " 0.27002058 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058\n",
      " 0.26632203 0.23274927 0.17163901 0.26632203 0.27002058 0.26632203\n",
      " 0.12302569 0.09812895 0.26632203 0.27002058 0.26632203 0.12302569\n",
      " 0.09812895 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23292134 0.17428399 0.26632203\n",
      " 0.27002058 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058\n",
      " 0.26632203 0.23274927 0.17163901 0.26632203 0.27002058 0.26632203\n",
      " 0.12302569 0.09812895 0.26632203 0.27002058 0.26632203 0.12302569\n",
      " 0.09812895 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23292134 0.17428399 0.26632203\n",
      " 0.27002058 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058\n",
      " 0.26632203 0.23292134 0.17163901 0.26632203 0.27002058 0.26632203\n",
      " 0.14593117 0.09812895 0.26632203 0.27002058 0.26632203 0.14593117\n",
      " 0.09812895 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23292134 0.17428399 0.26632203\n",
      " 0.27002058 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058\n",
      " 0.26632203 0.23292134 0.16940691 0.26632203 0.27002058 0.26632203\n",
      " 0.23251514 0.1555506  0.26632203 0.27002058 0.26632203 0.23251514\n",
      " 0.1555506  0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23292134 0.17428399 0.26632203\n",
      " 0.27002058 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058\n",
      " 0.26632203 0.23292134 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23323763 0.17359627 0.26632203 0.27002058 0.26632203 0.23323763\n",
      " 0.17359627 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23292134 0.17428399 0.26632203\n",
      " 0.27002058 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058\n",
      " 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058 0.26632203\n",
      " 0.23292134 0.17426672 0.26632203 0.27002058 0.26632203 0.23292134\n",
      " 0.17426672 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23292134 0.17428399 0.26632203\n",
      " 0.27002058 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058\n",
      " 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058 0.26632203\n",
      " 0.23292134 0.17426672 0.26632203 0.27002058 0.26632203 0.23292134\n",
      " 0.17426672 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23292134 0.17428399 0.26632203\n",
      " 0.27002058 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058\n",
      " 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058 0.26632203\n",
      " 0.23292134 0.17428399 0.26632203 0.27002058 0.26632203 0.23292134\n",
      " 0.17428399 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23292134 0.17428399 0.26632203\n",
      " 0.27002058 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058\n",
      " 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058 0.26632203\n",
      " 0.23292134 0.17428399 0.26632203 0.27002058 0.26632203 0.23292134\n",
      " 0.17428399 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23292134 0.17428399 0.26632203\n",
      " 0.27002058 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058\n",
      " 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058 0.26632203\n",
      " 0.23292134 0.17428399 0.26632203 0.27002058 0.26632203 0.23292134\n",
      " 0.17428399 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23292134 0.17428399 0.26632203\n",
      " 0.27002058 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058\n",
      " 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058 0.26632203\n",
      " 0.23292134 0.17428399 0.26632203 0.27002058 0.26632203 0.23292134\n",
      " 0.17428399 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23292134 0.17428399 0.26632203\n",
      " 0.27002058 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058\n",
      " 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058 0.26632203\n",
      " 0.23292134 0.17428399 0.26632203 0.27002058 0.26632203 0.23292134\n",
      " 0.17428399 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23292134 0.17428399 0.26632203\n",
      " 0.27002058 0.26632203 0.23292134 0.17428399 0.26632203 0.27002058]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.26985869022737996\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.5037406483790524\n",
      "GridSearchCV Runtime: 4.103474855422974 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2198581560283688\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.22727272727272727\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25874125874125875\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.25196850393700787\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.22916666666666666\n",
      "Ave Test Precision: 0.23740146252920588\n",
      "Stdev Test Precision: 0.01692451137791282\n",
      "Ave Test Accuracy: 0.4843283582089553\n",
      "Stdev Test Accuracy: 0.024666408777669267\n",
      "Ave Test Specificity: 0.48118811881188117\n",
      "Ave Test Recall: 0.49393939393939396\n",
      "Ave Test NPV: 0.7440879564869685\n",
      "Ave Test F1-Score: 0.3205012257582182\n",
      "Ave Test G-mean: 0.48688585434159287\n",
      "Ave Runtime: 0.004412937164306641\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseCleaning. 78 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.26223668 0.21429149 0.19053102 0.26223668 0.2562228  0.26223668\n",
      " 0.09594136 0.07375    0.26223668 0.2562228  0.26032097 0.09594136\n",
      " 0.07375    0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19483912 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231\n",
      " 0.26223668 0.21429149 0.19053102 0.26223668 0.2562228  0.26223668\n",
      " 0.09594136 0.07375    0.26223668 0.2562228  0.26032097 0.09594136\n",
      " 0.07375    0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19483912 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231\n",
      " 0.26223668 0.21429149 0.19053102 0.26223668 0.2562228  0.26223668\n",
      " 0.09594136 0.07375    0.26223668 0.2562228  0.26032097 0.09594136\n",
      " 0.07375    0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19483912 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231\n",
      " 0.26223668 0.21429149 0.19053102 0.26223668 0.2562228  0.26032097\n",
      " 0.16072077 0.07375    0.2615525  0.25430709 0.26032097 0.16072077\n",
      " 0.07375    0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19483912 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231\n",
      " 0.26223668 0.21355541 0.19745154 0.2615525  0.25430709 0.26032097\n",
      " 0.20415312 0.1675     0.2615525  0.25387231 0.26032097 0.20415312\n",
      " 0.1675     0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19483912 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19483912 0.2615525  0.25430709 0.26032097\n",
      " 0.21355541 0.19761689 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19761689 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19483912 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19483912 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19483912 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19483912 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19483912 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19483912 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19483912 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19483912 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19483912 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19483912 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19483912 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19483912 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19483912 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19483912 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19483912 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19483912 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19483912 0.2615525  0.25387231]\n",
      "One or more of the train scores are non-finite: [0.26573415 0.29861881 0.21988623 0.26573415 0.25991785 0.26573415\n",
      " 0.09851046 0.07368421 0.26573415 0.25991785 0.26671905 0.09851046\n",
      " 0.07368421 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.2666015  0.29871026 0.20894942 0.26712216\n",
      " 0.26084006 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006\n",
      " 0.26573415 0.29861881 0.21988623 0.26573415 0.25991785 0.26573415\n",
      " 0.09851046 0.07368421 0.26573415 0.25991785 0.26671905 0.09851046\n",
      " 0.07368421 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.2666015  0.29871026 0.20894942 0.26712216\n",
      " 0.26084006 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006\n",
      " 0.26573415 0.29861881 0.21988623 0.26573415 0.25991785 0.26573415\n",
      " 0.14851046 0.07368421 0.26573415 0.25991785 0.26671905 0.14851046\n",
      " 0.07368421 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.2666015  0.29871026 0.20894942 0.26712216\n",
      " 0.26084006 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006\n",
      " 0.2658846  0.29861881 0.22048955 0.2658846  0.2600683  0.26671905\n",
      " 0.21903108 0.0736138  0.26712463 0.26044862 0.26671905 0.21903108\n",
      " 0.0736138  0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.2666015  0.29871026 0.20894942 0.26712216\n",
      " 0.26084006 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006\n",
      " 0.26599952 0.29863095 0.22071591 0.26701111 0.26091754 0.26671905\n",
      " 0.23716334 0.17203037 0.26723955 0.26084006 0.26671905 0.23716334\n",
      " 0.17203037 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.2666015  0.29871026 0.20894942 0.26712216\n",
      " 0.26084006 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006\n",
      " 0.26649151 0.29871026 0.20913159 0.26712216 0.26087086 0.2666015\n",
      " 0.29863095 0.21201702 0.26712216 0.26084006 0.2666015  0.29863095\n",
      " 0.21201702 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.2666015  0.29871026 0.20894942 0.26712216\n",
      " 0.26084006 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006\n",
      " 0.2666015  0.29871026 0.2090736  0.26712216 0.26088814 0.2666015\n",
      " 0.29871026 0.20894942 0.26712216 0.26084006 0.2666015  0.29871026\n",
      " 0.20894942 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.2666015  0.29871026 0.20894942 0.26712216\n",
      " 0.26084006 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006\n",
      " 0.2666015  0.29871026 0.2090736  0.26712216 0.26084006 0.2666015\n",
      " 0.29871026 0.20894942 0.26712216 0.26084006 0.2666015  0.29871026\n",
      " 0.20894942 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.2666015  0.29871026 0.20894942 0.26712216\n",
      " 0.26084006 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006\n",
      " 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006 0.2666015\n",
      " 0.29871026 0.20894942 0.26712216 0.26084006 0.2666015  0.29871026\n",
      " 0.20894942 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.2666015  0.29871026 0.20894942 0.26712216\n",
      " 0.26084006 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006\n",
      " 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006 0.2666015\n",
      " 0.29871026 0.20894942 0.26712216 0.26084006 0.2666015  0.29871026\n",
      " 0.20894942 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.2666015  0.29871026 0.20894942 0.26712216\n",
      " 0.26084006 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006\n",
      " 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006 0.2666015\n",
      " 0.29871026 0.20894942 0.26712216 0.26084006 0.2666015  0.29871026\n",
      " 0.20894942 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.2666015  0.29871026 0.20894942 0.26712216\n",
      " 0.26084006 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006\n",
      " 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006 0.2666015\n",
      " 0.29871026 0.20894942 0.26712216 0.26084006 0.2666015  0.29871026\n",
      " 0.20894942 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.2666015  0.29871026 0.20894942 0.26712216\n",
      " 0.26084006 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006\n",
      " 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006 0.2666015\n",
      " 0.29871026 0.20894942 0.26712216 0.26084006 0.2666015  0.29871026\n",
      " 0.20894942 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.2666015  0.29871026 0.20894942 0.26712216\n",
      " 0.26084006 0.2666015  0.29871026 0.20894942 0.26712216 0.26084006]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2622366815064584\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.5972568578553616\n",
      "GridSearchCV Runtime: 4.303966999053955 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2524271844660194\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.27835051546391754\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.24175824175824176\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.23404255319148937\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3\n",
      "Ave Test Precision: 0.26131569897593365\n",
      "Stdev Test Precision: 0.027349517466009447\n",
      "Ave Test Accuracy: 0.5843283582089552\n",
      "Stdev Test Accuracy: 0.022788703708361904\n",
      "Ave Test Specificity: 0.6524752475247524\n",
      "Ave Test Recall: 0.37575757575757573\n",
      "Ave Test NPV: 0.7617886649403591\n",
      "Ave Test F1-Score: 0.30807785489489997\n",
      "Ave Test G-mean: 0.4945966523204481\n",
      "Ave Runtime: 0.008149528503417968\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with fare. 77 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27441813 0.165      0.10000781 0.27441813 0.27870385 0.27441813\n",
      " 0.         0.09969136 0.27441813 0.27870385 0.28951914 0.\n",
      " 0.09969136 0.28951914 0.27053624        nan        nan        nan\n",
      "        nan        nan 0.28951914 0.165      0.2038421  0.28951914\n",
      " 0.28442513 0.28951914 0.165      0.2038421  0.28951914 0.28442513\n",
      " 0.27441813 0.165      0.10000781 0.27441813 0.27870385 0.27441813\n",
      " 0.         0.09969136 0.27441813 0.27870385 0.28951914 0.\n",
      " 0.09969136 0.28951914 0.27053624        nan        nan        nan\n",
      "        nan        nan 0.28951914 0.165      0.2038421  0.28951914\n",
      " 0.28442513 0.28951914 0.165      0.2038421  0.28951914 0.28442513\n",
      " 0.27441813 0.165      0.10000781 0.27441813 0.27870385 0.27441813\n",
      " 0.         0.09969136 0.27441813 0.27870385 0.28951914 0.\n",
      " 0.09969136 0.28951914 0.27053624        nan        nan        nan\n",
      "        nan        nan 0.28951914 0.165      0.2038421  0.28951914\n",
      " 0.28442513 0.28951914 0.165      0.2038421  0.28951914 0.28442513\n",
      " 0.27441813 0.165      0.10031646 0.28451914 0.27870385 0.27441813\n",
      " 0.04       0.09969136 0.28951914 0.27053624 0.28951914 0.04\n",
      " 0.09969136 0.28951914 0.27053624        nan        nan        nan\n",
      "        nan        nan 0.28951914 0.165      0.2038421  0.28951914\n",
      " 0.28442513 0.28951914 0.165      0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.165      0.20031646 0.28951914 0.27410767 0.28951914\n",
      " 0.13333333 0.10000781 0.28951914 0.27053624 0.28951914 0.13333333\n",
      " 0.10000781 0.28951914 0.27053624        nan        nan        nan\n",
      "        nan        nan 0.28951914 0.165      0.2038421  0.28951914\n",
      " 0.28442513 0.28951914 0.165      0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.165      0.18396624 0.28951914 0.28442513 0.28951914\n",
      " 0.165      0.18396624 0.28951914 0.28442513 0.28951914 0.165\n",
      " 0.18396624 0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.28951914 0.165      0.2038421  0.28951914\n",
      " 0.28442513 0.28951914 0.165      0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.165      0.2038421  0.28951914 0.28442513 0.28951914\n",
      " 0.165      0.2038421  0.28951914 0.28442513 0.28951914 0.165\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.28951914 0.165      0.2038421  0.28951914\n",
      " 0.28442513 0.28951914 0.165      0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.165      0.2038421  0.28951914 0.28442513 0.28951914\n",
      " 0.165      0.2038421  0.28951914 0.28442513 0.28951914 0.165\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.28951914 0.165      0.2038421  0.28951914\n",
      " 0.28442513 0.28951914 0.165      0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.165      0.2038421  0.28951914 0.28442513 0.28951914\n",
      " 0.165      0.2038421  0.28951914 0.28442513 0.28951914 0.165\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.28951914 0.165      0.2038421  0.28951914\n",
      " 0.28442513 0.28951914 0.165      0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.165      0.2038421  0.28951914 0.28442513 0.28951914\n",
      " 0.165      0.2038421  0.28951914 0.28442513 0.28951914 0.165\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.28951914 0.165      0.2038421  0.28951914\n",
      " 0.28442513 0.28951914 0.165      0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.165      0.2038421  0.28951914 0.28442513 0.28951914\n",
      " 0.165      0.2038421  0.28951914 0.28442513 0.28951914 0.165\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.28951914 0.165      0.2038421  0.28951914\n",
      " 0.28442513 0.28951914 0.165      0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.165      0.2038421  0.28951914 0.28442513 0.28951914\n",
      " 0.165      0.2038421  0.28951914 0.28442513 0.28951914 0.165\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.28951914 0.165      0.2038421  0.28951914\n",
      " 0.28442513 0.28951914 0.165      0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.165      0.2038421  0.28951914 0.28442513 0.28951914\n",
      " 0.165      0.2038421  0.28951914 0.28442513 0.28951914 0.165\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.28951914 0.165      0.2038421  0.28951914\n",
      " 0.28442513 0.28951914 0.165      0.2038421  0.28951914 0.28442513]\n",
      "One or more of the train scores are non-finite: [0.29844374 0.18828206 0.21038545 0.29844374 0.29028487 0.29844374\n",
      " 0.         0.09809494 0.29844374 0.29028487 0.30185304 0.\n",
      " 0.09809494 0.30327687 0.28775907        nan        nan        nan\n",
      "        nan        nan 0.30185304 0.20237004 0.28396677 0.30255705\n",
      " 0.28211498 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498\n",
      " 0.29844374 0.18828206 0.21038545 0.29844374 0.29028487 0.29844374\n",
      " 0.         0.09809494 0.29844374 0.29028487 0.30185304 0.\n",
      " 0.09809494 0.30327687 0.28775907        nan        nan        nan\n",
      "        nan        nan 0.30185304 0.20237004 0.28396677 0.30255705\n",
      " 0.28211498 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498\n",
      " 0.29844374 0.18828206 0.21038545 0.29844374 0.29028487 0.29844374\n",
      " 0.025      0.09809494 0.29844374 0.29028487 0.30185304 0.025\n",
      " 0.09809494 0.30327687 0.28775907        nan        nan        nan\n",
      "        nan        nan 0.30185304 0.20237004 0.28396677 0.30255705\n",
      " 0.28211498 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498\n",
      " 0.29912007 0.18828206 0.21260768 0.30045688 0.29028487 0.29912007\n",
      " 0.0945098  0.09809494 0.30188072 0.28775907 0.30185304 0.0945098\n",
      " 0.09809494 0.30327687 0.28775907        nan        nan        nan\n",
      "        nan        nan 0.30185304 0.20237004 0.28396677 0.30255705\n",
      " 0.28211498 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498\n",
      " 0.30042424 0.19459226 0.27368189 0.30180954 0.28973949 0.30185304\n",
      " 0.19146658 0.23146237 0.30255705 0.28775907 0.30185304 0.19146658\n",
      " 0.23146237 0.30255705 0.28775907        nan        nan        nan\n",
      "        nan        nan 0.30185304 0.20237004 0.28396677 0.30255705\n",
      " 0.28211498 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498\n",
      " 0.30185304 0.20237004 0.28881972 0.30255705 0.28237949 0.30185304\n",
      " 0.20237004 0.28722799 0.30255705 0.28211498 0.30185304 0.20237004\n",
      " 0.28722799 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.30185304 0.20237004 0.28396677 0.30255705\n",
      " 0.28211498 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498\n",
      " 0.30185304 0.20237004 0.28452791 0.30255705 0.28211498 0.30185304\n",
      " 0.20237004 0.28452791 0.30255705 0.28211498 0.30185304 0.20237004\n",
      " 0.28452791 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.30185304 0.20237004 0.28396677 0.30255705\n",
      " 0.28211498 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498\n",
      " 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498 0.30185304\n",
      " 0.20237004 0.28396677 0.30255705 0.28211498 0.30185304 0.20237004\n",
      " 0.28396677 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.30185304 0.20237004 0.28396677 0.30255705\n",
      " 0.28211498 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498\n",
      " 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498 0.30185304\n",
      " 0.20237004 0.28396677 0.30255705 0.28211498 0.30185304 0.20237004\n",
      " 0.28396677 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.30185304 0.20237004 0.28396677 0.30255705\n",
      " 0.28211498 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498\n",
      " 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498 0.30185304\n",
      " 0.20237004 0.28396677 0.30255705 0.28211498 0.30185304 0.20237004\n",
      " 0.28396677 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.30185304 0.20237004 0.28396677 0.30255705\n",
      " 0.28211498 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498\n",
      " 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498 0.30185304\n",
      " 0.20237004 0.28396677 0.30255705 0.28211498 0.30185304 0.20237004\n",
      " 0.28396677 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.30185304 0.20237004 0.28396677 0.30255705\n",
      " 0.28211498 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498\n",
      " 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498 0.30185304\n",
      " 0.20237004 0.28396677 0.30255705 0.28211498 0.30185304 0.20237004\n",
      " 0.28396677 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.30185304 0.20237004 0.28396677 0.30255705\n",
      " 0.28211498 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498\n",
      " 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498 0.30185304\n",
      " 0.20237004 0.28396677 0.30255705 0.28211498 0.30185304 0.20237004\n",
      " 0.28396677 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.30185304 0.20237004 0.28396677 0.30255705\n",
      " 0.28211498 0.30185304 0.20237004 0.28396677 0.30255705 0.28211498]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.28951914098972925\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.7069825436408977\n",
      "GridSearchCV Runtime: 4.659212589263916 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.15151515151515152\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.22594142259414227\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.24166666666666667\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2384937238493724\n",
      "Ave Test Precision: 0.22152339292506656\n",
      "Stdev Test Precision: 0.04007908934430588\n",
      "Ave Test Accuracy: 0.43805970149253726\n",
      "Stdev Test Accuracy: 0.21520180709461456\n",
      "Ave Test Specificity: 0.39900990099009903\n",
      "Ave Test Recall: 0.5575757575757576\n",
      "Ave Test NPV: 0.6969918560156813\n",
      "Ave Test F1-Score: 0.27932863321447954\n",
      "Ave Test G-mean: 0.2928868017257362\n",
      "Ave Runtime: 0.006718492507934571\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with parking. 76 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27204502 0.29983092 0.35954643 0.27204502 0.27204502 0.27204502\n",
      " 0.09007597 0.09969136 0.27204502 0.27204502 0.27204502 0.09007597\n",
      " 0.09969136 0.26547744 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30600606 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.29983092 0.35954643 0.27204502 0.27204502 0.27204502\n",
      " 0.09007597 0.09969136 0.27204502 0.27204502 0.27204502 0.09007597\n",
      " 0.09969136 0.26547744 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30600606 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.29983092 0.35954643 0.27204502 0.27204502 0.27204502\n",
      " 0.09007597 0.09969136 0.27023935 0.27204502 0.27204502 0.09007597\n",
      " 0.09969136 0.26547744 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30600606 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.29983092 0.35954643 0.27204502 0.27204502 0.27204502\n",
      " 0.31785375 0.19969136 0.26886206 0.26633093 0.27204502 0.31785375\n",
      " 0.19969136 0.26886206 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30600606 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.29732465 0.35752391 0.26728311 0.2638985  0.27204502\n",
      " 0.30589724 0.17436668 0.26728311 0.26633093 0.27204502 0.30589724\n",
      " 0.17436668 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30600606 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30070303 0.33378765 0.26728311 0.26547744 0.27204502\n",
      " 0.30070303 0.35632733 0.26728311 0.26633093 0.27204502 0.30070303\n",
      " 0.35632733 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30600606 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30070303 0.34878794 0.26728311 0.26633093 0.27204502 0.30070303\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30600606 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30600606 0.34878794 0.26728311 0.26633093 0.27204502 0.30070303\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30600606 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30600606 0.34878794 0.26728311 0.26633093 0.27204502 0.30070303\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30600606 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30600606 0.34878794 0.26728311 0.26633093 0.27204502 0.30600606\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30600606 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30600606 0.34878794 0.26728311 0.26633093 0.27204502 0.30600606\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30600606 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30600606 0.34878794 0.26728311 0.26633093 0.27204502 0.30600606\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30600606 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30600606 0.34878794 0.26728311 0.26633093 0.27204502 0.30600606\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30600606 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30600606 0.34878794 0.26728311 0.26633093]\n",
      "One or more of the train scores are non-finite: [0.26599082 0.28487361 0.32934625 0.26599082 0.26599082 0.26599082\n",
      " 0.10161709 0.09809494 0.26599082 0.26599082 0.2660345  0.10161709\n",
      " 0.09809494 0.26698393 0.2669056         nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28401414 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28487361 0.32934625 0.26599082 0.26599082 0.26599082\n",
      " 0.10161709 0.09809494 0.26599082 0.26599082 0.2660345  0.10161709\n",
      " 0.09809494 0.26698393 0.2669056         nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28401414 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28487361 0.32934625 0.26599082 0.26599082 0.2660345\n",
      " 0.50161709 0.09809494 0.26633303 0.26599082 0.2660345  0.50161709\n",
      " 0.09809494 0.26698393 0.2669056         nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28401414 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28487361 0.32701934 0.26599082 0.26599082 0.2660345\n",
      " 0.40112705 0.29799025 0.26658529 0.2669056  0.2660345  0.40112705\n",
      " 0.29799025 0.26658529 0.2669056         nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28401414 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28367825 0.32860554 0.26664172 0.26704036 0.26599082\n",
      " 0.29233793 0.47220974 0.26668165 0.26687193 0.26599082 0.29233793\n",
      " 0.47220974 0.26668165 0.26687193        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28401414 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28568364 0.31904238 0.26664172 0.26695026 0.26599082\n",
      " 0.28568364 0.31148763 0.26664172 0.26683583 0.26599082 0.28568364\n",
      " 0.31148763 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28401414 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28568364 0.31073678 0.26664172 0.26683583 0.26599082 0.28568364\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28401414 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28401414 0.31073678 0.26664172 0.26683583 0.26599082 0.28568364\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28401414 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28401414 0.31073678 0.26664172 0.26683583 0.26599082 0.28568364\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28401414 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28401414 0.31073678 0.26664172 0.26683583 0.26599082 0.28401414\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28401414 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28401414 0.31073678 0.26664172 0.26683583 0.26599082 0.28401414\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28401414 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28401414 0.31073678 0.26664172 0.26683583 0.26599082 0.28401414\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28401414 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28401414 0.31073678 0.26664172 0.26683583 0.26599082 0.28401414\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28401414 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28401414 0.31073678 0.26664172 0.26683583]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.35954643048845947\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7518703241895262\n",
      "GridSearchCV Runtime: 4.3636980056762695 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.6\n",
      "Ave Test Precision: 0.24380952380952384\n",
      "Stdev Test Precision: 0.2527490576991346\n",
      "Ave Test Accuracy: 0.7462686567164178\n",
      "Stdev Test Accuracy: 0.012653600714785919\n",
      "Ave Test Specificity: 0.9752475247524753\n",
      "Ave Test Recall: 0.045454545454545456\n",
      "Ave Test NPV: 0.7577951556532673\n",
      "Ave Test F1-Score: 0.0747217595239871\n",
      "Ave Test G-mean: 0.1569204316091002\n",
      "Ave Runtime: 0.006520843505859375\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with gasoline. 75 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24641404 0.0238806  0.02469136 0.24411783 0.23983212 0.24641404\n",
      " 0.025      0.02469136 0.24411783 0.23983212 0.24966117 0.025\n",
      " 0.02469136 0.24363217 0.23172845        nan        nan        nan\n",
      "        nan        nan 0.2515293  0.02424242 0.02469136 0.24382265\n",
      " 0.23339189 0.2515293  0.0238806  0.02469136 0.24382265 0.23339189\n",
      " 0.24641404 0.0238806  0.02469136 0.24411783 0.23983212 0.24641404\n",
      " 0.025      0.02469136 0.24411783 0.23983212 0.24966117 0.025\n",
      " 0.02469136 0.24363217 0.23172845        nan        nan        nan\n",
      "        nan        nan 0.2515293  0.02424242 0.02469136 0.24382265\n",
      " 0.23339189 0.2515293  0.0238806  0.02469136 0.24382265 0.23339189\n",
      " 0.24641404 0.0238806  0.02469136 0.24411783 0.23983212 0.24641404\n",
      " 0.025      0.02469136 0.24411783 0.23983212 0.24966117 0.025\n",
      " 0.02469136 0.24363217 0.23172845        nan        nan        nan\n",
      "        nan        nan 0.2515293  0.02424242 0.02469136 0.24382265\n",
      " 0.23339189 0.2515293  0.0238806  0.02469136 0.24382265 0.23339189\n",
      " 0.24678572 0.0238806  0.02469136 0.24411783 0.23983212 0.24879603\n",
      " 0.02236842 0.02469136 0.24506806 0.23172845 0.24966117 0.02236842\n",
      " 0.02469136 0.24363217 0.23172845        nan        nan        nan\n",
      "        nan        nan 0.2515293  0.02424242 0.02469136 0.24382265\n",
      " 0.23339189 0.2515293  0.0238806  0.02469136 0.24382265 0.23339189\n",
      " 0.24779304 0.0238806  0.02469136 0.24506806 0.23316545 0.2515293\n",
      " 0.0238806  0.02469136 0.24363217 0.23172845 0.2515293  0.0238806\n",
      " 0.02469136 0.24363217 0.23172845        nan        nan        nan\n",
      "        nan        nan 0.2515293  0.02424242 0.02469136 0.24382265\n",
      " 0.23339189 0.2515293  0.0238806  0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.0238806  0.02469136 0.24382265 0.23244166 0.2515293\n",
      " 0.0238806  0.02469136 0.24382265 0.23244166 0.2515293  0.0238806\n",
      " 0.02469136 0.24382265 0.23244166        nan        nan        nan\n",
      "        nan        nan 0.2515293  0.02424242 0.02469136 0.24382265\n",
      " 0.23339189 0.2515293  0.0238806  0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.0238806  0.02469136 0.24382265 0.23244166 0.2515293\n",
      " 0.0238806  0.02469136 0.24382265 0.23244166 0.2515293  0.0238806\n",
      " 0.02469136 0.24382265 0.23244166        nan        nan        nan\n",
      "        nan        nan 0.2515293  0.02424242 0.02469136 0.24382265\n",
      " 0.23339189 0.2515293  0.0238806  0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.2515293\n",
      " 0.02424242 0.02469136 0.24382265 0.23339189 0.2515293  0.0238806\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.2515293  0.02424242 0.02469136 0.24382265\n",
      " 0.23339189 0.2515293  0.0238806  0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.2515293\n",
      " 0.02424242 0.02469136 0.24382265 0.23339189 0.2515293  0.0238806\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.2515293  0.02424242 0.02469136 0.24382265\n",
      " 0.23339189 0.2515293  0.0238806  0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.2515293\n",
      " 0.02424242 0.02469136 0.24382265 0.23339189 0.2515293  0.0238806\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.2515293  0.02424242 0.02469136 0.24382265\n",
      " 0.23339189 0.2515293  0.0238806  0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.2515293\n",
      " 0.02424242 0.02469136 0.24382265 0.23339189 0.2515293  0.0238806\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.2515293  0.02424242 0.02469136 0.24382265\n",
      " 0.23339189 0.2515293  0.0238806  0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.2515293\n",
      " 0.02424242 0.02469136 0.24382265 0.23339189 0.2515293  0.0238806\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.2515293  0.02424242 0.02469136 0.24382265\n",
      " 0.23339189 0.2515293  0.0238806  0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.2515293\n",
      " 0.02424242 0.02469136 0.24382265 0.23339189 0.2515293  0.0238806\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.2515293  0.02424242 0.02469136 0.24382265\n",
      " 0.23339189 0.2515293  0.0238806  0.02469136 0.24382265 0.23339189]\n",
      "One or more of the train scores are non-finite: [0.24699719 0.02435312 0.05795086 0.24378802 0.24354892 0.24699719\n",
      " 0.02451524 0.02454924 0.24378802 0.24354892 0.25033043 0.02451524\n",
      " 0.02454924 0.24545387 0.24206778        nan        nan        nan\n",
      "        nan        nan 0.25060776 0.0246875  0.27432807 0.24595344\n",
      " 0.24298467 0.25060776 0.02431193 0.27432807 0.24595344 0.24298467\n",
      " 0.24699719 0.02435312 0.05795086 0.24378802 0.24354892 0.24699719\n",
      " 0.02451524 0.02454924 0.24382763 0.24354892 0.25033043 0.02451524\n",
      " 0.02454924 0.24545387 0.24206778        nan        nan        nan\n",
      "        nan        nan 0.25060776 0.0246875  0.27432807 0.24595344\n",
      " 0.24298467 0.25060776 0.02431193 0.27432807 0.24595344 0.24298467\n",
      " 0.24703681 0.02435312 0.05795086 0.24382763 0.24354892 0.24703681\n",
      " 0.02458333 0.02454924 0.24382763 0.24354892 0.25033043 0.02458333\n",
      " 0.02454924 0.24545387 0.24206778        nan        nan        nan\n",
      "        nan        nan 0.25060776 0.0246875  0.27432807 0.24595344\n",
      " 0.24298467 0.25060776 0.02431193 0.27432807 0.24595344 0.24298467\n",
      " 0.24746876 0.02435312 0.05795086 0.24382763 0.24354892 0.24937752\n",
      " 0.02439716 0.02454924 0.24429012 0.24191092 0.25033043 0.02439716\n",
      " 0.02454924 0.24545387 0.24206778        nan        nan        nan\n",
      "        nan        nan 0.25060776 0.0246875  0.27432807 0.24595344\n",
      " 0.24298467 0.25060776 0.02431193 0.27432807 0.24595344 0.24298467\n",
      " 0.25005309 0.02427481 0.08295086 0.24432987 0.24228171 0.25060776\n",
      " 0.02435312 0.02458333 0.24549286 0.24202742 0.25060776 0.02435312\n",
      " 0.02458333 0.24545387 0.24202742        nan        nan        nan\n",
      "        nan        nan 0.25060776 0.0246875  0.27432807 0.24595344\n",
      " 0.24298467 0.25060776 0.02431193 0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.02431193 0.22475417 0.24595344 0.24240069 0.25060776\n",
      " 0.02431193 0.2450572  0.24595344 0.24248231 0.25060776 0.02431193\n",
      " 0.2450572  0.24595344 0.24248231        nan        nan        nan\n",
      "        nan        nan 0.25060776 0.0246875  0.27432807 0.24595344\n",
      " 0.24298467 0.25060776 0.02431193 0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.02431193 0.26244975 0.24595344 0.2425998  0.25060776\n",
      " 0.02431193 0.26244975 0.24595344 0.2425998  0.25060776 0.02431193\n",
      " 0.26244975 0.24595344 0.2425998         nan        nan        nan\n",
      "        nan        nan 0.25060776 0.0246875  0.27432807 0.24595344\n",
      " 0.24298467 0.25060776 0.02431193 0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25060776\n",
      " 0.0246875  0.27432807 0.24595344 0.24298467 0.25060776 0.02431193\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25060776 0.0246875  0.27432807 0.24595344\n",
      " 0.24298467 0.25060776 0.02431193 0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25060776\n",
      " 0.0246875  0.27432807 0.24595344 0.24298467 0.25060776 0.02431193\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25060776 0.0246875  0.27432807 0.24595344\n",
      " 0.24298467 0.25060776 0.02431193 0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25060776\n",
      " 0.0246875  0.27432807 0.24595344 0.24298467 0.25060776 0.02431193\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25060776 0.0246875  0.27432807 0.24595344\n",
      " 0.24298467 0.25060776 0.02431193 0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25060776\n",
      " 0.0246875  0.27432807 0.24595344 0.24298467 0.25060776 0.02431193\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25060776 0.0246875  0.27432807 0.24595344\n",
      " 0.24298467 0.25060776 0.02431193 0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25060776\n",
      " 0.0246875  0.27432807 0.24595344 0.24298467 0.25060776 0.02431193\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25060776 0.0246875  0.27432807 0.24595344\n",
      " 0.24298467 0.25060776 0.02431193 0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25060776\n",
      " 0.0246875  0.27432807 0.24595344 0.24298467 0.25060776 0.02431193\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25060776 0.0246875  0.27432807 0.24595344\n",
      " 0.24298467 0.25060776 0.02431193 0.27432807 0.24595344 0.24298467]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2515293021042443\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.3117206982543641\n",
      "GridSearchCV Runtime: 4.541171312332153 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2459016393442623\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25327510917030566\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25925925925925924\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.24\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.24663677130044842\n",
      "Ave Test Precision: 0.24901455581485515\n",
      "Stdev Test Precision: 0.007411352579782677\n",
      "Ave Test Accuracy: 0.3276119402985075\n",
      "Stdev Test Accuracy: 0.02708759912370378\n",
      "Ave Test Specificity: 0.15445544554455445\n",
      "Ave Test Recall: 0.8575757575757577\n",
      "Ave Test NPV: 0.7658099781355594\n",
      "Ave Test F1-Score: 0.3858474183465548\n",
      "Ave Test G-mean: 0.3601488659948557\n",
      "Ave Runtime: 0.00634150505065918\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with tuition. 74 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.26451764 0.21738796 0.3987294  0.26451764 0.25340652 0.26451764\n",
      " 0.04844136 0.0725     0.26451764 0.25340652 0.26451764 0.04844136\n",
      " 0.0725     0.26169099 0.25057988        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.22433983 0.31762233 0.26169099\n",
      " 0.25402191 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191\n",
      " 0.26451764 0.21738796 0.3987294  0.26451764 0.25340652 0.26451764\n",
      " 0.04844136 0.0725     0.26451764 0.25340652 0.26451764 0.04844136\n",
      " 0.0725     0.26169099 0.25057988        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.22433983 0.31762233 0.26169099\n",
      " 0.25402191 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191\n",
      " 0.26451764 0.21738796 0.3987294  0.26451764 0.25340652 0.26451764\n",
      " 0.04844136 0.0725     0.26451764 0.25340652 0.26451764 0.04844136\n",
      " 0.0725     0.26169099 0.25057988        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.22433983 0.31762233 0.26169099\n",
      " 0.25402191 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191\n",
      " 0.26451764 0.22433983 0.3987294  0.26451764 0.25340652 0.26451764\n",
      " 0.16319444 0.0725     0.26169099 0.25057988 0.26451764 0.16319444\n",
      " 0.0725     0.26169099 0.25057988        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.22433983 0.31762233 0.26169099\n",
      " 0.25402191 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191\n",
      " 0.26451764 0.22433983 0.40713624 0.26169099 0.25402191 0.26451764\n",
      " 0.25189743 0.21265152 0.26169099 0.25402191 0.26451764 0.25189743\n",
      " 0.21265152 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.22433983 0.31762233 0.26169099\n",
      " 0.25402191 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191\n",
      " 0.26451764 0.22433983 0.35114923 0.26169099 0.25402191 0.26451764\n",
      " 0.22433983 0.34658194 0.26169099 0.25402191 0.26451764 0.22433983\n",
      " 0.34658194 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.22433983 0.31762233 0.26169099\n",
      " 0.25402191 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191\n",
      " 0.26451764 0.22433983 0.34497276 0.26169099 0.25402191 0.26451764\n",
      " 0.22433983 0.34497276 0.26169099 0.25402191 0.26451764 0.22433983\n",
      " 0.34497276 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.22433983 0.31762233 0.26169099\n",
      " 0.25402191 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191\n",
      " 0.26451764 0.22433983 0.33151122 0.26169099 0.25402191 0.26451764\n",
      " 0.22433983 0.3334343  0.26169099 0.25402191 0.26451764 0.22433983\n",
      " 0.3334343  0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.22433983 0.31762233 0.26169099\n",
      " 0.25402191 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191\n",
      " 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191 0.26451764\n",
      " 0.22433983 0.31762233 0.26169099 0.25402191 0.26451764 0.22433983\n",
      " 0.31762233 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.22433983 0.31762233 0.26169099\n",
      " 0.25402191 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191\n",
      " 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191 0.26451764\n",
      " 0.22433983 0.31762233 0.26169099 0.25402191 0.26451764 0.22433983\n",
      " 0.31762233 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.22433983 0.31762233 0.26169099\n",
      " 0.25402191 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191\n",
      " 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191 0.26451764\n",
      " 0.22433983 0.31762233 0.26169099 0.25402191 0.26451764 0.22433983\n",
      " 0.31762233 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.22433983 0.31762233 0.26169099\n",
      " 0.25402191 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191\n",
      " 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191 0.26451764\n",
      " 0.22433983 0.31762233 0.26169099 0.25402191 0.26451764 0.22433983\n",
      " 0.31762233 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.22433983 0.31762233 0.26169099\n",
      " 0.25402191 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191\n",
      " 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191 0.26451764\n",
      " 0.22433983 0.31762233 0.26169099 0.25402191 0.26451764 0.22433983\n",
      " 0.31762233 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.22433983 0.31762233 0.26169099\n",
      " 0.25402191 0.26451764 0.22433983 0.31762233 0.26169099 0.25402191]\n",
      "One or more of the train scores are non-finite: [0.26182795 0.25656243 0.35783277 0.26182795 0.26035723 0.26182795\n",
      " 0.04920298 0.07382271 0.26182795 0.26035723 0.26173823 0.04920298\n",
      " 0.07382271 0.26135432 0.26004765        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.26249229 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25656243 0.35783277 0.26182795 0.26035723 0.26182795\n",
      " 0.04920298 0.07382271 0.26182795 0.26035723 0.26173823 0.04920298\n",
      " 0.07382271 0.26135432 0.26004765        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.26249229 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25656243 0.35783277 0.26182795 0.26035723 0.26182795\n",
      " 0.04920298 0.07382271 0.26182795 0.26035723 0.26173823 0.04920298\n",
      " 0.07382271 0.26135432 0.26004765        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.26249229 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.26249229 0.35773667 0.26182795 0.26035723 0.26173823\n",
      " 0.12886937 0.07382271 0.26135432 0.26004765 0.26173823 0.12886937\n",
      " 0.07382271 0.26135432 0.26004765        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.26249229 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.26249229 0.34956678 0.26166826 0.26009055 0.26182795\n",
      " 0.21045654 0.33155343 0.26145237 0.25978877 0.26182795 0.21045654\n",
      " 0.33155343 0.26145237 0.25978877        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.26249229 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.26249229 0.31846559 0.26145237 0.26000083 0.26182795\n",
      " 0.26095383 0.33241537 0.26145237 0.25990279 0.26182795 0.26095383\n",
      " 0.33241537 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.26249229 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.26249229 0.31177353 0.26145237 0.25990279 0.26182795\n",
      " 0.26249229 0.31177353 0.26145237 0.25990279 0.26182795 0.26249229\n",
      " 0.31177353 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.26249229 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.26249229 0.31036387 0.26145237 0.25990279 0.26182795\n",
      " 0.26249229 0.31088695 0.26145237 0.25990279 0.26182795 0.26249229\n",
      " 0.31088695 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.26249229 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.26249229 0.30514648 0.26145237 0.25990279 0.26182795 0.26249229\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.26249229 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.26249229 0.30514648 0.26145237 0.25990279 0.26182795 0.26249229\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.26249229 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.26249229 0.30514648 0.26145237 0.25990279 0.26182795 0.26249229\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.26249229 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.26249229 0.30514648 0.26145237 0.25990279 0.26182795 0.26249229\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.26249229 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.26249229 0.30514648 0.26145237 0.25990279 0.26182795 0.26249229\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.26249229 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.26249229 0.30514648 0.26145237 0.25990279]\n",
      "invalid value encountered in scalar divide\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.4071362433862434\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7169576059850374\n",
      "GridSearchCV Runtime: 4.296830415725708 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.5\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.5714285714285714\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.3135394456289978\n",
      "Stdev Test Precision: 0.22811397573313558\n",
      "Ave Test Accuracy: 0.6365671641791045\n",
      "Stdev Test Accuracy: 0.22115253301810775\n",
      "Ave Test Specificity: 0.7574257425742574\n",
      "Ave Test Recall: 0.26666666666666666\n",
      "Ave Test NPV: 0.7595910679276581\n",
      "Ave Test F1-Score: 0.17595972438684276\n",
      "Ave Test G-mean: 0.18754804595632607\n",
      "Ave Runtime: 0.005979013442993164\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with allowance. 73 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22085467 0.06113801 0.19930736 0.20449655 0.23293405 0.22085467\n",
      " 0.         0.02469136 0.20449655 0.23293405 0.22085467 0.\n",
      " 0.02469136 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06113801 0.19930736 0.20449655 0.23293405 0.22085467\n",
      " 0.         0.02469136 0.20449655 0.23293405 0.22085467 0.\n",
      " 0.02469136 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06113801 0.19930736 0.20449655 0.23293405 0.22085467\n",
      " 0.         0.02469136 0.20449655 0.23293405 0.22085467 0.\n",
      " 0.02469136 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06113801 0.19930736 0.20449655 0.23293405 0.22085467\n",
      " 0.         0.02469136 0.20449655 0.23293405 0.22085467 0.\n",
      " 0.02469136 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06113801 0.20930736 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.02469136 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.02469136 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.18264069 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.16597403 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.16597403 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.18264069 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.18264069 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405]\n",
      "One or more of the train scores are non-finite: [0.24999426 0.07955048 0.2219548  0.24644833 0.25376518 0.24999426\n",
      " 0.         0.02454924 0.24644833 0.25376518 0.24999426 0.\n",
      " 0.02454924 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.07980699 0.23509473 0.24644833\n",
      " 0.25376518 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518\n",
      " 0.24999426 0.07955048 0.2219548  0.24644833 0.25376518 0.24999426\n",
      " 0.         0.02454924 0.24644833 0.25376518 0.24999426 0.\n",
      " 0.02454924 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.07980699 0.23509473 0.24644833\n",
      " 0.25376518 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518\n",
      " 0.24999426 0.07955048 0.2219548  0.24644833 0.25376518 0.24999426\n",
      " 0.         0.02454924 0.24644833 0.25376518 0.24999426 0.\n",
      " 0.02454924 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.07980699 0.23509473 0.24644833\n",
      " 0.25376518 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518\n",
      " 0.24999426 0.07955048 0.2219548  0.24644833 0.25376518 0.24999426\n",
      " 0.05       0.02454924 0.24644833 0.25376518 0.24999426 0.05\n",
      " 0.02454924 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.07980699 0.23509473 0.24644833\n",
      " 0.25376518 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518\n",
      " 0.24999426 0.07955048 0.22576433 0.24644833 0.25376518 0.24999426\n",
      " 0.08213773 0.10788006 0.24644833 0.25376518 0.24999426 0.08213773\n",
      " 0.10788006 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.07980699 0.23509473 0.24644833\n",
      " 0.25376518 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518\n",
      " 0.24999426 0.07980699 0.22932177 0.24644833 0.25376518 0.24999426\n",
      " 0.07980699 0.22500536 0.24644833 0.25376518 0.24999426 0.07980699\n",
      " 0.22500536 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.07980699 0.23509473 0.24644833\n",
      " 0.25376518 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518\n",
      " 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518 0.24999426\n",
      " 0.07980699 0.22932177 0.24644833 0.25376518 0.24999426 0.07980699\n",
      " 0.22932177 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.07980699 0.23509473 0.24644833\n",
      " 0.25376518 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518\n",
      " 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518 0.24999426\n",
      " 0.07980699 0.23509473 0.24644833 0.25376518 0.24999426 0.07980699\n",
      " 0.23509473 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.07980699 0.23509473 0.24644833\n",
      " 0.25376518 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518\n",
      " 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518 0.24999426\n",
      " 0.07980699 0.23509473 0.24644833 0.25376518 0.24999426 0.07980699\n",
      " 0.23509473 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.07980699 0.23509473 0.24644833\n",
      " 0.25376518 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518\n",
      " 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518 0.24999426\n",
      " 0.07980699 0.23509473 0.24644833 0.25376518 0.24999426 0.07980699\n",
      " 0.23509473 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.07980699 0.23509473 0.24644833\n",
      " 0.25376518 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518\n",
      " 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518 0.24999426\n",
      " 0.07980699 0.23509473 0.24644833 0.25376518 0.24999426 0.07980699\n",
      " 0.23509473 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.07980699 0.23509473 0.24644833\n",
      " 0.25376518 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518\n",
      " 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518 0.24999426\n",
      " 0.07980699 0.23509473 0.24644833 0.25376518 0.24999426 0.07980699\n",
      " 0.23509473 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.07980699 0.23509473 0.24644833\n",
      " 0.25376518 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518\n",
      " 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518 0.24999426\n",
      " 0.07980699 0.23509473 0.24644833 0.25376518 0.24999426 0.07980699\n",
      " 0.23509473 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.07980699 0.23509473 0.24644833\n",
      " 0.25376518 0.24999426 0.07980699 0.23509473 0.24644833 0.25376518]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2329340536512668\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6172069825436409\n",
      "GridSearchCV Runtime: 4.259084939956665 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.17647058823529413\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2608695652173913\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25654450261780104\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2644230769230769\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.19672131147540983\n",
      "Ave Test Precision: 0.23100580889379466\n",
      "Stdev Test Precision: 0.04126226992340348\n",
      "Ave Test Accuracy: 0.47686567164179106\n",
      "Stdev Test Accuracy: 0.11547286724476011\n",
      "Ave Test Specificity: 0.45247524752475243\n",
      "Ave Test Recall: 0.5515151515151515\n",
      "Ave Test NPV: 0.7736593138389288\n",
      "Ave Test F1-Score: 0.30929361247642684\n",
      "Ave Test G-mean: 0.4196620982673546\n",
      "Ave Runtime: 0.006076240539550781\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with uniform. 72 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.18397856 0.06407379 0.23333333 0.20784816 0.20905861 0.18397856\n",
      " 0.07125    0.         0.20784816 0.20905861 0.18397856 0.07125\n",
      " 0.         0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18397856 0.06407379 0.35       0.20784816\n",
      " 0.20982682 0.18397856 0.06407379 0.35       0.20784816 0.20982682\n",
      " 0.18397856 0.06407379 0.23333333 0.20784816 0.20905861 0.18397856\n",
      " 0.07125    0.         0.20784816 0.20905861 0.18397856 0.07125\n",
      " 0.         0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18397856 0.06407379 0.35       0.20784816\n",
      " 0.20982682 0.18397856 0.06407379 0.35       0.20784816 0.20982682\n",
      " 0.18397856 0.06407379 0.23333333 0.20784816 0.20905861 0.18397856\n",
      " 0.07125    0.         0.20784816 0.20905861 0.18397856 0.07125\n",
      " 0.         0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18397856 0.06407379 0.35       0.20784816\n",
      " 0.20982682 0.18397856 0.06407379 0.35       0.20784816 0.20982682\n",
      " 0.18397856 0.06407379 0.23333333 0.20784816 0.20905861 0.18397856\n",
      " 0.17125    0.         0.20784816 0.20982682 0.18397856 0.17125\n",
      " 0.         0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18397856 0.06407379 0.35       0.20784816\n",
      " 0.20982682 0.18397856 0.06407379 0.35       0.20784816 0.20982682\n",
      " 0.18397856 0.06407379 0.33333333 0.20784816 0.20905861 0.18397856\n",
      " 0.07203431 0.1        0.20784816 0.20982682 0.18397856 0.07203431\n",
      " 0.1        0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18397856 0.06407379 0.35       0.20784816\n",
      " 0.20982682 0.18397856 0.06407379 0.35       0.20784816 0.20982682\n",
      " 0.18397856 0.06407379 0.43333333 0.20784816 0.20982682 0.18397856\n",
      " 0.06407379 0.33333333 0.20784816 0.20982682 0.18397856 0.06407379\n",
      " 0.33333333 0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18397856 0.06407379 0.35       0.20784816\n",
      " 0.20982682 0.18397856 0.06407379 0.35       0.20784816 0.20982682\n",
      " 0.18397856 0.06407379 0.35       0.20784816 0.20982682 0.18397856\n",
      " 0.06407379 0.35       0.20784816 0.20982682 0.18397856 0.06407379\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18397856 0.06407379 0.35       0.20784816\n",
      " 0.20982682 0.18397856 0.06407379 0.35       0.20784816 0.20982682\n",
      " 0.18397856 0.06407379 0.35       0.20784816 0.20982682 0.18397856\n",
      " 0.06407379 0.35       0.20784816 0.20982682 0.18397856 0.06407379\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18397856 0.06407379 0.35       0.20784816\n",
      " 0.20982682 0.18397856 0.06407379 0.35       0.20784816 0.20982682\n",
      " 0.18397856 0.06407379 0.35       0.20784816 0.20982682 0.18397856\n",
      " 0.06407379 0.35       0.20784816 0.20982682 0.18397856 0.06407379\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18397856 0.06407379 0.35       0.20784816\n",
      " 0.20982682 0.18397856 0.06407379 0.35       0.20784816 0.20982682\n",
      " 0.18397856 0.06407379 0.35       0.20784816 0.20982682 0.18397856\n",
      " 0.06407379 0.35       0.20784816 0.20982682 0.18397856 0.06407379\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18397856 0.06407379 0.35       0.20784816\n",
      " 0.20982682 0.18397856 0.06407379 0.35       0.20784816 0.20982682\n",
      " 0.18397856 0.06407379 0.35       0.20784816 0.20982682 0.18397856\n",
      " 0.06407379 0.35       0.20784816 0.20982682 0.18397856 0.06407379\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18397856 0.06407379 0.35       0.20784816\n",
      " 0.20982682 0.18397856 0.06407379 0.35       0.20784816 0.20982682\n",
      " 0.18397856 0.06407379 0.35       0.20784816 0.20982682 0.18397856\n",
      " 0.06407379 0.35       0.20784816 0.20982682 0.18397856 0.06407379\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18397856 0.06407379 0.35       0.20784816\n",
      " 0.20982682 0.18397856 0.06407379 0.35       0.20784816 0.20982682\n",
      " 0.18397856 0.06407379 0.35       0.20784816 0.20982682 0.18397856\n",
      " 0.06407379 0.35       0.20784816 0.20982682 0.18397856 0.06407379\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18397856 0.06407379 0.35       0.20784816\n",
      " 0.20982682 0.18397856 0.06407379 0.35       0.20784816 0.20982682]\n",
      "One or more of the train scores are non-finite: [0.23232448 0.12874357 0.48747666 0.20829643 0.23995611 0.23232448\n",
      " 0.07396122 0.         0.20829643 0.23995611 0.23290597 0.07396122\n",
      " 0.         0.20791805 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23290597 0.13134097 0.41932896 0.20829643\n",
      " 0.2400415  0.23290597 0.13134097 0.41932896 0.20829643 0.2400415\n",
      " 0.23232448 0.12874357 0.48747666 0.20829643 0.23995611 0.23232448\n",
      " 0.07396122 0.         0.20829643 0.23995611 0.23290597 0.07396122\n",
      " 0.         0.20791805 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23290597 0.13134097 0.41932896 0.20829643\n",
      " 0.2400415  0.23290597 0.13134097 0.41932896 0.20829643 0.2400415\n",
      " 0.23232448 0.12874357 0.48747666 0.20829643 0.23995611 0.23232448\n",
      " 0.07396122 0.         0.20829643 0.23995611 0.23290597 0.07396122\n",
      " 0.         0.20791805 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23290597 0.13134097 0.41932896 0.20829643\n",
      " 0.2400415  0.23290597 0.13134097 0.41932896 0.20829643 0.2400415\n",
      " 0.23290597 0.13134097 0.48747666 0.20829643 0.23995611 0.23290597\n",
      " 0.10582008 0.         0.20791805 0.2400415  0.23290597 0.10582008\n",
      " 0.         0.20791805 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23290597 0.13134097 0.41932896 0.20829643\n",
      " 0.2400415  0.23290597 0.13134097 0.41932896 0.20829643 0.2400415\n",
      " 0.23290597 0.13134097 0.44480159 0.20829643 0.23995611 0.23290597\n",
      " 0.13422773 0.23380952 0.20791805 0.2400415  0.23290597 0.13422773\n",
      " 0.23380952 0.20791805 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23290597 0.13134097 0.41932896 0.20829643\n",
      " 0.2400415  0.23290597 0.13134097 0.41932896 0.20829643 0.2400415\n",
      " 0.23290597 0.13134097 0.42309343 0.20829643 0.2400415  0.23290597\n",
      " 0.13124738 0.46358947 0.20829643 0.2400415  0.23290597 0.13124738\n",
      " 0.46358947 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23290597 0.13134097 0.41932896 0.20829643\n",
      " 0.2400415  0.23290597 0.13134097 0.41932896 0.20829643 0.2400415\n",
      " 0.23290597 0.13134097 0.41834784 0.20829643 0.2400415  0.23290597\n",
      " 0.13134097 0.41834784 0.20829643 0.2400415  0.23290597 0.13134097\n",
      " 0.41834784 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23290597 0.13134097 0.41932896 0.20829643\n",
      " 0.2400415  0.23290597 0.13134097 0.41932896 0.20829643 0.2400415\n",
      " 0.23290597 0.13134097 0.41932896 0.20829643 0.2400415  0.23290597\n",
      " 0.13134097 0.41932896 0.20829643 0.2400415  0.23290597 0.13134097\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23290597 0.13134097 0.41932896 0.20829643\n",
      " 0.2400415  0.23290597 0.13134097 0.41932896 0.20829643 0.2400415\n",
      " 0.23290597 0.13134097 0.41932896 0.20829643 0.2400415  0.23290597\n",
      " 0.13134097 0.41932896 0.20829643 0.2400415  0.23290597 0.13134097\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23290597 0.13134097 0.41932896 0.20829643\n",
      " 0.2400415  0.23290597 0.13134097 0.41932896 0.20829643 0.2400415\n",
      " 0.23290597 0.13134097 0.41932896 0.20829643 0.2400415  0.23290597\n",
      " 0.13134097 0.41932896 0.20829643 0.2400415  0.23290597 0.13134097\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23290597 0.13134097 0.41932896 0.20829643\n",
      " 0.2400415  0.23290597 0.13134097 0.41932896 0.20829643 0.2400415\n",
      " 0.23290597 0.13134097 0.41932896 0.20829643 0.2400415  0.23290597\n",
      " 0.13134097 0.41932896 0.20829643 0.2400415  0.23290597 0.13134097\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23290597 0.13134097 0.41932896 0.20829643\n",
      " 0.2400415  0.23290597 0.13134097 0.41932896 0.20829643 0.2400415\n",
      " 0.23290597 0.13134097 0.41932896 0.20829643 0.2400415  0.23290597\n",
      " 0.13134097 0.41932896 0.20829643 0.2400415  0.23290597 0.13134097\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23290597 0.13134097 0.41932896 0.20829643\n",
      " 0.2400415  0.23290597 0.13134097 0.41932896 0.20829643 0.2400415\n",
      " 0.23290597 0.13134097 0.41932896 0.20829643 0.2400415  0.23290597\n",
      " 0.13134097 0.41932896 0.20829643 0.2400415  0.23290597 0.13134097\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23290597 0.13134097 0.41932896 0.20829643\n",
      " 0.2400415  0.23290597 0.13134097 0.41932896 0.20829643 0.2400415 ]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.4333333333333333\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7082294264339152\n",
      "GridSearchCV Runtime: 4.34788179397583 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2702702702702703\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.38461538461538464\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.34285714285714286\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.0\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23868312757201646\n",
      "Ave Test Precision: 0.24728518506296285\n",
      "Stdev Test Precision: 0.1497880628691828\n",
      "Ave Test Accuracy: 0.6343283582089552\n",
      "Stdev Test Accuracy: 0.1994786030540845\n",
      "Ave Test Specificity: 0.7574257425742574\n",
      "Ave Test Recall: 0.25757575757575757\n",
      "Ave Test NPV: 0.7436960227794015\n",
      "Ave Test F1-Score: 0.18675706577662807\n",
      "Ave Test G-mean: 0.2610763812385221\n",
      "Ave Runtime: 0.006938934326171875\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with otherEducation. 71 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22800898 0.2151203  0.16995411 0.22621429 0.25942117 0.22800898\n",
      " 0.1475     0.07313272 0.22621429 0.25942117 0.22670954 0.1475\n",
      " 0.07313272 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.24112736 0.21407222 0.22621429\n",
      " 0.25942117 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117\n",
      " 0.22800898 0.2151203  0.16995411 0.22621429 0.25942117 0.22800898\n",
      " 0.1475     0.07313272 0.22621429 0.25942117 0.22670954 0.1475\n",
      " 0.07313272 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.24112736 0.21407222 0.22621429\n",
      " 0.25942117 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117\n",
      " 0.22800898 0.21356007 0.16995411 0.22621429 0.25942117 0.22800898\n",
      " 0.1475     0.07313272 0.22621429 0.25942117 0.22670954 0.1475\n",
      " 0.07313272 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.24112736 0.21407222 0.22621429\n",
      " 0.25942117 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117\n",
      " 0.22800898 0.21356007 0.16995411 0.22621429 0.25942117 0.22800898\n",
      " 0.1475     0.07313272 0.22621429 0.25942117 0.22670954 0.1475\n",
      " 0.07313272 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.24112736 0.21407222 0.22621429\n",
      " 0.25942117 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117\n",
      " 0.22670954 0.21742481 0.16614459 0.22621429 0.25942117 0.22670954\n",
      " 0.1871254  0.20844136 0.22621429 0.25942117 0.22670954 0.1871254\n",
      " 0.20844136 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.24112736 0.21407222 0.22621429\n",
      " 0.25942117 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117\n",
      " 0.22670954 0.24112736 0.21452676 0.22621429 0.25942117 0.22670954\n",
      " 0.21742481 0.21407222 0.22621429 0.25942117 0.22670954 0.21742481\n",
      " 0.21407222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.24112736 0.21407222 0.22621429\n",
      " 0.25942117 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117\n",
      " 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954\n",
      " 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954 0.24112736\n",
      " 0.21407222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.24112736 0.21407222 0.22621429\n",
      " 0.25942117 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117\n",
      " 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954\n",
      " 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954 0.24112736\n",
      " 0.21407222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.24112736 0.21407222 0.22621429\n",
      " 0.25942117 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117\n",
      " 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954\n",
      " 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954 0.24112736\n",
      " 0.21407222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.24112736 0.21407222 0.22621429\n",
      " 0.25942117 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117\n",
      " 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954\n",
      " 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954 0.24112736\n",
      " 0.21407222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.24112736 0.21407222 0.22621429\n",
      " 0.25942117 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117\n",
      " 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954\n",
      " 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954 0.24112736\n",
      " 0.21407222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.24112736 0.21407222 0.22621429\n",
      " 0.25942117 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117\n",
      " 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954\n",
      " 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954 0.24112736\n",
      " 0.21407222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.24112736 0.21407222 0.22621429\n",
      " 0.25942117 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117\n",
      " 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954\n",
      " 0.24112736 0.21407222 0.22621429 0.25942117 0.22670954 0.24112736\n",
      " 0.21407222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.24112736 0.21407222 0.22621429\n",
      " 0.25942117 0.22670954 0.24112736 0.21407222 0.22621429 0.25942117]\n",
      "One or more of the train scores are non-finite: [0.25228894 0.22534811 0.19792369 0.25245812 0.25089948 0.25228894\n",
      " 0.14736842 0.07375221 0.25245812 0.25089948 0.25225756 0.14736842\n",
      " 0.07375221 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.25174653 0.2158163  0.25210893\n",
      " 0.25138751 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138\n",
      " 0.25228894 0.22534811 0.19792369 0.25245812 0.25089948 0.25228894\n",
      " 0.14736842 0.07375221 0.25245812 0.25089948 0.25225756 0.14736842\n",
      " 0.07375221 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.25174653 0.2158163  0.25210893\n",
      " 0.25138751 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138\n",
      " 0.25228894 0.22558934 0.19792369 0.25245812 0.25089948 0.25228894\n",
      " 0.14736842 0.07375221 0.25245812 0.25089948 0.25225756 0.14736842\n",
      " 0.07375221 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.25174653 0.2158163  0.25210893\n",
      " 0.25138751 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138\n",
      " 0.25228894 0.22558934 0.19792369 0.25245812 0.25089948 0.25228894\n",
      " 0.17804778 0.07375221 0.25206264 0.25113231 0.25225756 0.17804778\n",
      " 0.07375221 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.25174653 0.2158163  0.25210893\n",
      " 0.25138751 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138\n",
      " 0.2525069  0.22595176 0.20785413 0.25221056 0.25113231 0.25225756\n",
      " 0.22230117 0.14970603 0.25210893 0.25148774 0.25225756 0.22230117\n",
      " 0.14970603 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.25174653 0.2158163  0.25210893\n",
      " 0.25138751 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138\n",
      " 0.25225756 0.25174653 0.21590301 0.25210893 0.25134138 0.25225756\n",
      " 0.22595176 0.21477595 0.25210893 0.25134138 0.25225756 0.22595176\n",
      " 0.21477595 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.25174653 0.2158163  0.25210893\n",
      " 0.25138751 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138\n",
      " 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138 0.25225756\n",
      " 0.25174653 0.2158163  0.25210893 0.25138751 0.25225756 0.25174653\n",
      " 0.2158163  0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.25174653 0.2158163  0.25210893\n",
      " 0.25138751 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138\n",
      " 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138 0.25225756\n",
      " 0.25174653 0.2158163  0.25210893 0.25138751 0.25225756 0.25174653\n",
      " 0.2158163  0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.25174653 0.2158163  0.25210893\n",
      " 0.25138751 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138\n",
      " 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138 0.25225756\n",
      " 0.25174653 0.2158163  0.25210893 0.25138751 0.25225756 0.25174653\n",
      " 0.2158163  0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.25174653 0.2158163  0.25210893\n",
      " 0.25138751 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138\n",
      " 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138 0.25225756\n",
      " 0.25174653 0.2158163  0.25210893 0.25138751 0.25225756 0.25174653\n",
      " 0.2158163  0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.25174653 0.2158163  0.25210893\n",
      " 0.25138751 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138\n",
      " 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138 0.25225756\n",
      " 0.25174653 0.2158163  0.25210893 0.25138751 0.25225756 0.25174653\n",
      " 0.2158163  0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.25174653 0.2158163  0.25210893\n",
      " 0.25138751 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138\n",
      " 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138 0.25225756\n",
      " 0.25174653 0.2158163  0.25210893 0.25138751 0.25225756 0.25174653\n",
      " 0.2158163  0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.25174653 0.2158163  0.25210893\n",
      " 0.25138751 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138\n",
      " 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138 0.25225756\n",
      " 0.25174653 0.2158163  0.25210893 0.25138751 0.25225756 0.25174653\n",
      " 0.2158163  0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.25174653 0.2158163  0.25210893\n",
      " 0.25138751 0.25225756 0.25174653 0.2158163  0.25210893 0.25134138]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.25942117019211575\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6172069825436409\n",
      "GridSearchCV Runtime: 4.298141717910767 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.3013698630136986\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2876712328767123\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2153846153846154\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2988505747126437\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3283582089552239\n",
      "Ave Test Precision: 0.28632689898857877\n",
      "Stdev Test Precision: 0.042380358287662984\n",
      "Ave Test Accuracy: 0.6380597014925373\n",
      "Stdev Test Accuracy: 0.020437408862133084\n",
      "Ave Test Specificity: 0.7425742574257427\n",
      "Ave Test Recall: 0.3181818181818182\n",
      "Ave Test NPV: 0.7695064320693652\n",
      "Ave Test F1-Score: 0.30062836854028296\n",
      "Ave Test G-mean: 0.48336706885810016\n",
      "Ave Runtime: 0.005597162246704102\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with emergency. 70 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24551362 0.25842314 0.18185728 0.24551362 0.24551362 0.24551362\n",
      " 0.09813272 0.07344136 0.24551362 0.24551362 0.24823343 0.09813272\n",
      " 0.07344136 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25842314 0.16724965 0.24823343\n",
      " 0.24641941 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941\n",
      " 0.24551362 0.25842314 0.18185728 0.24551362 0.24551362 0.24551362\n",
      " 0.09813272 0.07344136 0.24551362 0.24551362 0.24823343 0.09813272\n",
      " 0.07344136 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25842314 0.16724965 0.24823343\n",
      " 0.24641941 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941\n",
      " 0.24551362 0.25842314 0.18185728 0.24551362 0.24551362 0.24551362\n",
      " 0.19813272 0.07344136 0.24551362 0.24551362 0.24823343 0.19813272\n",
      " 0.07344136 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25842314 0.16724965 0.24823343\n",
      " 0.24641941 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941\n",
      " 0.24641941 0.25842314 0.17828585 0.24641941 0.24551362 0.24823343\n",
      " 0.23146605 0.07344136 0.24752779 0.24641941 0.24823343 0.23146605\n",
      " 0.07344136 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25842314 0.16724965 0.24823343\n",
      " 0.24641941 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941\n",
      " 0.24823343 0.25842314 0.17542116 0.24752779 0.24641941 0.24823343\n",
      " 0.29216638 0.15610644 0.24823343 0.24641941 0.24823343 0.29216638\n",
      " 0.15610644 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25842314 0.16724965 0.24823343\n",
      " 0.24641941 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941\n",
      " 0.24823343 0.25842314 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25842314 0.17057481 0.24823343 0.24641941 0.24823343 0.25842314\n",
      " 0.17057481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25842314 0.16724965 0.24823343\n",
      " 0.24641941 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941\n",
      " 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343\n",
      " 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343 0.25842314\n",
      " 0.16724965 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25842314 0.16724965 0.24823343\n",
      " 0.24641941 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941\n",
      " 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343\n",
      " 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343 0.25842314\n",
      " 0.16724965 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25842314 0.16724965 0.24823343\n",
      " 0.24641941 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941\n",
      " 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343\n",
      " 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343 0.25842314\n",
      " 0.16724965 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25842314 0.16724965 0.24823343\n",
      " 0.24641941 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941\n",
      " 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343\n",
      " 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343 0.25842314\n",
      " 0.16724965 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25842314 0.16724965 0.24823343\n",
      " 0.24641941 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941\n",
      " 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343\n",
      " 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343 0.25842314\n",
      " 0.16724965 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25842314 0.16724965 0.24823343\n",
      " 0.24641941 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941\n",
      " 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343\n",
      " 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343 0.25842314\n",
      " 0.16724965 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25842314 0.16724965 0.24823343\n",
      " 0.24641941 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941\n",
      " 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343\n",
      " 0.25842314 0.16724965 0.24823343 0.24641941 0.24823343 0.25842314\n",
      " 0.16724965 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25842314 0.16724965 0.24823343\n",
      " 0.24641941 0.24823343 0.25842314 0.16724965 0.24823343 0.24641941]\n",
      "One or more of the train scores are non-finite: [0.24638637 0.24124237 0.22794725 0.24638637 0.24638637 0.24638637\n",
      " 0.09826745 0.07371821 0.24638637 0.24638637 0.24768942 0.09826745\n",
      " 0.07371821 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.24151191 0.22706501 0.24768093\n",
      " 0.24659988 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988\n",
      " 0.24638637 0.24124237 0.22794725 0.24638637 0.24638637 0.24671087\n",
      " 0.09826745 0.07371821 0.24638637 0.24638637 0.24768942 0.09826745\n",
      " 0.07371821 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.24151191 0.22706501 0.24768093\n",
      " 0.24659988 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988\n",
      " 0.24638637 0.24124237 0.22794725 0.24638637 0.24638637 0.24671087\n",
      " 0.13739788 0.07371821 0.24638637 0.24638637 0.24768942 0.13739788\n",
      " 0.07371821 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.24151191 0.22706501 0.24768093\n",
      " 0.24659988 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988\n",
      " 0.24693017 0.24124237 0.22652057 0.24660567 0.24638637 0.24768942\n",
      " 0.23946309 0.07375221 0.24745093 0.24660567 0.24768942 0.23946309\n",
      " 0.07375221 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.24151191 0.22706501 0.24768093\n",
      " 0.24659988 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988\n",
      " 0.24737061 0.24151191 0.22581206 0.24745365 0.24692449 0.24768942\n",
      " 0.24494239 0.18453218 0.24768093 0.24692449 0.24768942 0.24494239\n",
      " 0.18453218 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.24151191 0.22706501 0.24768093\n",
      " 0.24659988 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988\n",
      " 0.24768942 0.24151191 0.22576041 0.24768093 0.24692449 0.24768942\n",
      " 0.24181267 0.22586501 0.24768093 0.24659988 0.24768942 0.24181267\n",
      " 0.22586501 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.24151191 0.22706501 0.24768093\n",
      " 0.24659988 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988\n",
      " 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942\n",
      " 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942 0.24151191\n",
      " 0.22706501 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.24151191 0.22706501 0.24768093\n",
      " 0.24659988 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988\n",
      " 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942\n",
      " 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942 0.24151191\n",
      " 0.22706501 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.24151191 0.22706501 0.24768093\n",
      " 0.24659988 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988\n",
      " 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942\n",
      " 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942 0.24151191\n",
      " 0.22706501 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.24151191 0.22706501 0.24768093\n",
      " 0.24659988 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988\n",
      " 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942\n",
      " 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942 0.24151191\n",
      " 0.22706501 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.24151191 0.22706501 0.24768093\n",
      " 0.24659988 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988\n",
      " 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942\n",
      " 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942 0.24151191\n",
      " 0.22706501 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.24151191 0.22706501 0.24768093\n",
      " 0.24659988 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988\n",
      " 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942\n",
      " 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942 0.24151191\n",
      " 0.22706501 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.24151191 0.22706501 0.24768093\n",
      " 0.24659988 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988\n",
      " 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942\n",
      " 0.24151191 0.22706501 0.24768093 0.24659988 0.24768942 0.24151191\n",
      " 0.22706501 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.24151191 0.22706501 0.24768093\n",
      " 0.24659988 0.24768942 0.24151191 0.22706501 0.24768093 0.24659988]\n",
      "invalid value encountered in scalar divide\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2921663829997163\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.6895261845386533\n",
      "GridSearchCV Runtime: 4.755168676376343 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2727272727272727\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2727272727272727\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2265193370165746\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.20364850783750757\n",
      "Stdev Test Precision: 0.11549460829366974\n",
      "Ave Test Accuracy: 0.5447761194029851\n",
      "Stdev Test Accuracy: 0.2202915047627054\n",
      "Ave Test Specificity: 0.5900990099009901\n",
      "Ave Test Recall: 0.40606060606060607\n",
      "Ave Test NPV: 0.7467176220448184\n",
      "Ave Test F1-Score: 0.2314526633152539\n",
      "Ave Test G-mean: 0.2411071361815237\n",
      "Ave Runtime: 0.008329010009765625\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with medicine. 69 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27731677 0.27690529 0.2955908  0.27731677 0.27731677 0.27731677\n",
      " 0.19719136 0.09844136 0.27731677 0.27731677 0.28411582 0.19719136\n",
      " 0.09844136 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27677604 0.26884325 0.28150191\n",
      " 0.27819121 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121\n",
      " 0.27731677 0.27690529 0.2955908  0.27731677 0.27731677 0.27731677\n",
      " 0.19719136 0.09844136 0.27731677 0.27731677 0.28411582 0.19719136\n",
      " 0.09844136 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27677604 0.26884325 0.28150191\n",
      " 0.27819121 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121\n",
      " 0.27731677 0.27690529 0.2955908  0.27731677 0.27731677 0.28062746\n",
      " 0.19680674 0.09844136 0.28178105 0.27731677 0.28411582 0.19680674\n",
      " 0.09844136 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27677604 0.26884325 0.28150191\n",
      " 0.27819121 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121\n",
      " 0.28062746 0.27690529 0.2955908  0.28273343 0.27941761 0.28411582\n",
      " 0.22283599 0.09651828 0.28150191 0.28150191 0.28411582 0.22283599\n",
      " 0.09651828 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27677604 0.26884325 0.28150191\n",
      " 0.27819121 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121\n",
      " 0.28065255 0.27610037 0.28096446 0.28150191 0.28150191 0.28180101\n",
      " 0.27020424 0.30604416 0.28150191 0.28150191 0.28180101 0.27020424\n",
      " 0.30420291 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27677604 0.26884325 0.28150191\n",
      " 0.27819121 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121\n",
      " 0.28065255 0.27677604 0.2701253  0.28150191 0.27819121 0.28065255\n",
      " 0.27610037 0.27869143 0.28150191 0.27819121 0.28065255 0.27610037\n",
      " 0.27869143 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27677604 0.26884325 0.28150191\n",
      " 0.27819121 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121\n",
      " 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255\n",
      " 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255 0.27677604\n",
      " 0.26884325 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27677604 0.26884325 0.28150191\n",
      " 0.27819121 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121\n",
      " 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255\n",
      " 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255 0.27677604\n",
      " 0.26884325 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27677604 0.26884325 0.28150191\n",
      " 0.27819121 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121\n",
      " 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255\n",
      " 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255 0.27677604\n",
      " 0.26884325 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27677604 0.26884325 0.28150191\n",
      " 0.27819121 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121\n",
      " 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255\n",
      " 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255 0.27677604\n",
      " 0.26884325 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27677604 0.26884325 0.28150191\n",
      " 0.27819121 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121\n",
      " 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255\n",
      " 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255 0.27677604\n",
      " 0.26884325 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27677604 0.26884325 0.28150191\n",
      " 0.27819121 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121\n",
      " 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255\n",
      " 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255 0.27677604\n",
      " 0.26884325 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27677604 0.26884325 0.28150191\n",
      " 0.27819121 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121\n",
      " 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255\n",
      " 0.27677604 0.26884325 0.28150191 0.27819121 0.28065255 0.27677604\n",
      " 0.26884325 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27677604 0.26884325 0.28150191\n",
      " 0.27819121 0.28065255 0.27677604 0.26884325 0.28150191 0.27819121]\n",
      "One or more of the train scores are non-finite: [0.27835387 0.2606658  0.37398884 0.27835387 0.27835387 0.27835387\n",
      " 0.19643289 0.09823345 0.27835387 0.27835387 0.28170213 0.19643289\n",
      " 0.09823345 0.28433758 0.28422363        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26072185 0.2990894  0.28377159\n",
      " 0.28354931 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931\n",
      " 0.27835387 0.2606658  0.37398884 0.27835387 0.27835387 0.27835387\n",
      " 0.19643289 0.09823345 0.27835387 0.27835387 0.28170213 0.19643289\n",
      " 0.09823345 0.28433758 0.28422363        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26072185 0.2990894  0.28377159\n",
      " 0.28354931 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931\n",
      " 0.27835387 0.2606658  0.37398884 0.27835387 0.27835387 0.27880549\n",
      " 0.22911995 0.09823345 0.28264191 0.27835387 0.28170213 0.22911995\n",
      " 0.09823345 0.28433758 0.28422363        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26072185 0.2990894  0.28377159\n",
      " 0.28354931 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931\n",
      " 0.27880549 0.2606658  0.37398884 0.28103984 0.27894232 0.28170213\n",
      " 0.33244968 0.54801597 0.28433758 0.28422363 0.28170213 0.33244968\n",
      " 0.54801597 0.28433758 0.28422363        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26072185 0.2990894  0.28377159\n",
      " 0.28354931 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931\n",
      " 0.27959101 0.26055682 0.37198209 0.28377159 0.28366063 0.28110459\n",
      " 0.26215226 0.3618235  0.28388674 0.28411268 0.28110459 0.26215226\n",
      " 0.36122955 0.28388674 0.28411268        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26072185 0.2990894  0.28377159\n",
      " 0.28354931 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931\n",
      " 0.27959101 0.26072185 0.31642337 0.28377159 0.28343416 0.27959101\n",
      " 0.26053217 0.32175155 0.28377159 0.28366105 0.27959101 0.26053217\n",
      " 0.32175155 0.28377159 0.28366105        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26072185 0.2990894  0.28377159\n",
      " 0.28354931 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931\n",
      " 0.27959101 0.26072185 0.29970946 0.28377159 0.28354931 0.27959101\n",
      " 0.26072185 0.29970946 0.28377159 0.28354931 0.27959101 0.26072185\n",
      " 0.29970946 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26072185 0.2990894  0.28377159\n",
      " 0.28354931 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931\n",
      " 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931 0.27959101\n",
      " 0.26072185 0.2990894  0.28377159 0.28354931 0.27959101 0.26072185\n",
      " 0.2990894  0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26072185 0.2990894  0.28377159\n",
      " 0.28354931 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931\n",
      " 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931 0.27959101\n",
      " 0.26072185 0.2990894  0.28377159 0.28354931 0.27959101 0.26072185\n",
      " 0.2990894  0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26072185 0.2990894  0.28377159\n",
      " 0.28354931 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931\n",
      " 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931 0.27959101\n",
      " 0.26072185 0.2990894  0.28377159 0.28354931 0.27959101 0.26072185\n",
      " 0.2990894  0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26072185 0.2990894  0.28377159\n",
      " 0.28354931 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931\n",
      " 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931 0.27959101\n",
      " 0.26072185 0.2990894  0.28377159 0.28354931 0.27959101 0.26072185\n",
      " 0.2990894  0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26072185 0.2990894  0.28377159\n",
      " 0.28354931 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931\n",
      " 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931 0.27959101\n",
      " 0.26072185 0.2990894  0.28377159 0.28354931 0.27959101 0.26072185\n",
      " 0.2990894  0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26072185 0.2990894  0.28377159\n",
      " 0.28354931 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931\n",
      " 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931 0.27959101\n",
      " 0.26072185 0.2990894  0.28377159 0.28354931 0.27959101 0.26072185\n",
      " 0.2990894  0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26072185 0.2990894  0.28377159\n",
      " 0.28354931 0.27959101 0.26072185 0.2990894  0.28377159 0.28354931]\n",
      "invalid value encountered in scalar divide\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "invalid value encountered in scalar divide\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3060441580763623\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.2456359102244389\n",
      "GridSearchCV Runtime: 4.769578218460083 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.0\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23404255319148937\n",
      "Ave Test Precision: 0.14531597332486504\n",
      "Stdev Test Precision: 0.1327485958018431\n",
      "Ave Test Accuracy: 0.4567164179104477\n",
      "Stdev Test Accuracy: 0.2699569489534585\n",
      "Ave Test Specificity: 0.42079207920792083\n",
      "Ave Test Recall: 0.5666666666666667\n",
      "Ave Test NPV: 0.7244023329047645\n",
      "Ave Test F1-Score: 0.23117353333200708\n",
      "Ave Test G-mean: 0.060252494025588944\n",
      "Ave Runtime: 0.007771539688110352\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with water. 68 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.3019967  0.28333835 0.423051   0.3019967  0.3019967  0.3019967\n",
      " 0.24563272 0.02469136 0.3019967  0.3019967  0.30270794 0.24563272\n",
      " 0.02469136 0.30161138 0.30862137        nan        nan        nan\n",
      "        nan        nan 0.3019967  0.2893364  0.40220092 0.30161138\n",
      " 0.3066983  0.3019967  0.2893364  0.40220092 0.30161138 0.3066983\n",
      " 0.3019967  0.28333835 0.423051   0.3019967  0.3019967  0.3019967\n",
      " 0.24563272 0.02469136 0.3019967  0.3019967  0.30270794 0.24563272\n",
      " 0.02469136 0.30161138 0.30862137        nan        nan        nan\n",
      "        nan        nan 0.3019967  0.2893364  0.40220092 0.30161138\n",
      " 0.3066983  0.3019967  0.2893364  0.40220092 0.30161138 0.3066983\n",
      " 0.3019967  0.28333835 0.423051   0.3019967  0.3019967  0.30270794\n",
      " 0.24563272 0.02469136 0.30391978 0.3019967  0.30270794 0.24563272\n",
      " 0.02469136 0.30161138 0.30862137        nan        nan        nan\n",
      "        nan        nan 0.3019967  0.2893364  0.40220092 0.30161138\n",
      " 0.3066983  0.3019967  0.2893364  0.40220092 0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.423051   0.3019967  0.3019967  0.30270794\n",
      " 0.25428342 0.02469136 0.30161138 0.30862137 0.30270794 0.25428342\n",
      " 0.02469136 0.30161138 0.30862137        nan        nan        nan\n",
      "        nan        nan 0.3019967  0.2893364  0.40220092 0.30161138\n",
      " 0.3066983  0.3019967  0.2893364  0.40220092 0.30161138 0.3066983\n",
      " 0.3019967  0.2893364  0.40506854 0.3019967  0.3019967  0.30270794\n",
      " 0.28266199 0.3141358  0.30161138 0.3066983  0.30270794 0.28266199\n",
      " 0.3141358  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.2893364  0.40220092 0.30161138\n",
      " 0.3066983  0.3019967  0.2893364  0.40220092 0.30161138 0.3066983\n",
      " 0.3019967  0.2893364  0.40220092 0.30080623 0.3066983  0.3019967\n",
      " 0.2893364  0.38926913 0.30161138 0.3066983  0.3019967  0.2893364\n",
      " 0.38926913 0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.2893364  0.40220092 0.30161138\n",
      " 0.3066983  0.3019967  0.2893364  0.40220092 0.30161138 0.3066983\n",
      " 0.3019967  0.2893364  0.40220092 0.30161138 0.3066983  0.3019967\n",
      " 0.2893364  0.40220092 0.30161138 0.3066983  0.3019967  0.2893364\n",
      " 0.40220092 0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.2893364  0.40220092 0.30161138\n",
      " 0.3066983  0.3019967  0.2893364  0.40220092 0.30161138 0.3066983\n",
      " 0.3019967  0.2893364  0.40220092 0.30161138 0.3066983  0.3019967\n",
      " 0.2893364  0.40220092 0.30161138 0.3066983  0.3019967  0.2893364\n",
      " 0.40220092 0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.2893364  0.40220092 0.30161138\n",
      " 0.3066983  0.3019967  0.2893364  0.40220092 0.30161138 0.3066983\n",
      " 0.3019967  0.2893364  0.40220092 0.30161138 0.3066983  0.3019967\n",
      " 0.2893364  0.40220092 0.30161138 0.3066983  0.3019967  0.2893364\n",
      " 0.40220092 0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.2893364  0.40220092 0.30161138\n",
      " 0.3066983  0.3019967  0.2893364  0.40220092 0.30161138 0.3066983\n",
      " 0.3019967  0.2893364  0.40220092 0.30161138 0.3066983  0.3019967\n",
      " 0.2893364  0.40220092 0.30161138 0.3066983  0.3019967  0.2893364\n",
      " 0.40220092 0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.2893364  0.40220092 0.30161138\n",
      " 0.3066983  0.3019967  0.2893364  0.40220092 0.30161138 0.3066983\n",
      " 0.3019967  0.2893364  0.40220092 0.30161138 0.3066983  0.3019967\n",
      " 0.2893364  0.40220092 0.30161138 0.3066983  0.3019967  0.2893364\n",
      " 0.40220092 0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.2893364  0.40220092 0.30161138\n",
      " 0.3066983  0.3019967  0.2893364  0.40220092 0.30161138 0.3066983\n",
      " 0.3019967  0.2893364  0.40220092 0.30161138 0.3066983  0.3019967\n",
      " 0.2893364  0.40220092 0.30161138 0.3066983  0.3019967  0.2893364\n",
      " 0.40220092 0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.2893364  0.40220092 0.30161138\n",
      " 0.3066983  0.3019967  0.2893364  0.40220092 0.30161138 0.3066983\n",
      " 0.3019967  0.2893364  0.40220092 0.30161138 0.3066983  0.3019967\n",
      " 0.2893364  0.40220092 0.30161138 0.3066983  0.3019967  0.2893364\n",
      " 0.40220092 0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.2893364  0.40220092 0.30161138\n",
      " 0.3066983  0.3019967  0.2893364  0.40220092 0.30161138 0.3066983 ]\n",
      "One or more of the train scores are non-finite: [0.30036814 0.28408129 0.32327209 0.30036814 0.30036814 0.30036814\n",
      " 0.24563587 0.02454924 0.30036814 0.30036814 0.30092026 0.24563587\n",
      " 0.02454924 0.30209065 0.30197741        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.2854498  0.3148977  0.30209065\n",
      " 0.30156868 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868\n",
      " 0.30036814 0.28408129 0.32327209 0.30036814 0.30036814 0.30036814\n",
      " 0.24563587 0.02454924 0.30036814 0.30036814 0.30092026 0.24563587\n",
      " 0.02454924 0.30209065 0.30197741        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.2854498  0.3148977  0.30209065\n",
      " 0.30156868 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868\n",
      " 0.30036814 0.28408129 0.32327209 0.30036814 0.30036814 0.30092026\n",
      " 0.24563587 0.02454924 0.30077686 0.30036814 0.30092026 0.24563587\n",
      " 0.02454924 0.30209065 0.30197741        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.2854498  0.3148977  0.30209065\n",
      " 0.30156868 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868\n",
      " 0.30036814 0.28635351 0.32327209 0.30036814 0.30036814 0.30092026\n",
      " 0.25874252 0.12454924 0.30209065 0.30197741 0.30092026 0.25874252\n",
      " 0.12454924 0.30209065 0.30197741        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.2854498  0.3148977  0.30209065\n",
      " 0.30156868 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868\n",
      " 0.30036814 0.2854498  0.32411311 0.30036814 0.30036814 0.30092026\n",
      " 0.27850702 0.35220389 0.30209065 0.30156868 0.30092026 0.27850702\n",
      " 0.35220389 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.2854498  0.3148977  0.30209065\n",
      " 0.30156868 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868\n",
      " 0.30036814 0.2854498  0.31622955 0.30158733 0.30156868 0.30036814\n",
      " 0.2854498  0.32388187 0.30209065 0.30156868 0.30036814 0.2854498\n",
      " 0.32388187 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.2854498  0.3148977  0.30209065\n",
      " 0.30156868 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868\n",
      " 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868 0.30036814\n",
      " 0.2854498  0.31485191 0.30209065 0.30156868 0.30036814 0.2854498\n",
      " 0.31485191 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.2854498  0.3148977  0.30209065\n",
      " 0.30156868 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868\n",
      " 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868 0.30036814\n",
      " 0.2854498  0.3148977  0.30209065 0.30156868 0.30036814 0.2854498\n",
      " 0.3148977  0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.2854498  0.3148977  0.30209065\n",
      " 0.30156868 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868\n",
      " 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868 0.30036814\n",
      " 0.2854498  0.3148977  0.30209065 0.30156868 0.30036814 0.2854498\n",
      " 0.3148977  0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.2854498  0.3148977  0.30209065\n",
      " 0.30156868 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868\n",
      " 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868 0.30036814\n",
      " 0.2854498  0.3148977  0.30209065 0.30156868 0.30036814 0.2854498\n",
      " 0.3148977  0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.2854498  0.3148977  0.30209065\n",
      " 0.30156868 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868\n",
      " 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868 0.30036814\n",
      " 0.2854498  0.3148977  0.30209065 0.30156868 0.30036814 0.2854498\n",
      " 0.3148977  0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.2854498  0.3148977  0.30209065\n",
      " 0.30156868 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868\n",
      " 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868 0.30036814\n",
      " 0.2854498  0.3148977  0.30209065 0.30156868 0.30036814 0.2854498\n",
      " 0.3148977  0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.2854498  0.3148977  0.30209065\n",
      " 0.30156868 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868\n",
      " 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868 0.30036814\n",
      " 0.2854498  0.3148977  0.30209065 0.30156868 0.30036814 0.2854498\n",
      " 0.3148977  0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.2854498  0.3148977  0.30209065\n",
      " 0.30156868 0.30036814 0.2854498  0.3148977  0.30209065 0.30156868]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.42305099870889346\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6795511221945137\n",
      "GridSearchCV Runtime: 4.511510610580444 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2222222222222222\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.37037037037037035\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.5714285714285714\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2391304347826087\n",
      "Ave Test Precision: 0.28063031976075453\n",
      "Stdev Test Precision: 0.210061909051099\n",
      "Ave Test Accuracy: 0.7126865671641791\n",
      "Stdev Test Accuracy: 0.04741889935499565\n",
      "Ave Test Specificity: 0.9108910891089108\n",
      "Ave Test Recall: 0.10606060606060605\n",
      "Ave Test NPV: 0.7569898962708257\n",
      "Ave Test F1-Score: 0.14025031122910045\n",
      "Ave Test G-mean: 0.26839637246299\n",
      "Ave Runtime: 0.006231975555419922\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with electricity. 67 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22726944 0.06822487 0.13940925 0.25005408 0.27193442 0.22726944\n",
      " 0.0725     0.09844136 0.25005408 0.27193442 0.22830941 0.0725\n",
      " 0.09844136 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.12064914 0.21266665 0.25069511\n",
      " 0.26645323 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323\n",
      " 0.22726944 0.06822487 0.13940925 0.25005408 0.27193442 0.22726944\n",
      " 0.0725     0.09844136 0.25005408 0.27193442 0.22830941 0.0725\n",
      " 0.09844136 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.12064914 0.21266665 0.25069511\n",
      " 0.26645323 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323\n",
      " 0.22726944 0.06822487 0.13940925 0.25005408 0.27193442 0.22726944\n",
      " 0.0725     0.09844136 0.25005408 0.27193442 0.22830941 0.0725\n",
      " 0.09844136 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.12064914 0.21266665 0.25069511\n",
      " 0.26645323 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323\n",
      " 0.22726944 0.06822487 0.13921221 0.25005408 0.27193442 0.22766838\n",
      " 0.07183544 0.09844136 0.25005408 0.2658122  0.22830941 0.07183544\n",
      " 0.09844136 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.12064914 0.21266665 0.25069511\n",
      " 0.26645323 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323\n",
      " 0.22830941 0.0685468  0.13882675 0.25005408 0.26791107 0.22830941\n",
      " 0.06689005 0.13994332 0.25005408 0.26645323 0.22830941 0.06689005\n",
      " 0.13994332 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.12064914 0.21266665 0.25069511\n",
      " 0.26645323 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323\n",
      " 0.22830941 0.12064914 0.23434595 0.25005408 0.26645323 0.22830941\n",
      " 0.0685468  0.13360631 0.25069511 0.26645323 0.22830941 0.0685468\n",
      " 0.13360631 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.12064914 0.21266665 0.25069511\n",
      " 0.26645323 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323\n",
      " 0.22830941 0.12064914 0.18648341 0.25069511 0.26645323 0.22830941\n",
      " 0.12064914 0.18648341 0.25069511 0.26645323 0.22830941 0.12064914\n",
      " 0.18648341 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.12064914 0.21266665 0.25069511\n",
      " 0.26645323 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323\n",
      " 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323 0.22830941\n",
      " 0.12064914 0.21266665 0.25069511 0.26645323 0.22830941 0.12064914\n",
      " 0.21266665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.12064914 0.21266665 0.25069511\n",
      " 0.26645323 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323\n",
      " 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323 0.22830941\n",
      " 0.12064914 0.21266665 0.25069511 0.26645323 0.22830941 0.12064914\n",
      " 0.21266665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.12064914 0.21266665 0.25069511\n",
      " 0.26645323 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323\n",
      " 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323 0.22830941\n",
      " 0.12064914 0.21266665 0.25069511 0.26645323 0.22830941 0.12064914\n",
      " 0.21266665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.12064914 0.21266665 0.25069511\n",
      " 0.26645323 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323\n",
      " 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323 0.22830941\n",
      " 0.12064914 0.21266665 0.25069511 0.26645323 0.22830941 0.12064914\n",
      " 0.21266665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.12064914 0.21266665 0.25069511\n",
      " 0.26645323 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323\n",
      " 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323 0.22830941\n",
      " 0.12064914 0.21266665 0.25069511 0.26645323 0.22830941 0.12064914\n",
      " 0.21266665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.12064914 0.21266665 0.25069511\n",
      " 0.26645323 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323\n",
      " 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323 0.22830941\n",
      " 0.12064914 0.21266665 0.25069511 0.26645323 0.22830941 0.12064914\n",
      " 0.21266665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.12064914 0.21266665 0.25069511\n",
      " 0.26645323 0.22830941 0.12064914 0.21266665 0.25069511 0.26645323]\n",
      "One or more of the train scores are non-finite: [0.24575859 0.07274453 0.14859558 0.24377289 0.2428185  0.24575859\n",
      " 0.07382271 0.09823345 0.24377289 0.2428185  0.24632226 0.07382271\n",
      " 0.09823345 0.24377289 0.23971829        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12258291 0.27106482 0.24382803\n",
      " 0.23986972 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972\n",
      " 0.24575859 0.07274453 0.14859558 0.24377289 0.2428185  0.24575859\n",
      " 0.07382271 0.09823345 0.24377289 0.2428185  0.24632226 0.07382271\n",
      " 0.09823345 0.24377289 0.23971829        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12258291 0.27106482 0.24382803\n",
      " 0.23986972 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972\n",
      " 0.24575859 0.07274453 0.14859558 0.24377289 0.2428185  0.24575859\n",
      " 0.0738911  0.09823345 0.24377289 0.24263866 0.24632226 0.0738911\n",
      " 0.09823345 0.24377289 0.23971829        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12258291 0.27106482 0.24382803\n",
      " 0.23986972 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972\n",
      " 0.24586809 0.07281782 0.14859986 0.24377289 0.24263866 0.24615511\n",
      " 0.07366246 0.09836974 0.24377289 0.23982751 0.24632226 0.07366246\n",
      " 0.09836974 0.24377289 0.23971829        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12258291 0.27106482 0.24382803\n",
      " 0.23986972 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972\n",
      " 0.24632226 0.07188196 0.14650008 0.24377289 0.24067302 0.24632226\n",
      " 0.07215355 0.14930038 0.24377289 0.23981822 0.24632226 0.07215355\n",
      " 0.14930038 0.24377289 0.23981822        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12258291 0.27106482 0.24382803\n",
      " 0.23986972 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972\n",
      " 0.24643316 0.12258291 0.21233413 0.24377289 0.24003407 0.24643316\n",
      " 0.0719019  0.20925105 0.24382803 0.23986972 0.24643316 0.0719019\n",
      " 0.20925105 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12258291 0.27106482 0.24382803\n",
      " 0.23986972 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972\n",
      " 0.24643316 0.12258291 0.27406487 0.24382803 0.23986972 0.24643316\n",
      " 0.12258291 0.27406487 0.24382803 0.23986972 0.24643316 0.12258291\n",
      " 0.27406487 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12258291 0.27106482 0.24382803\n",
      " 0.23986972 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972\n",
      " 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972 0.24643316\n",
      " 0.12258291 0.27106482 0.24382803 0.23986972 0.24643316 0.12258291\n",
      " 0.27106482 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12258291 0.27106482 0.24382803\n",
      " 0.23986972 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972\n",
      " 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972 0.24643316\n",
      " 0.12258291 0.27106482 0.24382803 0.23986972 0.24643316 0.12258291\n",
      " 0.27106482 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12258291 0.27106482 0.24382803\n",
      " 0.23986972 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972\n",
      " 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972 0.24643316\n",
      " 0.12258291 0.27106482 0.24382803 0.23986972 0.24643316 0.12258291\n",
      " 0.27106482 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12258291 0.27106482 0.24382803\n",
      " 0.23986972 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972\n",
      " 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972 0.24643316\n",
      " 0.12258291 0.27106482 0.24382803 0.23986972 0.24643316 0.12258291\n",
      " 0.27106482 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12258291 0.27106482 0.24382803\n",
      " 0.23986972 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972\n",
      " 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972 0.24643316\n",
      " 0.12258291 0.27106482 0.24382803 0.23986972 0.24643316 0.12258291\n",
      " 0.27106482 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12258291 0.27106482 0.24382803\n",
      " 0.23986972 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972\n",
      " 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972 0.24643316\n",
      " 0.12258291 0.27106482 0.24382803 0.23986972 0.24643316 0.12258291\n",
      " 0.27106482 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12258291 0.27106482 0.24382803\n",
      " 0.23986972 0.24643316 0.12258291 0.27106482 0.24382803 0.23986972]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.27193442236516463\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.5536159600997507\n",
      "GridSearchCV Runtime: 4.392178535461426 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.21774193548387097\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24444444444444444\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25210084033613445\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.25882352941176473\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23711340206185566\n",
      "Ave Test Precision: 0.24204483034761407\n",
      "Stdev Test Precision: 0.01583789688282497\n",
      "Ave Test Accuracy: 0.5544776119402985\n",
      "Stdev Test Accuracy: 0.04259291992574931\n",
      "Ave Test Specificity: 0.6128712871287129\n",
      "Ave Test Recall: 0.3757575757575758\n",
      "Ave Test NPV: 0.7496931540803278\n",
      "Ave Test F1-Score: 0.2928370900250544\n",
      "Ave Test G-mean: 0.4768950848912845\n",
      "Ave Runtime: 0.005201005935668945\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with rent. 66 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22769151 0.16713344 0.1649561  0.22769151 0.23226817 0.22769151\n",
      " 0.         0.09969136 0.22769151 0.23226817 0.22769151 0.\n",
      " 0.09969136 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16713344 0.1649561  0.22769151 0.23226817 0.22769151\n",
      " 0.         0.09969136 0.22769151 0.23226817 0.22769151 0.\n",
      " 0.09969136 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16713344 0.1649561  0.22769151 0.23226817 0.22769151\n",
      " 0.07333333 0.09969136 0.22769151 0.23226817 0.22769151 0.07333333\n",
      " 0.09969136 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16713344 0.1649561  0.22769151 0.23226817 0.22769151\n",
      " 0.10380952 0.09969136 0.22769151 0.23801047 0.22769151 0.10380952\n",
      " 0.09969136 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.22162276 0.22769151 0.23324856 0.22769151\n",
      " 0.15383587 0.14905063 0.22769151 0.23801047 0.22769151 0.15383587\n",
      " 0.14905063 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.22828943 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.22162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.22162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047]\n",
      "One or more of the train scores are non-finite: [0.22503123 0.24546136 0.307667   0.22503123 0.22756997 0.22503123\n",
      " 0.         0.09809494 0.22503123 0.22756997 0.22650063 0.\n",
      " 0.09809494 0.22928213 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.3097551  0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055\n",
      " 0.22503123 0.24546136 0.307667   0.22503123 0.22756997 0.22503123\n",
      " 0.         0.09809494 0.22503123 0.22756997 0.22650063 0.\n",
      " 0.09809494 0.22928213 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.3097551  0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055\n",
      " 0.22503123 0.24546136 0.307667   0.22503123 0.22756997 0.22503123\n",
      " 0.11933333 0.09809494 0.22503123 0.22756997 0.22650063 0.11933333\n",
      " 0.09809494 0.22928213 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.3097551  0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055\n",
      " 0.22503123 0.24546136 0.307667   0.22503123 0.22756997 0.22650063\n",
      " 0.24174901 0.09809494 0.22928213 0.22983473 0.22650063 0.24174901\n",
      " 0.09809494 0.22928213 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.3097551  0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.3082308  0.22637893 0.23038105 0.22650063\n",
      " 0.20358091 0.32279927 0.22791524 0.23275055 0.22650063 0.20358091\n",
      " 0.32279927 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.3097551  0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.30422004 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.30155337 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.30155337 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.3097551  0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.30733344 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.30729865 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.30729865 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.3097551  0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.3097551  0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.3097551  0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.3097551  0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.3097551  0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.3097551  0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.3097551  0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.3097551  0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.3097551  0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.3097551  0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.3097551  0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.3097551  0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.3097551  0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.3097551  0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.3097551  0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.3097551  0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.3097551  0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.3097551  0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.3097551  0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.3097551  0.22791524 0.23275055]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.26162276200250884\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.7418952618453866\n",
      "GridSearchCV Runtime: 4.678884506225586 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.46153846153846156\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.42857142857142855\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3076923076923077\n",
      "Ave Test Precision: 0.3633699633699634\n",
      "Stdev Test Precision: 0.07733189809993642\n",
      "Ave Test Accuracy: 0.7447761194029849\n",
      "Stdev Test Accuracy: 0.006243731541299092\n",
      "Ave Test Specificity: 0.9693069306930692\n",
      "Ave Test Recall: 0.05757575757575757\n",
      "Ave Test NPV: 0.7589708816345524\n",
      "Ave Test F1-Score: 0.09738891695126947\n",
      "Ave Test G-mean: 0.22534962951358578\n",
      "Ave Runtime: 0.007431745529174805\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with repair. 65 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.30469697 0.34525253 0.19874199 0.30469697 0.30469697 0.30469697\n",
      " 0.         0.09969136 0.30469697 0.30469697 0.2880303  0.\n",
      " 0.09969136 0.31166667 0.2880303         nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.28354417 0.30469697\n",
      " 0.30469697 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697\n",
      " 0.30469697 0.34525253 0.19874199 0.30469697 0.30469697 0.30469697\n",
      " 0.         0.09969136 0.30469697 0.30469697 0.2880303  0.\n",
      " 0.09969136 0.31166667 0.2880303         nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.28354417 0.30469697\n",
      " 0.30469697 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697\n",
      " 0.30469697 0.34525253 0.19874199 0.30469697 0.30469697 0.30469697\n",
      " 0.1        0.09969136 0.31166667 0.30469697 0.2880303  0.1\n",
      " 0.09969136 0.31166667 0.2880303         nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.28354417 0.30469697\n",
      " 0.30469697 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697\n",
      " 0.30469697 0.33025253 0.09874199 0.30469697 0.30469697 0.2880303\n",
      " 0.24916667 0.09874199 0.285      0.2880303  0.2880303  0.24916667\n",
      " 0.09874199 0.285      0.2880303         nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.28354417 0.30469697\n",
      " 0.30469697 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697\n",
      " 0.30469697 0.34085859 0.19874199 0.30469697 0.30469697 0.30469697\n",
      " 0.33025253 0.09874199 0.2880303  0.2880303  0.30469697 0.33025253\n",
      " 0.09874199 0.2880303  0.2880303         nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.28354417 0.30469697\n",
      " 0.30469697 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.33140131 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.23207532 0.30469697 0.30469697 0.30469697 0.32722222\n",
      " 0.23207532 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.28354417 0.30469697\n",
      " 0.30469697 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26195687 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26195687 0.30469697 0.30469697 0.30469697 0.32722222\n",
      " 0.26195687 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.28354417 0.30469697\n",
      " 0.30469697 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.28354417 0.30469697 0.30469697 0.30469697 0.32722222\n",
      " 0.28576639 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.28354417 0.30469697\n",
      " 0.30469697 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.28354417 0.30469697 0.30469697 0.30469697 0.32722222\n",
      " 0.28354417 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.28354417 0.30469697\n",
      " 0.30469697 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.28354417 0.30469697 0.30469697 0.30469697 0.32722222\n",
      " 0.28354417 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.28354417 0.30469697\n",
      " 0.30469697 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.28354417 0.30469697 0.30469697 0.30469697 0.32722222\n",
      " 0.28354417 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.28354417 0.30469697\n",
      " 0.30469697 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.28354417 0.30469697 0.30469697 0.30469697 0.32722222\n",
      " 0.28354417 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.28354417 0.30469697\n",
      " 0.30469697 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.28354417 0.30469697 0.30469697 0.30469697 0.32722222\n",
      " 0.28354417 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.28354417 0.30469697\n",
      " 0.30469697 0.30469697 0.32722222 0.28354417 0.30469697 0.30469697]\n",
      "One or more of the train scores are non-finite: [0.28740222 0.31833742 0.69781369 0.28740222 0.28740222 0.28740222\n",
      " 0.         0.09809494 0.28740222 0.28740222 0.28473788 0.\n",
      " 0.09809494 0.29416047 0.28017513        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.31816498 0.28740222\n",
      " 0.28740222 0.28740222 0.29868538 0.31816498 0.28755412 0.28740222\n",
      " 0.28740222 0.31833742 0.69781369 0.28740222 0.28740222 0.28740222\n",
      " 0.         0.09809494 0.28740222 0.28740222 0.28473788 0.\n",
      " 0.09809494 0.29416047 0.28017513        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.31816498 0.28740222\n",
      " 0.28740222 0.28740222 0.29868538 0.31816498 0.28755412 0.28740222\n",
      " 0.28740222 0.31833742 0.69781369 0.28740222 0.28740222 0.28740222\n",
      " 0.9        0.09809494 0.29400857 0.28740222 0.28473788 0.9\n",
      " 0.09809494 0.29416047 0.28017513        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.31816498 0.28740222\n",
      " 0.28740222 0.28740222 0.29868538 0.31816498 0.28755412 0.28740222\n",
      " 0.28740222 0.31793856 0.69781369 0.28740222 0.28740222 0.28473788\n",
      " 0.47266378 0.09809494 0.28858025 0.28017513 0.28473788 0.47266378\n",
      " 0.09809494 0.28858025 0.28017513        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.31816498 0.28740222\n",
      " 0.28740222 0.28740222 0.29868538 0.31816498 0.28755412 0.28740222\n",
      " 0.28740222 0.31726549 0.67781369 0.28740222 0.28740222 0.28755412\n",
      " 0.31861409 0.69778057 0.28481183 0.28298166 0.28755412 0.31861409\n",
      " 0.69778057 0.2839386  0.28298166        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.31816498 0.28740222\n",
      " 0.28740222 0.28740222 0.29868538 0.31816498 0.28755412 0.28740222\n",
      " 0.28740222 0.29770861 0.44956971 0.28740222 0.28740222 0.28740222\n",
      " 0.2986633  0.44023056 0.28755412 0.28740222 0.28755412 0.2986633\n",
      " 0.44023056 0.28755412 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.31816498 0.28740222\n",
      " 0.28740222 0.28740222 0.29868538 0.31816498 0.28755412 0.28740222\n",
      " 0.28740222 0.29868538 0.33554184 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33554184 0.28740222 0.28740222 0.28740222 0.29868538\n",
      " 0.33554184 0.28755412 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.31816498 0.28740222\n",
      " 0.28740222 0.28740222 0.29868538 0.31816498 0.28755412 0.28740222\n",
      " 0.28740222 0.29868538 0.31724222 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.31724222 0.28740222 0.28740222 0.28740222 0.29868538\n",
      " 0.31981907 0.28755412 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.31816498 0.28740222\n",
      " 0.28740222 0.28740222 0.29868538 0.31816498 0.28755412 0.28740222\n",
      " 0.28740222 0.29868538 0.31816498 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.31816498 0.28740222 0.28740222 0.28740222 0.29868538\n",
      " 0.31816498 0.28755412 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.31816498 0.28740222\n",
      " 0.28740222 0.28740222 0.29868538 0.31816498 0.28755412 0.28740222\n",
      " 0.28740222 0.29868538 0.31816498 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.31816498 0.28740222 0.28740222 0.28740222 0.29868538\n",
      " 0.31816498 0.28755412 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.31816498 0.28740222\n",
      " 0.28740222 0.28740222 0.29868538 0.31816498 0.28755412 0.28740222\n",
      " 0.28740222 0.29868538 0.31816498 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.31816498 0.28740222 0.28740222 0.28740222 0.29868538\n",
      " 0.31816498 0.28755412 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.31816498 0.28740222\n",
      " 0.28740222 0.28740222 0.29868538 0.31816498 0.28755412 0.28740222\n",
      " 0.28740222 0.29868538 0.31816498 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.31816498 0.28740222 0.28740222 0.28740222 0.29868538\n",
      " 0.31816498 0.28755412 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.31816498 0.28740222\n",
      " 0.28740222 0.28740222 0.29868538 0.31816498 0.28755412 0.28740222\n",
      " 0.28740222 0.29868538 0.31816498 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.31816498 0.28740222 0.28740222 0.28740222 0.29868538\n",
      " 0.31816498 0.28755412 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.31816498 0.28740222\n",
      " 0.28740222 0.28740222 0.29868538 0.31816498 0.28755412 0.28740222]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.34525252525252526\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7219451371571073\n",
      "GridSearchCV Runtime: 4.4585511684417725 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25925925925925924\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.4117647058823529\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.21428571428571427\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.19047619047619047\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.34782608695652173\n",
      "Ave Test Precision: 0.2847223913720077\n",
      "Stdev Test Precision: 0.09302401441538266\n",
      "Ave Test Accuracy: 0.7149253731343284\n",
      "Stdev Test Accuracy: 0.019673770636513536\n",
      "Ave Test Specificity: 0.9168316831683168\n",
      "Ave Test Recall: 0.09696969696969697\n",
      "Ave Test NPV: 0.7564760242817975\n",
      "Ave Test F1-Score: 0.14372024231185002\n",
      "Ave Test G-mean: 0.2963655869180332\n",
      "Ave Runtime: 0.006375646591186524\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with cinema. 64 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27909069 0.30374901 0.20538101 0.27909069 0.27909069 0.27909069\n",
      " 0.0725     0.09538101 0.27909069 0.27909069 0.28673775 0.0725\n",
      " 0.09135802 0.30285286 0.28472961        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.24781691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194\n",
      " 0.27909069 0.30374901 0.20538101 0.27909069 0.27909069 0.27909069\n",
      " 0.0725     0.09538101 0.27909069 0.27909069 0.28673775 0.0725\n",
      " 0.09135802 0.30285286 0.28472961        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.24781691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194\n",
      " 0.27909069 0.30374901 0.20538101 0.27909069 0.27909069 0.27909069\n",
      " 0.1725     0.09538101 0.30285286 0.27909069 0.28673775 0.1725\n",
      " 0.09135802 0.30285286 0.28472961        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.24781691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194\n",
      " 0.27909069 0.30374901 0.20538101 0.27909069 0.27909069 0.28673775\n",
      " 0.22153595 0.13421517 0.30285286 0.28472961 0.28673775 0.22153595\n",
      " 0.13421517 0.30285286 0.28472961        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.24781691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194\n",
      " 0.27909069 0.30930457 0.24871435 0.27909069 0.2750677  0.27909069\n",
      " 0.30374901 0.26135802 0.28271476 0.28472961 0.27909069 0.30374901\n",
      " 0.26135802 0.28271476 0.28472961        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.24781691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.22948358 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.22948358 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.24781691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.24781691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.24781691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.24781691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.24781691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.24781691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.24781691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.24781691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.24781691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.24781691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.24781691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.24781691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.24781691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.24781691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.24781691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.24781691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.24781691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.24781691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.24781691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.24781691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.24781691 0.27909069 0.27836194]\n",
      "One or more of the train scores are non-finite: [0.27992724 0.30099718 0.28628376 0.27992724 0.27992724 0.27992724\n",
      " 0.07382271 0.10223824 0.27992724 0.27992724 0.28128833 0.07382271\n",
      " 0.10467727 0.29252973 0.29444698        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.35954908 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292\n",
      " 0.27992724 0.30099718 0.28628376 0.27992724 0.27992724 0.27992724\n",
      " 0.07382271 0.10223824 0.27992724 0.27992724 0.28128833 0.07382271\n",
      " 0.10467727 0.29252973 0.29444698        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.35954908 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292\n",
      " 0.27992724 0.30099718 0.28628376 0.27992724 0.27992724 0.27992724\n",
      " 0.32382271 0.20223824 0.29235698 0.27992724 0.28128833 0.32382271\n",
      " 0.20467727 0.29252973 0.29444698        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.35954908 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292\n",
      " 0.27992724 0.30114973 0.28628376 0.27992724 0.27992724 0.28128833\n",
      " 0.32557084 0.3398124  0.29252973 0.29444698 0.28128833 0.32557084\n",
      " 0.3398124  0.29252973 0.29444698        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.35954908 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292\n",
      " 0.28009999 0.29386122 0.27443698 0.28189123 0.28236627 0.28009999\n",
      " 0.30045392 0.29517143 0.28551859 0.29265575 0.28009999 0.30045392\n",
      " 0.29517143 0.28551859 0.29265575        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.35954908 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36505274 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.3720387  0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.3720387  0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.35954908 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.35973163 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36254548 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36254548 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.35954908 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.35954908 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.35954908 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.35954908 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.35954908 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.35954908 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.35954908 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.35954908 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.35954908 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.35954908 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.35954908 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.35954908 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.35954908 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.35954908 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.35954908 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.35954908 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.35954908 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.35954908 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.35954908 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.35954908 0.28189123 0.28969292]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "invalid value encountered in scalar divide\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "invalid value encountered in scalar divide\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.31140247007894073\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.6670822942643392\n",
      "GridSearchCV Runtime: 4.450302839279175 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.375\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.30597014925373134\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.46153846153846156\n",
      "Ave Test Precision: 0.32700918484500574\n",
      "Stdev Test Precision: 0.09203525331426364\n",
      "Ave Test Accuracy: 0.5022388059701492\n",
      "Stdev Test Accuracy: 0.24416218056155595\n",
      "Ave Test Specificity: 0.47128712871287126\n",
      "Ave Test Recall: 0.5969696969696969\n",
      "Ave Test NPV: 0.7866522999973395\n",
      "Ave Test F1-Score: 0.3336214739076145\n",
      "Ave Test G-mean: 0.27142169529888394\n",
      "Ave Runtime: 0.008704471588134765\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with dineOut. 63 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.26852693 0.3192841  0.33340949 0.26852693 0.26852693 0.26852693\n",
      " 0.14625    0.09938272 0.26852693 0.26852693 0.26852693 0.14625\n",
      " 0.09938272 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.33339337 0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693\n",
      " 0.26852693 0.3192841  0.33340949 0.26852693 0.26852693 0.26852693\n",
      " 0.14625    0.09938272 0.26852693 0.26852693 0.26852693 0.14625\n",
      " 0.09938272 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.33339337 0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693\n",
      " 0.26852693 0.3192841  0.33340949 0.26852693 0.26852693 0.26852693\n",
      " 0.14837963 0.09938272 0.26852693 0.26852693 0.26852693 0.14837963\n",
      " 0.09938272 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.33339337 0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.33179224 0.26852693 0.26852693 0.26852693\n",
      " 0.21483999 0.12999496 0.26852693 0.26852693 0.26852693 0.21483999\n",
      " 0.12999496 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.33339337 0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.33109048 0.26852693 0.26852693 0.26852693\n",
      " 0.25156981 0.1801455  0.26852693 0.26852693 0.26852693 0.25156981\n",
      " 0.1801455  0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.33339337 0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.39339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.39339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.33339337 0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.33339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.33339337 0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.33339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.33339337 0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.33339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.33339337 0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.33339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.33339337 0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.33339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.33339337 0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.33339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.33339337 0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.33339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.33339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.33339337 0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.33339337 0.26852693 0.26852693]\n",
      "One or more of the train scores are non-finite: [0.26819894 0.30399233 0.31689096 0.26819894 0.26819894 0.26819894\n",
      " 0.14750693 0.09812895 0.26819894 0.26819894 0.26819894 0.14750693\n",
      " 0.09812895 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.32002389 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.31689096 0.26819894 0.26819894 0.26819894\n",
      " 0.14750693 0.09812895 0.26819894 0.26819894 0.26819894 0.14750693\n",
      " 0.09812895 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.32002389 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.31689096 0.26819894 0.26819894 0.26819894\n",
      " 0.15074126 0.09812895 0.26819894 0.26819894 0.26819894 0.15074126\n",
      " 0.09812895 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.32002389 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.31743482 0.26819894 0.26819894 0.26819894\n",
      " 0.22733617 0.12113364 0.26819894 0.26819894 0.26819894 0.22733617\n",
      " 0.12113364 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.32002389 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.31949741 0.26819894 0.26819894 0.26819894\n",
      " 0.32725985 0.25818509 0.26819894 0.26819894 0.26819894 0.32725985\n",
      " 0.25818509 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.32002389 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30934013 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30934013 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.32002389 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.32002389 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.32002389 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.32002389 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.32002389 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.32002389 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.32002389 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.32002389 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.32002389 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.32002389 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.32002389 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.32002389 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.32002389 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.32002389 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.32002389 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.32002389 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.32002389 0.26819894 0.26819894]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3933933663709828\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.25561097256857856\n",
      "GridSearchCV Runtime: 4.680717945098877 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.24609375\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.30612244897959184\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.4117647058823529\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2222222222222222\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.35\n",
      "Ave Test Precision: 0.30724062541683345\n",
      "Stdev Test Precision: 0.07700968621779661\n",
      "Ave Test Accuracy: 0.6238805970149254\n",
      "Stdev Test Accuracy: 0.199712284696006\n",
      "Ave Test Specificity: 0.7257425742574257\n",
      "Ave Test Recall: 0.31212121212121213\n",
      "Ave Test NPV: 0.7611986702552921\n",
      "Ave Test F1-Score: 0.23604753009459611\n",
      "Ave Test G-mean: 0.32517984198485406\n",
      "Ave Runtime: 0.008555984497070313\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with leisure. 62 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.2638219  0.21030003 0.19464286 0.25082755 0.25995343 0.2638219\n",
      " 0.12088272 0.         0.25082755 0.25995343 0.27214045 0.12088272\n",
      " 0.         0.25246785 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.21030003 0.19464286 0.25082755 0.25995343 0.2638219\n",
      " 0.12088272 0.         0.25082755 0.25995343 0.27214045 0.12088272\n",
      " 0.         0.25246785 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.21030003 0.19464286 0.25082755 0.25995343 0.2638219\n",
      " 0.12088272 0.         0.25416088 0.25995343 0.27214045 0.12088272\n",
      " 0.         0.25246785 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.21030003 0.19464286 0.25082755 0.25995343 0.27214045\n",
      " 0.29171605 0.1        0.25246785 0.26204622 0.27214045 0.29171605\n",
      " 0.1        0.25246785 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.16878816 0.253326   0.26387599 0.26423569\n",
      " 0.21581542 0.07857143 0.25403307 0.26204622 0.26423569 0.21581542\n",
      " 0.07857143 0.25403307 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.20890212 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.14890212 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.14890212 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19973304 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19973304 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19973304 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811]\n",
      "One or more of the train scores are non-finite: [0.26382465 0.23850866 0.21510597 0.26206883 0.26137565 0.26382465\n",
      " 0.1250453  0.         0.26206883 0.26137565 0.26377139 0.1250453\n",
      " 0.         0.26469351 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.39705724 0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811\n",
      " 0.26382465 0.23850866 0.21510597 0.26206883 0.26137565 0.26382465\n",
      " 0.1250453  0.         0.26206883 0.26137565 0.26377139 0.1250453\n",
      " 0.         0.26469351 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.39705724 0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811\n",
      " 0.26382465 0.23850866 0.21510597 0.26206883 0.26137565 0.26382465\n",
      " 0.45837863 0.         0.26208507 0.26137565 0.26377139 0.45837863\n",
      " 0.         0.26469351 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.39705724 0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811\n",
      " 0.26382465 0.23850866 0.21510597 0.26206883 0.26137565 0.26398216\n",
      " 0.24522127 0.43333333 0.26464266 0.26341731 0.26377139 0.24522127\n",
      " 0.43333333 0.26469351 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.39705724 0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811\n",
      " 0.26382465 0.23356884 0.21385186 0.26324413 0.26159239 0.26378117\n",
      " 0.24367805 0.22125566 0.26425056 0.26341731 0.26378117 0.24367805\n",
      " 0.22125566 0.26425056 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.39705724 0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.40172993 0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.40670506 0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.40670506 0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.39705724 0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.39774078 0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.39752187 0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.39752187 0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.39705724 0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.39705724 0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.39705724 0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.39705724 0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.39705724 0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.39705724 0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.39705724 0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.39705724 0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.39705724 0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.39705724 0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.39705724 0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.39705724 0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.39705724 0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.39705724 0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.39705724 0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.39705724 0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.39705724 0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.39705724 0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.39705724 0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.39705724 0.2635985  0.26295811]\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2917160493827161\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.2456359102244389\n",
      "GridSearchCV Runtime: 4.586446523666382 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2413793103448276\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.21428571428571427\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2462686567164179\n",
      "Ave Test Precision: 0.23889419895595912\n",
      "Stdev Test Precision: 0.013918523432490348\n",
      "Ave Test Accuracy: 0.3365671641791045\n",
      "Stdev Test Accuracy: 0.19983424853600126\n",
      "Ave Test Specificity: 0.18217821782178217\n",
      "Ave Test Recall: 0.809090909090909\n",
      "Ave Test NPV: 0.6607142857142857\n",
      "Ave Test F1-Score: 0.33972188358009725\n",
      "Ave Test G-mean: 0.08442067319195659\n",
      "Ave Runtime: 0.008642816543579101\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with personalCare. 61 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.26883361 0.27398236 0.28844136 0.26883361 0.26883361 0.26883361\n",
      " 0.07375    0.12344136 0.26883361 0.26883361 0.26883361 0.07375\n",
      " 0.12344136 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.27398236 0.28844136 0.26883361 0.26883361 0.26883361\n",
      " 0.07375    0.12344136 0.26883361 0.26883361 0.26883361 0.07375\n",
      " 0.12344136 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.27398236 0.28844136 0.26883361 0.26883361 0.26883361\n",
      " 0.12375    0.12344136 0.26883361 0.26883361 0.26883361 0.12375\n",
      " 0.12344136 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.27398236 0.28844136 0.26883361 0.26883361 0.26883361\n",
      " 0.3632622  0.12344136 0.26883361 0.26883361 0.26883361 0.3632622\n",
      " 0.12344136 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.29292854 0.26883361 0.26883361 0.26883361\n",
      " 0.33858161 0.22844136 0.26883361 0.26883361 0.26883361 0.33858161\n",
      " 0.22844136 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.25880978 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.25880978 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.25880978 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.25838138 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.25838138 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.25838138 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.25730446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.25730446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.25730446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361]\n",
      "One or more of the train scores are non-finite: [0.26911115 0.31031419 0.30488635 0.26911115 0.26911115 0.26911115\n",
      " 0.07368421 0.12274868 0.26911115 0.26911115 0.26923137 0.07368421\n",
      " 0.12274868 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.31118441 0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137\n",
      " 0.26911115 0.31031419 0.30488635 0.26911115 0.26911115 0.26911115\n",
      " 0.07368421 0.12274868 0.26911115 0.26911115 0.26923137 0.07368421\n",
      " 0.12274868 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.31118441 0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137\n",
      " 0.26911115 0.31031419 0.30488635 0.26911115 0.26911115 0.26911115\n",
      " 0.16422475 0.12274868 0.26911115 0.26911115 0.26923137 0.16422475\n",
      " 0.12274868 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.31118441 0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.31031419 0.30488635 0.26923137 0.26923137 0.26923137\n",
      " 0.37080947 0.23941535 0.26923137 0.26923137 0.26923137 0.37080947\n",
      " 0.23941535 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.31118441 0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.31118441 0.30206809 0.26923137 0.26923137 0.26923137\n",
      " 0.31714727 0.3184369  0.26923137 0.26923137 0.26923137 0.31714727\n",
      " 0.3184369  0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.31118441 0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.31118441 0.28874758 0.26923137 0.26923137 0.26923137\n",
      " 0.31118441 0.2898272  0.26923137 0.26923137 0.26923137 0.31118441\n",
      " 0.2898272  0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.31118441 0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.31118441 0.28734225 0.26923137 0.26923137 0.26923137\n",
      " 0.31118441 0.28734225 0.26923137 0.26923137 0.26923137 0.31118441\n",
      " 0.28734225 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.31118441 0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.31118441 0.28734225 0.26923137 0.26923137 0.26923137\n",
      " 0.31118441 0.28734225 0.26923137 0.26923137 0.26923137 0.31118441\n",
      " 0.28734225 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.31118441 0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.31118441 0.28489908 0.26923137 0.26923137 0.26923137 0.31118441\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.31118441 0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.31118441 0.28489908 0.26923137 0.26923137 0.26923137 0.31118441\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.31118441 0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.31118441 0.28489908 0.26923137 0.26923137 0.26923137 0.31118441\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.31118441 0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.31118441 0.28489908 0.26923137 0.26923137 0.26923137 0.31118441\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.31118441 0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.31118441 0.28489908 0.26923137 0.26923137 0.26923137 0.31118441\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.31118441 0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.31118441 0.28489908 0.26923137 0.26923137]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.36326219512195124\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.4688279301745636\n",
      "GridSearchCV Runtime: 4.495737552642822 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2446043165467626\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.5\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.23076923076923078\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2786885245901639\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23684210526315788\n",
      "Ave Test Precision: 0.29818083543386303\n",
      "Stdev Test Precision: 0.11433299283963919\n",
      "Ave Test Accuracy: 0.5470149253731343\n",
      "Stdev Test Accuracy: 0.16879122228621143\n",
      "Ave Test Specificity: 0.592079207920792\n",
      "Ave Test Recall: 0.40909090909090906\n",
      "Ave Test NPV: 0.7416904638467494\n",
      "Ave Test F1-Score: 0.28173414064330143\n",
      "Ave Test G-mean: 0.41513834886434486\n",
      "Ave Runtime: 0.007413387298583984\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with clothing. 60 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22356655 0.24221053 0.31955235 0.22356655 0.23483927 0.22356655\n",
      " 0.09813272 0.1        0.22356655 0.23483927 0.22356655 0.09813272\n",
      " 0.1        0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23643739 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.24221053 0.31955235 0.22356655 0.23483927 0.22356655\n",
      " 0.09813272 0.1        0.22356655 0.23483927 0.22356655 0.09813272\n",
      " 0.1        0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23643739 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.24221053 0.31955235 0.22356655 0.23483927 0.22356655\n",
      " 0.19813272 0.1        0.2243602  0.23483927 0.22356655 0.19813272\n",
      " 0.1        0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23643739 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23731922 0.31955235 0.22356655 0.23483927 0.22356655\n",
      " 0.34535494 0.09807692 0.2243602  0.23467599 0.22356655 0.34535494\n",
      " 0.09807692 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23643739 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23731922 0.31813725 0.22356655 0.23483927 0.22356655\n",
      " 0.18615012 0.15095238 0.2243602  0.23467599 0.22356655 0.18615012\n",
      " 0.15095238 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23643739 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23731922 0.31775264 0.2243602  0.23467599 0.22356655 0.23731922\n",
      " 0.31775264 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23643739 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23731922 0.31813725 0.2243602  0.23467599 0.22356655 0.23731922\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23643739 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655 0.23643739\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23643739 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655 0.23643739\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23643739 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655 0.23643739\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23643739 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655 0.23643739\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23643739 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655 0.23643739\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23643739 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23643739 0.31813725 0.2243602  0.23467599 0.22356655 0.23643739\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23643739 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23643739 0.31813725 0.2243602  0.23467599]\n",
      "One or more of the train scores are non-finite: [0.22455054 0.2690323  0.31304964 0.22455054 0.22701474 0.22455054\n",
      " 0.09826745 0.09806094 0.22455054 0.22701474 0.22463815 0.09826745\n",
      " 0.09806094 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26894088 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26894088 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.2690323  0.31304964 0.22455054 0.22701474 0.22455054\n",
      " 0.09826745 0.09806094 0.22455054 0.22701474 0.22463815 0.09826745\n",
      " 0.09806094 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26894088 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26894088 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.2690323  0.31304964 0.22455054 0.22701474 0.22463815\n",
      " 0.19493412 0.09806094 0.22463907 0.22701474 0.22463815 0.19493412\n",
      " 0.09806094 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26894088 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26894088 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26868095 0.31759492 0.22455054 0.22701474 0.22463815\n",
      " 0.42902634 0.09744039 0.22463907 0.22658    0.22463815 0.42902634\n",
      " 0.09744039 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26894088 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26894088 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26868095 0.35245767 0.22455054 0.22695958 0.22455054\n",
      " 0.28186242 0.44529656 0.22463907 0.22658    0.22455054 0.28186242\n",
      " 0.44529656 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26894088 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26894088 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26894088 0.41912434 0.22463907 0.22658    0.22455054\n",
      " 0.27337865 0.40676818 0.22463907 0.22658    0.22455054 0.27337865\n",
      " 0.40676818 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26894088 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26894088 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26894088 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26868095 0.41891329 0.22463907 0.22658    0.22455054 0.26868095\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26894088 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26894088 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26894088 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26894088 0.41891329 0.22463907 0.22658    0.22455054 0.26894088\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26894088 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26894088 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26894088 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26894088 0.41891329 0.22463907 0.22658    0.22455054 0.26894088\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26894088 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26894088 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26894088 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26894088 0.41891329 0.22463907 0.22658    0.22455054 0.26894088\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26894088 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26894088 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26894088 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26894088 0.41891329 0.22463907 0.22658    0.22455054 0.26894088\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26894088 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26894088 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26894088 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26894088 0.41891329 0.22463907 0.22658    0.22455054 0.26894088\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26894088 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26894088 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26894088 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26894088 0.41891329 0.22463907 0.22658    0.22455054 0.26894088\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26894088 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26894088 0.41891329 0.22463907 0.22658   ]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.34535493827160496\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.743142144638404\n",
      "GridSearchCV Runtime: 4.228301286697388 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.3125\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.17391304347826086\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2727272727272727\n",
      "Ave Test Precision: 0.1518280632411067\n",
      "Stdev Test Precision: 0.14749860850479246\n",
      "Ave Test Accuracy: 0.7231343283582088\n",
      "Stdev Test Accuracy: 0.046634327761098486\n",
      "Ave Test Specificity: 0.9435643564356436\n",
      "Ave Test Recall: 0.048484848484848485\n",
      "Ave Test NPV: 0.7518003492943965\n",
      "Ave Test F1-Score: 0.06854608805828319\n",
      "Ave Test G-mean: 0.15805631117441438\n",
      "Ave Runtime: 0.00652766227722168\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with mobileLoad. 59 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.31098952 0.28977326 0.25352807 0.31098952 0.31098952 0.31098952\n",
      " 0.12219136 0.14719136 0.31098952 0.31098952 0.30665091 0.12219136\n",
      " 0.14719136 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.2512847  0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024\n",
      " 0.31098952 0.28977326 0.25352807 0.31098952 0.31098952 0.31098952\n",
      " 0.12219136 0.14719136 0.31098952 0.31098952 0.30665091 0.12219136\n",
      " 0.14719136 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.2512847  0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024\n",
      " 0.3067626  0.28977326 0.25352807 0.31098952 0.31098952 0.30595647\n",
      " 0.27219136 0.14719136 0.31220058 0.31098952 0.30665091 0.27219136\n",
      " 0.14719136 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.2512847  0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024\n",
      " 0.30522888 0.28977326 0.25235847 0.30522888 0.30522888 0.30665091\n",
      " 0.27643848 0.14719136 0.30797366 0.30253024 0.30665091 0.27643848\n",
      " 0.14719136 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.2512847  0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024\n",
      " 0.30665091 0.28822565 0.25202924 0.30665091 0.30598646 0.30665091\n",
      " 0.27291963 0.23433723 0.30797366 0.30253024 0.30665091 0.27291963\n",
      " 0.23433723 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.2512847  0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25221106 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25221106 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.2512847  0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.2512847  0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.2512847  0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.2512847  0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.2512847  0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.2512847  0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.2512847  0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.2512847  0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.2512847  0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.2512847  0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.2512847  0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.2512847  0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.2512847  0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.2512847  0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.2512847  0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.2512847  0.30797366 0.30253024]\n",
      "One or more of the train scores are non-finite: [0.30421616 0.29448011 0.26578504 0.30421616 0.30421616 0.30421616\n",
      " 0.12288719 0.14740242 0.30421616 0.30421616 0.30604687 0.12288719\n",
      " 0.14740242 0.30649813 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29456925 0.26486848 0.30627022\n",
      " 0.30651448 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448\n",
      " 0.30421616 0.29459543 0.26578504 0.30421616 0.30421616 0.30421616\n",
      " 0.12288719 0.14740242 0.30421616 0.30421616 0.30604687 0.12288719\n",
      " 0.14740242 0.30649813 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29456925 0.26486848 0.30627022\n",
      " 0.30651448 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448\n",
      " 0.30479255 0.29459543 0.26578504 0.30465601 0.30454464 0.30528731\n",
      " 0.25788719 0.14740242 0.30583764 0.30454464 0.30604687 0.25788719\n",
      " 0.14740242 0.30649813 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29456925 0.26486848 0.30627022\n",
      " 0.30651448 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448\n",
      " 0.30548326 0.29470661 0.26576093 0.30548326 0.30539179 0.30604687\n",
      " 0.29733701 0.14740242 0.30649813 0.30651448 0.30604687 0.29733701\n",
      " 0.14740242 0.30649813 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29456925 0.26486848 0.30627022\n",
      " 0.30651448 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448\n",
      " 0.30593539 0.29422785 0.26539888 0.30604968 0.30570789 0.30604687\n",
      " 0.28949607 0.34012919 0.30627022 0.30651448 0.30604687 0.28949607\n",
      " 0.34012919 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29456925 0.26486848 0.30627022\n",
      " 0.30651448 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448\n",
      " 0.30593539 0.29456925 0.26495644 0.30627022 0.30640157 0.30593539\n",
      " 0.29456925 0.26759559 0.30627022 0.30651448 0.30593539 0.29456925\n",
      " 0.26759559 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29456925 0.26486848 0.30627022\n",
      " 0.30651448 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448\n",
      " 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448 0.30593539\n",
      " 0.29456925 0.26573892 0.30627022 0.30651448 0.30593539 0.29456925\n",
      " 0.26573892 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29456925 0.26486848 0.30627022\n",
      " 0.30651448 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448\n",
      " 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448 0.30593539\n",
      " 0.29456925 0.26486848 0.30627022 0.30651448 0.30593539 0.29456925\n",
      " 0.26486848 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29456925 0.26486848 0.30627022\n",
      " 0.30651448 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448\n",
      " 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448 0.30593539\n",
      " 0.29456925 0.26486848 0.30627022 0.30651448 0.30593539 0.29456925\n",
      " 0.26486848 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29456925 0.26486848 0.30627022\n",
      " 0.30651448 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448\n",
      " 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448 0.30593539\n",
      " 0.29456925 0.26486848 0.30627022 0.30651448 0.30593539 0.29456925\n",
      " 0.26486848 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29456925 0.26486848 0.30627022\n",
      " 0.30651448 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448\n",
      " 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448 0.30593539\n",
      " 0.29456925 0.26486848 0.30627022 0.30651448 0.30593539 0.29456925\n",
      " 0.26486848 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29456925 0.26486848 0.30627022\n",
      " 0.30651448 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448\n",
      " 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448 0.30593539\n",
      " 0.29456925 0.26486848 0.30627022 0.30651448 0.30593539 0.29456925\n",
      " 0.26486848 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29456925 0.26486848 0.30627022\n",
      " 0.30651448 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448\n",
      " 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448 0.30593539\n",
      " 0.29456925 0.26486848 0.30627022 0.30651448 0.30593539 0.29456925\n",
      " 0.26486848 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29456925 0.26486848 0.30627022\n",
      " 0.30651448 0.30593539 0.29456925 0.26486848 0.30627022 0.30651448]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.0001, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.31220058120792593\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.0001, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.6059850374064838\n",
      "GridSearchCV Runtime: 4.2533860206604 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.34523809523809523\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.26595744680851063\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.34782608695652173\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3300970873786408\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3118279569892473\n",
      "Ave Test Precision: 0.32018933467420313\n",
      "Stdev Test Precision: 0.033542526531335014\n",
      "Ave Test Accuracy: 0.6283582089552239\n",
      "Stdev Test Accuracy: 0.026463616755965876\n",
      "Ave Test Specificity: 0.6861386138613862\n",
      "Ave Test Recall: 0.4515151515151515\n",
      "Ave Test NPV: 0.7929462152040863\n",
      "Ave Test F1-Score: 0.3742753391850138\n",
      "Ave Test G-mean: 0.5557923962795617\n",
      "Ave Runtime: 0.006184530258178711\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with internet. 58 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.29904762 0.37595238 0.09969136 0.29904762 0.29904762 0.29904762\n",
      " 0.         0.09969136 0.29904762 0.29904762 0.29904762 0.\n",
      " 0.09969136 0.30904762 0.29904762        nan        nan        nan\n",
      "        nan        nan 0.27603175 0.31904762 0.33540866 0.2831746\n",
      " 0.29904762 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762\n",
      " 0.29904762 0.37595238 0.09969136 0.29904762 0.29904762 0.29904762\n",
      " 0.         0.09969136 0.29904762 0.29904762 0.29904762 0.\n",
      " 0.09969136 0.30904762 0.29904762        nan        nan        nan\n",
      "        nan        nan 0.27603175 0.31904762 0.33540866 0.2831746\n",
      " 0.29904762 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762\n",
      " 0.29904762 0.37595238 0.09969136 0.29904762 0.29904762 0.29904762\n",
      " 0.1        0.09969136 0.30904762 0.29904762 0.29904762 0.1\n",
      " 0.09969136 0.30904762 0.29904762        nan        nan        nan\n",
      "        nan        nan 0.27603175 0.31904762 0.33540866 0.2831746\n",
      " 0.29904762 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762\n",
      " 0.29904762 0.37595238 0.09969136 0.29904762 0.29904762 0.29904762\n",
      " 0.28333333 0.09969136 0.29904762 0.29904762 0.29904762 0.28333333\n",
      " 0.09969136 0.29904762 0.29904762        nan        nan        nan\n",
      "        nan        nan 0.27603175 0.31904762 0.33540866 0.2831746\n",
      " 0.29904762 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762\n",
      " 0.29904762 0.37761905 0.09874199 0.29904762 0.29904762 0.29904762\n",
      " 0.40928571 0.09874199 0.29904762 0.29904762 0.29904762 0.40928571\n",
      " 0.09874199 0.29904762 0.29904762        nan        nan        nan\n",
      "        nan        nan 0.27603175 0.31904762 0.33540866 0.2831746\n",
      " 0.29904762 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762\n",
      " 0.29190476 0.30904762 0.14874199 0.29904762 0.29904762 0.29190476\n",
      " 0.30904762 0.09874199 0.29904762 0.29904762 0.29190476 0.29404762\n",
      " 0.09874199 0.29904762 0.29904762        nan        nan        nan\n",
      "        nan        nan 0.27603175 0.31904762 0.33540866 0.2831746\n",
      " 0.29904762 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762\n",
      " 0.27603175 0.31904762 0.28207532 0.2831746  0.29904762 0.27603175\n",
      " 0.31904762 0.28207532 0.2831746  0.29904762 0.27603175 0.31904762\n",
      " 0.28207532 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.27603175 0.31904762 0.33540866 0.2831746\n",
      " 0.29904762 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762\n",
      " 0.27603175 0.31904762 0.34040866 0.2831746  0.29904762 0.27603175\n",
      " 0.31904762 0.34040866 0.2831746  0.29904762 0.27603175 0.31904762\n",
      " 0.34040866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.27603175 0.31904762 0.33540866 0.2831746\n",
      " 0.29904762 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762\n",
      " 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762 0.27603175\n",
      " 0.31904762 0.33540866 0.2831746  0.29904762 0.27603175 0.31904762\n",
      " 0.33540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.27603175 0.31904762 0.33540866 0.2831746\n",
      " 0.29904762 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762\n",
      " 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762 0.27603175\n",
      " 0.31904762 0.33540866 0.2831746  0.29904762 0.27603175 0.31904762\n",
      " 0.33540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.27603175 0.31904762 0.33540866 0.2831746\n",
      " 0.29904762 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762\n",
      " 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762 0.27603175\n",
      " 0.31904762 0.33540866 0.2831746  0.29904762 0.27603175 0.31904762\n",
      " 0.33540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.27603175 0.31904762 0.33540866 0.2831746\n",
      " 0.29904762 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762\n",
      " 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762 0.27603175\n",
      " 0.31904762 0.33540866 0.2831746  0.29904762 0.27603175 0.31904762\n",
      " 0.33540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.27603175 0.31904762 0.33540866 0.2831746\n",
      " 0.29904762 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762\n",
      " 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762 0.27603175\n",
      " 0.31904762 0.33540866 0.2831746  0.29904762 0.27603175 0.31904762\n",
      " 0.33540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.27603175 0.31904762 0.33540866 0.2831746\n",
      " 0.29904762 0.27603175 0.31904762 0.33540866 0.2831746  0.29904762]\n",
      "One or more of the train scores are non-finite: [0.33676665 0.36760226 0.69809494 0.33676665 0.33676665 0.33676665\n",
      " 0.         0.09809494 0.33676665 0.33676665 0.3375537  0.\n",
      " 0.09809494 0.33715043 0.33678325        nan        nan        nan\n",
      "        nan        nan 0.32503055 0.33417494 0.35264673 0.32837576\n",
      " 0.3223538  0.32503055 0.33417494 0.35264673 0.32837576 0.3223538\n",
      " 0.33676665 0.36760226 0.69809494 0.33676665 0.33676665 0.33676665\n",
      " 0.         0.09809494 0.33676665 0.33676665 0.3375537  0.\n",
      " 0.09809494 0.33715043 0.33678325        nan        nan        nan\n",
      "        nan        nan 0.32503055 0.33417494 0.35264673 0.32837576\n",
      " 0.3223538  0.32503055 0.33417494 0.35264673 0.32837576 0.3223538\n",
      " 0.33676665 0.36760226 0.69809494 0.33676665 0.33676665 0.33693686\n",
      " 0.9        0.09809494 0.33715043 0.33676665 0.3375537  0.9\n",
      " 0.09809494 0.33715043 0.33678325        nan        nan        nan\n",
      "        nan        nan 0.32503055 0.33417494 0.35264673 0.32837576\n",
      " 0.3223538  0.32503055 0.33417494 0.35264673 0.32837576 0.3223538\n",
      " 0.33676665 0.36383033 0.69809494 0.33676665 0.33676665 0.3375537\n",
      " 0.65656566 0.09809494 0.33776078 0.33678325 0.3375537  0.65656566\n",
      " 0.09809494 0.33776078 0.33678325        nan        nan        nan\n",
      "        nan        nan 0.32503055 0.33417494 0.35264673 0.32837576\n",
      " 0.3223538  0.32503055 0.33417494 0.35264673 0.32837576 0.3223538\n",
      " 0.33738349 0.36574518 0.69809494 0.33738349 0.33676665 0.33738349\n",
      " 0.36483112 0.69809494 0.33738349 0.33676665 0.33738349 0.36483112\n",
      " 0.69809494 0.33738349 0.33676665        nan        nan        nan\n",
      "        nan        nan 0.32503055 0.33417494 0.35264673 0.32837576\n",
      " 0.3223538  0.32503055 0.33417494 0.35264673 0.32837576 0.3223538\n",
      " 0.33381819 0.33419986 0.40809494 0.33316006 0.33246627 0.33185741\n",
      " 0.33200206 0.56476161 0.33316006 0.33246627 0.33185741 0.3345266\n",
      " 0.56476161 0.33316006 0.33246627        nan        nan        nan\n",
      "        nan        nan 0.32503055 0.33417494 0.35264673 0.32837576\n",
      " 0.3223538  0.32503055 0.33417494 0.35264673 0.32837576 0.3223538\n",
      " 0.32503055 0.33496762 0.38354949 0.32837576 0.32295712 0.32503055\n",
      " 0.33496762 0.38021616 0.32837576 0.3223538  0.32503055 0.3358387\n",
      " 0.38021616 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.32503055 0.33417494 0.35264673 0.32837576\n",
      " 0.3223538  0.32503055 0.33417494 0.35264673 0.32837576 0.3223538\n",
      " 0.32503055 0.33417494 0.34749657 0.32837576 0.3223538  0.32503055\n",
      " 0.33417494 0.34749657 0.32837576 0.3223538  0.32503055 0.33496762\n",
      " 0.34749657 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.32503055 0.33417494 0.35264673 0.32837576\n",
      " 0.3223538  0.32503055 0.33417494 0.35264673 0.32837576 0.3223538\n",
      " 0.32503055 0.33417494 0.35264673 0.32837576 0.3223538  0.32503055\n",
      " 0.33417494 0.35264673 0.32837576 0.3223538  0.32503055 0.33417494\n",
      " 0.35264673 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.32503055 0.33417494 0.35264673 0.32837576\n",
      " 0.3223538  0.32503055 0.33417494 0.35264673 0.32837576 0.3223538\n",
      " 0.32503055 0.33417494 0.35264673 0.32837576 0.3223538  0.32503055\n",
      " 0.33417494 0.35264673 0.32837576 0.3223538  0.32503055 0.33417494\n",
      " 0.35264673 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.32503055 0.33417494 0.35264673 0.32837576\n",
      " 0.3223538  0.32503055 0.33417494 0.35264673 0.32837576 0.3223538\n",
      " 0.32503055 0.33417494 0.35264673 0.32837576 0.3223538  0.32503055\n",
      " 0.33417494 0.35264673 0.32837576 0.3223538  0.32503055 0.33417494\n",
      " 0.35264673 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.32503055 0.33417494 0.35264673 0.32837576\n",
      " 0.3223538  0.32503055 0.33417494 0.35264673 0.32837576 0.3223538\n",
      " 0.32503055 0.33417494 0.35264673 0.32837576 0.3223538  0.32503055\n",
      " 0.33417494 0.35264673 0.32837576 0.3223538  0.32503055 0.33417494\n",
      " 0.35264673 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.32503055 0.33417494 0.35264673 0.32837576\n",
      " 0.3223538  0.32503055 0.33417494 0.35264673 0.32837576 0.3223538\n",
      " 0.32503055 0.33417494 0.35264673 0.32837576 0.3223538  0.32503055\n",
      " 0.33417494 0.35264673 0.32837576 0.3223538  0.32503055 0.33417494\n",
      " 0.35264673 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.32503055 0.33417494 0.35264673 0.32837576\n",
      " 0.3223538  0.32503055 0.33417494 0.35264673 0.32837576 0.3223538 ]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.40928571428571425\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7369077306733167\n",
      "GridSearchCV Runtime: 4.5854833126068115 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.29411764705882354\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.625\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 1.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.47619047619047616\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3333333333333333\n",
      "Ave Test Precision: 0.5457282913165267\n",
      "Stdev Test Precision: 0.28543906150033305\n",
      "Ave Test Accuracy: 0.7470149253731344\n",
      "Stdev Test Accuracy: 0.01891610724721874\n",
      "Ave Test Specificity: 0.9603960396039604\n",
      "Ave Test Recall: 0.09393939393939395\n",
      "Ave Test NPV: 0.7643842395111005\n",
      "Ave Test F1-Score: 0.15214147496656846\n",
      "Ave Test G-mean: 0.295799345376871\n",
      "Ave Runtime: 0.00840296745300293\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with vehicleLoan. 57 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.42916667 0.4        0.19969136 0.42916667 0.42916667 0.42916667\n",
      " 0.         0.09969136 0.42916667 0.42916667 0.42916667 0.\n",
      " 0.09969136 0.37916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4        0.19969136 0.42916667 0.42916667 0.42916667\n",
      " 0.         0.09969136 0.42916667 0.42916667 0.42916667 0.\n",
      " 0.09969136 0.37916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4        0.19969136 0.42916667 0.42916667 0.42916667\n",
      " 0.1        0.09969136 0.37916667 0.42916667 0.42916667 0.1\n",
      " 0.09969136 0.37916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4        0.19969136 0.42916667 0.42916667 0.42916667\n",
      " 0.46       0.09969136 0.37916667 0.42916667 0.42916667 0.46\n",
      " 0.09969136 0.37916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4        0.29969136 0.42916667 0.42916667 0.42916667\n",
      " 0.4        0.09969136 0.42916667 0.42916667 0.42916667 0.4\n",
      " 0.09969136 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.45       0.35874199 0.42916667 0.42916667 0.42916667\n",
      " 0.45       0.24874199 0.42916667 0.42916667 0.42916667 0.45\n",
      " 0.24874199 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.34207532 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.34110161 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.34110161 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667]\n",
      "One or more of the train scores are non-finite: [0.40663038 0.49242746 0.69809494 0.40663038 0.40663038 0.40663038\n",
      " 0.         0.09809494 0.40663038 0.40663038 0.40663038 0.\n",
      " 0.09809494 0.4359584  0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.49242746 0.69809494 0.40663038 0.40663038 0.40663038\n",
      " 0.         0.09809494 0.40663038 0.40663038 0.40663038 0.\n",
      " 0.09809494 0.4359584  0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.49242746 0.69809494 0.40663038 0.40663038 0.40663038\n",
      " 0.8        0.09809494 0.4359584  0.40663038 0.40663038 0.8\n",
      " 0.09809494 0.4359584  0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.49242746 0.69809494 0.40663038 0.40663038 0.40663038\n",
      " 0.63547053 0.09809494 0.43050386 0.40663038 0.40663038 0.63547053\n",
      " 0.09809494 0.43050386 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.47419124 0.5052378  0.40663038 0.40663038 0.40663038\n",
      " 0.49743999 0.69809494 0.40663038 0.40663038 0.40663038 0.49743999\n",
      " 0.69809494 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.45368924 0.4667004  0.40663038 0.40663038 0.40663038\n",
      " 0.45783002 0.4780786  0.40663038 0.40663038 0.40663038 0.45783002\n",
      " 0.4780786  0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.44393896 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.45005007 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.45005007 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.45999999999999996\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7593516209476309\n",
      "GridSearchCV Runtime: 4.4202492237091064 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.5\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 1.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25\n",
      "Ave Test Precision: 0.4833333333333333\n",
      "Stdev Test Precision: 0.30276503540974914\n",
      "Ave Test Accuracy: 0.7507462686567165\n",
      "Stdev Test Accuracy: 0.004865076421793002\n",
      "Ave Test Specificity: 0.9900990099009901\n",
      "Ave Test Recall: 0.01818181818181818\n",
      "Ave Test NPV: 0.7552846911240231\n",
      "Ave Test F1-Score: 0.034475000469580006\n",
      "Ave Test G-mean: 0.13257561126489192\n",
      "Ave Runtime: 0.007851457595825196\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with informalLenders. 56 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.2399706  0.03333333 0.09806798 0.24459398 0.2646733  0.2399706\n",
      " 0.         0.09969136 0.24459398 0.2646733  0.2399706  0.\n",
      " 0.09969136 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.2399706  0.03333333 0.09766433 0.24459398\n",
      " 0.2646733  0.2399706  0.03333333 0.09766433 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09806798 0.24459398 0.2646733  0.2399706\n",
      " 0.         0.09969136 0.24459398 0.2646733  0.2399706  0.\n",
      " 0.09969136 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.2399706  0.03333333 0.09766433 0.24459398\n",
      " 0.2646733  0.2399706  0.03333333 0.09766433 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09806798 0.24459398 0.2646733  0.2399706\n",
      " 0.         0.09969136 0.24459398 0.2646733  0.2399706  0.\n",
      " 0.09969136 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.2399706  0.03333333 0.09766433 0.24459398\n",
      " 0.2646733  0.2399706  0.03333333 0.09766433 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09806798 0.24459398 0.2646733  0.2399706\n",
      " 0.04       0.09969136 0.24459398 0.2646733  0.2399706  0.04\n",
      " 0.09969136 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.2399706  0.03333333 0.09766433 0.24459398\n",
      " 0.2646733  0.2399706  0.03333333 0.09766433 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09806798 0.24459398 0.2646733  0.2399706\n",
      " 0.03333333 0.09874199 0.24459398 0.2646733  0.2399706  0.03333333\n",
      " 0.09874199 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.2399706  0.03333333 0.09766433 0.24459398\n",
      " 0.2646733  0.2399706  0.03333333 0.09766433 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09735802 0.24459398 0.2646733  0.2399706\n",
      " 0.03333333 0.09806798 0.24459398 0.2646733  0.2399706  0.03333333\n",
      " 0.09806798 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.2399706  0.03333333 0.09766433 0.24459398\n",
      " 0.2646733  0.2399706  0.03333333 0.09766433 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09766433 0.24459398 0.2646733  0.2399706\n",
      " 0.03333333 0.09735802 0.24459398 0.2646733  0.2399706  0.03333333\n",
      " 0.09735802 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.2399706  0.03333333 0.09766433 0.24459398\n",
      " 0.2646733  0.2399706  0.03333333 0.09766433 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09766433 0.24459398 0.2646733  0.2399706\n",
      " 0.03333333 0.09766433 0.24459398 0.2646733  0.2399706  0.03333333\n",
      " 0.09766433 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.2399706  0.03333333 0.09766433 0.24459398\n",
      " 0.2646733  0.2399706  0.03333333 0.09766433 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09766433 0.24459398 0.2646733  0.2399706\n",
      " 0.03333333 0.09766433 0.24459398 0.2646733  0.2399706  0.03333333\n",
      " 0.09766433 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.2399706  0.03333333 0.09766433 0.24459398\n",
      " 0.2646733  0.2399706  0.03333333 0.09766433 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09766433 0.24459398 0.2646733  0.2399706\n",
      " 0.03333333 0.09766433 0.24459398 0.2646733  0.2399706  0.03333333\n",
      " 0.09766433 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.2399706  0.03333333 0.09766433 0.24459398\n",
      " 0.2646733  0.2399706  0.03333333 0.09766433 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09766433 0.24459398 0.2646733  0.2399706\n",
      " 0.03333333 0.09766433 0.24459398 0.2646733  0.2399706  0.03333333\n",
      " 0.09766433 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.2399706  0.03333333 0.09766433 0.24459398\n",
      " 0.2646733  0.2399706  0.03333333 0.09766433 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09766433 0.24459398 0.2646733  0.2399706\n",
      " 0.03333333 0.09766433 0.24459398 0.2646733  0.2399706  0.03333333\n",
      " 0.09766433 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.2399706  0.03333333 0.09766433 0.24459398\n",
      " 0.2646733  0.2399706  0.03333333 0.09766433 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09766433 0.24459398 0.2646733  0.2399706\n",
      " 0.03333333 0.09766433 0.24459398 0.2646733  0.2399706  0.03333333\n",
      " 0.09766433 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.2399706  0.03333333 0.09766433 0.24459398\n",
      " 0.2646733  0.2399706  0.03333333 0.09766433 0.24459398 0.2646733 ]\n",
      "One or more of the train scores are non-finite: [0.25401405 0.10640437 0.39797966 0.25715111 0.2509052  0.25401405\n",
      " 0.         0.09809494 0.25715111 0.2509052  0.25401405 0.\n",
      " 0.09809494 0.2522688  0.25124006        nan        nan        nan\n",
      "        nan        nan 0.25401405 0.10640437 0.45661938 0.25557798\n",
      " 0.25124006 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.39797966 0.25715111 0.2509052  0.25401405\n",
      " 0.         0.09809494 0.25715111 0.2509052  0.25401405 0.\n",
      " 0.09809494 0.2522688  0.25124006        nan        nan        nan\n",
      "        nan        nan 0.25401405 0.10640437 0.45661938 0.25557798\n",
      " 0.25124006 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.39797966 0.25715111 0.2509052  0.25401405\n",
      " 0.1        0.09809494 0.25715111 0.2509052  0.25401405 0.1\n",
      " 0.09809494 0.2522688  0.25124006        nan        nan        nan\n",
      "        nan        nan 0.25401405 0.10640437 0.45661938 0.25557798\n",
      " 0.25124006 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.39804774 0.25715111 0.2509052  0.25401405\n",
      " 0.07908623 0.09809494 0.2522688  0.25101686 0.25401405 0.07908623\n",
      " 0.09809494 0.2522688  0.25124006        nan        nan        nan\n",
      "        nan        nan 0.25401405 0.10640437 0.45661938 0.25557798\n",
      " 0.25124006 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.46467847 0.25715111 0.2509052  0.25401405\n",
      " 0.10886726 0.14791657 0.25393547 0.2511284  0.25401405 0.10886726\n",
      " 0.14791657 0.25393547 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.25401405 0.10640437 0.45661938 0.25557798\n",
      " 0.25124006 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.46470746 0.25557798 0.2511284  0.25401405\n",
      " 0.10640437 0.45484911 0.25557798 0.2511284  0.25401405 0.10640437\n",
      " 0.45484911 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.25401405 0.10640437 0.45661938 0.25557798\n",
      " 0.25124006 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.456513   0.25557798 0.25124006 0.25401405\n",
      " 0.10640437 0.456513   0.25557798 0.25124006 0.25401405 0.10640437\n",
      " 0.456513   0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.25401405 0.10640437 0.45661938 0.25557798\n",
      " 0.25124006 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006 0.25401405\n",
      " 0.10640437 0.45661938 0.25557798 0.25124006 0.25401405 0.10640437\n",
      " 0.45661938 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.25401405 0.10640437 0.45661938 0.25557798\n",
      " 0.25124006 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006 0.25401405\n",
      " 0.10640437 0.45661938 0.25557798 0.25124006 0.25401405 0.10640437\n",
      " 0.45661938 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.25401405 0.10640437 0.45661938 0.25557798\n",
      " 0.25124006 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006 0.25401405\n",
      " 0.10640437 0.45661938 0.25557798 0.25124006 0.25401405 0.10640437\n",
      " 0.45661938 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.25401405 0.10640437 0.45661938 0.25557798\n",
      " 0.25124006 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006 0.25401405\n",
      " 0.10640437 0.45661938 0.25557798 0.25124006 0.25401405 0.10640437\n",
      " 0.45661938 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.25401405 0.10640437 0.45661938 0.25557798\n",
      " 0.25124006 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006 0.25401405\n",
      " 0.10640437 0.45661938 0.25557798 0.25124006 0.25401405 0.10640437\n",
      " 0.45661938 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.25401405 0.10640437 0.45661938 0.25557798\n",
      " 0.25124006 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006 0.25401405\n",
      " 0.10640437 0.45661938 0.25557798 0.25124006 0.25401405 0.10640437\n",
      " 0.45661938 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.25401405 0.10640437 0.45661938 0.25557798\n",
      " 0.25124006 0.25401405 0.10640437 0.45661938 0.25557798 0.25124006]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2646733036185091\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7231920199501247\n",
      "GridSearchCV Runtime: 4.454787492752075 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.15\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2388663967611336\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25301204819277107\n",
      "Ave Test Precision: 0.21837568899078094\n",
      "Stdev Test Precision: 0.04368221698058891\n",
      "Ave Test Accuracy: 0.4544776119402985\n",
      "Stdev Test Accuracy: 0.23423505605187847\n",
      "Ave Test Specificity: 0.4138613861386139\n",
      "Ave Test Recall: 0.5787878787878789\n",
      "Ave Test NPV: 0.7636455628105071\n",
      "Ave Test F1-Score: 0.26341294722814\n",
      "Ave Test G-mean: 0.2365069405488125\n",
      "Ave Runtime: 0.005787992477416992\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with companyLoan. 55 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.38190476 0.29       0.23333333 0.38190476 0.38190476 0.38190476\n",
      " 0.         0.         0.38190476 0.38190476 0.35690476 0.\n",
      " 0.         0.35690476 0.35690476        nan        nan        nan\n",
      "        nan        nan 0.38190476 0.38166667 0.18333333 0.35690476\n",
      " 0.38190476 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476\n",
      " 0.38190476 0.29       0.23333333 0.38190476 0.38190476 0.38190476\n",
      " 0.         0.         0.38190476 0.38190476 0.35690476 0.\n",
      " 0.         0.35690476 0.35690476        nan        nan        nan\n",
      "        nan        nan 0.38190476 0.38166667 0.18333333 0.35690476\n",
      " 0.38190476 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476\n",
      " 0.38190476 0.29       0.23333333 0.38190476 0.38190476 0.38190476\n",
      " 0.         0.         0.38190476 0.38190476 0.35690476 0.\n",
      " 0.         0.35690476 0.35690476        nan        nan        nan\n",
      "        nan        nan 0.38190476 0.38166667 0.18333333 0.35690476\n",
      " 0.38190476 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476\n",
      " 0.38190476 0.34       0.23333333 0.38190476 0.38190476 0.35690476\n",
      " 0.20333333 0.         0.35690476 0.35690476 0.35690476 0.20333333\n",
      " 0.         0.35690476 0.35690476        nan        nan        nan\n",
      "        nan        nan 0.38190476 0.38166667 0.18333333 0.35690476\n",
      " 0.38190476 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476\n",
      " 0.38190476 0.37333333 0.20833333 0.35690476 0.38190476 0.35690476\n",
      " 0.33       0.4        0.35690476 0.38190476 0.35690476 0.33\n",
      " 0.4        0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.38190476 0.38166667 0.18333333 0.35690476\n",
      " 0.38190476 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476\n",
      " 0.37333333 0.18666667 0.35690476 0.38190476 0.38190476 0.37333333\n",
      " 0.18666667 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.38190476 0.38166667 0.18333333 0.35690476\n",
      " 0.38190476 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476\n",
      " 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476 0.38166667\n",
      " 0.18333333 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.38190476 0.38166667 0.18333333 0.35690476\n",
      " 0.38190476 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476\n",
      " 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476 0.38166667\n",
      " 0.18333333 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.38190476 0.38166667 0.18333333 0.35690476\n",
      " 0.38190476 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476\n",
      " 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476 0.38166667\n",
      " 0.18333333 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.38190476 0.38166667 0.18333333 0.35690476\n",
      " 0.38190476 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476\n",
      " 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476 0.38166667\n",
      " 0.18333333 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.38190476 0.38166667 0.18333333 0.35690476\n",
      " 0.38190476 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476\n",
      " 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476 0.38166667\n",
      " 0.18333333 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.38190476 0.38166667 0.18333333 0.35690476\n",
      " 0.38190476 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476\n",
      " 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476 0.38166667\n",
      " 0.18333333 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.38190476 0.38166667 0.18333333 0.35690476\n",
      " 0.38190476 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476\n",
      " 0.38166667 0.18333333 0.35690476 0.38190476 0.38190476 0.38166667\n",
      " 0.18333333 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.38190476 0.38166667 0.18333333 0.35690476\n",
      " 0.38190476 0.38190476 0.38166667 0.18333333 0.35690476 0.38190476]\n",
      "One or more of the train scores are non-finite: [0.36193696 0.35424498 0.3946287  0.36193696 0.36193696 0.36193696\n",
      " 0.         0.         0.36193696 0.36193696 0.35872259 0.\n",
      " 0.         0.35652595 0.3589305         nan        nan        nan\n",
      "        nan        nan 0.3596441  0.36830088 0.36189026 0.3570888\n",
      " 0.36128761 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761\n",
      " 0.36193696 0.35424498 0.3946287  0.36193696 0.36193696 0.36193696\n",
      " 0.         0.         0.36193696 0.36193696 0.35872259 0.\n",
      " 0.         0.35652595 0.3589305         nan        nan        nan\n",
      "        nan        nan 0.3596441  0.36830088 0.36189026 0.3570888\n",
      " 0.36128761 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761\n",
      " 0.36193696 0.35424498 0.3946287  0.36193696 0.36193696 0.3605099\n",
      " 0.1        0.         0.35791948 0.36193696 0.35872259 0.1\n",
      " 0.         0.35652595 0.3589305         nan        nan        nan\n",
      "        nan        nan 0.3596441  0.36830088 0.36189026 0.3570888\n",
      " 0.36128761 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761\n",
      " 0.36193696 0.35938784 0.39084083 0.36128761 0.36193696 0.35872259\n",
      " 0.34443747 0.         0.35652595 0.3589305  0.35872259 0.34443747\n",
      " 0.         0.35652595 0.3589305         nan        nan        nan\n",
      "        nan        nan 0.3596441  0.36830088 0.36189026 0.3570888\n",
      " 0.36128761 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761\n",
      " 0.3605099  0.35938784 0.38607708 0.35793962 0.36128761 0.36053003\n",
      " 0.33929245 0.54365079 0.3570888  0.36072476 0.36053003 0.33929245\n",
      " 0.54365079 0.3570888  0.36072476        nan        nan        nan\n",
      "        nan        nan 0.3596441  0.36830088 0.36189026 0.3570888\n",
      " 0.36128761 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.33960108 0.3570888  0.36128761 0.3596441\n",
      " 0.35938784 0.35378947 0.3570888  0.36072476 0.3596441  0.35938784\n",
      " 0.35378947 0.3570888  0.36072476        nan        nan        nan\n",
      "        nan        nan 0.3596441  0.36830088 0.36189026 0.3570888\n",
      " 0.36128761 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.36836979 0.3570888  0.36128761 0.3596441\n",
      " 0.36830088 0.36945204 0.3570888  0.36128761 0.3596441  0.36830088\n",
      " 0.36945204 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.3596441  0.36830088 0.36189026 0.3570888\n",
      " 0.36128761 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.3604632  0.3570888  0.36128761 0.3596441\n",
      " 0.36830088 0.3618466  0.3570888  0.36128761 0.3596441  0.36830088\n",
      " 0.3618466  0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.3596441  0.36830088 0.36189026 0.3570888\n",
      " 0.36128761 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761 0.3596441\n",
      " 0.36830088 0.36189026 0.3570888  0.36128761 0.3596441  0.36830088\n",
      " 0.36189026 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.3596441  0.36830088 0.36189026 0.3570888\n",
      " 0.36128761 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761 0.3596441\n",
      " 0.36830088 0.36189026 0.3570888  0.36128761 0.3596441  0.36830088\n",
      " 0.36189026 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.3596441  0.36830088 0.36189026 0.3570888\n",
      " 0.36128761 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761 0.3596441\n",
      " 0.36830088 0.36189026 0.3570888  0.36128761 0.3596441  0.36830088\n",
      " 0.36189026 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.3596441  0.36830088 0.36189026 0.3570888\n",
      " 0.36128761 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761 0.3596441\n",
      " 0.36830088 0.36189026 0.3570888  0.36128761 0.3596441  0.36830088\n",
      " 0.36189026 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.3596441  0.36830088 0.36189026 0.3570888\n",
      " 0.36128761 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761 0.3596441\n",
      " 0.36830088 0.36189026 0.3570888  0.36128761 0.3596441  0.36830088\n",
      " 0.36189026 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.3596441  0.36830088 0.36189026 0.3570888\n",
      " 0.36128761 0.3596441  0.36830088 0.36189026 0.3570888  0.36128761]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.4\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7518703241895262\n",
      "GridSearchCV Runtime: 4.379002571105957 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.0\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.0\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.06666666666666667\n",
      "Stdev Test Precision: 0.14907119849998599\n",
      "Ave Test Accuracy: 0.7492537313432835\n",
      "Stdev Test Accuracy: 0.006131222658639893\n",
      "Ave Test Specificity: 0.993069306930693\n",
      "Ave Test Recall: 0.0030303030303030303\n",
      "Ave Test NPV: 0.7529977312926569\n",
      "Ave Test F1-Score: 0.005797101449275362\n",
      "Ave Test G-mean: 0.0244961222645667\n",
      "Ave Runtime: 0.007677936553955078\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with privateLoans. 54 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.23011216 0.         0.12105594 0.23011216 0.22816313 0.23011216\n",
      " 0.         0.12188272 0.23011216 0.22816313 0.23380233 0.\n",
      " 0.12188272 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23011216 0.         0.12105594 0.23011216 0.22816313 0.23011216\n",
      " 0.         0.12188272 0.23011216 0.22816313 0.23380233 0.\n",
      " 0.12188272 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23011216 0.         0.12105594 0.23011216 0.22816313 0.23011216\n",
      " 0.         0.12188272 0.23011216 0.22816313 0.23380233 0.\n",
      " 0.12188272 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23011216 0.         0.12173174 0.23011216 0.22816313 0.23360101\n",
      " 0.         0.12091753 0.23331562 0.22503704 0.23380233 0.\n",
      " 0.12091753 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23331562 0.22693564 0.23380233\n",
      " 0.         0.12153481 0.23351695 0.22869198 0.23380233 0.\n",
      " 0.12153481 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22838855 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22906215 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925]\n",
      "One or more of the train scores are non-finite: [0.24752566 0.         0.31492336 0.24752566 0.24878257 0.24752566\n",
      " 0.         0.12292119 0.24752566 0.24878257 0.24889402 0.\n",
      " 0.12292119 0.24745294 0.25004889        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.2057855  0.24770359\n",
      " 0.25009048 0.24907515 0.         0.2057855  0.24770359 0.25009048\n",
      " 0.24752566 0.         0.31492336 0.24752566 0.24878257 0.24752566\n",
      " 0.         0.12292119 0.24752566 0.24878257 0.24889402 0.\n",
      " 0.12292119 0.24745294 0.25004889        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.2057855  0.24770359\n",
      " 0.25009048 0.24907515 0.         0.2057855  0.24770359 0.25009048\n",
      " 0.24752566 0.         0.31507314 0.24752566 0.24878257 0.24752566\n",
      " 0.         0.12292119 0.24752566 0.24878257 0.24889402 0.\n",
      " 0.12292119 0.24745294 0.25004889        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.2057855  0.24770359\n",
      " 0.25009048 0.24907515 0.         0.2057855  0.24770359 0.25009048\n",
      " 0.24731012 0.         0.21503751 0.24731012 0.24917162 0.24852749\n",
      " 0.         0.12277969 0.24689279 0.25000992 0.24889402 0.\n",
      " 0.12277969 0.24745294 0.25004889        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.2057855  0.24770359\n",
      " 0.25009048 0.24907515 0.         0.2057855  0.24770359 0.25009048\n",
      " 0.24879977 0.         0.28053718 0.24767236 0.24927375 0.24903122\n",
      " 0.         0.32259561 0.24770359 0.25004889 0.24907515 0.\n",
      " 0.32259561 0.24770359 0.25004889        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.2057855  0.24770359\n",
      " 0.25009048 0.24907515 0.         0.2057855  0.24770359 0.25009048\n",
      " 0.24903122 0.         0.2057146  0.24770359 0.24999717 0.24907515\n",
      " 0.         0.27231187 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.27231187 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.2057855  0.24770359\n",
      " 0.25009048 0.24907515 0.         0.2057855  0.24770359 0.25009048\n",
      " 0.24907515 0.         0.20575016 0.24770359 0.25009048 0.24907515\n",
      " 0.         0.22241682 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.22241682 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.2057855  0.24770359\n",
      " 0.25009048 0.24907515 0.         0.2057855  0.24770359 0.25009048\n",
      " 0.24907515 0.         0.2057855  0.24770359 0.25009048 0.24907515\n",
      " 0.         0.2057855  0.24770359 0.25009048 0.24907515 0.\n",
      " 0.20575016 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.2057855  0.24770359\n",
      " 0.25009048 0.24907515 0.         0.2057855  0.24770359 0.25009048\n",
      " 0.24907515 0.         0.2057855  0.24770359 0.25009048 0.24907515\n",
      " 0.         0.2057855  0.24770359 0.25009048 0.24907515 0.\n",
      " 0.2057855  0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.2057855  0.24770359\n",
      " 0.25009048 0.24907515 0.         0.2057855  0.24770359 0.25009048\n",
      " 0.24907515 0.         0.2057855  0.24770359 0.25009048 0.24907515\n",
      " 0.         0.2057855  0.24770359 0.25009048 0.24907515 0.\n",
      " 0.2057855  0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.2057855  0.24770359\n",
      " 0.25009048 0.24907515 0.         0.2057855  0.24770359 0.25009048\n",
      " 0.24907515 0.         0.2057855  0.24770359 0.25009048 0.24907515\n",
      " 0.         0.2057855  0.24770359 0.25009048 0.24907515 0.\n",
      " 0.2057855  0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.2057855  0.24770359\n",
      " 0.25009048 0.24907515 0.         0.2057855  0.24770359 0.25009048\n",
      " 0.24907515 0.         0.2057855  0.24770359 0.25009048 0.24907515\n",
      " 0.         0.2057855  0.24770359 0.25009048 0.24907515 0.\n",
      " 0.2057855  0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.2057855  0.24770359\n",
      " 0.25009048 0.24907515 0.         0.2057855  0.24770359 0.25009048\n",
      " 0.24907515 0.         0.2057855  0.24770359 0.25009048 0.24907515\n",
      " 0.         0.2057855  0.24770359 0.25009048 0.24907515 0.\n",
      " 0.2057855  0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.2057855  0.24770359\n",
      " 0.25009048 0.24907515 0.         0.2057855  0.24770359 0.25009048]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.23380233332464367\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.3690773067331671\n",
      "GridSearchCV Runtime: 4.335151195526123 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25471698113207547\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.22033898305084745\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.26288659793814434\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.23696682464454977\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23214285714285715\n",
      "Ave Test Precision: 0.24141044878169488\n",
      "Stdev Test Precision: 0.017233176608540145\n",
      "Ave Test Accuracy: 0.4776119402985074\n",
      "Stdev Test Accuracy: 0.14702155451377272\n",
      "Ave Test Specificity: 0.4544554455445544\n",
      "Ave Test Recall: 0.5484848484848485\n",
      "Ave Test NPV: 0.7597442623758413\n",
      "Ave Test F1-Score: 0.3125844970728113\n",
      "Ave Test G-mean: 0.4146271054803149\n",
      "Ave Runtime: 0.0050052165985107425\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with governmentLoans. 53 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.32759019 0.17619048 0.09905033 0.32759019 0.32759019 0.32759019\n",
      " 0.         0.09969136 0.32759019 0.32759019 0.32759019 0.\n",
      " 0.09969136 0.33203463 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32759019 0.18619048 0.31738367 0.32759019\n",
      " 0.32759019 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019\n",
      " 0.32759019 0.17619048 0.09905033 0.32759019 0.32759019 0.32759019\n",
      " 0.         0.09969136 0.32759019 0.32759019 0.32759019 0.\n",
      " 0.09969136 0.33203463 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32759019 0.18619048 0.31738367 0.32759019\n",
      " 0.32759019 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019\n",
      " 0.32759019 0.17619048 0.09905033 0.32759019 0.32759019 0.32759019\n",
      " 0.         0.09969136 0.33203463 0.32759019 0.32759019 0.\n",
      " 0.09969136 0.33203463 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32759019 0.18619048 0.31738367 0.32759019\n",
      " 0.32759019 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019\n",
      " 0.32759019 0.16285714 0.09905033 0.32759019 0.32759019 0.32759019\n",
      " 0.05       0.09874199 0.33203463 0.32759019 0.32759019 0.05\n",
      " 0.09874199 0.33203463 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32759019 0.18619048 0.31738367 0.32759019\n",
      " 0.32759019 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019\n",
      " 0.32759019 0.15619048 0.09905033 0.32759019 0.32759019 0.32759019\n",
      " 0.17666667 0.09905033 0.32759019 0.32759019 0.32759019 0.17666667\n",
      " 0.09905033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32759019 0.18619048 0.31738367 0.32759019\n",
      " 0.32759019 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019\n",
      " 0.32759019 0.18619048 0.09905033 0.32759019 0.32759019 0.32759019\n",
      " 0.15285714 0.09905033 0.32759019 0.32759019 0.32759019 0.15285714\n",
      " 0.09905033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32759019 0.18619048 0.31738367 0.32759019\n",
      " 0.32759019 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019\n",
      " 0.32759019 0.18619048 0.32738367 0.32759019 0.32759019 0.32759019\n",
      " 0.18619048 0.32738367 0.32759019 0.32759019 0.32759019 0.18619048\n",
      " 0.32738367 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32759019 0.18619048 0.31738367 0.32759019\n",
      " 0.32759019 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019\n",
      " 0.32759019 0.18619048 0.31988367 0.32759019 0.32759019 0.32759019\n",
      " 0.18619048 0.31988367 0.32759019 0.32759019 0.32759019 0.18619048\n",
      " 0.31988367 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32759019 0.18619048 0.31738367 0.32759019\n",
      " 0.32759019 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019\n",
      " 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019 0.32759019\n",
      " 0.18619048 0.31738367 0.32759019 0.32759019 0.32759019 0.18619048\n",
      " 0.31738367 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32759019 0.18619048 0.31738367 0.32759019\n",
      " 0.32759019 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019\n",
      " 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019 0.32759019\n",
      " 0.18619048 0.31738367 0.32759019 0.32759019 0.32759019 0.18619048\n",
      " 0.31738367 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32759019 0.18619048 0.31738367 0.32759019\n",
      " 0.32759019 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019\n",
      " 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019 0.32759019\n",
      " 0.18619048 0.31738367 0.32759019 0.32759019 0.32759019 0.18619048\n",
      " 0.31738367 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32759019 0.18619048 0.31738367 0.32759019\n",
      " 0.32759019 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019\n",
      " 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019 0.32759019\n",
      " 0.18619048 0.31738367 0.32759019 0.32759019 0.32759019 0.18619048\n",
      " 0.31738367 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32759019 0.18619048 0.31738367 0.32759019\n",
      " 0.32759019 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019\n",
      " 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019 0.32759019\n",
      " 0.18619048 0.31738367 0.32759019 0.32759019 0.32759019 0.18619048\n",
      " 0.31738367 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32759019 0.18619048 0.31738367 0.32759019\n",
      " 0.32759019 0.32759019 0.18619048 0.31738367 0.32759019 0.32759019]\n",
      "One or more of the train scores are non-finite: [0.32446574 0.30635892 0.29801791 0.32446574 0.32446574 0.32446574\n",
      " 0.         0.09809494 0.32446574 0.32446574 0.32446574 0.\n",
      " 0.09809494 0.32787225 0.32638497        nan        nan        nan\n",
      "        nan        nan 0.32446574 0.30838527 0.3015087  0.32446574\n",
      " 0.32509431 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431\n",
      " 0.32446574 0.30635892 0.29801791 0.32446574 0.32446574 0.32446574\n",
      " 0.         0.09809494 0.32446574 0.32446574 0.32446574 0.\n",
      " 0.09809494 0.32787225 0.32638497        nan        nan        nan\n",
      "        nan        nan 0.32446574 0.30838527 0.3015087  0.32446574\n",
      " 0.32509431 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431\n",
      " 0.32446574 0.30635892 0.29801791 0.32446574 0.32446574 0.32446574\n",
      " 0.6        0.09809494 0.32733462 0.32446574 0.32446574 0.6\n",
      " 0.09809494 0.32787225 0.32638497        nan        nan        nan\n",
      "        nan        nan 0.32446574 0.30838527 0.3015087  0.32446574\n",
      " 0.32509431 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431\n",
      " 0.32446574 0.31026583 0.29801791 0.32446574 0.32446574 0.32446574\n",
      " 0.30712121 0.09809494 0.32739148 0.32638497 0.32446574 0.30712121\n",
      " 0.09809494 0.32739148 0.32638497        nan        nan        nan\n",
      "        nan        nan 0.32446574 0.30838527 0.3015087  0.32446574\n",
      " 0.32509431 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431\n",
      " 0.32446574 0.31251203 0.28135124 0.32446574 0.32446574 0.32446574\n",
      " 0.32100064 0.69816304 0.32491566 0.32593504 0.32446574 0.32100064\n",
      " 0.69816304 0.32491566 0.32593504        nan        nan        nan\n",
      "        nan        nan 0.32446574 0.30838527 0.3015087  0.32446574\n",
      " 0.32509431 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431\n",
      " 0.32446574 0.30959457 0.34028926 0.32446574 0.32455902 0.32446574\n",
      " 0.3059552  0.32637317 0.32446574 0.32503392 0.32446574 0.3059552\n",
      " 0.32637317 0.32446574 0.32503392        nan        nan        nan\n",
      "        nan        nan 0.32446574 0.30838527 0.3015087  0.32446574\n",
      " 0.32509431 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431\n",
      " 0.32446574 0.30838527 0.30023847 0.32446574 0.32509431 0.32446574\n",
      " 0.30838527 0.30142895 0.32446574 0.32509431 0.32446574 0.30838527\n",
      " 0.30142895 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.32446574 0.30838527 0.3015087  0.32446574\n",
      " 0.32509431 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431\n",
      " 0.32446574 0.30838527 0.29896303 0.32446574 0.32509431 0.32446574\n",
      " 0.30838527 0.29896303 0.32446574 0.32509431 0.32446574 0.30838527\n",
      " 0.29896303 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.32446574 0.30838527 0.3015087  0.32446574\n",
      " 0.32509431 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431\n",
      " 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431 0.32446574\n",
      " 0.30838527 0.3015087  0.32446574 0.32509431 0.32446574 0.30838527\n",
      " 0.3015087  0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.32446574 0.30838527 0.3015087  0.32446574\n",
      " 0.32509431 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431\n",
      " 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431 0.32446574\n",
      " 0.30838527 0.3015087  0.32446574 0.32509431 0.32446574 0.30838527\n",
      " 0.3015087  0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.32446574 0.30838527 0.3015087  0.32446574\n",
      " 0.32509431 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431\n",
      " 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431 0.32446574\n",
      " 0.30838527 0.3015087  0.32446574 0.32509431 0.32446574 0.30838527\n",
      " 0.3015087  0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.32446574 0.30838527 0.3015087  0.32446574\n",
      " 0.32509431 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431\n",
      " 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431 0.32446574\n",
      " 0.30838527 0.3015087  0.32446574 0.32509431 0.32446574 0.30838527\n",
      " 0.3015087  0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.32446574 0.30838527 0.3015087  0.32446574\n",
      " 0.32509431 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431\n",
      " 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431 0.32446574\n",
      " 0.30838527 0.3015087  0.32446574 0.32509431 0.32446574 0.30838527\n",
      " 0.3015087  0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.32446574 0.30838527 0.3015087  0.32446574\n",
      " 0.32509431 0.32446574 0.30838527 0.3015087  0.32446574 0.32509431]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.332034632034632\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.7219451371571073\n",
      "GridSearchCV Runtime: 4.390851020812988 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.17647058823529413\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.375\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3548387096774194\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.36363636363636365\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2631578947368421\n",
      "Ave Test Precision: 0.3066207112571838\n",
      "Stdev Test Precision: 0.0852636470885228\n",
      "Ave Test Accuracy: 0.7231343283582089\n",
      "Stdev Test Accuracy: 0.008089359473387608\n",
      "Ave Test Specificity: 0.9237623762376238\n",
      "Ave Test Recall: 0.1090909090909091\n",
      "Ave Test NPV: 0.7605155277471761\n",
      "Ave Test F1-Score: 0.1597117041959115\n",
      "Ave Test G-mean: 0.3099792244094661\n",
      "Ave Runtime: 0.0056021690368652345\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with smoking. 52 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.23617077 0.23434343 0.09874199 0.23617077 0.23617077 0.23617077\n",
      " 0.02375    0.09969136 0.23617077 0.23617077 0.23617077 0.02375\n",
      " 0.09969136 0.23617077 0.23617077        nan        nan        nan\n",
      "        nan        nan 0.23617077 0.24632035 0.24874199 0.23617077\n",
      " 0.23452242 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.23434343 0.09874199 0.23617077 0.23617077 0.23617077\n",
      " 0.02375    0.09969136 0.23617077 0.23617077 0.23617077 0.02375\n",
      " 0.09969136 0.23617077 0.23617077        nan        nan        nan\n",
      "        nan        nan 0.23617077 0.24632035 0.24874199 0.23617077\n",
      " 0.23452242 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.23434343 0.09874199 0.23617077 0.23617077 0.23617077\n",
      " 0.12375    0.09969136 0.23617077 0.23617077 0.23617077 0.12375\n",
      " 0.09969136 0.23617077 0.23617077        nan        nan        nan\n",
      "        nan        nan 0.23617077 0.24632035 0.24874199 0.23617077\n",
      " 0.23452242 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.23434343 0.09874199 0.23617077 0.23617077 0.23617077\n",
      " 0.24642857 0.09874199 0.23617077 0.23617077 0.23617077 0.24642857\n",
      " 0.09874199 0.23617077 0.23617077        nan        nan        nan\n",
      "        nan        nan 0.23617077 0.24632035 0.24874199 0.23617077\n",
      " 0.23452242 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.2260101  0.09874199 0.23617077 0.23617077 0.23617077\n",
      " 0.23910534 0.09874199 0.23617077 0.23617077 0.23617077 0.23910534\n",
      " 0.09874199 0.23617077 0.23617077        nan        nan        nan\n",
      "        nan        nan 0.23617077 0.24632035 0.24874199 0.23617077\n",
      " 0.23452242 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24632035 0.09874199 0.23617077 0.23617077 0.23617077\n",
      " 0.24126984 0.09874199 0.23617077 0.23452242 0.23617077 0.24126984\n",
      " 0.09874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.23617077 0.24632035 0.24874199 0.23617077\n",
      " 0.23452242 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077\n",
      " 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077 0.24632035\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.23617077 0.24632035 0.24874199 0.23617077\n",
      " 0.23452242 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077\n",
      " 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077 0.24632035\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.23617077 0.24632035 0.24874199 0.23617077\n",
      " 0.23452242 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077\n",
      " 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077 0.24632035\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.23617077 0.24632035 0.24874199 0.23617077\n",
      " 0.23452242 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077\n",
      " 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077 0.24632035\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.23617077 0.24632035 0.24874199 0.23617077\n",
      " 0.23452242 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077\n",
      " 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077 0.24632035\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.23617077 0.24632035 0.24874199 0.23617077\n",
      " 0.23452242 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077\n",
      " 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077 0.24632035\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.23617077 0.24632035 0.24874199 0.23617077\n",
      " 0.23452242 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077\n",
      " 0.24632035 0.24874199 0.23617077 0.23452242 0.23617077 0.24632035\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.23617077 0.24632035 0.24874199 0.23617077\n",
      " 0.23452242 0.23617077 0.24632035 0.24874199 0.23617077 0.23452242]\n",
      "One or more of the train scores are non-finite: [0.22268463 0.30410917 0.69770313 0.22268463 0.22268463 0.22268463\n",
      " 0.02465374 0.09809494 0.22268463 0.22268463 0.22326428 0.02465374\n",
      " 0.09809494 0.22283146 0.22375177        nan        nan        nan\n",
      "        nan        nan 0.22188705 0.24116661 0.62666031 0.2231764\n",
      " 0.22363311 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311\n",
      " 0.22268463 0.30410917 0.69770313 0.22268463 0.22268463 0.22268463\n",
      " 0.02465374 0.09809494 0.22268463 0.22268463 0.22326428 0.02465374\n",
      " 0.09809494 0.22283146 0.22375177        nan        nan        nan\n",
      "        nan        nan 0.22188705 0.24116661 0.62666031 0.2231764\n",
      " 0.22363311 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311\n",
      " 0.22268463 0.30410917 0.69770313 0.22297849 0.22268463 0.22297849\n",
      " 0.72465374 0.09809494 0.22311752 0.22297849 0.22326428 0.72465374\n",
      " 0.09809494 0.22283146 0.22375177        nan        nan        nan\n",
      "        nan        nan 0.22188705 0.24116661 0.62666031 0.2231764\n",
      " 0.22363311 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311\n",
      " 0.22297849 0.26648131 0.69770313 0.22346571 0.22297849 0.22326428\n",
      " 0.70070251 0.09809494 0.22283146 0.22350486 0.22326428 0.70070251\n",
      " 0.09809494 0.22283146 0.22375177        nan        nan        nan\n",
      "        nan        nan 0.22188705 0.24116661 0.62666031 0.2231764\n",
      " 0.22363311 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311\n",
      " 0.22270722 0.26255935 0.69749364 0.22346571 0.22323386 0.22297822\n",
      " 0.30566668 0.69767578 0.22283146 0.22350486 0.22297822 0.30566668\n",
      " 0.69767578 0.22283146 0.22350486        nan        nan        nan\n",
      "        nan        nan 0.22188705 0.24116661 0.62666031 0.2231764\n",
      " 0.22363311 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311\n",
      " 0.22214523 0.24878037 0.69749364 0.22283146 0.22319223 0.22214523\n",
      " 0.2507017  0.69749364 0.22283146 0.22413257 0.22214523 0.2507017\n",
      " 0.69749364 0.22283146 0.22413257        nan        nan        nan\n",
      "        nan        nan 0.22188705 0.24116661 0.62666031 0.2231764\n",
      " 0.22363311 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311\n",
      " 0.22214523 0.24116661 0.62666031 0.2231764  0.22363311 0.22214523\n",
      " 0.24116661 0.62666031 0.2231764  0.22363311 0.22214523 0.24116661\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22188705 0.24116661 0.62666031 0.2231764\n",
      " 0.22363311 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311\n",
      " 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311 0.22188705\n",
      " 0.24116661 0.62666031 0.2231764  0.22363311 0.22188705 0.24116661\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22188705 0.24116661 0.62666031 0.2231764\n",
      " 0.22363311 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311\n",
      " 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311 0.22188705\n",
      " 0.24116661 0.62666031 0.2231764  0.22363311 0.22188705 0.24116661\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22188705 0.24116661 0.62666031 0.2231764\n",
      " 0.22363311 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311\n",
      " 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311 0.22188705\n",
      " 0.24116661 0.62666031 0.2231764  0.22363311 0.22188705 0.24116661\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22188705 0.24116661 0.62666031 0.2231764\n",
      " 0.22363311 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311\n",
      " 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311 0.22188705\n",
      " 0.24116661 0.62666031 0.2231764  0.22363311 0.22188705 0.24116661\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22188705 0.24116661 0.62666031 0.2231764\n",
      " 0.22363311 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311\n",
      " 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311 0.22188705\n",
      " 0.24116661 0.62666031 0.2231764  0.22363311 0.22188705 0.24116661\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22188705 0.24116661 0.62666031 0.2231764\n",
      " 0.22363311 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311\n",
      " 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311 0.22188705\n",
      " 0.24116661 0.62666031 0.2231764  0.22363311 0.22188705 0.24116661\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22188705 0.24116661 0.62666031 0.2231764\n",
      " 0.22363311 0.22188705 0.24116661 0.62666031 0.2231764  0.22363311]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.2487419909360838\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.756857855361596\n",
      "GridSearchCV Runtime: 4.375006437301636 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.5\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.29411764705882354\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.5\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3333333333333333\n",
      "Ave Test Precision: 0.3254901960784314\n",
      "Stdev Test Precision: 0.20487615936624454\n",
      "Ave Test Accuracy: 0.7432835820895523\n",
      "Stdev Test Accuracy: 0.014306186203493094\n",
      "Ave Test Specificity: 0.9712871287128714\n",
      "Ave Test Recall: 0.045454545454545456\n",
      "Ave Test NPV: 0.7570103826792703\n",
      "Ave Test F1-Score: 0.07359121795789358\n",
      "Ave Test G-mean: 0.17542434945353558\n",
      "Ave Runtime: 0.007426071166992188\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with alcohol. 51 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.32333333 0.17       0.09969136 0.32333333 0.32333333 0.32333333\n",
      " 0.         0.09969136 0.32333333 0.32333333 0.32333333 0.\n",
      " 0.09969136 0.22333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.32333333 0.17       0.09969136 0.32333333 0.32333333 0.32333333\n",
      " 0.         0.09969136 0.32333333 0.32333333 0.32333333 0.\n",
      " 0.09969136 0.22333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.32333333 0.17       0.09969136 0.32333333 0.32333333 0.32333333\n",
      " 0.1        0.09969136 0.22333333 0.32333333 0.32333333 0.1\n",
      " 0.09969136 0.22333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.32333333 0.17       0.09969136 0.32333333 0.32333333 0.32333333\n",
      " 0.15       0.09969136 0.22333333 0.32333333 0.32333333 0.15\n",
      " 0.09969136 0.22333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.32333333 0.20333333 0.09969136 0.32333333 0.32333333 0.32333333\n",
      " 0.2        0.09874199 0.32333333 0.32333333 0.32333333 0.2\n",
      " 0.09874199 0.32333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.23666667 0.09874199 0.32333333 0.32333333 0.42333333\n",
      " 0.23666667 0.09874199 0.32333333 0.32333333 0.42333333 0.23666667\n",
      " 0.09874199 0.32333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.22333333 0.29776828 0.42333333 0.37333333 0.42333333\n",
      " 0.22333333 0.29776828 0.42333333 0.37333333 0.42333333 0.22333333\n",
      " 0.29776828 0.42333333 0.37333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333]\n",
      "One or more of the train scores are non-finite: [0.31955041 0.48305322 0.69809494 0.31955041 0.31955041 0.31955041\n",
      " 0.         0.09809494 0.31955041 0.31955041 0.31955041 0.\n",
      " 0.09809494 0.32583511 0.31762734        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32801431 0.42809494 0.33478261\n",
      " 0.31272465 0.32658103 0.32801431 0.42809494 0.33478261 0.31272465\n",
      " 0.31955041 0.48305322 0.69809494 0.31955041 0.31955041 0.31955041\n",
      " 0.         0.09809494 0.31955041 0.31955041 0.31955041 0.\n",
      " 0.09809494 0.32583511 0.31762734        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32801431 0.42809494 0.33478261\n",
      " 0.31272465 0.32658103 0.32801431 0.42809494 0.33478261 0.31272465\n",
      " 0.31955041 0.48305322 0.69809494 0.31955041 0.31955041 0.31955041\n",
      " 0.9        0.09809494 0.32224034 0.31955041 0.31955041 0.9\n",
      " 0.09809494 0.32583511 0.31762734        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32801431 0.42809494 0.33478261\n",
      " 0.31272465 0.32658103 0.32801431 0.42809494 0.33478261 0.31272465\n",
      " 0.31955041 0.48464052 0.69809494 0.31955041 0.31955041 0.31955041\n",
      " 0.905      0.09809494 0.32116228 0.31762734 0.31955041 0.905\n",
      " 0.09809494 0.32475705 0.31762734        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32801431 0.42809494 0.33478261\n",
      " 0.31272465 0.32658103 0.32801431 0.42809494 0.33478261 0.31272465\n",
      " 0.31955041 0.43652544 0.69809494 0.32314518 0.31762734 0.31955041\n",
      " 0.4952381  0.69809494 0.32314518 0.31762734 0.31955041 0.4952381\n",
      " 0.69809494 0.32314518 0.31762734        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32801431 0.42809494 0.33478261\n",
      " 0.31272465 0.32658103 0.32801431 0.42809494 0.33478261 0.31272465\n",
      " 0.34589372 0.35896242 0.69809494 0.33580772 0.31762734 0.34589372\n",
      " 0.35896242 0.69809494 0.33580772 0.32403198 0.34589372 0.35896242\n",
      " 0.69809494 0.33580772 0.32078523        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32801431 0.42809494 0.33478261\n",
      " 0.31272465 0.32658103 0.32801431 0.42809494 0.33478261 0.31272465\n",
      " 0.34033816 0.32084811 0.55809494 0.34033816 0.32756199 0.34033816\n",
      " 0.32084811 0.55809494 0.34033816 0.32756199 0.34033816 0.32084811\n",
      " 0.55809494 0.34033816 0.32422866        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32801431 0.42809494 0.33478261\n",
      " 0.31272465 0.32658103 0.32801431 0.42809494 0.33478261 0.31272465\n",
      " 0.33213658 0.32801431 0.46738066 0.33478261 0.3208971  0.33213658\n",
      " 0.32801431 0.46738066 0.33478261 0.3208971  0.33213658 0.32801431\n",
      " 0.46738066 0.33478261 0.3208971         nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32801431 0.42809494 0.33478261\n",
      " 0.31272465 0.32658103 0.32801431 0.42809494 0.33478261 0.31272465\n",
      " 0.32658103 0.32801431 0.42493705 0.33478261 0.31496824 0.32658103\n",
      " 0.32801431 0.42493705 0.33478261 0.31496824 0.32658103 0.32801431\n",
      " 0.42493705 0.33478261 0.31496824        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32801431 0.42809494 0.33478261\n",
      " 0.31272465 0.32658103 0.32801431 0.42809494 0.33478261 0.31272465\n",
      " 0.32221739 0.32801431 0.42809494 0.33478261 0.31272465 0.32221739\n",
      " 0.32801431 0.42809494 0.33478261 0.31272465 0.32658103 0.32801431\n",
      " 0.42809494 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32801431 0.42809494 0.33478261\n",
      " 0.31272465 0.32658103 0.32801431 0.42809494 0.33478261 0.31272465\n",
      " 0.32221739 0.32801431 0.42809494 0.33478261 0.31272465 0.32221739\n",
      " 0.32801431 0.42809494 0.33478261 0.31272465 0.32658103 0.32801431\n",
      " 0.42809494 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32801431 0.42809494 0.33478261\n",
      " 0.31272465 0.32658103 0.32801431 0.42809494 0.33478261 0.31272465\n",
      " 0.32221739 0.32801431 0.42809494 0.33478261 0.31272465 0.32221739\n",
      " 0.32801431 0.42809494 0.33478261 0.31272465 0.32658103 0.32801431\n",
      " 0.42809494 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32801431 0.42809494 0.33478261\n",
      " 0.31272465 0.32658103 0.32801431 0.42809494 0.33478261 0.31272465\n",
      " 0.32221739 0.32801431 0.42809494 0.33478261 0.31272465 0.32221739\n",
      " 0.32801431 0.42809494 0.33478261 0.31272465 0.32658103 0.32801431\n",
      " 0.42809494 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32801431 0.42809494 0.33478261\n",
      " 0.31272465 0.32658103 0.32801431 0.42809494 0.33478261 0.31272465]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.42333333333333334\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.7418952618453866\n",
      "GridSearchCV Runtime: 4.581512451171875 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.1111111111111111\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.16666666666666666\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.4166666666666667\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.125\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.14285714285714285\n",
      "Ave Test Precision: 0.19246031746031744\n",
      "Stdev Test Precision: 0.12704611582208034\n",
      "Ave Test Accuracy: 0.7358208955223882\n",
      "Stdev Test Accuracy: 0.007177382112563941\n",
      "Ave Test Specificity: 0.9673267326732674\n",
      "Ave Test Recall: 0.02727272727272727\n",
      "Ave Test NPV: 0.7527239500774888\n",
      "Ave Test F1-Score: 0.04741477199011446\n",
      "Ave Test G-mean: 0.15096146967093066\n",
      "Ave Runtime: 0.006392049789428711\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with gambling. 50 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.325      0.10833333 0.09874199 0.325      0.325      0.325\n",
      " 0.         0.09969136 0.325      0.325      0.325      0.\n",
      " 0.09969136 0.33166667 0.325             nan        nan        nan\n",
      "        nan        nan 0.33666667 0.245      0.22374199 0.34\n",
      " 0.28666667 0.33666667 0.245      0.22374199 0.34       0.28666667\n",
      " 0.325      0.10833333 0.09874199 0.325      0.325      0.325\n",
      " 0.         0.09969136 0.325      0.325      0.325      0.\n",
      " 0.09969136 0.33166667 0.325             nan        nan        nan\n",
      "        nan        nan 0.33666667 0.245      0.22374199 0.34\n",
      " 0.28666667 0.33666667 0.245      0.22374199 0.34       0.28666667\n",
      " 0.325      0.10833333 0.09874199 0.325      0.325      0.325\n",
      " 0.         0.09969136 0.33166667 0.325      0.325      0.\n",
      " 0.09969136 0.33166667 0.325             nan        nan        nan\n",
      "        nan        nan 0.33666667 0.245      0.22374199 0.34\n",
      " 0.28666667 0.33666667 0.245      0.22374199 0.34       0.28666667\n",
      " 0.325      0.10833333 0.09874199 0.325      0.325      0.325\n",
      " 0.1        0.09874199 0.325      0.325      0.325      0.1\n",
      " 0.09874199 0.325      0.325             nan        nan        nan\n",
      "        nan        nan 0.33666667 0.245      0.22374199 0.34\n",
      " 0.28666667 0.33666667 0.245      0.22374199 0.34       0.28666667\n",
      " 0.325      0.10833333 0.09874199 0.325      0.325      0.325\n",
      " 0.15833333 0.09874199 0.325      0.325      0.325      0.15833333\n",
      " 0.09874199 0.325      0.325             nan        nan        nan\n",
      "        nan        nan 0.33666667 0.245      0.22374199 0.34\n",
      " 0.28666667 0.33666667 0.245      0.22374199 0.34       0.28666667\n",
      " 0.345      0.14       0.09874199 0.325      0.325      0.345\n",
      " 0.14       0.09874199 0.325      0.325      0.345      0.14\n",
      " 0.09874199 0.325      0.325             nan        nan        nan\n",
      "        nan        nan 0.33666667 0.245      0.22374199 0.34\n",
      " 0.28666667 0.33666667 0.245      0.22374199 0.34       0.28666667\n",
      " 0.34       0.23333333 0.24874199 0.34       0.345      0.34\n",
      " 0.23333333 0.24874199 0.34       0.345      0.34       0.23333333\n",
      " 0.24874199 0.34       0.345             nan        nan        nan\n",
      "        nan        nan 0.33666667 0.245      0.22374199 0.34\n",
      " 0.28666667 0.33666667 0.245      0.22374199 0.34       0.28666667\n",
      " 0.33666667 0.245      0.16874199 0.34       0.28666667 0.33666667\n",
      " 0.245      0.16874199 0.34       0.28666667 0.33666667 0.245\n",
      " 0.16874199 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.245      0.22374199 0.34\n",
      " 0.28666667 0.33666667 0.245      0.22374199 0.34       0.28666667\n",
      " 0.33666667 0.245      0.23207532 0.34       0.28666667 0.33666667\n",
      " 0.245      0.23207532 0.34       0.28666667 0.33666667 0.245\n",
      " 0.23207532 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.245      0.22374199 0.34\n",
      " 0.28666667 0.33666667 0.245      0.22374199 0.34       0.28666667\n",
      " 0.33666667 0.245      0.22374199 0.34       0.28666667 0.33666667\n",
      " 0.245      0.22374199 0.34       0.28666667 0.33666667 0.245\n",
      " 0.22374199 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.245      0.22374199 0.34\n",
      " 0.28666667 0.33666667 0.245      0.22374199 0.34       0.28666667\n",
      " 0.33666667 0.245      0.22374199 0.34       0.28666667 0.33666667\n",
      " 0.245      0.22374199 0.34       0.28666667 0.33666667 0.245\n",
      " 0.22374199 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.245      0.22374199 0.34\n",
      " 0.28666667 0.33666667 0.245      0.22374199 0.34       0.28666667\n",
      " 0.33666667 0.245      0.22374199 0.34       0.28666667 0.33666667\n",
      " 0.245      0.22374199 0.34       0.28666667 0.33666667 0.245\n",
      " 0.22374199 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.245      0.22374199 0.34\n",
      " 0.28666667 0.33666667 0.245      0.22374199 0.34       0.28666667\n",
      " 0.33666667 0.245      0.22374199 0.34       0.28666667 0.33666667\n",
      " 0.245      0.22374199 0.34       0.28666667 0.33666667 0.245\n",
      " 0.22374199 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.245      0.22374199 0.34\n",
      " 0.28666667 0.33666667 0.245      0.22374199 0.34       0.28666667]\n",
      "One or more of the train scores are non-finite: [0.31147315 0.45560492 0.69809494 0.31147315 0.31147315 0.31147315\n",
      " 0.         0.09809494 0.31147315 0.31147315 0.31147315 0.\n",
      " 0.09809494 0.34795899 0.31147315        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.33464915 0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.33464915 0.32778318 0.32134401\n",
      " 0.31147315 0.45560492 0.69809494 0.31147315 0.31147315 0.31147315\n",
      " 0.         0.09809494 0.31147315 0.31147315 0.31147315 0.\n",
      " 0.09809494 0.34795899 0.31147315        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.33464915 0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.33464915 0.32778318 0.32134401\n",
      " 0.31147315 0.45560492 0.69809494 0.31147315 0.31147315 0.31147315\n",
      " 0.9        0.09809494 0.34499456 0.31147315 0.31147315 0.9\n",
      " 0.09809494 0.34795899 0.31147315        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.33464915 0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.33464915 0.32778318 0.32134401\n",
      " 0.31147315 0.45560492 0.69809494 0.31147315 0.31147315 0.31147315\n",
      " 0.8        0.09809494 0.32366835 0.31147315 0.31147315 0.8\n",
      " 0.09809494 0.32663278 0.31147315        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.33464915 0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.33464915 0.32778318 0.32134401\n",
      " 0.31147315 0.4296309  0.69805815 0.31443758 0.31147315 0.31147315\n",
      " 0.47116883 0.69809494 0.31443758 0.31147315 0.31147315 0.47116883\n",
      " 0.69809494 0.31443758 0.31147315        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.33464915 0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.33464915 0.32778318 0.32134401\n",
      " 0.33137897 0.33812389 0.69798696 0.3203863  0.31147315 0.33137897\n",
      " 0.33812389 0.69795297 0.3203863  0.31147315 0.33137897 0.33812389\n",
      " 0.69795297 0.31777091 0.31147315        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.33464915 0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.33464915 0.32778318 0.32134401\n",
      " 0.33271175 0.2949359  0.41179648 0.32778318 0.32900398 0.33271175\n",
      " 0.2949359  0.41465363 0.32778318 0.32900398 0.33271175 0.2949359\n",
      " 0.41465363 0.32778318 0.32900398        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.33464915 0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.33464915 0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.37585379 0.32778318 0.32638105 0.33025287\n",
      " 0.2926978  0.387975   0.32778318 0.32638105 0.33025287 0.2926978\n",
      " 0.387975   0.32778318 0.32638105        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.33464915 0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.33464915 0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.33506572 0.32778318 0.32134401 0.33025287\n",
      " 0.2926978  0.33506572 0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.33506572 0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.33464915 0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.33464915 0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.33781247 0.32778318 0.32134401 0.33025287\n",
      " 0.2926978  0.33781247 0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.33781247 0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.33464915 0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.33464915 0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.33731582 0.32778318 0.32134401 0.33025287\n",
      " 0.2926978  0.33731582 0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.33731582 0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.33464915 0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.33464915 0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.33731582 0.32778318 0.32134401 0.33025287\n",
      " 0.2926978  0.33731582 0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.33731582 0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.33464915 0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.33464915 0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.33731582 0.32778318 0.32134401 0.33025287\n",
      " 0.2926978  0.33464915 0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.33464915 0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.33464915 0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.33464915 0.32778318 0.32134401]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.34500000000000003\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.743142144638404\n",
      "GridSearchCV Runtime: 4.548974990844727 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.0\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.16666666666666666\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2350597609561753\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.0\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.08034528552456839\n",
      "Stdev Test Precision: 0.11264329148736506\n",
      "Ave Test Accuracy: 0.6402985074626866\n",
      "Stdev Test Accuracy: 0.21401782892609703\n",
      "Ave Test Specificity: 0.7900990099009901\n",
      "Ave Test Recall: 0.18181818181818182\n",
      "Ave Test NPV: 0.7174583960075192\n",
      "Ave Test F1-Score: 0.08000350508236945\n",
      "Ave Test G-mean: 0.06638517884013932\n",
      "Ave Runtime: 0.005616378784179687\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with smallLottery. 49 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.30833333 0.21190476 0.09874199 0.30833333 0.30833333 0.30833333\n",
      " 0.         0.09969136 0.30833333 0.30833333 0.30833333 0.\n",
      " 0.09969136 0.30833333 0.30833333        nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.14874199 0.3\n",
      " 0.25       0.3        0.31190476 0.09874199 0.3        0.25\n",
      " 0.30833333 0.21190476 0.09874199 0.30833333 0.30833333 0.30833333\n",
      " 0.         0.09969136 0.30833333 0.30833333 0.30833333 0.\n",
      " 0.09969136 0.30833333 0.30833333        nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.14874199 0.3\n",
      " 0.25       0.3        0.31190476 0.09874199 0.3        0.25\n",
      " 0.30833333 0.21190476 0.09874199 0.30833333 0.30833333 0.30833333\n",
      " 0.1        0.09969136 0.30833333 0.30833333 0.30833333 0.1\n",
      " 0.09969136 0.30833333 0.30833333        nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.14874199 0.3\n",
      " 0.25       0.3        0.31190476 0.09874199 0.3        0.25\n",
      " 0.30833333 0.17857143 0.09874199 0.30833333 0.30833333 0.30833333\n",
      " 0.15       0.09874199 0.30833333 0.30833333 0.30833333 0.15\n",
      " 0.09874199 0.30833333 0.30833333        nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.14874199 0.3\n",
      " 0.25       0.3        0.31190476 0.09874199 0.3        0.25\n",
      " 0.30833333 0.17857143 0.09874199 0.30833333 0.30833333 0.3\n",
      " 0.14       0.09874199 0.30833333 0.30833333 0.3        0.14\n",
      " 0.09874199 0.30833333 0.30833333        nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.14874199 0.3\n",
      " 0.25       0.3        0.31190476 0.09874199 0.3        0.25\n",
      " 0.3        0.21190476 0.09874199 0.3        0.30833333 0.3\n",
      " 0.21190476 0.09874199 0.3        0.30833333 0.3        0.21190476\n",
      " 0.09874199 0.3        0.30833333        nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.14874199 0.3\n",
      " 0.25       0.3        0.31190476 0.09874199 0.3        0.25\n",
      " 0.3        0.31190476 0.09874199 0.3        0.25       0.3\n",
      " 0.31190476 0.09874199 0.3        0.25       0.3        0.31190476\n",
      " 0.09874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.14874199 0.3\n",
      " 0.25       0.3        0.31190476 0.09874199 0.3        0.25\n",
      " 0.3        0.31190476 0.19874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.09874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.14874199 0.3\n",
      " 0.25       0.3        0.31190476 0.09874199 0.3        0.25\n",
      " 0.3        0.31190476 0.14874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.09874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.14874199 0.3\n",
      " 0.25       0.3        0.31190476 0.09874199 0.3        0.25\n",
      " 0.3        0.31190476 0.14874199 0.3        0.25       0.3\n",
      " 0.31190476 0.14874199 0.3        0.25       0.3        0.31190476\n",
      " 0.09874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.14874199 0.3\n",
      " 0.25       0.3        0.31190476 0.09874199 0.3        0.25\n",
      " 0.3        0.31190476 0.14874199 0.3        0.25       0.3\n",
      " 0.31190476 0.14874199 0.3        0.25       0.3        0.31190476\n",
      " 0.09874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.14874199 0.3\n",
      " 0.25       0.3        0.31190476 0.09874199 0.3        0.25\n",
      " 0.3        0.31190476 0.14874199 0.3        0.25       0.3\n",
      " 0.31190476 0.14874199 0.3        0.25       0.3        0.31190476\n",
      " 0.09874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.14874199 0.3\n",
      " 0.25       0.3        0.31190476 0.09874199 0.3        0.25\n",
      " 0.3        0.31190476 0.14874199 0.3        0.25       0.3\n",
      " 0.31190476 0.14874199 0.3        0.25       0.3        0.31190476\n",
      " 0.09874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.14874199 0.3\n",
      " 0.25       0.3        0.31190476 0.09874199 0.3        0.25      ]\n",
      "One or more of the train scores are non-finite: [0.25950436 0.35787007 0.69809494 0.25950436 0.25950436 0.25950436\n",
      " 0.         0.09809494 0.25950436 0.25950436 0.26117103 0.\n",
      " 0.09809494 0.28763232 0.25963846        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.49465692 0.25445307\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25950436 0.35787007 0.69809494 0.25950436 0.25950436 0.25950436\n",
      " 0.         0.09809494 0.25950436 0.25950436 0.26117103 0.\n",
      " 0.09809494 0.28763232 0.25963846        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.49465692 0.25445307\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25950436 0.35787007 0.69809494 0.26117103 0.25950436 0.26117103\n",
      " 0.9        0.09809494 0.28906089 0.26117103 0.26117103 0.9\n",
      " 0.09809494 0.28763232 0.25963846        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.49465692 0.25445307\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.26117103 0.37710084 0.69809494 0.26117103 0.26117103 0.26117103\n",
      " 0.86666667 0.09809494 0.2703447  0.25963846 0.26117103 0.86666667\n",
      " 0.09809494 0.26891613 0.25963846        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.49465692 0.25445307\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25974246 0.36884921 0.69809494 0.25974246 0.26117103 0.25974246\n",
      " 0.40416667 0.69809494 0.25974246 0.25963846 0.25974246 0.40416667\n",
      " 0.69809494 0.25974246 0.25963846        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.49465692 0.25445307\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25218646 0.31819687 0.69799025 0.25844375 0.25963846 0.25110421\n",
      " 0.31819687 0.69799025 0.25844375 0.25963846 0.25218646 0.31819687\n",
      " 0.69799025 0.25844375 0.25963846        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.49465692 0.25445307\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.28388626 0.64799025 0.25445307 0.25307864 0.25110421\n",
      " 0.28388626 0.64799025 0.25445307 0.25367388 0.25110421 0.28388626\n",
      " 0.64799025 0.25445307 0.25307864        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.49465692 0.25445307\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.55215692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.55215692 0.25445307 0.25090944 0.25110421 0.2763847\n",
      " 0.55215692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.49465692 0.25445307\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.49799025 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54799025 0.25445307 0.25090944 0.25110421 0.2763847\n",
      " 0.54799025 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.49465692 0.25445307\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.49465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.49465692 0.25445307 0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.49465692 0.25445307\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.49465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.49465692 0.25445307 0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.49465692 0.25445307\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.49465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.49465692 0.25445307 0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.49465692 0.25445307\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.49465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.49465692 0.25445307 0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.49465692 0.25445307\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.31190476190476185\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.7406483790523691\n",
      "GridSearchCV Runtime: 4.512614965438843 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.14285714285714285\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.14285714285714285\n",
      "Ave Test Precision: 0.09714285714285714\n",
      "Stdev Test Precision: 0.09169603734789265\n",
      "Ave Test Accuracy: 0.741044776119403\n",
      "Stdev Test Accuracy: 0.007737478116913324\n",
      "Ave Test Specificity: 0.9801980198019802\n",
      "Ave Test Recall: 0.00909090909090909\n",
      "Ave Test NPV: 0.7516997526244917\n",
      "Ave Test F1-Score: 0.016592706926490453\n",
      "Ave Test G-mean: 0.07287318157601129\n",
      "Ave Runtime: 0.007072544097900391\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with otherVices. 48 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.32348641 0.40151942 0.18219136 0.32348641 0.32348641 0.32348641\n",
      " 0.0975     0.09969136 0.32348641 0.32348641 0.32645677 0.0975\n",
      " 0.09969136 0.3284154  0.33647913        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.40866228 0.23678343 0.32626419\n",
      " 0.33533067 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067\n",
      " 0.32348641 0.40151942 0.18219136 0.32348641 0.32348641 0.32348641\n",
      " 0.0975     0.09969136 0.32348641 0.32348641 0.32645677 0.0975\n",
      " 0.09969136 0.3284154  0.33647913        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.40866228 0.23678343 0.32626419\n",
      " 0.33533067 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067\n",
      " 0.32348641 0.40151942 0.18219136 0.32348641 0.32348641 0.32626419\n",
      " 0.0975     0.09969136 0.32626419 0.32348641 0.32645677 0.0975\n",
      " 0.09969136 0.3284154  0.33647913        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.40866228 0.23678343 0.32626419\n",
      " 0.33533067 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067\n",
      " 0.32348641 0.40151942 0.18219136 0.32626419 0.32348641 0.32645677\n",
      " 0.2975     0.09969136 0.3284154  0.33647913 0.32645677 0.2975\n",
      " 0.09969136 0.3284154  0.33647913        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.40866228 0.23678343 0.32626419\n",
      " 0.33533067 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067\n",
      " 0.32645677 0.40151942 0.21552469 0.32626419 0.33336988 0.32645677\n",
      " 0.40248168 0.20171839 0.32388324 0.33647913 0.32645677 0.40248168\n",
      " 0.20171839 0.32388324 0.33647913        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.40866228 0.23678343 0.32626419\n",
      " 0.33533067 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067\n",
      " 0.32645677 0.40866228 0.19314374 0.32626419 0.33533067 0.32645677\n",
      " 0.40151942 0.20981041 0.32626419 0.33533067 0.32645677 0.40151942\n",
      " 0.20981041 0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.40866228 0.23678343 0.32626419\n",
      " 0.33533067 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067\n",
      " 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677\n",
      " 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677 0.40866228\n",
      " 0.23678343 0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.40866228 0.23678343 0.32626419\n",
      " 0.33533067 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067\n",
      " 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677\n",
      " 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677 0.40866228\n",
      " 0.23678343 0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.40866228 0.23678343 0.32626419\n",
      " 0.33533067 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067\n",
      " 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677\n",
      " 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677 0.40866228\n",
      " 0.23678343 0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.40866228 0.23678343 0.32626419\n",
      " 0.33533067 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067\n",
      " 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677\n",
      " 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677 0.40866228\n",
      " 0.23678343 0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.40866228 0.23678343 0.32626419\n",
      " 0.33533067 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067\n",
      " 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677\n",
      " 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677 0.40866228\n",
      " 0.23678343 0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.40866228 0.23678343 0.32626419\n",
      " 0.33533067 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067\n",
      " 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677\n",
      " 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677 0.40866228\n",
      " 0.23678343 0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.40866228 0.23678343 0.32626419\n",
      " 0.33533067 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067\n",
      " 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677\n",
      " 0.40866228 0.23678343 0.32626419 0.33533067 0.32645677 0.40866228\n",
      " 0.23678343 0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.40866228 0.23678343 0.32626419\n",
      " 0.33533067 0.32645677 0.40866228 0.23678343 0.32626419 0.33533067]\n",
      "One or more of the train scores are non-finite: [0.31350617 0.28662152 0.33469768 0.31350617 0.31350617 0.31350617\n",
      " 0.09833795 0.09809494 0.31350617 0.31350617 0.32336281 0.09833795\n",
      " 0.09809494 0.32292046 0.32667809        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.29033442 0.31174655 0.31555701\n",
      " 0.32313527 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527\n",
      " 0.31350617 0.28662152 0.33469768 0.31350617 0.31350617 0.31350617\n",
      " 0.09833795 0.09809494 0.31350617 0.31350617 0.32336281 0.09833795\n",
      " 0.09809494 0.32292046 0.32667809        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.29033442 0.31174655 0.31555701\n",
      " 0.32313527 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527\n",
      " 0.31350617 0.28662152 0.33469768 0.31350617 0.31350617 0.31725081\n",
      " 0.19833795 0.09809494 0.31555701 0.31350617 0.32336281 0.19833795\n",
      " 0.09809494 0.32292046 0.32667809        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.29033442 0.31174655 0.31555701\n",
      " 0.32313527 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527\n",
      " 0.31539457 0.28662152 0.33469768 0.31555701 0.31371646 0.32315252\n",
      " 0.29450401 0.28142828 0.32103206 0.32667809 0.32336281 0.29450401\n",
      " 0.28142828 0.32292046 0.32667809        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.29033442 0.31174655 0.31555701\n",
      " 0.32313527 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527\n",
      " 0.32074527 0.28662152 0.29801551 0.31555701 0.32242885 0.32291979\n",
      " 0.28387522 0.31842792 0.31744442 0.32667809 0.32291979 0.28387522\n",
      " 0.31842792 0.31744442 0.32667809        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.29033442 0.31174655 0.31555701\n",
      " 0.32313527 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527\n",
      " 0.32074527 0.29033442 0.30027505 0.31555701 0.32313527 0.32099387\n",
      " 0.28662152 0.30020275 0.31555701 0.32313527 0.32099387 0.28662152\n",
      " 0.30020275 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.29033442 0.31174655 0.31555701\n",
      " 0.32313527 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527\n",
      " 0.32074527 0.29033442 0.30992115 0.31555701 0.32313527 0.32074527\n",
      " 0.29033442 0.30992115 0.31555701 0.32313527 0.32074527 0.29033442\n",
      " 0.30992115 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.29033442 0.31174655 0.31555701\n",
      " 0.32313527 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527\n",
      " 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527 0.32074527\n",
      " 0.29033442 0.31174655 0.31555701 0.32313527 0.32074527 0.29033442\n",
      " 0.31174655 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.29033442 0.31174655 0.31555701\n",
      " 0.32313527 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527\n",
      " 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527 0.32074527\n",
      " 0.29033442 0.31174655 0.31555701 0.32313527 0.32074527 0.29033442\n",
      " 0.31174655 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.29033442 0.31174655 0.31555701\n",
      " 0.32313527 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527\n",
      " 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527 0.32074527\n",
      " 0.29033442 0.31174655 0.31555701 0.32313527 0.32074527 0.29033442\n",
      " 0.31174655 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.29033442 0.31174655 0.31555701\n",
      " 0.32313527 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527\n",
      " 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527 0.32074527\n",
      " 0.29033442 0.31174655 0.31555701 0.32313527 0.32074527 0.29033442\n",
      " 0.31174655 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.29033442 0.31174655 0.31555701\n",
      " 0.32313527 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527\n",
      " 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527 0.32074527\n",
      " 0.29033442 0.31174655 0.31555701 0.32313527 0.32074527 0.29033442\n",
      " 0.31174655 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.29033442 0.31174655 0.31555701\n",
      " 0.32313527 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527\n",
      " 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527 0.32074527\n",
      " 0.29033442 0.31174655 0.31555701 0.32313527 0.32074527 0.29033442\n",
      " 0.31174655 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.29033442 0.31174655 0.31555701\n",
      " 0.32313527 0.32074527 0.29033442 0.31174655 0.31555701 0.32313527]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "invalid value encountered in scalar divide\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "invalid value encountered in scalar divide\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.40866228070175437\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.2456359102244389\n",
      "GridSearchCV Runtime: 4.342209100723267 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3076923076923077\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2543859649122807\n",
      "Ave Test Precision: 0.2775897838741515\n",
      "Stdev Test Precision: 0.040354517943496794\n",
      "Ave Test Accuracy: 0.49850746268656715\n",
      "Stdev Test Accuracy: 0.2421868880643179\n",
      "Ave Test Specificity: 0.4900990099009901\n",
      "Ave Test Recall: 0.5242424242424242\n",
      "Ave Test NPV: 0.7592944460842189\n",
      "Ave Test F1-Score: 0.27782370598824646\n",
      "Ave Test G-mean: 0.2156041540326256\n",
      "Ave Runtime: 0.007597589492797851\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with savings. 47 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "910 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "208 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "598 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.26203686 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.26203686 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.26203686 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.26203686 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.26203686 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.26203686 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.26203686 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.26203686 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.26203686 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.26203686 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.26203686 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.26203686 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.26203686 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.26203686 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.26203686 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.26203686 0.23572107\n",
      " 0.28297619 0.                nan 0.26203686 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.26203686 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.26203686 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.26203686 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.26203686 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.26203686 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.26203686 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.26203686 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.26203686 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.26203686 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.26203686 0.23572107]\n",
      "One or more of the train scores are non-finite: [0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.28821975 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.28821975 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.28821975 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.28821975 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.28821975 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.28821975 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.28821975 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.28821975 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.28821975 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.28821975 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.28821975 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.28821975 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.28821975 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.28821975 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.28821975 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.28821975 0.28506087\n",
      " 0.29082237 0.                nan 0.28821975 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.28821975 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.28821975 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.28821975 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.28821975 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.28821975 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.28821975 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.28821975 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.28821975 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.28821975 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.28821975 0.28506087]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2829761904761905\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7256857855361596\n",
      "GridSearchCV Runtime: 4.1647114753723145 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2777777777777778\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2289156626506024\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.22672064777327935\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.20382567478318903\n",
      "Stdev Test Precision: 0.11712520129947807\n",
      "Ave Test Accuracy: 0.5440298507462686\n",
      "Stdev Test Accuracy: 0.26862302856781217\n",
      "Ave Test Specificity: 0.602970297029703\n",
      "Ave Test Recall: 0.36363636363636365\n",
      "Ave Test NPV: 0.6629291857194883\n",
      "Ave Test F1-Score: 0.17871487550773296\n",
      "Ave Test G-mean: 0.17197406061792794\n",
      "Ave Runtime: 0.005707597732543946\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanOthers. 46 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "910 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "208 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "598 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.                nan 0.33936508 0.16707281 0.25936508 0.\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.                nan 0.33936508 0.16707281 0.25936508 0.\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.                nan 0.33936508 0.16707281 0.25936508 0.\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281]\n",
      "One or more of the train scores are non-finite: [0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.                nan 0.29226564 0.28410767 0.29296519 0.\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.                nan 0.29226564 0.28410767 0.29296519 0.\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.                nan 0.29226564 0.28410767 0.29296519 0.\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.33936507936507937\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7169576059850374\n",
      "GridSearchCV Runtime: 4.188599109649658 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4166666666666667\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.35714285714285715\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.34615384615384615\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.35714285714285715\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4166666666666667\n",
      "Ave Test Precision: 0.3787545787545788\n",
      "Stdev Test Precision: 0.03489840113765008\n",
      "Ave Test Accuracy: 0.7298507462686568\n",
      "Stdev Test Accuracy: 0.00817496354485325\n",
      "Ave Test Specificity: 0.9198019801980198\n",
      "Ave Test Recall: 0.1484848484848485\n",
      "Ave Test NPV: 0.7677559499616132\n",
      "Ave Test F1-Score: 0.21312570665022096\n",
      "Ave Test G-mean: 0.36948706056345093\n",
      "Ave Runtime: 0.005201959609985351\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with payInsurance. 45 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.         0.         0.26114724 0.26114724 0.26114724 0.\n",
      " 0.         0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.         0.         0.26114724 0.26114724 0.26114724 0.\n",
      " 0.         0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.         0.         0.26114724 0.26114724 0.26114724 0.\n",
      " 0.         0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.         0.19302725 0.26114724 0.26114724 0.26114724 0.\n",
      " 0.19302725 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724]\n",
      "One or more of the train scores are non-finite: [0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.         0.         0.26070874 0.26070874 0.26070874 0.\n",
      " 0.         0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.         0.         0.26070874 0.26070874 0.26070874 0.\n",
      " 0.         0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.         0.         0.26070874 0.26070874 0.26070874 0.\n",
      " 0.         0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.         0.18132127 0.26070874 0.26070874 0.26070874 0.\n",
      " 0.18132127 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2611472392843984\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.4476309226932668\n",
      "GridSearchCV Runtime: 4.375215530395508 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.26666666666666666\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2807017543859649\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.26737967914438504\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23125\n",
      "Ave Test Precision: 0.2591996200394033\n",
      "Stdev Test Precision: 0.019044856280428506\n",
      "Ave Test Accuracy: 0.4417910447761194\n",
      "Stdev Test Accuracy: 0.01839366797591975\n",
      "Ave Test Specificity: 0.3623762376237624\n",
      "Ave Test Recall: 0.6848484848484849\n",
      "Ave Test NPV: 0.7808509638191767\n",
      "Ave Test F1-Score: 0.37586845534160446\n",
      "Ave Test G-mean: 0.4965086177546289\n",
      "Ave Runtime: 0.006467485427856445\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanSSS. 44 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "780 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "624 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.025             nan 0.21008292 0.18422306 0.24495471 0.025\n",
      "        nan 0.25674958 0.20836099        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.25674958 0.20836099\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.025             nan 0.21008292 0.18422306 0.24495471 0.025\n",
      "        nan 0.25674958 0.20836099        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.25674958 0.20836099\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.025             nan 0.21008292 0.18422306 0.24495471 0.025\n",
      "        nan 0.25674958 0.20836099        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.25674958 0.20836099\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.025             nan 0.21008292 0.18422306 0.24495471 0.025\n",
      "        nan 0.25674958 0.20836099        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.25674958 0.20836099\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.10599795        nan 0.21008292 0.18422306 0.24495471 0.10599795\n",
      "        nan 0.25674958 0.20836099        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.25674958 0.20836099\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.25674958 0.20836099        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.25674958 0.20836099\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.25674958 0.20836099        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.25674958 0.20836099\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.25674958 0.20836099        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.25674958 0.20836099\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.25674958 0.20836099        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.25674958 0.20836099\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.25674958 0.20836099        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.25674958 0.20836099\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.25674958 0.20836099        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.25674958 0.20836099\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.25674958 0.20836099        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.25674958 0.20836099\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.25674958 0.20836099        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.25674958 0.20836099]\n",
      "One or more of the train scores are non-finite: [0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.02451524        nan 0.22828644 0.20153897 0.25179811 0.02451524\n",
      "        nan 0.25145717 0.22596375        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.25145717 0.22596375\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.02451524        nan 0.22828644 0.20153897 0.25179811 0.02451524\n",
      "        nan 0.25145717 0.22596375        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.25145717 0.22596375\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.02451524        nan 0.22828644 0.20153897 0.25179811 0.02451524\n",
      "        nan 0.25145717 0.22596375        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.25145717 0.22596375\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.02451524        nan 0.22828644 0.20153897 0.25179811 0.02451524\n",
      "        nan 0.25145717 0.22596375        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.25145717 0.22596375\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.12732249        nan 0.22828644 0.20153897 0.25179811 0.12732249\n",
      "        nan 0.25145717 0.22596375        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.25145717 0.22596375\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.25145717 0.22596375        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.25145717 0.22596375\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.25145717 0.22596375        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.25145717 0.22596375\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.25145717 0.22596375        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.25145717 0.22596375\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.25145717 0.22596375        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.25145717 0.22596375\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.25145717 0.22596375        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.25145717 0.22596375\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.25145717 0.22596375        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.25145717 0.22596375\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.25145717 0.22596375        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.25145717 0.22596375\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.25145717 0.22596375        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.25145717 0.22596375]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2567495834059302\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.6433915211970075\n",
      "GridSearchCV Runtime: 4.3347015380859375 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.26865671641791045\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.20634920634920634\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.23414634146341465\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.18571428571428572\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25\n",
      "Ave Test Precision: 0.22897330998896342\n",
      "Stdev Test Precision: 0.03326728099785323\n",
      "Ave Test Accuracy: 0.5634328358208955\n",
      "Stdev Test Accuracy: 0.12231181644092361\n",
      "Ave Test Specificity: 0.6396039603960396\n",
      "Ave Test Recall: 0.3303030303030303\n",
      "Ave Test NPV: 0.7408532782187678\n",
      "Ave Test F1-Score: 0.25427568712669285\n",
      "Ave Test G-mean: 0.41136314228611387\n",
      "Ave Runtime: 0.005599212646484375\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with payFamilySupport. 43 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "780 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "624 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.                nan 0.26579155 0.26579155 0.26579155 0.\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.                nan 0.26579155 0.26579155 0.26579155 0.\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.                nan 0.26579155 0.26579155 0.26579155 0.\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.04744898        nan 0.26579155 0.26579155 0.26579155 0.04744898\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155]\n",
      "One or more of the train scores are non-finite: [0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.                nan 0.26605232 0.26605232 0.26605232 0.\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.                nan 0.26605232 0.26605232 0.26605232 0.\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.                nan 0.26605232 0.26605232 0.26605232 0.\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.05379098        nan 0.26605232 0.26605232 0.26605232 0.05379098\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2657915475780755\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.43640897755610975\n",
      "GridSearchCV Runtime: 4.340700149536133 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25925925925925924\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.288135593220339\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2617801047120419\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2541436464088398\n",
      "Ave Test Precision: 0.26266372072009603\n",
      "Stdev Test Precision: 0.014951161430659526\n",
      "Ave Test Accuracy: 0.4298507462686567\n",
      "Stdev Test Accuracy: 0.025086173602942703\n",
      "Ave Test Specificity: 0.3326732673267327\n",
      "Ave Test Recall: 0.7272727272727273\n",
      "Ave Test NPV: 0.7886334523402065\n",
      "Ave Test F1-Score: 0.3858555739079204\n",
      "Ave Test G-mean: 0.4912617142364824\n",
      "Ave Runtime: 0.005180740356445312\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanPagIbig. 42 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.                nan 0.056      0.11641026 0.07907692 0.\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.07907692 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.                nan 0.056      0.11641026 0.07907692 0.\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.07907692 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.07907692 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.07907692 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.07907692 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.07907692 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.07907692 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.07907692 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.07907692 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.07907692 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.07907692 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.07907692 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.07907692 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.07907692 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.07907692 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.07907692 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.07907692 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.07907692 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.07907692 0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.07907692 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.07907692 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.07907692 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.07907692 0.11641026]\n",
      "One or more of the train scores are non-finite: [0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.                nan 0.22826937 0.18445088 0.28148467 0.\n",
      "        nan 0.28139003 0.21445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.28148467 0.21445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.                nan 0.22826937 0.18445088 0.28148467 0.\n",
      "        nan 0.28139003 0.21445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.28148467 0.21445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.05631579        nan 0.22826937 0.18445088 0.28148467 0.05631579\n",
      "        nan 0.28139003 0.21445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.28148467 0.21445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.28139003 0.21445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.28148467 0.21445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.28148467 0.21445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.28148467 0.21445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.28148467 0.21445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.28148467 0.21445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.28148467 0.21445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.28148467 0.21445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.28148467 0.21445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.28148467 0.21445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.21445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.28148467 0.21445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.28148467 0.21445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.28148467 0.21445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.28148467 0.21445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.28148467 0.21445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.28148467 0.21445088\n",
      " 0.28148467 0.20215995        nan 0.25291324 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.28148467 0.21445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.28148467 0.21445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.28148467 0.21445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.28148467 0.21445088]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.11641025641025642\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7418952618453866\n",
      "GridSearchCV Runtime: 4.333142518997192 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.125\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.14285714285714285\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.23846153846153847\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2471042471042471\n",
      "Ave Test Precision: 0.1906845856845857\n",
      "Stdev Test Precision: 0.05512377309109978\n",
      "Ave Test Accuracy: 0.5417910447761194\n",
      "Stdev Test Accuracy: 0.26134590467245156\n",
      "Ave Test Specificity: 0.5900990099009901\n",
      "Ave Test Recall: 0.3939393939393939\n",
      "Ave Test NPV: 0.7061347233360065\n",
      "Ave Test F1-Score: 0.17625402365080622\n",
      "Ave Test G-mean: 0.1464972227672114\n",
      "Ave Runtime: 0.005399036407470703\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanGSIS. 41 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.                nan 0.34202381 0.34202381 0.34202381 0.\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.                nan 0.34202381 0.34202381 0.34202381 0.\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.                nan 0.34202381 0.34202381 0.34202381 0.\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381]\n",
      "One or more of the train scores are non-finite: [0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.                nan 0.32278992 0.32278992 0.32278992 0.\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.                nan 0.32278992 0.32278992 0.32278992 0.\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.                nan 0.32278992 0.32278992 0.32278992 0.\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.09406699        nan 0.32278992 0.32278992 0.32278992 0.09406699\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.3420238095238095\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.726932668329177\n",
      "GridSearchCV Runtime: 4.3460235595703125 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.35\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2727272727272727\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4\n",
      "Ave Test Precision: 0.3283549783549783\n",
      "Stdev Test Precision: 0.051329743836055444\n",
      "Ave Test Accuracy: 0.7276119402985074\n",
      "Stdev Test Accuracy: 0.008750775671312359\n",
      "Ave Test Specificity: 0.9316831683168317\n",
      "Ave Test Recall: 0.10303030303030303\n",
      "Ave Test NPV: 0.760743992229473\n",
      "Ave Test F1-Score: 0.15606447568489804\n",
      "Ave Test G-mean: 0.30746008147610804\n",
      "Ave Runtime: 0.006763362884521484\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanPersonal. 40 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.                nan 0.35       0.32368421 0.35       0.\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.25              nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.3               nan 0.35       0.32368421 0.35\n",
      " 0.                nan 0.35       0.32368421 0.35       0.\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.25              nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.3               nan 0.35       0.32368421 0.35\n",
      " 0.                nan 0.35       0.32368421 0.35       0.\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.25              nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.3               nan 0.35       0.32368421 0.35\n",
      " 0.18333333        nan 0.35       0.32368421 0.35       0.18333333\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.25              nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.25              nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.3               nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.25              nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.3               nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.25              nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.25              nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.3               nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.25              nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.3               nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.25              nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.3               nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.25              nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.25              nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.25              nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421]\n",
      "One or more of the train scores are non-finite: [0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.                nan 0.34779107 0.34053645 0.34779107 0.\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.28287879        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.31621212        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.                nan 0.34779107 0.34053645 0.34779107 0.\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.28287879        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.31621212        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.                nan 0.34779107 0.34053645 0.34779107 0.\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.28287879        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.31621212        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.14015152        nan 0.34779107 0.34053645 0.34779107 0.14015152\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.28287879        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.28287879        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.31621212        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.28287879        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.31621212        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.28287879        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.28287879        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.31621212        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.28287879        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.31621212        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.28287879        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.31621212        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.28287879        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.28287879        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.28287879        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.35\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7456359102244389\n",
      "GridSearchCV Runtime: 4.720113515853882 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.18181818181818182\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.6\n",
      "Ave Test Precision: 0.32303030303030306\n",
      "Stdev Test Precision: 0.16764999177716466\n",
      "Ave Test Accuracy: 0.7425373134328359\n",
      "Stdev Test Accuracy: 0.01087864159486063\n",
      "Ave Test Specificity: 0.9742574257425742\n",
      "Ave Test Recall: 0.03333333333333334\n",
      "Ave Test NPV: 0.7551639783529711\n",
      "Ave Test F1-Score: 0.059825406452537665\n",
      "Ave Test G-mean: 0.1771968772153508\n",
      "Ave Runtime: 0.007419967651367187\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasPensioner. 39 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "975 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "130 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "260 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "585 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.02469136        nan 0.21913087 0.19959269 0.20788755 0.02469136\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.20788755 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.02469136        nan 0.21913087 0.19959269 0.20788755 0.02469136\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.20788755 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.02469136        nan 0.21913087 0.19959269 0.20788755 0.02469136\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.20788755 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.02469136        nan 0.21913087 0.19959269 0.20788755 0.02469136\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.20788755 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.04719742        nan 0.21913087 0.19959269 0.20788755 0.04719742\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.20788755 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.20788755 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.20788755 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.20788755 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.20788755 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.20788755 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.20788755 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.20788755 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.20788755 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.20788755 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.20788755 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.20788755 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.20788755 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.20788755 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.20788755 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.20788755 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.20788755 0.05628833        nan 0.21913087 0.19959269]\n",
      "One or more of the train scores are non-finite: [0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.02454924        nan 0.24800822 0.19612667 0.24819663 0.02454924\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24819663 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.02454924        nan 0.24800822 0.19612667 0.24819663 0.02454924\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24819663 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.02454924        nan 0.24800822 0.19612667 0.24819663 0.02454924\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24819663 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.02454924        nan 0.24800822 0.19612667 0.24819663 0.02454924\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24819663 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.04960083        nan 0.24800822 0.19612667 0.24819663 0.04960083\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24819663 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24819663 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24819663 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24819663 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24819663 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24819663 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24819663 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24819663 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24819663 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24819663 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24819663 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24819663 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24819663 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24819663 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24819663 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24819663 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24819663 0.0747542         nan 0.24800822 0.19612667]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.21913087316206684\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.64214463840399\n",
      "GridSearchCV Runtime: 4.55214262008667 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.29411764705882354\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.16666666666666666\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2376237623762376\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.24757281553398058\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.21311475409836064\n",
      "Ave Test Precision: 0.23181912914681382\n",
      "Stdev Test Precision: 0.04679122446818023\n",
      "Ave Test Accuracy: 0.5253731343283582\n",
      "Stdev Test Accuracy: 0.15146220562463722\n",
      "Ave Test Specificity: 0.5613861386138614\n",
      "Ave Test Recall: 0.41515151515151516\n",
      "Ave Test NPV: 0.745008957070668\n",
      "Ave Test F1-Score: 0.27061475596262297\n",
      "Ave Test G-mean: 0.39941349497781664\n",
      "Ave Runtime: 0.0062291622161865234\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasPrivateEmployee. 38 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12219136 0.17219136 0.22178444 0.20281837 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12219136 0.17219136 0.22178444 0.20281837 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12219136 0.17219136 0.22178444 0.20281837 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12219136 0.17219136 0.22178444 0.20281837 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.10613367 0.17688219 0.22178444 0.20281837 0.22294525 0.10613367\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.22374892 0.12254079 0.17688219 0.22178444 0.2335876  0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.22669897 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837]\n",
      "One or more of the train scores are non-finite: [0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.12288719 0.17191766 0.22392204 0.22153363 0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.12288719 0.17191766 0.22392204 0.22153363 0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.12288719 0.17191766 0.22392204 0.22153363 0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.12288719 0.17191766 0.22392204 0.22153363 0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.12296463 0.17368407 0.22392204 0.22153363 0.24411135 0.12296463\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.24649096 0.11341356 0.17368407 0.22392204 0.24303901 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.24649389 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 100.0, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.23358760016957086\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=100.0, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.3179551122194514\n",
      "GridSearchCV Runtime: 4.768408298492432 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2145748987854251\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3448275862068966\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.4166666666666667\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2647058823529412\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23529411764705882\n",
      "Ave Test Precision: 0.29521383033179766\n",
      "Stdev Test Precision: 0.08401794505470427\n",
      "Ave Test Accuracy: 0.6119402985074627\n",
      "Stdev Test Accuracy: 0.21562838485790042\n",
      "Ave Test Specificity: 0.7178217821782178\n",
      "Ave Test Recall: 0.2878787878787878\n",
      "Ave Test NPV: 0.6870724359723062\n",
      "Ave Test F1-Score: 0.23666042196263065\n",
      "Ave Test G-mean: 0.334148623767373\n",
      "Ave Runtime: 0.00639801025390625\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasBusiness. 37 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.         0.025      0.19670077 0.21408065 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.         0.025      0.19670077 0.21408065 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.         0.025      0.19670077 0.21408065 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.         0.025      0.19670077 0.21408065 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.         0.025      0.19670077 0.21408065 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065]\n",
      "One or more of the train scores are non-finite: [0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.         0.02451524 0.24679654 0.24336471 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.         0.02451524 0.24679654 0.24336471 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.         0.02451524 0.24679654 0.24336471 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.         0.02451524 0.24679654 0.24336471 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.0255814  0.02451524 0.24679654 0.24336471 0.2416157  0.0255814\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2140806482100642\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7182044887780549\n",
      "GridSearchCV Runtime: 4.838996648788452 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.42105263157894735\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.24302788844621515\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.24302788844621515\n",
      "Ave Test Precision: 0.23856453883713263\n",
      "Stdev Test Precision: 0.15205088358273713\n",
      "Ave Test Accuracy: 0.5522388059701492\n",
      "Stdev Test Accuracy: 0.2557535744506536\n",
      "Ave Test Specificity: 0.598019801980198\n",
      "Ave Test Recall: 0.4121212121212121\n",
      "Ave Test NPV: 0.7379298685002439\n",
      "Ave Test F1-Score: 0.2191764833856963\n",
      "Ave Test G-mean: 0.21945645154447554\n",
      "Ave Runtime: 0.0069964885711669925\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasFreelancer. 36 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22453706 0.11070112        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.07243421        nan 0.22453706 0.24919459 0.23882277 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11778445        nan 0.22453706\n",
      " 0.24919459 0.23882277 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.07243421        nan 0.22453706 0.24919459 0.23882277 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11778445        nan 0.22453706\n",
      " 0.24919459 0.23882277 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11070112        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.07243421        nan 0.22453706 0.24919459 0.23882277 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11778445        nan 0.22453706\n",
      " 0.24919459 0.23882277 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.07243421        nan 0.22453706 0.24919459 0.23882277 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11778445        nan 0.22453706\n",
      " 0.24919459 0.23882277 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.09417334        nan 0.22453706 0.24919459 0.23882277 0.09417334\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11778445        nan 0.22453706\n",
      " 0.24919459 0.23882277 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.23882277 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11778445        nan 0.22453706\n",
      " 0.24919459 0.23882277 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.23882277 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11778445        nan 0.22453706\n",
      " 0.24919459 0.23882277 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11070112        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.23882277 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11778445        nan 0.22453706\n",
      " 0.24919459 0.23882277 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11070112        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.23882277 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11778445        nan 0.22453706\n",
      " 0.24919459 0.23882277 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11070112        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.23882277 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11778445        nan 0.22453706\n",
      " 0.24919459 0.23882277 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11070112        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.23882277 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11778445        nan 0.22453706\n",
      " 0.24919459 0.23882277 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.23882277 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11778445        nan 0.22453706\n",
      " 0.24919459 0.23882277 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11070112        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.23882277 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11778445        nan 0.22453706\n",
      " 0.24919459 0.23882277 0.11070112        nan 0.23882277 0.24919459]\n",
      "One or more of the train scores are non-finite: [0.22435173 0.11721755        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.07031843        nan 0.22435173 0.24931368 0.24589019 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.12065917        nan 0.22435173\n",
      " 0.24931368 0.24589019 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.07031843        nan 0.22435173 0.24931368 0.24589019 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.12065917        nan 0.22435173\n",
      " 0.24931368 0.24589019 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.11721755        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.07031843        nan 0.22435173 0.24931368 0.24589019 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.12065917        nan 0.22435173\n",
      " 0.24931368 0.24589019 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.07031843        nan 0.22435173 0.24931368 0.24589019 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.12065917        nan 0.22435173\n",
      " 0.24931368 0.24589019 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.09558318        nan 0.22435173 0.24931368 0.24589019 0.09558318\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.12065917        nan 0.22435173\n",
      " 0.24931368 0.24589019 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.24589019 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.12065917        nan 0.22435173\n",
      " 0.24931368 0.24589019 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.24589019 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.12065917        nan 0.22435173\n",
      " 0.24931368 0.24589019 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.11721755        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.24589019 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.12065917        nan 0.22435173\n",
      " 0.24931368 0.24589019 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.11721755        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.24589019 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.12065917        nan 0.22435173\n",
      " 0.24931368 0.24589019 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.11721755        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.24589019 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.12065917        nan 0.22435173\n",
      " 0.24931368 0.24589019 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.11721755        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.24589019 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.12065917        nan 0.22435173\n",
      " 0.24931368 0.24589019 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.24589019 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.12065917        nan 0.22435173\n",
      " 0.24931368 0.24589019 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.11721755        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.24589019 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.12065917        nan 0.22435173\n",
      " 0.24931368 0.24589019 0.11721755        nan 0.24589019 0.24931368]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.24919459136576155\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.29800498753117205\n",
      "GridSearchCV Runtime: 4.996249675750732 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25311203319502074\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.26141078838174275\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25210084033613445\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.24481327800829875\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2625\n",
      "Ave Test Precision: 0.25478738798423933\n",
      "Stdev Test Precision: 0.007294879003087672\n",
      "Ave Test Accuracy: 0.31417910447761194\n",
      "Stdev Test Accuracy: 0.013297410209854815\n",
      "Ave Test Specificity: 0.11386138613861385\n",
      "Ave Test Recall: 0.9272727272727274\n",
      "Ave Test NPV: 0.8274603174603176\n",
      "Ave Test F1-Score: 0.39973679168221377\n",
      "Ave Test G-mean: 0.324780082136426\n",
      "Ave Runtime: 0.006830883026123047\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasGovtEmployee. 35 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.                nan 0.26361631 0.26098127 0.2399486  0.\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.                nan 0.26361631 0.26098127 0.2399486  0.\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.                nan 0.26361631 0.26098127 0.2399486  0.\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.14032614        nan 0.26361631 0.26098127 0.2399486  0.14032614\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127]\n",
      "One or more of the train scores are non-finite: [0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.                nan 0.26821344 0.26531763 0.26822072 0.\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.                nan 0.26821344 0.26531763 0.26822072 0.\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.                nan 0.26821344 0.26531763 0.26822072 0.\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.16335038        nan 0.26821344 0.26531763 0.26822072 0.16335038\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2636163131673967\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6596009975062345\n",
      "GridSearchCV Runtime: 4.715744495391846 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.36538461538461536\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.27586206896551724\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3275862068965517\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3103448275862069\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2903225806451613\n",
      "Ave Test Precision: 0.3139000598956105\n",
      "Stdev Test Precision: 0.03483315960478191\n",
      "Ave Test Accuracy: 0.673134328358209\n",
      "Stdev Test Accuracy: 0.01858193969849065\n",
      "Ave Test Specificity: 0.803960396039604\n",
      "Ave Test Recall: 0.2727272727272727\n",
      "Ave Test NPV: 0.7717843016386705\n",
      "Ave Test F1-Score: 0.2916245215965008\n",
      "Ave Test G-mean: 0.46809626526532977\n",
      "Ave Runtime: 0.006126928329467774\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasOFW. 34 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.09844136 0.         0.29961993 0.29961993 0.29961993 0.09844136\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.09844136 0.         0.29961993 0.29961993 0.29961993 0.09844136\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.09844136 0.         0.29961993 0.29961993 0.29961993 0.09844136\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.14510802 0.         0.29961993 0.29961993 0.29961993 0.14510802\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993]\n",
      "One or more of the train scores are non-finite: [0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.09823345 0.         0.30323976 0.30323976 0.30323976 0.09823345\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.09823345 0.         0.30323976 0.30323976 0.30323976 0.09823345\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.09823345 0.         0.30323976 0.30323976 0.30323976 0.09823345\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.16032457 0.         0.30323976 0.30323976 0.30323976 0.16032457\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2996199340112383\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6346633416458853\n",
      "GridSearchCV Runtime: 4.872777700424194 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.20512820512820512\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.38666666666666666\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2328767123287671\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2558139534883721\n",
      "Ave Test Precision: 0.26609710752240223\n",
      "Stdev Test Precision: 0.07021873047074419\n",
      "Ave Test Accuracy: 0.6134328358208955\n",
      "Stdev Test Accuracy: 0.04403933619050918\n",
      "Ave Test Specificity: 0.7089108910891089\n",
      "Ave Test Recall: 0.32121212121212117\n",
      "Ave Test NPV: 0.7615295046437669\n",
      "Ave Test F1-Score: 0.29067240528485866\n",
      "Ave Test G-mean: 0.47496608666078843\n",
      "Ave Runtime: 0.007794952392578125\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseOnlyFamily. 33 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.                nan 0.31037195 0.31037195 0.31037195 0.\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.17807872        nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.                nan 0.31037195 0.31037195 0.31037195 0.\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.17807872        nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.17807872        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.                nan 0.31037195 0.31037195 0.31037195 0.\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.17807872        nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.17807872        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.14936077        nan 0.31037195 0.31037195 0.31037195 0.14936077\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.17807872        nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.14936077        nan 0.31037195 0.31037195 0.31037195 0.14936077\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.17807872        nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.17807872        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.17807872        nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.17807872        nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.17807872        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.17807872        nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.17807872        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.17807872        nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.17807872        nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.17807872        nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.17807872        nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.17807872        nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195]\n",
      "One or more of the train scores are non-finite: [0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.                nan 0.31918216 0.31918216 0.31918216 0.\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.19214184        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.                nan 0.31918216 0.31918216 0.31918216 0.\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.19214184        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.19214184        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.                nan 0.31918216 0.31918216 0.31918216 0.\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.19214184        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.19214184        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.12503367        nan 0.31918216 0.31918216 0.31918216 0.12503367\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.19214184        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.12503367        nan 0.31918216 0.31918216 0.31918216 0.12503367\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.19214184        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.19214184        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.19214184        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.19214184        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.19214184        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.19214184        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.19214184        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.19214184        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.19214184        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.19214184        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.19214184        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.19214184        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.3103719517496607\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6795511221945137\n",
      "GridSearchCV Runtime: 4.792355298995972 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.24193548387096775\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2698412698412698\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.4230769230769231\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2978723404255319\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25396825396825395\n",
      "Ave Test Precision: 0.2973388542365893\n",
      "Stdev Test Precision: 0.07334338096633872\n",
      "Ave Test Accuracy: 0.6649253731343284\n",
      "Stdev Test Accuracy: 0.03818023057398948\n",
      "Ave Test Specificity: 0.7990099009900989\n",
      "Ave Test Recall: 0.2545454545454545\n",
      "Ave Test NPV: 0.7661005067693928\n",
      "Ave Test F1-Score: 0.27333437470568334\n",
      "Ave Test G-mean: 0.4497861963040686\n",
      "Ave Runtime: 0.008499336242675782\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseExtendedFamily. 32 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.29206492 0.30814967 0.1375     0.29206492 0.27019904 0.29206492\n",
      " 0.04875    0.         0.29206492 0.27019904 0.29683645 0.04875\n",
      " 0.         0.29892314 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29403251 0.37098909 0.35527766 0.29838401\n",
      " 0.27554335 0.29544649 0.37384623 0.35527766 0.30113126 0.27554335\n",
      " 0.29206492 0.30814967 0.1375     0.29206492 0.27019904 0.29206492\n",
      " 0.04875    0.         0.29206492 0.27019904 0.29683645 0.04875\n",
      " 0.         0.29892314 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29403251 0.37098909 0.35527766 0.29838401\n",
      " 0.27554335 0.29544649 0.37384623 0.35527766 0.30113126 0.27554335\n",
      " 0.29206492 0.30814967 0.1375     0.29206492 0.27019904 0.29206492\n",
      " 0.04875    0.         0.29483949 0.27019904 0.29683645 0.04875\n",
      " 0.         0.29892314 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29403251 0.37098909 0.35527766 0.29838401\n",
      " 0.27554335 0.29544649 0.37384623 0.35527766 0.30113126 0.27554335\n",
      " 0.29014184 0.28560273 0.1375     0.29483949 0.27019904 0.29118351\n",
      " 0.11492647 0.         0.29474731 0.27195558 0.29683645 0.11492647\n",
      " 0.         0.29892314 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29403251 0.37098909 0.35527766 0.29838401\n",
      " 0.27554335 0.29544649 0.37384623 0.35527766 0.30113126 0.27554335\n",
      " 0.2943808  0.26099843 0.15416667 0.29609234 0.27362512 0.29683645\n",
      " 0.35698637 0.05       0.29816683 0.27554335 0.29683645 0.35890945\n",
      " 0.05       0.29816683 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29403251 0.37098909 0.35527766 0.29838401\n",
      " 0.27554335 0.29544649 0.37384623 0.35527766 0.30113126 0.27554335\n",
      " 0.29667801 0.37371021 0.41837302 0.30113126 0.27601103 0.29667801\n",
      " 0.37456128 0.43582945 0.30113126 0.27554335 0.29544649 0.37456128\n",
      " 0.43582945 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29403251 0.37098909 0.35527766 0.29838401\n",
      " 0.27554335 0.29544649 0.37384623 0.35527766 0.30113126 0.27554335\n",
      " 0.29544649 0.37339169 0.34218243 0.29838401 0.27554335 0.29403251\n",
      " 0.37339169 0.34218243 0.30113126 0.27554335 0.29544649 0.37339169\n",
      " 0.35218243 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29403251 0.37098909 0.35527766 0.29838401\n",
      " 0.27554335 0.29544649 0.37384623 0.35527766 0.30113126 0.27554335\n",
      " 0.29544649 0.37098909 0.35527766 0.29838401 0.27554335 0.29403251\n",
      " 0.37098909 0.35527766 0.29838401 0.27554335 0.29544649 0.37384623\n",
      " 0.35527766 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29403251 0.37098909 0.35527766 0.29838401\n",
      " 0.27554335 0.29544649 0.37384623 0.35527766 0.30113126 0.27554335\n",
      " 0.29544649 0.37098909 0.35527766 0.29838401 0.27554335 0.29403251\n",
      " 0.37098909 0.35527766 0.29838401 0.27554335 0.29544649 0.37384623\n",
      " 0.35527766 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29403251 0.37098909 0.35527766 0.29838401\n",
      " 0.27554335 0.29544649 0.37384623 0.35527766 0.30113126 0.27554335\n",
      " 0.29544649 0.37098909 0.35527766 0.29838401 0.27554335 0.29403251\n",
      " 0.37098909 0.35527766 0.29838401 0.27554335 0.29544649 0.37384623\n",
      " 0.35527766 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29403251 0.37098909 0.35527766 0.29838401\n",
      " 0.27554335 0.29544649 0.37384623 0.35527766 0.30113126 0.27554335\n",
      " 0.29544649 0.37098909 0.35527766 0.29838401 0.27554335 0.29403251\n",
      " 0.37098909 0.35527766 0.29838401 0.27554335 0.29544649 0.37384623\n",
      " 0.35527766 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29403251 0.37098909 0.35527766 0.29838401\n",
      " 0.27554335 0.29544649 0.37384623 0.35527766 0.30113126 0.27554335\n",
      " 0.29544649 0.37098909 0.35527766 0.29838401 0.27554335 0.29403251\n",
      " 0.37098909 0.35527766 0.29838401 0.27554335 0.29544649 0.37384623\n",
      " 0.35527766 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29403251 0.37098909 0.35527766 0.29838401\n",
      " 0.27554335 0.29544649 0.37384623 0.35527766 0.30113126 0.27554335\n",
      " 0.29544649 0.37098909 0.35527766 0.29838401 0.27554335 0.29403251\n",
      " 0.37098909 0.35527766 0.29838401 0.27554335 0.29544649 0.37384623\n",
      " 0.35527766 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29403251 0.37098909 0.35527766 0.29838401\n",
      " 0.27554335 0.29544649 0.37384623 0.35527766 0.30113126 0.27554335]\n",
      "One or more of the train scores are non-finite: [0.2946369  0.2990482  0.41915966 0.2946369  0.26097805 0.2946369\n",
      " 0.04916898 0.         0.2946369  0.26097805 0.29542802 0.04916898\n",
      " 0.         0.29778371 0.26544085        nan        nan        nan\n",
      "        nan        nan 0.29589295 0.30435377 0.33021165 0.29860028\n",
      " 0.26442213 0.29520762 0.30444427 0.33021165 0.29870723 0.26442213\n",
      " 0.2946369  0.2990482  0.41915966 0.2946369  0.26097805 0.2946369\n",
      " 0.04916898 0.         0.2946369  0.26097805 0.29542802 0.04916898\n",
      " 0.         0.29778371 0.26544085        nan        nan        nan\n",
      "        nan        nan 0.29589295 0.30435377 0.33021165 0.29860028\n",
      " 0.26442213 0.29520762 0.30444427 0.33021165 0.29870723 0.26442213\n",
      " 0.29438156 0.30612225 0.41915966 0.29480491 0.26083163 0.29438156\n",
      " 0.04916898 0.         0.29553472 0.26083163 0.29542802 0.04916898\n",
      " 0.         0.29778371 0.26544085        nan        nan        nan\n",
      "        nan        nan 0.29589295 0.30435377 0.33021165 0.29860028\n",
      " 0.26442213 0.29520762 0.30444427 0.33021165 0.29870723 0.26442213\n",
      " 0.29487813 0.31395727 0.43392157 0.29676979 0.26075137 0.29549355\n",
      " 0.15028009 0.         0.29756252 0.26481124 0.29542802 0.15028009\n",
      " 0.         0.29747248 0.26544085        nan        nan        nan\n",
      "        nan        nan 0.29589295 0.30435377 0.33021165 0.29860028\n",
      " 0.26442213 0.29520762 0.30444427 0.33021165 0.29870723 0.26442213\n",
      " 0.29582184 0.30454318 0.30975832 0.29839943 0.26317885 0.29561266\n",
      " 0.29888268 0.24       0.29864174 0.26515107 0.29571861 0.29888268\n",
      " 0.24       0.2987538  0.26515107        nan        nan        nan\n",
      "        nan        nan 0.29589295 0.30435377 0.33021165 0.29860028\n",
      " 0.26442213 0.29520762 0.30444427 0.33021165 0.29870723 0.26442213\n",
      " 0.29536663 0.3028977  0.34848701 0.29870723 0.2648293  0.29505333\n",
      " 0.30126918 0.3473073  0.29870723 0.26440827 0.29520762 0.30167605\n",
      " 0.3473073  0.29870723 0.26440827        nan        nan        nan\n",
      "        nan        nan 0.29589295 0.30435377 0.33021165 0.29860028\n",
      " 0.26442213 0.29520762 0.30444427 0.33021165 0.29870723 0.26442213\n",
      " 0.29520762 0.30430306 0.33138565 0.29860028 0.26442213 0.29589295\n",
      " 0.30430306 0.334602   0.29870723 0.26442213 0.29520762 0.30468528\n",
      " 0.334602   0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29589295 0.30435377 0.33021165 0.29860028\n",
      " 0.26442213 0.29520762 0.30444427 0.33021165 0.29870723 0.26442213\n",
      " 0.29520762 0.30435377 0.33057397 0.29860028 0.26442213 0.29589295\n",
      " 0.30435377 0.33057397 0.29860028 0.26442213 0.29520762 0.30444427\n",
      " 0.33057397 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29589295 0.30435377 0.33021165 0.29860028\n",
      " 0.26442213 0.29520762 0.30444427 0.33021165 0.29870723 0.26442213\n",
      " 0.29520762 0.30435377 0.33021165 0.29860028 0.26442213 0.29589295\n",
      " 0.30435377 0.33021165 0.29860028 0.26442213 0.29520762 0.30444427\n",
      " 0.33021165 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29589295 0.30435377 0.33021165 0.29860028\n",
      " 0.26442213 0.29520762 0.30444427 0.33021165 0.29870723 0.26442213\n",
      " 0.29520762 0.30435377 0.33021165 0.29860028 0.26442213 0.29589295\n",
      " 0.30435377 0.33021165 0.29860028 0.26442213 0.29520762 0.30444427\n",
      " 0.33021165 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29589295 0.30435377 0.33021165 0.29860028\n",
      " 0.26442213 0.29520762 0.30444427 0.33021165 0.29870723 0.26442213\n",
      " 0.29520762 0.30435377 0.33021165 0.29860028 0.26442213 0.29589295\n",
      " 0.30435377 0.33021165 0.29860028 0.26442213 0.29520762 0.30444427\n",
      " 0.33021165 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29589295 0.30435377 0.33021165 0.29860028\n",
      " 0.26442213 0.29520762 0.30444427 0.33021165 0.29870723 0.26442213\n",
      " 0.29520762 0.30435377 0.33021165 0.29860028 0.26442213 0.29589295\n",
      " 0.30435377 0.33021165 0.29860028 0.26442213 0.29520762 0.30444427\n",
      " 0.33021165 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29589295 0.30435377 0.33021165 0.29860028\n",
      " 0.26442213 0.29520762 0.30444427 0.33021165 0.29870723 0.26442213\n",
      " 0.29520762 0.30435377 0.33021165 0.29860028 0.26442213 0.29589295\n",
      " 0.30435377 0.33021165 0.29860028 0.26442213 0.29520762 0.30444427\n",
      " 0.33021165 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29589295 0.30435377 0.33021165 0.29860028\n",
      " 0.26442213 0.29520762 0.30444427 0.33021165 0.29870723 0.26442213]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.43582944832944837\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7369077306733167\n",
      "GridSearchCV Runtime: 4.826724290847778 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.23529411764705882\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.5\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.0\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.14705882352941177\n",
      "Stdev Test Precision: 0.22205395397855143\n",
      "Ave Test Accuracy: 0.7455223880597015\n",
      "Stdev Test Accuracy: 0.01430618620349312\n",
      "Ave Test Specificity: 0.9841584158415841\n",
      "Ave Test Recall: 0.015151515151515152\n",
      "Ave Test NPV: 0.7535952932729595\n",
      "Ave Test F1-Score: 0.025159461374911412\n",
      "Ave Test G-mean: 0.07218319116457171\n",
      "Ave Runtime: 0.008187294006347656\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyUtilityBills. 31 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27154304 0.2875     0.09806798 0.27154304 0.27154304 0.27154304\n",
      " 0.         0.09969136 0.27154304 0.27154304 0.28507937 0.\n",
      " 0.09969136 0.20600733 0.27873016        nan        nan        nan\n",
      "        nan        nan 0.2649534  0.26767399 0.27806798 0.26642399\n",
      " 0.27144405 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.27154304 0.2875     0.09806798 0.27154304 0.27154304 0.27154304\n",
      " 0.         0.09969136 0.27154304 0.27154304 0.28507937 0.\n",
      " 0.09969136 0.20600733 0.27873016        nan        nan        nan\n",
      "        nan        nan 0.2649534  0.26767399 0.27806798 0.26642399\n",
      " 0.27144405 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.27154304 0.2875     0.09806798 0.27987637 0.27154304 0.27987637\n",
      " 0.         0.09969136 0.20600733 0.29175325 0.28507937 0.\n",
      " 0.09969136 0.20600733 0.27873016        nan        nan        nan\n",
      "        nan        nan 0.2649534  0.26767399 0.27806798 0.26642399\n",
      " 0.27144405 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.27930819 0.2875     0.09806798 0.28675325 0.29237637 0.27257937\n",
      " 0.2        0.09874199 0.20600733 0.27873016 0.28507937 0.2\n",
      " 0.09874199 0.20600733 0.27873016        nan        nan        nan\n",
      "        nan        nan 0.2649534  0.26767399 0.27806798 0.26642399\n",
      " 0.27144405 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.27002248 0.26916667 0.09806798 0.27675325 0.29341991 0.27168914\n",
      " 0.19333333 0.09905033 0.26984127 0.28301587 0.27168914 0.19333333\n",
      " 0.09905033 0.26984127 0.28301587        nan        nan        nan\n",
      "        nan        nan 0.2649534  0.26767399 0.27806798 0.26642399\n",
      " 0.27144405 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.26975733 0.29416667 0.09806798 0.27142399 0.27258658 0.26642399\n",
      " 0.29416667 0.09806798 0.27142399 0.27258658 0.26642399 0.29416667\n",
      " 0.09806798 0.27142399 0.27258658        nan        nan        nan\n",
      "        nan        nan 0.2649534  0.26767399 0.27806798 0.26642399\n",
      " 0.27144405 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26410256 0.29806798 0.26642399 0.27144405 0.2649534\n",
      " 0.26410256 0.29806798 0.26642399 0.27144405 0.2649534  0.26410256\n",
      " 0.29806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.2649534  0.26767399 0.27806798 0.26642399\n",
      " 0.27144405 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26410256 0.31306798 0.26642399 0.27144405 0.2649534\n",
      " 0.26410256 0.31306798 0.26642399 0.27144405 0.2649534  0.26410256\n",
      " 0.31306798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.2649534  0.26767399 0.27806798 0.26642399\n",
      " 0.27144405 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405 0.2649534\n",
      " 0.26767399 0.27806798 0.26642399 0.27144405 0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.2649534  0.26767399 0.27806798 0.26642399\n",
      " 0.27144405 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405 0.2649534\n",
      " 0.26767399 0.27806798 0.26642399 0.27144405 0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.2649534  0.26767399 0.27806798 0.26642399\n",
      " 0.27144405 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405 0.2649534\n",
      " 0.26767399 0.27806798 0.26642399 0.27144405 0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.2649534  0.26767399 0.27806798 0.26642399\n",
      " 0.27144405 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405 0.2649534\n",
      " 0.26767399 0.27806798 0.26642399 0.27144405 0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.2649534  0.26767399 0.27806798 0.26642399\n",
      " 0.27144405 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405 0.2649534\n",
      " 0.26767399 0.27806798 0.26642399 0.27144405 0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.2649534  0.26767399 0.27806798 0.26642399\n",
      " 0.27144405 0.2649534  0.26767399 0.27806798 0.26642399 0.27144405]\n",
      "One or more of the train scores are non-finite: [0.27190773 0.52017667 0.69791537 0.27190773 0.27190773 0.27190773\n",
      " 0.         0.09809494 0.27190773 0.27190773 0.29476205 0.\n",
      " 0.09809494 0.27867104 0.29216514        nan        nan        nan\n",
      "        nan        nan 0.26522162 0.32169057 0.32203852 0.26735748\n",
      " 0.26685776 0.26522162 0.32290025 0.32203852 0.2676002  0.26685776\n",
      " 0.27190773 0.52017667 0.69791537 0.27215003 0.27190773 0.27190773\n",
      " 0.         0.09809494 0.27215003 0.27190773 0.29476205 0.\n",
      " 0.09809494 0.27867104 0.29216514        nan        nan        nan\n",
      "        nan        nan 0.26522162 0.32169057 0.32203852 0.26735748\n",
      " 0.26685776 0.26522162 0.32290025 0.32203852 0.2676002  0.26685776\n",
      " 0.27240644 0.52017667 0.69791537 0.27544227 0.27215003 0.27834382\n",
      " 0.6        0.09809494 0.27867104 0.29597423 0.29476205 0.6\n",
      " 0.09809494 0.27867104 0.29216514        nan        nan        nan\n",
      "        nan        nan 0.26522162 0.32169057 0.32203852 0.26735748\n",
      " 0.26685776 0.26522162 0.32290025 0.32203852 0.2676002  0.26685776\n",
      " 0.2847581  0.50684784 0.69794946 0.29684652 0.28081632 0.29247866\n",
      " 0.83333333 0.09809494 0.27867104 0.29216514 0.29476205 0.83333333\n",
      " 0.09809494 0.27867104 0.29216514        nan        nan        nan\n",
      "        nan        nan 0.26522162 0.32169057 0.32203852 0.26735748\n",
      " 0.26685776 0.26522162 0.32290025 0.32203852 0.2676002  0.26685776\n",
      " 0.27891872 0.45539523 0.69798364 0.29451607 0.29911565 0.28206015\n",
      " 0.53537675 0.69791907 0.28982273 0.29038399 0.28206015 0.54537675\n",
      " 0.69791907 0.28982273 0.29038399        nan        nan        nan\n",
      "        nan        nan 0.26522162 0.32169057 0.32203852 0.26735748\n",
      " 0.26685776 0.26522162 0.32290025 0.32203852 0.2676002  0.26685776\n",
      " 0.26986052 0.36723507 0.69791163 0.27451934 0.27895494 0.26986052\n",
      " 0.37430161 0.69798364 0.27451934 0.27895494 0.26986052 0.37430161\n",
      " 0.69798364 0.27451934 0.27895494        nan        nan        nan\n",
      "        nan        nan 0.26522162 0.32169057 0.32203852 0.26735748\n",
      " 0.26685776 0.26522162 0.32290025 0.32203852 0.2676002  0.26685776\n",
      " 0.26522162 0.33439409 0.36210022 0.26831482 0.26687159 0.26522162\n",
      " 0.33439409 0.35344637 0.26831482 0.26687159 0.26522162 0.33439409\n",
      " 0.35344637 0.26831482 0.26687159        nan        nan        nan\n",
      "        nan        nan 0.26522162 0.32169057 0.32203852 0.26735748\n",
      " 0.26685776 0.26522162 0.32290025 0.32203852 0.2676002  0.26685776\n",
      " 0.26522162 0.33197752 0.33717739 0.2676002  0.26685776 0.26522162\n",
      " 0.33197752 0.33717739 0.2676002  0.26685776 0.26522162 0.33280808\n",
      " 0.33717739 0.2676002  0.26685776        nan        nan        nan\n",
      "        nan        nan 0.26522162 0.32169057 0.32203852 0.26735748\n",
      " 0.26685776 0.26522162 0.32290025 0.32203852 0.2676002  0.26685776\n",
      " 0.26522162 0.32169057 0.32341106 0.2676002  0.26685776 0.26522162\n",
      " 0.32169057 0.32341106 0.2676002  0.26685776 0.26522162 0.32290025\n",
      " 0.32341106 0.2676002  0.26685776        nan        nan        nan\n",
      "        nan        nan 0.26522162 0.32169057 0.32203852 0.26735748\n",
      " 0.26685776 0.26522162 0.32290025 0.32203852 0.2676002  0.26685776\n",
      " 0.26522162 0.32169057 0.32203852 0.26735748 0.26685776 0.26522162\n",
      " 0.32169057 0.32203852 0.26735748 0.26685776 0.26522162 0.32290025\n",
      " 0.32203852 0.2676002  0.26685776        nan        nan        nan\n",
      "        nan        nan 0.26522162 0.32169057 0.32203852 0.26735748\n",
      " 0.26685776 0.26522162 0.32290025 0.32203852 0.2676002  0.26685776\n",
      " 0.26522162 0.32169057 0.32203852 0.26735748 0.26685776 0.26522162\n",
      " 0.32169057 0.32203852 0.26735748 0.26685776 0.26522162 0.32290025\n",
      " 0.32203852 0.2676002  0.26685776        nan        nan        nan\n",
      "        nan        nan 0.26522162 0.32169057 0.32203852 0.26735748\n",
      " 0.26685776 0.26522162 0.32290025 0.32203852 0.2676002  0.26685776\n",
      " 0.26522162 0.32169057 0.32203852 0.26735748 0.26685776 0.26522162\n",
      " 0.32169057 0.32203852 0.26735748 0.26685776 0.26522162 0.32290025\n",
      " 0.32203852 0.2676002  0.26685776        nan        nan        nan\n",
      "        nan        nan 0.26522162 0.32169057 0.32203852 0.26735748\n",
      " 0.26685776 0.26522162 0.32290025 0.32203852 0.2676002  0.26685776\n",
      " 0.26522162 0.32169057 0.32203852 0.26735748 0.26685776 0.26522162\n",
      " 0.32169057 0.32203852 0.26735748 0.26685776 0.26522162 0.32290025\n",
      " 0.32203852 0.2676002  0.26685776        nan        nan        nan\n",
      "        nan        nan 0.26522162 0.32169057 0.32203852 0.26735748\n",
      " 0.26685776 0.26522162 0.32290025 0.32203852 0.2676002  0.26685776]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 10.0, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.31306798140131475\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=10.0, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7481296758104738\n",
      "GridSearchCV Runtime: 5.053122282028198 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.42857142857142855\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4444444444444444\n",
      "Ave Test Precision: 0.37460317460317455\n",
      "Stdev Test Precision: 0.056789027999994655\n",
      "Ave Test Accuracy: 0.7455223880597014\n",
      "Stdev Test Accuracy: 0.006131222658639906\n",
      "Ave Test Specificity: 0.9732673267326731\n",
      "Ave Test Recall: 0.048484848484848485\n",
      "Ave Test NPV: 0.7579160862810017\n",
      "Ave Test F1-Score: 0.08459443197263328\n",
      "Ave Test G-mean: 0.21183891925610882\n",
      "Ave Runtime: 0.010548591613769531\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyVices. 30 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.28050847 0.28567283 0.36375    0.28050847 0.28050847 0.28050847\n",
      " 0.12219136 0.         0.28050847 0.28050847 0.27902625 0.12219136\n",
      " 0.         0.28040213 0.27753464        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28362744 0.35348973 0.28103142\n",
      " 0.2768262  0.28308099 0.28362744 0.35348973 0.28103142 0.2768262\n",
      " 0.28050847 0.28567283 0.36375    0.28050847 0.28050847 0.28050847\n",
      " 0.12219136 0.         0.28050847 0.28050847 0.27902625 0.12219136\n",
      " 0.         0.28040213 0.27753464        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28362744 0.35348973 0.28103142\n",
      " 0.2768262  0.28308099 0.28362744 0.35348973 0.28103142 0.2768262\n",
      " 0.28050847 0.2865251  0.36375    0.28150562 0.28050847 0.28092935\n",
      " 0.30461993 0.         0.28215312 0.28050847 0.27902625 0.30461993\n",
      " 0.         0.28040213 0.27753464        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28362744 0.35348973 0.28103142\n",
      " 0.2768262  0.28308099 0.28362744 0.35348973 0.28103142 0.2768262\n",
      " 0.28319243 0.28187038 0.36880051 0.28465442 0.28219528 0.27902625\n",
      " 0.27672222 0.1        0.28040213 0.27880542 0.27902625 0.27672222\n",
      " 0.1        0.28040213 0.27753464        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28362744 0.35348973 0.28103142\n",
      " 0.2768262  0.28308099 0.28362744 0.35348973 0.28103142 0.2768262\n",
      " 0.28497493 0.2827079  0.33051587 0.27973501 0.27729024 0.27784107\n",
      " 0.28174992 0.475      0.28169809 0.27607186 0.27784107 0.28174992\n",
      " 0.475      0.28169809 0.27607186        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28362744 0.35348973 0.28103142\n",
      " 0.2768262  0.28308099 0.28362744 0.35348973 0.28103142 0.2768262\n",
      " 0.28308099 0.28362744 0.36890873 0.28103142 0.2768262  0.28371591\n",
      " 0.28520639 0.35645563 0.28103142 0.2768262  0.28371591 0.28520639\n",
      " 0.35645563 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28362744 0.35348973 0.28103142\n",
      " 0.2768262  0.28308099 0.28362744 0.35348973 0.28103142 0.2768262\n",
      " 0.28308099 0.28362744 0.35348973 0.28103142 0.2768262  0.28308099\n",
      " 0.28362744 0.35793417 0.28103142 0.2768262  0.28308099 0.28362744\n",
      " 0.35793417 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28362744 0.35348973 0.28103142\n",
      " 0.2768262  0.28308099 0.28362744 0.35348973 0.28103142 0.2768262\n",
      " 0.28308099 0.28362744 0.35348973 0.28103142 0.2768262  0.28308099\n",
      " 0.28362744 0.35348973 0.28103142 0.2768262  0.28308099 0.28362744\n",
      " 0.35348973 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28362744 0.35348973 0.28103142\n",
      " 0.2768262  0.28308099 0.28362744 0.35348973 0.28103142 0.2768262\n",
      " 0.28308099 0.28362744 0.35348973 0.28103142 0.2768262  0.28308099\n",
      " 0.28362744 0.35348973 0.28103142 0.2768262  0.28308099 0.28362744\n",
      " 0.35348973 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28362744 0.35348973 0.28103142\n",
      " 0.2768262  0.28308099 0.28362744 0.35348973 0.28103142 0.2768262\n",
      " 0.28308099 0.28362744 0.35348973 0.28103142 0.2768262  0.28308099\n",
      " 0.28362744 0.35348973 0.28103142 0.2768262  0.28308099 0.28362744\n",
      " 0.35348973 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28362744 0.35348973 0.28103142\n",
      " 0.2768262  0.28308099 0.28362744 0.35348973 0.28103142 0.2768262\n",
      " 0.28308099 0.28362744 0.35348973 0.28103142 0.2768262  0.28308099\n",
      " 0.28362744 0.35348973 0.28103142 0.2768262  0.28308099 0.28362744\n",
      " 0.35348973 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28362744 0.35348973 0.28103142\n",
      " 0.2768262  0.28308099 0.28362744 0.35348973 0.28103142 0.2768262\n",
      " 0.28308099 0.28362744 0.35348973 0.28103142 0.2768262  0.28308099\n",
      " 0.28362744 0.35348973 0.28103142 0.2768262  0.28308099 0.28362744\n",
      " 0.35348973 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28362744 0.35348973 0.28103142\n",
      " 0.2768262  0.28308099 0.28362744 0.35348973 0.28103142 0.2768262\n",
      " 0.28308099 0.28362744 0.35348973 0.28103142 0.2768262  0.28308099\n",
      " 0.28362744 0.35348973 0.28103142 0.2768262  0.28308099 0.28362744\n",
      " 0.35348973 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28362744 0.35348973 0.28103142\n",
      " 0.2768262  0.28308099 0.28362744 0.35348973 0.28103142 0.2768262 ]\n",
      "One or more of the train scores are non-finite: [0.27972173 0.27754988 0.38266317 0.27972173 0.27972173 0.27972173\n",
      " 0.12288719 0.         0.27972173 0.27972173 0.27898251 0.12288719\n",
      " 0.         0.27847205 0.28029106        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27651053 0.34827293 0.27896561\n",
      " 0.28034557 0.27911561 0.27651053 0.34827293 0.27896561 0.28034557\n",
      " 0.27972173 0.27754988 0.38266317 0.27972173 0.27972173 0.27972173\n",
      " 0.22288719 0.         0.27972173 0.27972173 0.27898251 0.22288719\n",
      " 0.         0.27847205 0.28029106        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27651053 0.34827293 0.27896561\n",
      " 0.28034557 0.27911561 0.27651053 0.34827293 0.27896561 0.28034557\n",
      " 0.27988329 0.27773052 0.38212983 0.28035272 0.27999777 0.27938359\n",
      " 0.33094837 0.         0.27898969 0.27999777 0.27898251 0.33094837\n",
      " 0.         0.27847205 0.28029106        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27651053 0.34827293 0.27896561\n",
      " 0.28034557 0.27911561 0.27651053 0.34827293 0.27896561 0.28034557\n",
      " 0.27919051 0.276963   0.38230829 0.27967567 0.27943025 0.27898251\n",
      " 0.28157789 0.35       0.27867001 0.28015493 0.27898251 0.28157789\n",
      " 0.35       0.27867001 0.28015878        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27651053 0.34827293 0.27896561\n",
      " 0.28034557 0.27911561 0.27651053 0.34827293 0.27896561 0.28034557\n",
      " 0.27902206 0.27679952 0.37135833 0.27925153 0.27981628 0.27836029\n",
      " 0.27884271 0.44893273 0.2781197  0.28005644 0.27836029 0.27884271\n",
      " 0.44893273 0.2781197  0.28005644        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27651053 0.34827293 0.27896561\n",
      " 0.28034557 0.27911561 0.27651053 0.34827293 0.27896561 0.28034557\n",
      " 0.27911561 0.27654755 0.34898601 0.27904163 0.27964261 0.27856852\n",
      " 0.27637361 0.36587858 0.27908564 0.28019789 0.27856852 0.27637361\n",
      " 0.36636036 0.27930996 0.28019789        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27651053 0.34827293 0.27896561\n",
      " 0.28034557 0.27911561 0.27651053 0.34827293 0.27896561 0.28034557\n",
      " 0.27911561 0.27651053 0.34832698 0.27926877 0.28020991 0.27911561\n",
      " 0.27629302 0.34720098 0.27896561 0.28003396 0.27851643 0.27654755\n",
      " 0.34720098 0.27896561 0.28008838        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27651053 0.34827293 0.27896561\n",
      " 0.28034557 0.27911561 0.27651053 0.34827293 0.27896561 0.28034557\n",
      " 0.27911561 0.27651053 0.34827293 0.27896561 0.28020991 0.27911561\n",
      " 0.27651053 0.34905661 0.27896561 0.28034557 0.27911561 0.27665232\n",
      " 0.34905661 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27651053 0.34827293 0.27896561\n",
      " 0.28034557 0.27911561 0.27651053 0.34827293 0.27896561 0.28034557\n",
      " 0.27911561 0.27651053 0.34827293 0.27896561 0.28020991 0.27911561\n",
      " 0.27651053 0.34827293 0.27896561 0.28034557 0.27911561 0.27651053\n",
      " 0.34827293 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27651053 0.34827293 0.27896561\n",
      " 0.28034557 0.27911561 0.27651053 0.34827293 0.27896561 0.28034557\n",
      " 0.27911561 0.27651053 0.34827293 0.27896561 0.28020991 0.27911561\n",
      " 0.27651053 0.34827293 0.27896561 0.28034557 0.27911561 0.27651053\n",
      " 0.34827293 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27651053 0.34827293 0.27896561\n",
      " 0.28034557 0.27911561 0.27651053 0.34827293 0.27896561 0.28034557\n",
      " 0.27911561 0.27651053 0.34827293 0.27896561 0.28020991 0.27911561\n",
      " 0.27651053 0.34827293 0.27896561 0.28034557 0.27911561 0.27651053\n",
      " 0.34827293 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27651053 0.34827293 0.27896561\n",
      " 0.28034557 0.27911561 0.27651053 0.34827293 0.27896561 0.28034557\n",
      " 0.27911561 0.27651053 0.34827293 0.27896561 0.28020991 0.27911561\n",
      " 0.27651053 0.34827293 0.27896561 0.28034557 0.27911561 0.27651053\n",
      " 0.34827293 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27651053 0.34827293 0.27896561\n",
      " 0.28034557 0.27911561 0.27651053 0.34827293 0.27896561 0.28034557\n",
      " 0.27911561 0.27651053 0.34827293 0.27896561 0.28020991 0.27911561\n",
      " 0.27651053 0.34827293 0.27896561 0.28034557 0.27911561 0.27651053\n",
      " 0.34827293 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27651053 0.34827293 0.27896561\n",
      " 0.28034557 0.27911561 0.27651053 0.34827293 0.27896561 0.28034557]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.475\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7518703241895262\n",
      "GridSearchCV Runtime: 5.308799505233765 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4117647058823529\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.5\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.125\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.375\n",
      "Ave Test Precision: 0.34901960784313724\n",
      "Stdev Test Precision: 0.13949717813149945\n",
      "Ave Test Accuracy: 0.7432835820895523\n",
      "Stdev Test Accuracy: 0.008908459989786045\n",
      "Ave Test Specificity: 0.9702970297029703\n",
      "Ave Test Recall: 0.04848484848484848\n",
      "Ave Test NPV: 0.7574402159650047\n",
      "Ave Test F1-Score: 0.08008809223782021\n",
      "Ave Test G-mean: 0.19553329118967955\n",
      "Ave Runtime: 0.00858449935913086\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyExpenses. 29 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.19749532 0.27469057 0.         0.19749532 0.2880568  0.19749532\n",
      " 0.09813272 0.         0.19749532 0.2880568  0.20121743 0.09813272\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.19749532 0.27498675 0.         0.19749532 0.2880568  0.19749532\n",
      " 0.09813272 0.         0.19749532 0.2880568  0.20121743 0.09813272\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.19768856 0.25832009 0.         0.19749532 0.28651834 0.19768856\n",
      " 0.09813272 0.         0.19749532 0.28021214 0.20121743 0.09813272\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20009875 0.25554323 0.         0.19872282 0.27760256 0.19957936\n",
      " 0.27460799 0.         0.19806372 0.28242606 0.20121743 0.27460799\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20055298 0.25649601 0.         0.20001438 0.27697187 0.20121743\n",
      " 0.26569488 0.         0.19760485 0.26994124 0.20121743 0.26533271\n",
      " 0.         0.19760485 0.26994124        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.24690454 0.275      0.19760485 0.27134615 0.20121743\n",
      " 0.25134898 0.1        0.19760485 0.27000482 0.20121743 0.25134898\n",
      " 0.1        0.19760485 0.27138326        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.36666667 0.19760485 0.27063283 0.20121743\n",
      " 0.23692618 0.36666667 0.19760485 0.27063283 0.20121743 0.23692618\n",
      " 0.36666667 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743\n",
      " 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743 0.23732245\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743\n",
      " 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743 0.23732245\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743\n",
      " 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743 0.23732245\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743\n",
      " 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743 0.23732245\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743\n",
      " 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743 0.23732245\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743\n",
      " 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743 0.23732245\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283]\n",
      "One or more of the train scores are non-finite: [0.21013357 0.28610787 0.06       0.21013357 0.27674454 0.21013357\n",
      " 0.09826745 0.         0.21013357 0.27674454 0.21190105 0.09826745\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.21013357 0.28610787 0.06       0.21013357 0.27674454 0.21013357\n",
      " 0.09826745 0.         0.21013357 0.27674454 0.21190105 0.09826745\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.21022921 0.28707353 0.06       0.21003911 0.27637863 0.21022921\n",
      " 0.17343793 0.         0.21003911 0.2743568  0.21190105 0.17343793\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.20951384 0.2752861  0.05454545 0.20925355 0.27394389 0.2113608\n",
      " 0.29855767 0.         0.20951325 0.27670069 0.21190105 0.29855767\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.21116886 0.27570013 0.1        0.21037945 0.26866329 0.21163474\n",
      " 0.29150852 0.06666667 0.21040251 0.26425128 0.21163474 0.28286277\n",
      " 0.06666667 0.21059004 0.26425128        nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.21153733 0.26447815 0.45666667 0.21079549 0.26295842 0.2116334\n",
      " 0.26845463 0.47740741 0.2106925  0.26216079 0.21182487 0.26845463\n",
      " 0.47740741 0.21077353 0.26248218        nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.21168281 0.26451259 0.43639443 0.21098925 0.26238847 0.21163606\n",
      " 0.26471854 0.43981589 0.21086072 0.26229773 0.2118757  0.26477016\n",
      " 0.43981589 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.2117339  0.26451259 0.40842028 0.21098925 0.26210887 0.21163606\n",
      " 0.26451259 0.40842028 0.21086072 0.26210887 0.21192679 0.26444097\n",
      " 0.40842028 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.2117339  0.26451259 0.40635768 0.21098925 0.26210887 0.21163606\n",
      " 0.26451259 0.40635768 0.21086072 0.26210887 0.21192679 0.26444097\n",
      " 0.40635768 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.2117339  0.26451259 0.40635768 0.21098925 0.26210887 0.21163606\n",
      " 0.26451259 0.40635768 0.21086072 0.26210887 0.21192679 0.26444097\n",
      " 0.40635768 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.2117339  0.26451259 0.40635768 0.21098925 0.26210887 0.21163606\n",
      " 0.26451259 0.40635768 0.21086072 0.26210887 0.21192679 0.26444097\n",
      " 0.40635768 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.2117339  0.26451259 0.40635768 0.21098925 0.26210887 0.21163606\n",
      " 0.26451259 0.40635768 0.21086072 0.26210887 0.21192679 0.26444097\n",
      " 0.40635768 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.2117339  0.26451259 0.40635768 0.21098925 0.26210887 0.21163606\n",
      " 0.26451259 0.40635768 0.21086072 0.26210887 0.21192679 0.26444097\n",
      " 0.40635768 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036 ]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3733333333333333\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.7481296758104738\n",
      "GridSearchCV Runtime: 5.134933233261108 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3611111111111111\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3076923076923077\n",
      "Ave Test Precision: 0.3309035409035409\n",
      "Stdev Test Precision: 0.048017384689393074\n",
      "Ave Test Accuracy: 0.7335820895522387\n",
      "Stdev Test Accuracy: 0.011068952965814453\n",
      "Ave Test Specificity: 0.9455445544554454\n",
      "Ave Test Recall: 0.08484848484848485\n",
      "Ave Test NPV: 0.7599647714031972\n",
      "Ave Test F1-Score: 0.12807566197698267\n",
      "Ave Test G-mean: 0.2700808635673128\n",
      "Ave Runtime: 0.009134960174560548\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlySoloNetIncome. 28 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1105 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "182 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "364 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "559 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25989423 0.25989423        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.25989423        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.25989423        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.25989423        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.25989423        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.25989423        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423]\n",
      "One or more of the train scores are non-finite: [0.25887731 0.25887731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.25887731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.25887731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.25887731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.25887731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.25887731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.259894230383722\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.415211970074813\n",
      "GridSearchCV Runtime: 5.06951379776001 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2736842105263158\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25263157894736843\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.22941176470588234\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.28888888888888886\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2512820512820513\n",
      "Ave Test Precision: 0.25917969887010134\n",
      "Stdev Test Precision: 0.022827600872137695\n",
      "Ave Test Accuracy: 0.42164179104477606\n",
      "Stdev Test Accuracy: 0.03099486516014207\n",
      "Ave Test Specificity: 0.32178217821782173\n",
      "Ave Test Recall: 0.7272727272727272\n",
      "Ave Test NPV: 0.7844531528484562\n",
      "Ave Test F1-Score: 0.38200032588432253\n",
      "Ave Test G-mean: 0.48221180258827284\n",
      "Ave Runtime: 0.009199810028076173\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positiveMonthlySoloNetIncome. 27 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24158884 0.24164378 0.04       0.24158884 0.22303846 0.24158884\n",
      " 0.19105928 0.         0.24158884 0.22303846 0.23957897 0.19175658\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.24158884 0.24164378 0.04       0.24158884 0.22303846 0.24158884\n",
      " 0.19105928 0.         0.24158884 0.22303846 0.23957897 0.19175658\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.24069486 0.24200663 0.04       0.24069486 0.22184798 0.24105328\n",
      " 0.19019136 0.         0.24105328 0.22043372 0.23957897 0.19175658\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23966941 0.24322832 0.04       0.23966941 0.22175027 0.23887209\n",
      " 0.20949341 0.         0.24102687 0.22811944 0.24001419 0.20949341\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.24177967 0.23956775 0.02857143 0.24414526 0.23219099 0.23678385\n",
      " 0.24070582 0.         0.23598754 0.24888676 0.23678385 0.24070582\n",
      " 0.         0.23598754 0.24888676        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23748021 0.23961182 0.16666667 0.23375239 0.25130584 0.23680155\n",
      " 0.23800509 0.06666667 0.23271704 0.24922126 0.23680155 0.23800509\n",
      " 0.06666667 0.23232382 0.24922126        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.29166667 0.23169206 0.24426356 0.23663853\n",
      " 0.2407121  0.29166667 0.23083185 0.24657957 0.23663853 0.24041386\n",
      " 0.29166667 0.23169206 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.24940476 0.23083185 0.24742561 0.23663853\n",
      " 0.2407121  0.24940476 0.23083185 0.24657957 0.23663853 0.2407121\n",
      " 0.24940476 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2407121  0.24345238 0.23083185 0.24657957 0.23663853 0.2407121\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853\n",
      " 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853 0.2407121\n",
      " 0.24206349 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853\n",
      " 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853 0.2407121\n",
      " 0.24206349 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853\n",
      " 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853 0.2407121\n",
      " 0.24206349 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853\n",
      " 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853 0.2407121\n",
      " 0.24206349 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957]\n",
      "One or more of the train scores are non-finite: [0.24450882 0.24063572 0.04       0.24450882 0.25095615 0.24450882\n",
      " 0.1967365  0.         0.24450882 0.25095615 0.2455207  0.19639977\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24450882 0.24063572 0.04       0.24450882 0.25095615 0.24450882\n",
      " 0.1967365  0.         0.24450882 0.25070214 0.2455207  0.19639977\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.244272   0.24112365 0.04       0.244272   0.25165609 0.24417507\n",
      " 0.1968839  0.         0.24417507 0.24955013 0.2455207  0.19663873\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24447766 0.24074119 0.04       0.24435617 0.25110435 0.24529999\n",
      " 0.26304175 0.         0.24526786 0.25115591 0.24556684 0.26304175\n",
      " 0.         0.24526234 0.25075688        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24569559 0.24017643 0.0375     0.24557898 0.25253171 0.24382634\n",
      " 0.2389948  0.04285714 0.24429269 0.25082822 0.24382634 0.2389948\n",
      " 0.04285714 0.24429269 0.25068856        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24276663 0.23960321 0.21130952 0.24387648 0.25230745 0.24264476\n",
      " 0.24003406 0.17       0.24385221 0.24998015 0.24274032 0.23995086\n",
      " 0.17       0.24400453 0.25007882        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24142589 0.23833481 0.30164683 0.24290218 0.25033588 0.2411694\n",
      " 0.23842306 0.30467449 0.24295086 0.24976889 0.24167907 0.23834304\n",
      " 0.30546717 0.24305657 0.24976889        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24111965 0.23825684 0.33850422 0.24289462 0.24985954 0.24111965\n",
      " 0.23825684 0.33897042 0.24289462 0.24985954 0.24111879 0.23840975\n",
      " 0.34020446 0.24295086 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24111965 0.23825684 0.33170754 0.24289462 0.24995002 0.24111965\n",
      " 0.23825684 0.33170754 0.24289462 0.24990549 0.24111879 0.23840975\n",
      " 0.33219934 0.24299974 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24111965 0.23825684 0.33079904 0.24289462 0.24995002 0.24111965\n",
      " 0.23825684 0.33079904 0.24289462 0.24990549 0.24111879 0.23840975\n",
      " 0.33079904 0.24299974 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24111965 0.23825684 0.33079904 0.24289462 0.24995002 0.24111965\n",
      " 0.23825684 0.33079904 0.24289462 0.24990549 0.24111879 0.23840975\n",
      " 0.33079904 0.24299974 0.24990549        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24111965 0.23825684 0.33079904 0.24289462 0.24995002 0.24111965\n",
      " 0.23825684 0.33079904 0.24289462 0.24990549 0.24111879 0.23840975\n",
      " 0.33079904 0.24299974 0.24990549        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24111965 0.23825684 0.33079904 0.24289462 0.24995002 0.24111965\n",
      " 0.23825684 0.33079904 0.24289462 0.24990549 0.24111879 0.23840975\n",
      " 0.33079904 0.24299974 0.24990549        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.2916666666666667\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7481296758104738\n",
      "GridSearchCV Runtime: 5.036940574645996 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.42857142857142855\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.39285714285714285\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3076923076923077\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.4\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.32\n",
      "Ave Test Precision: 0.3698241758241758\n",
      "Stdev Test Precision: 0.052998007449563206\n",
      "Ave Test Accuracy: 0.7283582089552239\n",
      "Stdev Test Accuracy: 0.02033496506483393\n",
      "Ave Test Specificity: 0.9257425742574258\n",
      "Ave Test Recall: 0.12424242424242424\n",
      "Ave Test NPV: 0.7639623591077968\n",
      "Ave Test F1-Score: 0.1787402630963661\n",
      "Ave Test G-mean: 0.3317475209636972\n",
      "Ave Runtime: 0.008187580108642577\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyNetIncome. 26 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.21436226 0.\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.11441919        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.21436226 0.\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.21436226 0.\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.11441919        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.21436226 0.\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.11441919        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.11441919        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531]\n",
      "One or more of the train scores are non-finite: [0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25148267 0.\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.15479481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25148267 0.\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25148267 0.\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.15479481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25148267 0.\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.15479481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.15479481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2556990398972856\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.6907730673316709\n",
      "GridSearchCV Runtime: 5.196823596954346 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.21621621621621623\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24669603524229075\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2409090909090909\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2565217391304348\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2297872340425532\n",
      "Ave Test Precision: 0.2380260631081172\n",
      "Stdev Test Precision: 0.015565330252285143\n",
      "Ave Test Accuracy: 0.3888059701492537\n",
      "Stdev Test Accuracy: 0.16168819361247286\n",
      "Ave Test Specificity: 0.2881188118811881\n",
      "Ave Test Recall: 0.696969696969697\n",
      "Ave Test NPV: 0.7372670173215744\n",
      "Ave Test F1-Score: 0.33313487430823757\n",
      "Ave Test G-mean: 0.34361955132076955\n",
      "Ave Runtime: 0.00799713134765625\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positiveMonthlyFamilyNetIncome. 25 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.23020708 0.21807797 0.         0.25229283 0.26415682 0.23020708\n",
      " 0.19594136 0.         0.25229283 0.26415682 0.23118435 0.19594136\n",
      " 0.         0.25455179 0.28232675        nan        nan        nan\n",
      "        nan        nan 0.22785102 0.2171362  0.42698413 0.25482069\n",
      " 0.28162192 0.22785102 0.21986347 0.42698413 0.25374965 0.28162192\n",
      " 0.23020708 0.21807797 0.         0.25229283 0.26415682 0.23020708\n",
      " 0.19594136 0.         0.25229283 0.26615112 0.23118435 0.19594136\n",
      " 0.         0.25455179 0.28232675        nan        nan        nan\n",
      "        nan        nan 0.22785102 0.2171362  0.42698413 0.25482069\n",
      " 0.28162192 0.22785102 0.21986347 0.42698413 0.25374965 0.28162192\n",
      " 0.23020708 0.21807797 0.         0.25229283 0.26415682 0.23020708\n",
      " 0.19499199 0.         0.25229283 0.26615112 0.23118435 0.19499199\n",
      " 0.         0.25455179 0.28232675        nan        nan        nan\n",
      "        nan        nan 0.22785102 0.2171362  0.42698413 0.25482069\n",
      " 0.28162192 0.22785102 0.21986347 0.42698413 0.25374965 0.28162192\n",
      " 0.22905437 0.210105   0.         0.25487337 0.27027102 0.231137\n",
      " 0.19537299 0.         0.25370668 0.28269956 0.23118435 0.19537299\n",
      " 0.         0.25455179 0.28232675        nan        nan        nan\n",
      "        nan        nan 0.22785102 0.2171362  0.42698413 0.25482069\n",
      " 0.28162192 0.22785102 0.21986347 0.42698413 0.25374965 0.28162192\n",
      " 0.22856494 0.21331683 0.09       0.2532446  0.27432294 0.22942997\n",
      " 0.24247342 0.         0.25455179 0.28232675 0.22942997 0.24247342\n",
      " 0.         0.25455179 0.28232675        nan        nan        nan\n",
      "        nan        nan 0.22785102 0.2171362  0.42698413 0.25482069\n",
      " 0.28162192 0.22785102 0.21986347 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21986347 0.35166667 0.25374965 0.27985869 0.22785102\n",
      " 0.21144857 0.38857143 0.25482069 0.28273079 0.22785102 0.21144857\n",
      " 0.38857143 0.25374965 0.28273079        nan        nan        nan\n",
      "        nan        nan 0.22785102 0.2171362  0.42698413 0.25482069\n",
      " 0.28162192 0.22785102 0.21986347 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21986347 0.45638889 0.25374965 0.28162192 0.22785102\n",
      " 0.21986347 0.45638889 0.25482069 0.28162192 0.22785102 0.21986347\n",
      " 0.45638889 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.22785102 0.2171362  0.42698413 0.25482069\n",
      " 0.28162192 0.22785102 0.21986347 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.2171362  0.41698413 0.25374965 0.28162192 0.22785102\n",
      " 0.2171362  0.41698413 0.25482069 0.28162192 0.22785102 0.21986347\n",
      " 0.41698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.22785102 0.2171362  0.42698413 0.25482069\n",
      " 0.28162192 0.22785102 0.21986347 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.2171362  0.42698413 0.25374965 0.28162192 0.22785102\n",
      " 0.2171362  0.42698413 0.25482069 0.28162192 0.22785102 0.21986347\n",
      " 0.43365079 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.22785102 0.2171362  0.42698413 0.25482069\n",
      " 0.28162192 0.22785102 0.21986347 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.2171362  0.42698413 0.25374965 0.28162192 0.22785102\n",
      " 0.2171362  0.42698413 0.25482069 0.28162192 0.22785102 0.21986347\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.22785102 0.2171362  0.42698413 0.25482069\n",
      " 0.28162192 0.22785102 0.21986347 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.2171362  0.42698413 0.25374965 0.28162192 0.22785102\n",
      " 0.2171362  0.42698413 0.25482069 0.28162192 0.22785102 0.21986347\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.22785102 0.2171362  0.42698413 0.25482069\n",
      " 0.28162192 0.22785102 0.21986347 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.2171362  0.42698413 0.25374965 0.28162192 0.22785102\n",
      " 0.2171362  0.42698413 0.25482069 0.28162192 0.22785102 0.21986347\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.22785102 0.2171362  0.42698413 0.25482069\n",
      " 0.28162192 0.22785102 0.21986347 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.2171362  0.42698413 0.25374965 0.28162192 0.22785102\n",
      " 0.2171362  0.42698413 0.25482069 0.28162192 0.22785102 0.21986347\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.22785102 0.2171362  0.42698413 0.25482069\n",
      " 0.28162192 0.22785102 0.21986347 0.42698413 0.25374965 0.28162192]\n",
      "One or more of the train scores are non-finite: [0.27033275 0.2800086  0.03333333 0.28394181 0.27409043 0.27033275\n",
      " 0.1965714  0.         0.28394181 0.27409043 0.26901286 0.1965714\n",
      " 0.         0.28356117 0.2757061         nan        nan        nan\n",
      "        nan        nan 0.27063464 0.28038313 0.37322683 0.28350788\n",
      " 0.27680581 0.27036265 0.27984235 0.37393271 0.28331083 0.27680581\n",
      " 0.27033275 0.27918277 0.03333333 0.28394181 0.27409043 0.27033275\n",
      " 0.1965714  0.         0.28394181 0.27472024 0.26901286 0.1965714\n",
      " 0.         0.28356117 0.2757061         nan        nan        nan\n",
      "        nan        nan 0.27063464 0.28038313 0.37322683 0.28350788\n",
      " 0.27680581 0.27036265 0.27984235 0.37393271 0.28331083 0.27680581\n",
      " 0.27005936 0.27840041 0.03333333 0.28390931 0.27507648 0.26994732\n",
      " 0.19667117 0.         0.28390931 0.2745347  0.26901286 0.19667117\n",
      " 0.         0.28356117 0.2757061         nan        nan        nan\n",
      "        nan        nan 0.27063464 0.28038313 0.37322683 0.28350788\n",
      " 0.27680581 0.27036265 0.27984235 0.37393271 0.28331083 0.27680581\n",
      " 0.26928908 0.2823665  0.07333333 0.28311898 0.27540181 0.26942352\n",
      " 0.24778926 0.         0.28306857 0.2740498  0.26905018 0.24778926\n",
      " 0.         0.28356117 0.2757061         nan        nan        nan\n",
      "        nan        nan 0.27063464 0.28038313 0.37322683 0.28350788\n",
      " 0.27680581 0.27036265 0.27984235 0.37393271 0.28331083 0.27680581\n",
      " 0.27007443 0.28148903 0.35621118 0.28299677 0.27304643 0.26880455\n",
      " 0.23170721 0.05714286 0.28327549 0.27576658 0.26865946 0.23170721\n",
      " 0.05714286 0.28345234 0.27572421        nan        nan        nan\n",
      "        nan        nan 0.27063464 0.28038313 0.37322683 0.28350788\n",
      " 0.27680581 0.27036265 0.27984235 0.37393271 0.28331083 0.27680581\n",
      " 0.27026192 0.28000858 0.45708598 0.2830814  0.27399222 0.27015145\n",
      " 0.28369665 0.48946244 0.28345885 0.27660166 0.27031492 0.28260375\n",
      " 0.48946244 0.28331083 0.27660166        nan        nan        nan\n",
      "        nan        nan 0.27063464 0.28038313 0.37322683 0.28350788\n",
      " 0.27680581 0.27036265 0.27984235 0.37393271 0.28331083 0.27680581\n",
      " 0.27040912 0.28038313 0.37752401 0.28317719 0.27661003 0.27033056\n",
      " 0.27930933 0.37572062 0.28350788 0.27664447 0.27036265 0.27973627\n",
      " 0.37572062 0.28331083 0.27664447        nan        nan        nan\n",
      "        nan        nan 0.27063464 0.28038313 0.37322683 0.28350788\n",
      " 0.27680581 0.27036265 0.27984235 0.37393271 0.28331083 0.27680581\n",
      " 0.27040912 0.28038313 0.37870884 0.28317719 0.27680581 0.27063464\n",
      " 0.28038313 0.37947414 0.28350788 0.27680581 0.27036265 0.27984235\n",
      " 0.3789761  0.28331083 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.27063464 0.28038313 0.37322683 0.28350788\n",
      " 0.27680581 0.27036265 0.27984235 0.37393271 0.28331083 0.27680581\n",
      " 0.27040912 0.28038313 0.37322683 0.28317719 0.27680581 0.27063464\n",
      " 0.28038313 0.37322683 0.28350788 0.27680581 0.27036265 0.27984235\n",
      " 0.37393271 0.28331083 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.27063464 0.28038313 0.37322683 0.28350788\n",
      " 0.27680581 0.27036265 0.27984235 0.37393271 0.28331083 0.27680581\n",
      " 0.27040912 0.28038313 0.37322683 0.28317719 0.27680581 0.27063464\n",
      " 0.28038313 0.37322683 0.28350788 0.27680581 0.27036265 0.27984235\n",
      " 0.37393271 0.28331083 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.27063464 0.28038313 0.37322683 0.28350788\n",
      " 0.27680581 0.27036265 0.27984235 0.37393271 0.28331083 0.27680581\n",
      " 0.27040912 0.28038313 0.37322683 0.28317719 0.27680581 0.27063464\n",
      " 0.28038313 0.37322683 0.28350788 0.27680581 0.27036265 0.27984235\n",
      " 0.37393271 0.28331083 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.27063464 0.28038313 0.37322683 0.28350788\n",
      " 0.27680581 0.27036265 0.27984235 0.37393271 0.28331083 0.27680581\n",
      " 0.27040912 0.28038313 0.37322683 0.28317719 0.27680581 0.27063464\n",
      " 0.28038313 0.37322683 0.28350788 0.27680581 0.27036265 0.27984235\n",
      " 0.37393271 0.28331083 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.27063464 0.28038313 0.37322683 0.28350788\n",
      " 0.27680581 0.27036265 0.27984235 0.37393271 0.28331083 0.27680581\n",
      " 0.27040912 0.28038313 0.37322683 0.28317719 0.27680581 0.27063464\n",
      " 0.28038313 0.37322683 0.28350788 0.27680581 0.27036265 0.27984235\n",
      " 0.37393271 0.28331083 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.27063464 0.28038313 0.37322683 0.28350788\n",
      " 0.27680581 0.27036265 0.27984235 0.37393271 0.28331083 0.27680581]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.4563888888888889\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7381546134663342\n",
      "GridSearchCV Runtime: 5.922937631607056 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2692307692307692\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.47058823529411764\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.23529411764705882\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3\n",
      "Ave Test Precision: 0.3350226244343891\n",
      "Stdev Test Precision: 0.09759741503057694\n",
      "Ave Test Accuracy: 0.7291044776119403\n",
      "Stdev Test Accuracy: 0.016811761680699696\n",
      "Ave Test Specificity: 0.9366336633663366\n",
      "Ave Test Recall: 0.09393939393939395\n",
      "Ave Test NPV: 0.7598042127272903\n",
      "Ave Test F1-Score: 0.14580271428371616\n",
      "Ave Test G-mean: 0.2948018035280308\n",
      "Ave Runtime: 0.008198022842407227\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlySoloNetIncomeWithSavings. 24 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "910 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "208 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "598 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.                nan 0.2274904  0.26063011 0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.05746677        nan 0.2274904\n",
      " 0.26063011 0.20721634 0.05746677        nan 0.2274904  0.26063011\n",
      " 0.20721634 0.0805437         nan 0.2274904  0.26063011 0.20721634\n",
      " 0.                nan 0.2274904  0.26063011 0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.05746677        nan 0.2274904\n",
      " 0.26063011 0.20721634 0.05746677        nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.                nan 0.2274904  0.26063011 0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.05746677        nan 0.2274904\n",
      " 0.26063011 0.20721634 0.05746677        nan 0.2274904  0.26063011\n",
      " 0.20721634 0.0805437         nan 0.2274904  0.26063011 0.20721634\n",
      " 0.                nan 0.2274904  0.26063011 0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.05746677        nan 0.2274904\n",
      " 0.26063011 0.20721634 0.05746677        nan 0.2274904  0.26063011\n",
      " 0.20721634 0.0805437         nan 0.2274904  0.26063011 0.20721634\n",
      " 0.02774327        nan 0.2274904  0.26063011 0.20721634 0.02774327\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.05746677        nan 0.2274904\n",
      " 0.26063011 0.20721634 0.05746677        nan 0.2274904  0.26063011\n",
      " 0.20721634 0.0805437         nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.05746677        nan 0.2274904\n",
      " 0.26063011 0.20721634 0.05746677        nan 0.2274904  0.26063011\n",
      " 0.20721634 0.0805437         nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.05746677        nan 0.2274904\n",
      " 0.26063011 0.20721634 0.05746677        nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.05746677        nan 0.2274904\n",
      " 0.26063011 0.20721634 0.05746677        nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.05746677        nan 0.2274904\n",
      " 0.26063011 0.20721634 0.05746677        nan 0.2274904  0.26063011\n",
      " 0.20721634 0.0805437         nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.05746677        nan 0.2274904\n",
      " 0.26063011 0.20721634 0.05746677        nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.05746677        nan 0.2274904\n",
      " 0.26063011 0.20721634 0.05746677        nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.05746677        nan 0.2274904\n",
      " 0.26063011 0.20721634 0.05746677        nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.05746677        nan 0.2274904\n",
      " 0.26063011 0.20721634 0.05746677        nan 0.2274904  0.26063011]\n",
      "One or more of the train scores are non-finite: [0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.                nan 0.24849319 0.24447839 0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.10161665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.10161665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.12661665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.                nan 0.24849319 0.24447839 0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.10161665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.10161665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.                nan 0.24849319 0.24447839 0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.10161665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.10161665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.12661665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.                nan 0.24849319 0.24447839 0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.10161665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.10161665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.12661665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.05141887        nan 0.24849319 0.24447839 0.2468814  0.05141887\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.10161665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.10161665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.12661665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.10161665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.10161665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.12661665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.10161665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.10161665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.10161665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.10161665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.10161665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.10161665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.12661665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.10161665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.10161665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.10161665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.10161665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.10161665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.10161665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.10161665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.10161665        nan 0.24849319 0.24447839]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2606301088307362\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6384039900249376\n",
      "GridSearchCV Runtime: 6.2774341106414795 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.16393442622950818\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24390243902439024\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.22613065326633167\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2660098522167488\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.24537037037037038\n",
      "Ave Test Precision: 0.22906954822146983\n",
      "Stdev Test Precision: 0.039056173430170205\n",
      "Ave Test Accuracy: 0.41044776119402987\n",
      "Stdev Test Accuracy: 0.10865835655642565\n",
      "Ave Test Specificity: 0.3346534653465346\n",
      "Ave Test Recall: 0.6424242424242423\n",
      "Ave Test NPV: 0.7473074268726443\n",
      "Ave Test F1-Score: 0.32869603203544856\n",
      "Ave Test G-mean: 0.40319526001692435\n",
      "Ave Runtime: 0.008349752426147461\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positiveMonthlySoloNetIncomeWithSavings. 23 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24354457 0.25053046 0.         0.24354457 0.23830236 0.24354457\n",
      " 0.22250386 0.         0.24354457 0.23830236 0.24458784 0.22013491\n",
      " 0.         0.24621365 0.23911882        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24776077 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463\n",
      " 0.24354457 0.25053046 0.         0.24354457 0.23830236 0.24354457\n",
      " 0.22250386 0.         0.24354457 0.23830236 0.24458784 0.22013491\n",
      " 0.         0.24621365 0.23911882        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24776077 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463\n",
      " 0.24354457 0.25053046 0.         0.24354457 0.23830236 0.24354457\n",
      " 0.22250386 0.         0.24354457 0.23119197 0.24458784 0.22013491\n",
      " 0.         0.24621365 0.23911882        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24776077 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463\n",
      " 0.24354457 0.2514853  0.         0.24354457 0.23830236 0.24345959\n",
      " 0.25581355 0.         0.24530565 0.23192449 0.24458784 0.2534446\n",
      " 0.         0.24621365 0.23911882        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24776077 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463\n",
      " 0.24419236 0.25096832 0.025      0.24486367 0.23118922 0.24534578\n",
      " 0.25193293 0.         0.24408384 0.23874288 0.24534578 0.25193293\n",
      " 0.         0.24408384 0.23911882        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24776077 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463\n",
      " 0.24520716 0.24881292 0.06666667 0.24108347 0.23132611 0.24419163\n",
      " 0.24881292 0.05333333 0.24108347 0.23644849 0.24373965 0.24881292\n",
      " 0.05333333 0.24108347 0.23644849        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24776077 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463\n",
      " 0.24419163 0.24847545 0.4        0.24108347 0.23527869 0.24459076\n",
      " 0.24847545 0.4        0.24108347 0.23565463 0.24419163 0.24847545\n",
      " 0.4        0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24776077 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24776077 0.3552381  0.24108347 0.23565463 0.24459076\n",
      " 0.24776077 0.33857143 0.24108347 0.23565463 0.24459076 0.24776077\n",
      " 0.33857143 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24776077 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24776077 0.39166667 0.24108347 0.23565463 0.24459076 0.24776077\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24776077 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24776077 0.39166667 0.24108347 0.23565463 0.24459076 0.24776077\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24776077 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24776077 0.39166667 0.24108347 0.23565463 0.24459076 0.24776077\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24776077 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24776077 0.39166667 0.24108347 0.23565463 0.24459076 0.24776077\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24776077 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24776077 0.39166667 0.24108347 0.23565463 0.24459076 0.24776077\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24776077 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24776077 0.39166667 0.24108347 0.23565463]\n",
      "One or more of the train scores are non-finite: [0.24318343 0.24208175 0.04615385 0.24318343 0.25413466 0.24318343\n",
      " 0.22014414 0.         0.24318343 0.25413466 0.24093331 0.22017984\n",
      " 0.         0.24101197 0.25910633        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24231663 0.36347605 0.24201209\n",
      " 0.25938768 0.24130573 0.2422778  0.36347605 0.24196504 0.25934383\n",
      " 0.24318343 0.24208175 0.04615385 0.24318343 0.25413466 0.24318343\n",
      " 0.22014414 0.         0.24318343 0.25499625 0.24093331 0.22017984\n",
      " 0.         0.24101197 0.25910633        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24231663 0.36347605 0.24201209\n",
      " 0.25938768 0.24130573 0.2422778  0.36347605 0.24196504 0.25934383\n",
      " 0.24303643 0.24232981 0.04615385 0.24303643 0.25359338 0.24303643\n",
      " 0.22065598 0.         0.24303643 0.25607679 0.24093331 0.22069168\n",
      " 0.         0.24101197 0.25910633        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24231663 0.36347605 0.24201209\n",
      " 0.25938768 0.24130573 0.2422778  0.36347605 0.24196504 0.25934383\n",
      " 0.243531   0.24159806 0.04285714 0.24301432 0.25496763 0.2417113\n",
      " 0.25390393 0.         0.24190584 0.25852105 0.24093331 0.25400491\n",
      " 0.         0.24101197 0.25914979        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24231663 0.36347605 0.24201209\n",
      " 0.25938768 0.24130573 0.2422778  0.36347605 0.24196504 0.25934383\n",
      " 0.24141542 0.24124673 0.04375    0.24134836 0.25549291 0.24141221\n",
      " 0.2387183  0.04285714 0.24056896 0.25808889 0.24141221 0.2387183\n",
      " 0.04285714 0.24056896 0.25829609        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24231663 0.36347605 0.24201209\n",
      " 0.25938768 0.24130573 0.2422778  0.36347605 0.24196504 0.25934383\n",
      " 0.24234624 0.24190629 0.19333333 0.24198152 0.25380968 0.24129756\n",
      " 0.24161349 0.19067669 0.24150633 0.25838027 0.24192757 0.24169123\n",
      " 0.19067669 0.24146213 0.25838027        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24231663 0.36347605 0.24201209\n",
      " 0.25938768 0.24130573 0.2422778  0.36347605 0.24196504 0.25934383\n",
      " 0.24130573 0.24198206 0.37146063 0.24191817 0.25841375 0.24130573\n",
      " 0.24198206 0.37544517 0.24191817 0.25893866 0.24130573 0.24198206\n",
      " 0.37544517 0.24187316 0.25884606        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24231663 0.36347605 0.24201209\n",
      " 0.25938768 0.24130573 0.2422778  0.36347605 0.24196504 0.25934383\n",
      " 0.24135079 0.2422778  0.3707086  0.24196504 0.25944955 0.24135079\n",
      " 0.24185728 0.37367157 0.24201209 0.2594934  0.24130573 0.24185728\n",
      " 0.37668914 0.24196504 0.25944955        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24231663 0.36347605 0.24201209\n",
      " 0.25938768 0.24130573 0.2422778  0.36347605 0.24196504 0.25934383\n",
      " 0.24135079 0.24231663 0.36514536 0.24196504 0.25934383 0.24135079\n",
      " 0.2422778  0.36514536 0.24201209 0.25938768 0.24130573 0.2422778\n",
      " 0.36614109 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24231663 0.36347605 0.24201209\n",
      " 0.25938768 0.24130573 0.2422778  0.36347605 0.24196504 0.25934383\n",
      " 0.24135079 0.24231663 0.36347605 0.24196504 0.25934383 0.24135079\n",
      " 0.24231663 0.36347605 0.24201209 0.25938768 0.24130573 0.2422778\n",
      " 0.36347605 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24231663 0.36347605 0.24201209\n",
      " 0.25938768 0.24130573 0.2422778  0.36347605 0.24196504 0.25934383\n",
      " 0.24135079 0.24231663 0.36347605 0.24196504 0.25934383 0.24135079\n",
      " 0.24231663 0.36347605 0.24201209 0.25938768 0.24130573 0.2422778\n",
      " 0.36347605 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24231663 0.36347605 0.24201209\n",
      " 0.25938768 0.24130573 0.2422778  0.36347605 0.24196504 0.25934383\n",
      " 0.24135079 0.24231663 0.36347605 0.24196504 0.25934383 0.24135079\n",
      " 0.24231663 0.36347605 0.24201209 0.25938768 0.24130573 0.2422778\n",
      " 0.36347605 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24231663 0.36347605 0.24201209\n",
      " 0.25938768 0.24130573 0.2422778  0.36347605 0.24196504 0.25934383\n",
      " 0.24135079 0.24231663 0.36347605 0.24196504 0.25934383 0.24135079\n",
      " 0.24231663 0.36347605 0.24201209 0.25938768 0.24130573 0.2422778\n",
      " 0.36347605 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24231663 0.36347605 0.24201209\n",
      " 0.25938768 0.24130573 0.2422778  0.36347605 0.24196504 0.25934383]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.4\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7456359102244389\n",
      "GridSearchCV Runtime: 4.632385730743408 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4375\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.4782608695652174\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.32558139534883723\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3103448275862069\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.27586206896551724\n",
      "Ave Test Precision: 0.36550983229315576\n",
      "Stdev Test Precision: 0.08742076404260002\n",
      "Ave Test Accuracy: 0.7223880597014926\n",
      "Stdev Test Accuracy: 0.02412422369439497\n",
      "Ave Test Specificity: 0.9099009900990099\n",
      "Ave Test Recall: 0.1484848484848485\n",
      "Ave Test NPV: 0.7658201121452765\n",
      "Ave Test F1-Score: 0.2065396378680175\n",
      "Ave Test G-mean: 0.36411450610030377\n",
      "Ave Runtime: 0.007566595077514648\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyNetIncomeWithSavings. 22 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.                nan 0.21092713 0.1853669  0.15540956 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.15540956 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.                nan 0.21092713 0.1853669  0.15540956 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.15540956 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.                nan 0.21092713 0.1853669  0.15540956 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.15540956 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.                nan 0.21092713 0.1853669  0.15540956 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.15540956 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.15540956 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.15540956 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.15540956 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.15540956 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.15540956 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.15540956 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.15540956 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.15540956 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.15540956 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.15540956 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.15540956 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.15540956 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.15540956 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.15540956 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.15540956 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.15540956 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.15540956 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.15540956 0.05095238        nan 0.21092713 0.1853669 ]\n",
      "One or more of the train scores are non-finite: [0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.                nan 0.25330918 0.20137462 0.23054683 0.\n",
      "        nan 0.25330918 0.22776351        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.23054683 0.07827183        nan 0.25330918 0.22776351\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.                nan 0.25330918 0.20137462 0.23054683 0.\n",
      "        nan 0.25330918 0.22776351        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.23054683 0.07827183        nan 0.25330918 0.22776351\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.                nan 0.25330918 0.20137462 0.23054683 0.\n",
      "        nan 0.25330918 0.22776351        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.23054683 0.07827183        nan 0.25330918 0.22776351\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.                nan 0.25330918 0.20137462 0.23054683 0.\n",
      "        nan 0.25330918 0.22776351        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.23054683 0.07827183        nan 0.25330918 0.22776351\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.23054683 0.07827183\n",
      "        nan 0.25330918 0.22776351        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.23054683 0.07827183        nan 0.25330918 0.22776351\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.23054683 0.07827183\n",
      "        nan 0.25330918 0.22776351        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.23054683 0.07827183        nan 0.25330918 0.22776351\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.23054683 0.07827183\n",
      "        nan 0.25330918 0.22776351        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.23054683 0.07827183        nan 0.25330918 0.22776351\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.23054683 0.07827183\n",
      "        nan 0.25330918 0.22776351        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.23054683 0.07827183        nan 0.25330918 0.22776351\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.23054683 0.07827183\n",
      "        nan 0.25330918 0.22776351        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.23054683 0.07827183        nan 0.25330918 0.22776351\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.23054683 0.07827183\n",
      "        nan 0.25330918 0.22776351        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.23054683 0.07827183        nan 0.25330918 0.22776351\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.23054683 0.07827183\n",
      "        nan 0.25330918 0.22776351        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.23054683 0.07827183        nan 0.25330918 0.22776351\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.23054683 0.07827183\n",
      "        nan 0.25330918 0.22776351        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.23054683 0.07827183        nan 0.25330918 0.22776351\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.23054683 0.07827183\n",
      "        nan 0.25330918 0.22776351        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.23054683 0.07827183        nan 0.25330918 0.22776351]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.21092712842712844\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7082294264339152\n",
      "GridSearchCV Runtime: 4.724727153778076 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.15384615384615385\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24583333333333332\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.23809523809523808\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2489451476793249\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.22950819672131148\n",
      "Ave Test Precision: 0.22324561393507233\n",
      "Stdev Test Precision: 0.039517994141190345\n",
      "Ave Test Accuracy: 0.3716417910447761\n",
      "Stdev Test Accuracy: 0.17704427659305888\n",
      "Ave Test Specificity: 0.26237623762376244\n",
      "Ave Test Recall: 0.7060606060606062\n",
      "Ave Test NPV: 0.7108062474631389\n",
      "Ave Test F1-Score: 0.318735414723443\n",
      "Ave Test G-mean: 0.2866295334225509\n",
      "Ave Runtime: 0.0065271854400634766\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positiveMonthlyFamilyNetIncomeWithSavings. 21 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24644433 0.24405549 0.         0.24644433 0.2763146  0.24644433\n",
      " 0.16386385 0.         0.24644433 0.2763146  0.24786284 0.16320925\n",
      " 0.         0.24753768 0.26561708        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18357576 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18357576 0.24796617 0.26709332\n",
      " 0.24644433 0.24405549 0.         0.24644433 0.2763146  0.24644433\n",
      " 0.16386385 0.         0.24644433 0.2763146  0.24786284 0.16320925\n",
      " 0.         0.24753768 0.26561708        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18357576 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18357576 0.24796617 0.26709332\n",
      " 0.24644433 0.24405549 0.         0.24644433 0.2763146  0.24644433\n",
      " 0.16386385 0.         0.24644433 0.27397677 0.24786284 0.16320925\n",
      " 0.         0.24753768 0.26561708        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18357576 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18357576 0.24796617 0.26709332\n",
      " 0.24677089 0.24299316 0.         0.24677089 0.27610792 0.24685825\n",
      " 0.18019237 0.         0.24581211 0.27156411 0.24786284 0.18019237\n",
      " 0.         0.24753768 0.26561708        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18357576 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18357576 0.24796617 0.26709332\n",
      " 0.24743978 0.239229   0.02916667 0.24708313 0.27291673 0.24696866\n",
      " 0.24866684 0.         0.24606789 0.26703643 0.24696866 0.24866684\n",
      " 0.         0.24606789 0.26561708        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18357576 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18357576 0.24796617 0.26709332\n",
      " 0.24479608 0.23292743 0.02790698 0.24603326 0.26612041 0.24479608\n",
      " 0.23725361 0.02790698 0.24687881 0.26596361 0.24479608 0.23725361\n",
      " 0.02790698 0.24687881 0.26596361        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18357576 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18357576 0.24796617 0.26709332\n",
      " 0.24533351 0.23670355 0.15933333 0.24796617 0.26874723 0.24533351\n",
      " 0.23670355 0.15933333 0.24796617 0.26874723 0.24533351 0.23670355\n",
      " 0.15933333 0.24796617 0.26874723        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18357576 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18357576 0.24796617 0.26709332\n",
      " 0.24750934 0.2352906  0.18357576 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18357576 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18347126 0.24796617 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18357576 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18357576 0.24796617 0.26709332\n",
      " 0.24750934 0.2352906  0.18357576 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18357576 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18357576 0.24796617 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18357576 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18357576 0.24796617 0.26709332\n",
      " 0.24750934 0.2352906  0.18357576 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18357576 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18357576 0.24796617 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18357576 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18357576 0.24796617 0.26709332\n",
      " 0.24750934 0.2352906  0.18357576 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18357576 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18357576 0.24796617 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18357576 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18357576 0.24796617 0.26709332\n",
      " 0.24750934 0.2352906  0.18357576 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18357576 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18357576 0.24796617 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18357576 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18357576 0.24796617 0.26709332\n",
      " 0.24750934 0.2352906  0.18357576 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18357576 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18357576 0.24796617 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18357576 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18357576 0.24796617 0.26709332]\n",
      "One or more of the train scores are non-finite: [0.25240699 0.25523339 0.18333333 0.25240699 0.24146081 0.25240699\n",
      " 0.17339621 0.         0.25240699 0.24146081 0.25184383 0.17334813\n",
      " 0.         0.25184939 0.24227824        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.25135908 0.30715842 0.25202742\n",
      " 0.2429277  0.25228095 0.25131983 0.30715842 0.25219421 0.2429277\n",
      " 0.25240699 0.25523339 0.18333333 0.25240699 0.24146081 0.25240699\n",
      " 0.17339621 0.         0.25240699 0.24146081 0.25184383 0.17334813\n",
      " 0.         0.25184939 0.24227824        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.25135908 0.30715842 0.25202742\n",
      " 0.2429277  0.25228095 0.25131983 0.30715842 0.25219421 0.2429277\n",
      " 0.25231542 0.25518525 0.18103448 0.25231542 0.24146081 0.25205229\n",
      " 0.1734644  0.         0.25214454 0.24132054 0.25184383 0.17341633\n",
      " 0.         0.25184939 0.24227824        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.25135908 0.30715842 0.25202742\n",
      " 0.2429277  0.25228095 0.25131983 0.30715842 0.25219421 0.2429277\n",
      " 0.25172546 0.25340293 0.17647059 0.25191065 0.24147052 0.25169808\n",
      " 0.30181963 0.         0.25152083 0.24232689 0.25184383 0.30181963\n",
      " 0.         0.25170357 0.24227824        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.25135908 0.30715842 0.25202742\n",
      " 0.2429277  0.25228095 0.25131983 0.30715842 0.25219421 0.2429277\n",
      " 0.25159641 0.25212558 0.17564103 0.25166131 0.2433999  0.25156427\n",
      " 0.25250803 0.08       0.25174371 0.24173583 0.25156427 0.25250803\n",
      " 0.08       0.25169381 0.2423596         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.25135908 0.30715842 0.25202742\n",
      " 0.2429277  0.25228095 0.25131983 0.30715842 0.25219421 0.2429277\n",
      " 0.25176414 0.24938688 0.19552283 0.25146648 0.24264281 0.25181766\n",
      " 0.25152041 0.23385538 0.25203628 0.24279718 0.2519151  0.25152041\n",
      " 0.23385538 0.25183767 0.24279718        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.25135908 0.30715842 0.25202742\n",
      " 0.2429277  0.25228095 0.25131983 0.30715842 0.25219421 0.2429277\n",
      " 0.25197117 0.24920161 0.3053589  0.25219421 0.24251386 0.25197117\n",
      " 0.24920161 0.31440931 0.25219421 0.24262022 0.25197117 0.24920161\n",
      " 0.31440931 0.25219421 0.24262022        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.25135908 0.30715842 0.25202742\n",
      " 0.2429277  0.25228095 0.25131983 0.30715842 0.25219421 0.2429277\n",
      " 0.25228095 0.25131983 0.30436057 0.25202742 0.2429277  0.25228095\n",
      " 0.24965317 0.30436057 0.25202742 0.2429277  0.25228095 0.24953257\n",
      " 0.30456008 0.25219421 0.24306754        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.25135908 0.30715842 0.25202742\n",
      " 0.2429277  0.25228095 0.25131983 0.30715842 0.25219421 0.2429277\n",
      " 0.25228095 0.25131983 0.30844047 0.25202742 0.2429277  0.25228095\n",
      " 0.25131983 0.30715842 0.25202742 0.2429277  0.25228095 0.25131983\n",
      " 0.30844047 0.25219421 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.25135908 0.30715842 0.25202742\n",
      " 0.2429277  0.25228095 0.25131983 0.30715842 0.25219421 0.2429277\n",
      " 0.25228095 0.25135908 0.30715842 0.25202742 0.2429277  0.25228095\n",
      " 0.25135908 0.30715842 0.25202742 0.2429277  0.25228095 0.25131983\n",
      " 0.30715842 0.25219421 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.25135908 0.30715842 0.25202742\n",
      " 0.2429277  0.25228095 0.25131983 0.30715842 0.25219421 0.2429277\n",
      " 0.25228095 0.25135908 0.30715842 0.25202742 0.2429277  0.25228095\n",
      " 0.25135908 0.30715842 0.25202742 0.2429277  0.25228095 0.25131983\n",
      " 0.30715842 0.25219421 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.25135908 0.30715842 0.25202742\n",
      " 0.2429277  0.25228095 0.25131983 0.30715842 0.25219421 0.2429277\n",
      " 0.25228095 0.25135908 0.30715842 0.25202742 0.2429277  0.25228095\n",
      " 0.25135908 0.30715842 0.25202742 0.2429277  0.25228095 0.25131983\n",
      " 0.30715842 0.25219421 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.25135908 0.30715842 0.25202742\n",
      " 0.2429277  0.25228095 0.25131983 0.30715842 0.25219421 0.2429277\n",
      " 0.25228095 0.25135908 0.30715842 0.25202742 0.2429277  0.25228095\n",
      " 0.25135908 0.30715842 0.25202742 0.2429277  0.25228095 0.25131983\n",
      " 0.30715842 0.25219421 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.25135908 0.30715842 0.25202742\n",
      " 0.2429277  0.25228095 0.25131983 0.30715842 0.25219421 0.2429277 ]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2763145999890244\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.3827930174563591\n",
      "GridSearchCV Runtime: 4.582400798797607 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.22727272727272727\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2562814070351759\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2702702702702703\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.22981366459627328\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2654320987654321\n",
      "Ave Test Precision: 0.24981403358797577\n",
      "Stdev Test Precision: 0.020076937942556078\n",
      "Ave Test Accuracy: 0.441044776119403\n",
      "Stdev Test Accuracy: 0.08146813641576774\n",
      "Ave Test Specificity: 0.3811881188118812\n",
      "Ave Test Recall: 0.6242424242424243\n",
      "Ave Test NPV: 0.75306003262724\n",
      "Ave Test F1-Score: 0.35359639480347205\n",
      "Ave Test G-mean: 0.4731300236823454\n",
      "Ave Runtime: 0.005915641784667969\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyIncome - basicMonthlySalary. 20 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.02469136        nan 0.23816419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.02469136        nan 0.23816419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.13472673        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.02469136        nan 0.23816419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.02469136        nan 0.23816419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.0614375         nan 0.23816419 0.23857484 0.22847171 0.0614375\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.13472673        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.13472673        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484]\n",
      "One or more of the train scores are non-finite: [0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.02454924        nan 0.24653004 0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.02454924        nan 0.24653004 0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.17496007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.02454924        nan 0.24653004 0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.02454924        nan 0.24653004 0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.07504597        nan 0.24653004 0.24416029 0.24763775 0.07504597\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.17496007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.17496007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.23857484303136473\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6159600997506235\n",
      "GridSearchCV Runtime: 4.532003402709961 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2345679012345679\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.22702702702702704\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2289156626506024\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.25\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.18810211818243947\n",
      "Stdev Test Precision: 0.10553875777822423\n",
      "Ave Test Accuracy: 0.5350746268656716\n",
      "Stdev Test Accuracy: 0.16393723703808416\n",
      "Ave Test Specificity: 0.5792079207920793\n",
      "Ave Test Recall: 0.4\n",
      "Ave Test NPV: 0.7451700861988897\n",
      "Ave Test F1-Score: 0.24555207135696358\n",
      "Ave Test G-mean: 0.3489706892191612\n",
      "Ave Runtime: 0.007153654098510742\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positive monthlyFamilyIncome - basicMonthlySalary. 19 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.19749532 0.27469057 0.         0.19749532 0.2880568  0.19749532\n",
      " 0.09813272 0.         0.19749532 0.2880568  0.20121743 0.09813272\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.19749532 0.27498675 0.         0.19749532 0.2880568  0.19749532\n",
      " 0.09813272 0.         0.19749532 0.2880568  0.20121743 0.09813272\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.19768856 0.25832009 0.         0.19749532 0.28651834 0.19768856\n",
      " 0.09813272 0.         0.19749532 0.28021214 0.20121743 0.09813272\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20009875 0.25554323 0.         0.19872282 0.27760256 0.19957936\n",
      " 0.27460799 0.         0.19806372 0.28242606 0.20121743 0.27460799\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20055298 0.25649601 0.         0.20001438 0.27697187 0.20121743\n",
      " 0.26569488 0.         0.19760485 0.26994124 0.20121743 0.26533271\n",
      " 0.         0.19760485 0.26994124        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.24690454 0.275      0.19760485 0.27134615 0.20121743\n",
      " 0.25134898 0.1        0.19760485 0.27000482 0.20121743 0.25134898\n",
      " 0.1        0.19760485 0.27138326        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.36666667 0.19760485 0.27063283 0.20121743\n",
      " 0.23692618 0.36666667 0.19760485 0.27063283 0.20121743 0.23692618\n",
      " 0.36666667 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743\n",
      " 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743 0.23732245\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743\n",
      " 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743 0.23732245\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743\n",
      " 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743 0.23732245\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743\n",
      " 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743 0.23732245\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743\n",
      " 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743 0.23732245\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743\n",
      " 0.23732245 0.37333333 0.19760485 0.27063283 0.20121743 0.23732245\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20121743 0.23732245 0.37333333 0.19760485\n",
      " 0.27063283 0.20121743 0.23732245 0.37333333 0.19760485 0.27063283]\n",
      "One or more of the train scores are non-finite: [0.21013357 0.28610787 0.06       0.21013357 0.27674454 0.21013357\n",
      " 0.09826745 0.         0.21013357 0.27674454 0.21190105 0.09826745\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.21013357 0.28610787 0.06       0.21013357 0.27674454 0.21013357\n",
      " 0.09826745 0.         0.21013357 0.27674454 0.21190105 0.09826745\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.21022921 0.28707353 0.06       0.21003911 0.27637863 0.21022921\n",
      " 0.17343793 0.         0.21003911 0.2743568  0.21190105 0.17343793\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.20951384 0.2752861  0.05454545 0.20925355 0.27394389 0.2113608\n",
      " 0.29855767 0.         0.20951325 0.27670069 0.21190105 0.29855767\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.21116886 0.27570013 0.1        0.21037945 0.26866329 0.21163474\n",
      " 0.29150852 0.06666667 0.21040251 0.26425128 0.21163474 0.28286277\n",
      " 0.06666667 0.21059004 0.26425128        nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.21153733 0.26447815 0.45666667 0.21079549 0.26295842 0.2116334\n",
      " 0.26845463 0.47740741 0.2106925  0.26216079 0.21182487 0.26845463\n",
      " 0.47740741 0.21077353 0.26248218        nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.21168281 0.26451259 0.43639443 0.21098925 0.26238847 0.21163606\n",
      " 0.26471854 0.43981589 0.21086072 0.26229773 0.2118757  0.26477016\n",
      " 0.43981589 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.2117339  0.26451259 0.40842028 0.21098925 0.26210887 0.21163606\n",
      " 0.26451259 0.40842028 0.21086072 0.26210887 0.21192679 0.26444097\n",
      " 0.40842028 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.2117339  0.26451259 0.40635768 0.21098925 0.26210887 0.21163606\n",
      " 0.26451259 0.40635768 0.21086072 0.26210887 0.21192679 0.26444097\n",
      " 0.40635768 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.2117339  0.26451259 0.40635768 0.21098925 0.26210887 0.21163606\n",
      " 0.26451259 0.40635768 0.21086072 0.26210887 0.21192679 0.26444097\n",
      " 0.40635768 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.2117339  0.26451259 0.40635768 0.21098925 0.26210887 0.21163606\n",
      " 0.26451259 0.40635768 0.21086072 0.26210887 0.21192679 0.26444097\n",
      " 0.40635768 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.2117339  0.26451259 0.40635768 0.21098925 0.26210887 0.21163606\n",
      " 0.26451259 0.40635768 0.21086072 0.26210887 0.21192679 0.26444097\n",
      " 0.40635768 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036\n",
      " 0.2117339  0.26451259 0.40635768 0.21098925 0.26210887 0.21163606\n",
      " 0.26451259 0.40635768 0.21086072 0.26210887 0.21192679 0.26444097\n",
      " 0.40635768 0.21094175 0.2625036         nan        nan        nan\n",
      "        nan        nan 0.21163606 0.26451259 0.40635768 0.21086072\n",
      " 0.26210887 0.21192679 0.26444097 0.40635768 0.21094175 0.2625036 ]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3733333333333333\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.7481296758104738\n",
      "GridSearchCV Runtime: 4.44258713722229 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3611111111111111\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3076923076923077\n",
      "Ave Test Precision: 0.3309035409035409\n",
      "Stdev Test Precision: 0.048017384689393074\n",
      "Ave Test Accuracy: 0.7335820895522387\n",
      "Stdev Test Accuracy: 0.011068952965814453\n",
      "Ave Test Specificity: 0.9455445544554454\n",
      "Ave Test Recall: 0.08484848484848485\n",
      "Ave Test NPV: 0.7599647714031972\n",
      "Ave Test F1-Score: 0.12807566197698267\n",
      "Ave Test G-mean: 0.2700808635673128\n",
      "Ave Runtime: 0.008236455917358398\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with basicMonthlySalary - monthlyExpenses. 18 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1105 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "182 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "364 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "559 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25989423 0.25989423        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.25989423        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.25989423        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.25989423        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.25989423        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.25989423        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.22527885        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423]\n",
      "One or more of the train scores are non-finite: [0.25887731 0.25887731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.25887731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.25887731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.25887731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.25887731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.25887731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.23387731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.259894230383722\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.415211970074813\n",
      "GridSearchCV Runtime: 4.311971187591553 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2736842105263158\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25263157894736843\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.22941176470588234\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.28888888888888886\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2512820512820513\n",
      "Ave Test Precision: 0.25917969887010134\n",
      "Stdev Test Precision: 0.022827600872137695\n",
      "Ave Test Accuracy: 0.42164179104477606\n",
      "Stdev Test Accuracy: 0.03099486516014207\n",
      "Ave Test Specificity: 0.32178217821782173\n",
      "Ave Test Recall: 0.7272727272727272\n",
      "Ave Test NPV: 0.7844531528484562\n",
      "Ave Test F1-Score: 0.38200032588432253\n",
      "Ave Test G-mean: 0.48221180258827284\n",
      "Ave Runtime: 0.006941747665405273\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positive basicMonthlySalary - monthlyExpenses. 17 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24158884 0.24164378 0.04       0.24158884 0.22303846 0.24158884\n",
      " 0.19105928 0.         0.24158884 0.22303846 0.23957897 0.19175658\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.24158884 0.24164378 0.04       0.24158884 0.22303846 0.24158884\n",
      " 0.19105928 0.         0.24158884 0.22303846 0.23957897 0.19175658\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.24069486 0.24200663 0.04       0.24069486 0.22184798 0.24105328\n",
      " 0.19019136 0.         0.24105328 0.22043372 0.23957897 0.19175658\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23966941 0.24322832 0.04       0.23966941 0.22175027 0.23887209\n",
      " 0.20949341 0.         0.24102687 0.22811944 0.24001419 0.20949341\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.24177967 0.23956775 0.02857143 0.24414526 0.23219099 0.23678385\n",
      " 0.24070582 0.         0.23598754 0.24888676 0.23678385 0.24070582\n",
      " 0.         0.23598754 0.24888676        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23748021 0.23961182 0.16666667 0.23375239 0.25130584 0.23680155\n",
      " 0.23800509 0.06666667 0.23271704 0.24922126 0.23680155 0.23800509\n",
      " 0.06666667 0.23232382 0.24922126        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.29166667 0.23169206 0.24426356 0.23663853\n",
      " 0.2407121  0.29166667 0.23083185 0.24657957 0.23663853 0.24041386\n",
      " 0.29166667 0.23169206 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.24940476 0.23083185 0.24742561 0.23663853\n",
      " 0.2407121  0.24940476 0.23083185 0.24657957 0.23663853 0.2407121\n",
      " 0.24940476 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2407121  0.24345238 0.23083185 0.24657957 0.23663853 0.2407121\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853\n",
      " 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853 0.2407121\n",
      " 0.24206349 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853\n",
      " 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853 0.2407121\n",
      " 0.24206349 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853\n",
      " 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853 0.2407121\n",
      " 0.24206349 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957\n",
      " 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853\n",
      " 0.2407121  0.24206349 0.23083185 0.24657957 0.23663853 0.2407121\n",
      " 0.24206349 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2407121  0.24206349 0.23083185\n",
      " 0.24657957 0.23663853 0.2407121  0.24206349 0.23083185 0.24657957]\n",
      "One or more of the train scores are non-finite: [0.24450882 0.24063572 0.04       0.24450882 0.25095615 0.24450882\n",
      " 0.1967365  0.         0.24450882 0.25095615 0.2455207  0.19639977\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24450882 0.24063572 0.04       0.24450882 0.25095615 0.24450882\n",
      " 0.1967365  0.         0.24450882 0.25070214 0.2455207  0.19639977\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.244272   0.24112365 0.04       0.244272   0.25165609 0.24417507\n",
      " 0.1968839  0.         0.24417507 0.24955013 0.2455207  0.19663873\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24447766 0.24074119 0.04       0.24435617 0.25110435 0.24529999\n",
      " 0.26304175 0.         0.24526786 0.25115591 0.24556684 0.26304175\n",
      " 0.         0.24526234 0.25075688        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24569559 0.24017643 0.0375     0.24557898 0.25253171 0.24382634\n",
      " 0.2389948  0.04285714 0.24429269 0.25082822 0.24382634 0.2389948\n",
      " 0.04285714 0.24429269 0.25068856        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24276663 0.23960321 0.21130952 0.24387648 0.25230745 0.24264476\n",
      " 0.24003406 0.17       0.24385221 0.24998015 0.24274032 0.23995086\n",
      " 0.17       0.24400453 0.25007882        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24142589 0.23833481 0.30164683 0.24290218 0.25033588 0.2411694\n",
      " 0.23842306 0.30467449 0.24295086 0.24976889 0.24167907 0.23834304\n",
      " 0.30546717 0.24305657 0.24976889        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24111965 0.23825684 0.33850422 0.24289462 0.24985954 0.24111965\n",
      " 0.23825684 0.33897042 0.24289462 0.24985954 0.24111879 0.23840975\n",
      " 0.34020446 0.24295086 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24111965 0.23825684 0.33170754 0.24289462 0.24995002 0.24111965\n",
      " 0.23825684 0.33170754 0.24289462 0.24990549 0.24111879 0.23840975\n",
      " 0.33219934 0.24299974 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24111965 0.23825684 0.33079904 0.24289462 0.24995002 0.24111965\n",
      " 0.23825684 0.33079904 0.24289462 0.24990549 0.24111879 0.23840975\n",
      " 0.33079904 0.24299974 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24111965 0.23825684 0.33079904 0.24289462 0.24995002 0.24111965\n",
      " 0.23825684 0.33079904 0.24289462 0.24990549 0.24111879 0.23840975\n",
      " 0.33079904 0.24299974 0.24990549        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24111965 0.23825684 0.33079904 0.24289462 0.24995002 0.24111965\n",
      " 0.23825684 0.33079904 0.24289462 0.24990549 0.24111879 0.23840975\n",
      " 0.33079904 0.24299974 0.24990549        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549\n",
      " 0.24111965 0.23825684 0.33079904 0.24289462 0.24995002 0.24111965\n",
      " 0.23825684 0.33079904 0.24289462 0.24990549 0.24111879 0.23840975\n",
      " 0.33079904 0.24299974 0.24990549        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23825684 0.33079904 0.24289462\n",
      " 0.24990549 0.24111879 0.23840975 0.33079904 0.24299974 0.24990549]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.2916666666666667\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7481296758104738\n",
      "GridSearchCV Runtime: 4.465470790863037 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.42857142857142855\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.39285714285714285\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3076923076923077\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.4\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.32\n",
      "Ave Test Precision: 0.3698241758241758\n",
      "Stdev Test Precision: 0.052998007449563206\n",
      "Ave Test Accuracy: 0.7283582089552239\n",
      "Stdev Test Accuracy: 0.02033496506483393\n",
      "Ave Test Specificity: 0.9257425742574258\n",
      "Ave Test Recall: 0.12424242424242424\n",
      "Ave Test NPV: 0.7639623591077968\n",
      "Ave Test F1-Score: 0.1787402630963661\n",
      "Ave Test G-mean: 0.3317475209636972\n",
      "Ave Runtime: 0.007487964630126953\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyIncome - monthlyExpenses. 16 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.21436226 0.\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.11441919        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.21436226 0.\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.21436226 0.\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.11441919        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.21436226 0.\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.11441919        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.11441919        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.21436226 0.08775253\n",
      "        nan 0.25569904 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.08775253        nan 0.20528776\n",
      " 0.22013531 0.21436226 0.11441919        nan 0.25569904 0.22013531]\n",
      "One or more of the train scores are non-finite: [0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25148267 0.\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.15479481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25148267 0.\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25148267 0.\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.15479481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25148267 0.\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.15479481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.15479481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25148267 0.12979481\n",
      "        nan 0.24835469 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.12979481        nan 0.19958898\n",
      " 0.22476988 0.25148267 0.15479481        nan 0.24835469 0.22476988]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2556990398972856\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.6907730673316709\n",
      "GridSearchCV Runtime: 4.475640296936035 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.21621621621621623\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24669603524229075\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2409090909090909\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2565217391304348\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2297872340425532\n",
      "Ave Test Precision: 0.2380260631081172\n",
      "Stdev Test Precision: 0.015565330252285143\n",
      "Ave Test Accuracy: 0.3888059701492537\n",
      "Stdev Test Accuracy: 0.16168819361247286\n",
      "Ave Test Specificity: 0.2881188118811881\n",
      "Ave Test Recall: 0.696969696969697\n",
      "Ave Test NPV: 0.7372670173215744\n",
      "Ave Test F1-Score: 0.33313487430823757\n",
      "Ave Test G-mean: 0.34361955132076955\n",
      "Ave Runtime: 0.006736040115356445\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positive monthlyFamilyIncome - monthlyExpenses. 15 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.21976074 0.19685885 0.12884615 0.21763354 0.24015845 0.21976074\n",
      " 0.17896605 0.02469136 0.21763354 0.24015845 0.219761   0.14563272\n",
      " 0.02469136 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.21976074 0.19685885 0.12884615 0.21763354 0.24015845 0.21976074\n",
      " 0.14563272 0.02469136 0.21763354 0.24015845 0.219761   0.14563272\n",
      " 0.02469136 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.21976074 0.19685885 0.12884615 0.21763354 0.24015845 0.21976074\n",
      " 0.14563272 0.02469136 0.21763354 0.24015845 0.219761   0.14563272\n",
      " 0.02469136 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.21897023 0.19685885 0.12384615 0.21763354 0.24211923 0.2193096\n",
      " 0.14157969 0.02533333 0.21763354 0.24330374 0.219761   0.14157969\n",
      " 0.02533333 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.22171221 0.19275878 0.1635037  0.21817191 0.24211923 0.219761\n",
      " 0.2085524  0.12258621 0.21817191 0.24386924 0.219761   0.2085524\n",
      " 0.12258621 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.19260778 0.21817191 0.24440905 0.219761\n",
      " 0.18942545 0.18650575 0.21817191 0.24386924 0.219761   0.18942545\n",
      " 0.18650575 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18609016 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18609016 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18609016 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924]\n",
      "One or more of the train scores are non-finite: [0.24224698 0.20025436 0.22280112 0.25895857 0.26435773 0.24224698\n",
      " 0.16777695 0.02454924 0.25895857 0.26435773 0.24204582 0.14757493\n",
      " 0.02454924 0.26007573 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25963595\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25945083 0.26516277\n",
      " 0.24224698 0.20025436 0.22280112 0.25895857 0.26435773 0.24224698\n",
      " 0.14757493 0.02454924 0.25895857 0.26435773 0.24204582 0.14757493\n",
      " 0.02454924 0.26007573 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25963595\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25945083 0.26516277\n",
      " 0.24230529 0.20025436 0.22280112 0.25895857 0.26427693 0.24230529\n",
      " 0.14757493 0.02454924 0.25895857 0.26427693 0.24204582 0.14757493\n",
      " 0.02454924 0.26007573 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25963595\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25945083 0.26516277\n",
      " 0.24240859 0.20195615 0.22937577 0.25897608 0.26439114 0.24209957\n",
      " 0.14894941 0.02472089 0.25891379 0.26438586 0.24204582 0.14894941\n",
      " 0.02472089 0.26007573 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25963595\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25945083 0.26516277\n",
      " 0.24240472 0.20283599 0.2149194  0.25904315 0.26392049 0.24216726\n",
      " 0.18185437 0.23537734 0.25948243 0.26491422 0.24216726 0.18185437\n",
      " 0.23537734 0.25999406 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25963595\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25945083 0.26516277\n",
      " 0.24174702 0.20519439 0.21590658 0.25893919 0.26518118 0.24160947\n",
      " 0.20331222 0.21454206 0.25948243 0.26524178 0.24160947 0.20331222\n",
      " 0.21454206 0.25999406 0.26524178        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25963595\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25945083 0.26516277\n",
      " 0.24160947 0.20318846 0.21798596 0.25893919 0.26524178 0.24166765\n",
      " 0.20344    0.21798596 0.25963595 0.26516277 0.24160947 0.20344\n",
      " 0.21798596 0.25945083 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25963595\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25945083 0.26516277\n",
      " 0.24166765 0.20318846 0.21879701 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21879701 0.25963595 0.26516277 0.24166765 0.20318846\n",
      " 0.21879701 0.25945083 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25963595\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25945083 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25963595 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25945083 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25963595\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25945083 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25963595 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25945083 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25963595\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25945083 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25963595 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25945083 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25963595\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25945083 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25963595 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25945083 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25963595\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25945083 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25963595 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25945083 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25963595\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25945083 0.26516277]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2444090484291062\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.543640897755611\n",
      "GridSearchCV Runtime: 4.2514190673828125 secs\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.226890756302521\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.28225806451612906\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.24431818181818182\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2570281124497992\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2920353982300885\n",
      "Ave Test Precision: 0.2605061026633439\n",
      "Stdev Test Precision: 0.02679263439619645\n",
      "Ave Test Accuracy: 0.47238805970149256\n",
      "Stdev Test Accuracy: 0.11294943147244595\n",
      "Ave Test Specificity: 0.4267326732673268\n",
      "Ave Test Recall: 0.6121212121212121\n",
      "Ave Test NPV: 0.7909621744156162\n",
      "Ave Test F1-Score: 0.3581498270996012\n",
      "Ave Test G-mean: 0.46471910674316863\n",
      "Ave Runtime: 0.006035566329956055\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with basicMonthlySalary / monthlyFamilyIncome. 14 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3575 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "702 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 293, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 250, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 422, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 106, in fit_resample\n",
      "    X, y, binarize_y = self._check_X_y(X, y)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 161, in _check_X_y\n",
      "    X, y = self._validate_data(X, y, reset=True, accept_sparse=accept_sparse)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 622, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1146, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 957, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 122, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 171, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "SMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "702 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 293, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 250, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 422, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 106, in fit_resample\n",
      "    X, y, binarize_y = self._check_X_y(X, y)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 161, in _check_X_y\n",
      "    X, y = self._validate_data(X, y, reset=True, accept_sparse=accept_sparse)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 622, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1146, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 957, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 122, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 171, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "ADASYN does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "702 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 293, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 250, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 422, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 106, in fit_resample\n",
      "    X, y, binarize_y = self._check_X_y(X, y)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py\", line 161, in _check_X_y\n",
      "    X, y = self._validate_data(X, y, reset=True, accept_sparse=accept_sparse)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 622, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1146, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 957, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 122, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 171, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "EditedNearestNeighbours does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1404 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1208, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 622, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1146, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 957, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 122, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 171, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "65 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "One or more of the train scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Libraries\\Documents\\carlo1coder\\MSDS2023\\Capstone\\EDA deltaT.ipynb Cell 32\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, col \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(columns):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     df_2 \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcopy()\u001b[39m.\u001b[39mloc[:, [\u001b[39m'\u001b[39m\u001b[39muserId\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlastFirstName\u001b[39m\u001b[39m'\u001b[39m, col, \u001b[39m'\u001b[39m\u001b[39mhome_ownership_class\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     t01_logreg_gs, t01_logreg_be, t01_logreg_model_info, t01_logreg_metrics_df \u001b[39m=\u001b[39m logreg_class2(df_2, \u001b[39m'\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                                                 \u001b[39m'\u001b[39;49m\u001b[39mhome_ownership_class\u001b[39;49m\u001b[39m'\u001b[39;49m, scaler, random_state\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     new_row \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mVariable\u001b[39m\u001b[39m'\u001b[39m: col,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mMean Test Precision\u001b[39m\u001b[39m'\u001b[39m: t01_logreg_model_info[\u001b[39m'\u001b[39m\u001b[39maverage_test_precision\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mStd Test Accuracy\u001b[39m\u001b[39m'\u001b[39m: t01_logreg_model_info[\u001b[39m'\u001b[39m\u001b[39mstdev_test_accuracy\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     }\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     res_df \u001b[39m=\u001b[39m res_df\u001b[39m.\u001b[39mappend(new_row, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32md:\\Libraries\\Documents\\carlo1coder\\MSDS2023\\Capstone\\EDA deltaT.ipynb Cell 32\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=397'>398</a>\u001b[0m start_time_cv \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=398'>399</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(pipeline, param_grid,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=399'>400</a>\u001b[0m                            cv\u001b[39m=\u001b[39mskf, scoring\u001b[39m=\u001b[39mscorer,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=400'>401</a>\u001b[0m                            n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=401'>402</a>\u001b[0m                            verbose\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, return_train_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=402'>403</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=404'>405</a>\u001b[0m best_params \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Libraries/Documents/carlo1coder/MSDS2023/Capstone/EDA%20deltaT.ipynb#X56sZmlsZQ%3D%3D?line=405'>406</a>\u001b[0m best_score \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_score_\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:936\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    934\u001b[0m refit_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    935\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 936\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_estimator_\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    937\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    938\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py:293\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m    292\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 293\u001b[0m Xt, yt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[0;32m    294\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    295\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py:250\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    240\u001b[0m     X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    241\u001b[0m         cloned_transformer,\n\u001b[0;32m    242\u001b[0m         X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    248\u001b[0m     )\n\u001b[0;32m    249\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(cloned_transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_resample\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 250\u001b[0m     X, y, fitted_transformer \u001b[39m=\u001b[39m fit_resample_one_cached(\n\u001b[0;32m    251\u001b[0m         cloned_transformer,\n\u001b[0;32m    252\u001b[0m         X,\n\u001b[0;32m    253\u001b[0m         y,\n\u001b[0;32m    254\u001b[0m         message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    255\u001b[0m         message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[0;32m    256\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    258\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py:422\u001b[0m, in \u001b[0;36m_fit_resample_one\u001b[1;34m(sampler, X, y, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit_resample_one\u001b[39m(sampler, X, y, message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params):\n\u001b[0;32m    421\u001b[0m     \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m--> 422\u001b[0m         X_res, y_res \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39;49mfit_resample(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    424\u001b[0m         \u001b[39mreturn\u001b[39;00m X_res, y_res, sampler\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_resample(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py:106\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    104\u001b[0m check_classification_targets(y)\n\u001b[0;32m    105\u001b[0m arrays_transformer \u001b[39m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m--> 106\u001b[0m X, y, binarize_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X_y(X, y)\n\u001b[0;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy_ \u001b[39m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy, y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[0;32m    112\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_resample(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\base.py:161\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    159\u001b[0m     accept_sparse \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    160\u001b[0m y, binarize_y \u001b[39m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 161\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, reset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse)\n\u001b[0;32m    162\u001b[0m \u001b[39mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1149\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1150\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1151\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1152\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1153\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1154\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1155\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1156\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1157\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1158\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[0;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m         )\n\u001b[0;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 957\u001b[0m         _assert_all_finite(\n\u001b[0;32m    958\u001b[0m             array,\n\u001b[0;32m    959\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    960\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    961\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    964\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    123\u001b[0m     X,\n\u001b[0;32m    124\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[0;32m    125\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[0;32m    126\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[0;32m    127\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    128\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    129\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    155\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "res_df = pd.DataFrame(columns=[\n",
    "    'Variable',\n",
    "    'Mean Test Precision', 'Std Test Precision', 'Val Precision of Best Model',\n",
    "    'Train Precision of Best Model', 'Mean Test Accuracy', 'Std Test Accuracy'\n",
    "])\n",
    "\n",
    "for index, col in enumerate(columns):\n",
    "    df_2 = df.copy().loc[:, ['userId', 'lastFirstName', col, 'home_ownership_class']]\n",
    "    t01_logreg_gs, t01_logreg_be, t01_logreg_model_info, t01_logreg_metrics_df = logreg_class2(df_2, 'precision',\n",
    "                                                                'home_ownership_class', scaler, random_state=0)\n",
    "    new_row = {\n",
    "        'Variable': col,\n",
    "        'Mean Test Precision': t01_logreg_model_info['average_test_precision'],\n",
    "        'Std Test Precision': t01_logreg_model_info['stdev_test_precision'],\n",
    "        'Val Precision of Best Model': t01_logreg_model_info['best_cv_score'],\n",
    "        'Train Precision of Best Model': t01_logreg_model_info['train_score'],\n",
    "        'Mean Test Accuracy': t01_logreg_model_info['average_test_accuracy'],\n",
    "        'Std Test Accuracy': t01_logreg_model_info['stdev_test_accuracy']\n",
    "    }\n",
    "    res_df = res_df.append(new_row, ignore_index=True)\n",
    "    print(f'\\n=====================\\n\\n\\nDone with {col}. {len(columns) - index + 1} columns left\\n\\n\\n=====================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Mean Test Precision</th>\n",
       "      <th>Std Test Precision</th>\n",
       "      <th>Val Precision of Best Model</th>\n",
       "      <th>Train Precision of Best Model</th>\n",
       "      <th>Mean Test Accuracy</th>\n",
       "      <th>Std Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>vehicleLoan</td>\n",
       "      <td>0.545728</td>\n",
       "      <td>0.285439</td>\n",
       "      <td>0.409286</td>\n",
       "      <td>0.736908</td>\n",
       "      <td>0.747015</td>\n",
       "      <td>0.018916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>informalLenders</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.302765</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.759352</td>\n",
       "      <td>0.750746</td>\n",
       "      <td>0.004865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>basicMonthlySalary</td>\n",
       "      <td>0.406079</td>\n",
       "      <td>0.079838</td>\n",
       "      <td>0.380606</td>\n",
       "      <td>0.714464</td>\n",
       "      <td>0.725373</td>\n",
       "      <td>0.030611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>payInsurance</td>\n",
       "      <td>0.378755</td>\n",
       "      <td>0.034898</td>\n",
       "      <td>0.339365</td>\n",
       "      <td>0.716958</td>\n",
       "      <td>0.729851</td>\n",
       "      <td>0.008175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>monthlyVices</td>\n",
       "      <td>0.374603</td>\n",
       "      <td>0.056789</td>\n",
       "      <td>0.313068</td>\n",
       "      <td>0.748130</td>\n",
       "      <td>0.745522</td>\n",
       "      <td>0.006131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>monthlyFamilyNetIncome</td>\n",
       "      <td>0.369824</td>\n",
       "      <td>0.052998</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.748130</td>\n",
       "      <td>0.728358</td>\n",
       "      <td>0.020335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>monthlyFamilyIncome</td>\n",
       "      <td>0.368796</td>\n",
       "      <td>0.042731</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.725686</td>\n",
       "      <td>0.723134</td>\n",
       "      <td>0.019281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>preferredNetDisposableIncomeId</td>\n",
       "      <td>0.368088</td>\n",
       "      <td>0.030940</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>0.638404</td>\n",
       "      <td>0.663433</td>\n",
       "      <td>0.022911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>monthlyFamilyNetIncomeWithSavings</td>\n",
       "      <td>0.365510</td>\n",
       "      <td>0.087421</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.745636</td>\n",
       "      <td>0.722388</td>\n",
       "      <td>0.024124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>repair</td>\n",
       "      <td>0.363370</td>\n",
       "      <td>0.077332</td>\n",
       "      <td>0.261623</td>\n",
       "      <td>0.741895</td>\n",
       "      <td>0.744776</td>\n",
       "      <td>0.006244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Variable  Mean Test Precision  \\\n",
       "29                        vehicleLoan             0.545728   \n",
       "30                    informalLenders             0.483333   \n",
       "1                  basicMonthlySalary             0.406079   \n",
       "41                       payInsurance             0.378755   \n",
       "56                       monthlyVices             0.374603   \n",
       "60             monthlyFamilyNetIncome             0.369824   \n",
       "5                 monthlyFamilyIncome             0.368796   \n",
       "2      preferredNetDisposableIncomeId             0.368088   \n",
       "64  monthlyFamilyNetIncomeWithSavings             0.365510   \n",
       "21                             repair             0.363370   \n",
       "\n",
       "    Std Test Precision  Val Precision of Best Model  \\\n",
       "29            0.285439                     0.409286   \n",
       "30            0.302765                     0.460000   \n",
       "1             0.079838                     0.380606   \n",
       "41            0.034898                     0.339365   \n",
       "56            0.056789                     0.313068   \n",
       "60            0.052998                     0.291667   \n",
       "5             0.042731                     0.325000   \n",
       "2             0.030940                     0.330400   \n",
       "64            0.087421                     0.400000   \n",
       "21            0.077332                     0.261623   \n",
       "\n",
       "    Train Precision of Best Model  Mean Test Accuracy  Std Test Accuracy  \n",
       "29                       0.736908            0.747015           0.018916  \n",
       "30                       0.759352            0.750746           0.004865  \n",
       "1                        0.714464            0.725373           0.030611  \n",
       "41                       0.716958            0.729851           0.008175  \n",
       "56                       0.748130            0.745522           0.006131  \n",
       "60                       0.748130            0.728358           0.020335  \n",
       "5                        0.725686            0.723134           0.019281  \n",
       "2                        0.638404            0.663433           0.022911  \n",
       "64                       0.745636            0.722388           0.024124  \n",
       "21                       0.741895            0.744776           0.006244  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_res_df = res_df.copy().rename(columns=ren).drop(70)\n",
    "sorted_res_df = sorted_res_df.sort_values(by='Mean Test Precision', ascending=False)\n",
    "# sorted_res_df[sorted_res_df['Variable'] == 'basicMonthlySalary']\n",
    "sorted_res_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22517611 0.19630974 0.025      0.26182082 0.23276093 0.22573166\n",
      " 0.17743827 0.         0.26182082 0.23290937 0.22310612 0.17743827\n",
      " 0.         0.27194076 0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22528242 0.19630974 0.025      0.26182082 0.23140749 0.22528242\n",
      " 0.17743827 0.         0.26182082 0.23196304 0.22310612 0.17743827\n",
      " 0.         0.27194076 0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22528242 0.19630974 0.02       0.26182082 0.23196304 0.22557659\n",
      " 0.17743827 0.         0.269853   0.23196304 0.22310612 0.17743827\n",
      " 0.         0.27194076 0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22528242 0.19630974 0.02       0.26182082 0.23196304 0.22597693\n",
      " 0.2060097  0.         0.271331   0.23104251 0.22310612 0.2060097\n",
      " 0.         0.27194076 0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.21976117 0.20059584 0.06428571 0.2640333  0.23196304 0.22310612\n",
      " 0.20075568 0.         0.271331   0.23516006 0.22310612 0.20075568\n",
      " 0.         0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.21976117 0.19540925 0.13095238 0.26495575 0.23516006 0.22310612\n",
      " 0.19540925 0.13928571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13928571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.14095238 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.14095238 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.14095238 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.13428571 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.13428571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13428571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.13428571 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.13428571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13428571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.13428571 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.13428571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13428571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.13428571 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.13428571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13428571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.13428571 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.13428571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13428571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006\n",
      " 0.22310612 0.19540925 0.13428571 0.271331   0.23516006 0.22310612\n",
      " 0.19540925 0.13428571 0.271331   0.23516006 0.22310612 0.19540925\n",
      " 0.13428571 0.271331   0.23516006        nan        nan        nan\n",
      "        nan        nan 0.22310612 0.19540925 0.13428571 0.271331\n",
      " 0.23516006 0.22310612 0.19540925 0.13428571 0.271331   0.23516006]\n",
      "One or more of the train scores are non-finite: [0.25290316 0.19977741 0.021875   0.25625752 0.25174875 0.2530831\n",
      " 0.17296195 0.         0.25625752 0.25138793 0.25606611 0.17296195\n",
      " 0.         0.26232921 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25306213 0.19977741 0.021875   0.25625752 0.25124635 0.25306213\n",
      " 0.17296195 0.         0.25625752 0.25142628 0.25606611 0.17296195\n",
      " 0.         0.26232921 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25306213 0.19977741 0.0275     0.25625752 0.25142628 0.25585681\n",
      " 0.17296195 0.         0.25983608 0.25142628 0.25606611 0.17296195\n",
      " 0.         0.26232921 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25306213 0.20001976 0.0275     0.25625752 0.25142628 0.25605975\n",
      " 0.19609628 0.         0.26192757 0.25384962 0.25606611 0.19609628\n",
      " 0.         0.26232921 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25602463 0.20002023 0.06305263 0.26020752 0.25142628 0.25606611\n",
      " 0.2003313  0.01666667 0.26192757 0.25426513 0.25606611 0.2003313\n",
      " 0.01666667 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25602463 0.2004652  0.09853207 0.26177347 0.25426513 0.25606611\n",
      " 0.2004652  0.09881136 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.09881136 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11541015 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11541015 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11541015 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11365185 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11365185 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11365185 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11365185 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11365185 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513\n",
      " 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611\n",
      " 0.2004652  0.11365185 0.26192757 0.25426513 0.25606611 0.2004652\n",
      " 0.11365185 0.26192757 0.25426513        nan        nan        nan\n",
      "        nan        nan 0.25606611 0.2004652  0.11365185 0.26192757\n",
      " 0.25426513 0.25606611 0.2004652  0.11365185 0.26192757 0.25426513]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2719407594857509\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.5336658354114713\n",
      "GridSearchCV Runtime: 7.1358802318573 secs\n",
      "Efron R-squared: -0.3491711090899354\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2711864406779661\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2605042016806723\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2127659574468085\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.25806451612903225\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.265625\n",
      "Ave Test Precision: 0.25362922318689585\n",
      "Stdev Test Precision: 0.023392831097881244\n",
      "Ave Test Accuracy: 0.5208955223880597\n",
      "Stdev Test Accuracy: 0.03989223062956446\n",
      "Ave Test Specificity: 0.5336633663366336\n",
      "Ave Test Recall: 0.4818181818181818\n",
      "Ave Test NPV: 0.7580573795725198\n",
      "Ave Test F1-Score: 0.33203477274722176\n",
      "Ave Test G-mean: 0.5067362124311666\n",
      "Ave Runtime: 0.006033086776733398\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with age. 76 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.3198298  0.40019254 0.33417039 0.3198298  0.28051706 0.3198298\n",
      " 0.05       0.         0.3198298  0.28051706 0.34898253 0.05\n",
      " 0.         0.35279991 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.32043888 0.40019254 0.33417039 0.32043888 0.28051706 0.32151889\n",
      " 0.05       0.         0.34730131 0.28590403 0.34898253 0.05\n",
      " 0.         0.35279991 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.32133631 0.41792755 0.33090241 0.32133631 0.28112614 0.34321436\n",
      " 0.08333333 0.         0.35279991 0.30006485 0.34898253 0.08333333\n",
      " 0.         0.35279991 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.32930214 0.31101301 0.32574689 0.32690583 0.28467052 0.34479093\n",
      " 0.18116654 0.2        0.35279991 0.30006485 0.35017301 0.18116654\n",
      " 0.2        0.35279991 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34602388 0.31807018 0.3325518  0.34820177 0.29164388 0.34599214\n",
      " 0.29626437 0.31166667 0.35370221 0.30006485 0.35017301 0.29626437\n",
      " 0.31166667 0.35370221 0.30006485        nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.35212335 0.29626437 0.32992968 0.35010734 0.29780076 0.34482547\n",
      " 0.29626437 0.31432151 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.31432151 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35094031 0.2976455  0.34482547\n",
      " 0.29626437 0.32516777 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32516777 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35370221 0.2976455  0.34482547\n",
      " 0.29626437 0.32863098 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32863098 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35370221 0.2976455  0.34482547\n",
      " 0.29626437 0.32863098 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32863098 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35370221 0.2976455  0.34482547\n",
      " 0.29626437 0.32863098 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32863098 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35370221 0.2976455  0.34482547\n",
      " 0.29626437 0.32863098 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32863098 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35370221 0.2976455  0.34482547\n",
      " 0.29626437 0.32863098 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32863098 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455\n",
      " 0.34900634 0.29626437 0.32863098 0.35370221 0.2976455  0.34482547\n",
      " 0.29626437 0.32863098 0.35370221 0.2976455  0.34900634 0.29626437\n",
      " 0.32863098 0.35370221 0.2976455         nan        nan        nan\n",
      "        nan        nan 0.34482547 0.29626437 0.32863098 0.35370221\n",
      " 0.2976455  0.34900634 0.29626437 0.32863098 0.35370221 0.2976455 ]\n",
      "One or more of the train scores are non-finite: [0.31899892 0.34347137 0.33292962 0.31899892 0.27516701 0.31899892\n",
      " 0.04903047 0.         0.31899892 0.27516701 0.3369827  0.04903047\n",
      " 0.         0.3378266  0.30521314        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.31971635 0.32807183 0.33292962 0.31971635 0.27516701 0.32941341\n",
      " 0.04903047 0.         0.3349485  0.28717644 0.3369827  0.04903047\n",
      " 0.         0.3378266  0.30521314        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.3205307  0.32285482 0.33207943 0.3205307  0.2755598  0.33826158\n",
      " 0.19314812 0.         0.3378266  0.30537151 0.3369827  0.19314812\n",
      " 0.         0.3378266  0.30521314        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.32799459 0.31354538 0.3315562  0.32903233 0.27933125 0.33602989\n",
      " 0.35519003 0.50217391 0.33776536 0.30554522 0.33774473 0.35519003\n",
      " 0.50217391 0.33776536 0.30554522        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33723794 0.35911284 0.32048266 0.33829892 0.2968398  0.33557773\n",
      " 0.37120765 0.33476722 0.33803269 0.30519213 0.33760229 0.37120765\n",
      " 0.33476722 0.33803269 0.30528526        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33793385 0.37146532 0.31794624 0.33768384 0.30231787 0.33586879\n",
      " 0.37185748 0.31943517 0.33803269 0.30496637 0.33789334 0.37185748\n",
      " 0.31943517 0.33803269 0.30496637        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33798374 0.30614854 0.33586879\n",
      " 0.37185748 0.31817972 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31817972 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33803269 0.30512572 0.33586879\n",
      " 0.37185748 0.31810269 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31810269 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33803269 0.30508579 0.33586879\n",
      " 0.37185748 0.31810269 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31810269 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33803269 0.30508579 0.33586879\n",
      " 0.37185748 0.31810269 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31810269 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33803269 0.30508579 0.33586879\n",
      " 0.37185748 0.31810269 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31810269 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33803269 0.30508579 0.33586879\n",
      " 0.37185748 0.31810269 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31810269 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579\n",
      " 0.33819028 0.37185748 0.31810269 0.33803269 0.30508579 0.33586879\n",
      " 0.37185748 0.31810269 0.33803269 0.30517893 0.33789334 0.37185748\n",
      " 0.31810269 0.33803269 0.30508579        nan        nan        nan\n",
      "        nan        nan 0.33586879 0.37185748 0.31810269 0.33803269\n",
      " 0.30517893 0.33789334 0.37185748 0.31810269 0.33803269 0.30508579]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.0001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.41792755045602437\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.0001, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6670822942643392\n",
      "GridSearchCV Runtime: 4.550188779830933 secs\n",
      "Efron R-squared: -0.34885168744003936\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.42424242424242425\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3063063063063063\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.32947976878612717\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.26842105263157895\n",
      "Ave Test Precision: 0.332356577059954\n",
      "Stdev Test Precision: 0.057486100235996\n",
      "Ave Test Accuracy: 0.5813432835820895\n",
      "Stdev Test Accuracy: 0.109786493619808\n",
      "Ave Test Specificity: 0.5722772277227722\n",
      "Ave Test Recall: 0.6090909090909091\n",
      "Ave Test NPV: 0.8242029995309803\n",
      "Ave Test F1-Score: 0.41475705396397194\n",
      "Ave Test G-mean: 0.5638672192060966\n",
      "Ave Runtime: 0.006598997116088867\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with basicMonthlySalary. 75 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.33040034 0.31535089 0.         0.33040034 0.33040034 0.33040034\n",
      " 0.07469136 0.         0.33040034 0.33040034 0.33040034 0.07469136\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.         0.33040034 0.33040034 0.33040034\n",
      " 0.07469136 0.         0.33040034 0.33040034 0.33040034 0.07469136\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.         0.33040034 0.33040034 0.33040034\n",
      " 0.07469136 0.         0.33040034 0.33040034 0.33040034 0.07469136\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.         0.33040034 0.33040034 0.33040034\n",
      " 0.17734287 0.         0.33040034 0.33040034 0.33040034 0.17734287\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.         0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.         0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034\n",
      " 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034\n",
      " 0.31535089 0.22048655 0.33040034 0.33040034 0.33040034 0.31535089\n",
      " 0.22048655 0.33040034 0.33040034        nan        nan        nan\n",
      "        nan        nan 0.33040034 0.31535089 0.22048655 0.33040034\n",
      " 0.33040034 0.33040034 0.31535089 0.22048655 0.33040034 0.33040034]\n",
      "One or more of the train scores are non-finite: [0.33330429 0.31136017 0.         0.33330429 0.33330429 0.33330429\n",
      " 0.07357971 0.         0.33330429 0.33330429 0.33330429 0.07357971\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.         0.33330429 0.33330429 0.33330429\n",
      " 0.07357971 0.         0.33330429 0.33330429 0.33330429 0.07357971\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.         0.33330429 0.33330429 0.33330429\n",
      " 0.07357971 0.         0.33330429 0.33330429 0.33330429 0.07357971\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.         0.33330429 0.33330429 0.33330429\n",
      " 0.1731696  0.         0.33330429 0.33330429 0.33330429 0.1731696\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.         0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.         0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429\n",
      " 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429\n",
      " 0.31136017 0.23451077 0.33330429 0.33330429 0.33330429 0.31136017\n",
      " 0.23451077 0.33330429 0.33330429        nan        nan        nan\n",
      "        nan        nan 0.33330429 0.31136017 0.23451077 0.33330429\n",
      " 0.33330429 0.33330429 0.31136017 0.23451077 0.33330429 0.33330429]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.33040033960292586\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6384039900249376\n",
      "GridSearchCV Runtime: 4.262330532073975 secs\n",
      "Efron R-squared: -0.34917180672223136\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4044943820224719\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.32978723404255317\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.39325842696629215\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3645833333333333\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.34831460674157305\n",
      "Ave Test Precision: 0.3680875966212447\n",
      "Stdev Test Precision: 0.03094023871571617\n",
      "Ave Test Accuracy: 0.6634328358208956\n",
      "Stdev Test Accuracy: 0.022910569302998132\n",
      "Ave Test Specificity: 0.7138613861386138\n",
      "Ave Test Recall: 0.509090909090909\n",
      "Ave Test NPV: 0.8164610334822197\n",
      "Ave Test F1-Score: 0.4271455595380327\n",
      "Ave Test G-mean: 0.6026404897216738\n",
      "Ave Runtime: 0.006203556060791015\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with preferredNetDisposableIncomeId. 74 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27245955 0.18305297 0.14844396 0.22709238 0.25021041 0.27245955\n",
      " 0.17219136 0.14719136 0.22709238 0.25021041 0.27245955 0.14719136\n",
      " 0.14719136 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.25315313 0.18305297 0.14844396 0.22069131 0.25021041 0.27245955\n",
      " 0.14719136 0.14719136 0.22069131 0.25021041 0.27245955 0.14719136\n",
      " 0.14719136 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.18305297 0.14844396 0.22463868 0.23315737 0.27245955\n",
      " 0.14719136 0.14719136 0.22069131 0.23315737 0.27245955 0.14719136\n",
      " 0.14719136 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.19943073 0.14844396 0.22069131 0.23315737 0.27245955\n",
      " 0.14248121 0.14719136 0.22069131 0.23315737 0.27245955 0.14248121\n",
      " 0.14719136 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.24727715 0.14770163 0.22069131 0.23315737 0.27245955\n",
      " 0.19843073 0.14984477 0.22069131 0.23315737 0.27245955 0.19843073\n",
      " 0.14984477 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.17133863 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.17306672 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.17306672 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737\n",
      " 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955\n",
      " 0.23477715 0.19464781 0.22069131 0.23315737 0.27245955 0.23477715\n",
      " 0.19464781 0.22069131 0.23315737        nan        nan        nan\n",
      "        nan        nan 0.27245955 0.23477715 0.19464781 0.22069131\n",
      " 0.23315737 0.27245955 0.23477715 0.19464781 0.22069131 0.23315737]\n",
      "One or more of the train scores are non-finite: [0.27465602 0.23174272 0.14586824 0.27110458 0.25684778 0.27465602\n",
      " 0.17523747 0.14740242 0.27110458 0.25684778 0.27465602 0.14740242\n",
      " 0.14740242 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27005669 0.23174272 0.14586824 0.26677571 0.25684778 0.27465602\n",
      " 0.14740242 0.14740242 0.26677571 0.25526413 0.27465602 0.14740242\n",
      " 0.14740242 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.23174272 0.14586824 0.26791354 0.2471273  0.27465602\n",
      " 0.14740242 0.14740242 0.26677571 0.2471273  0.27465602 0.14740242\n",
      " 0.14740242 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25650871 0.14586824 0.26677571 0.2471273  0.27465602\n",
      " 0.14740642 0.14740242 0.26677571 0.2471273  0.27465602 0.14740642\n",
      " 0.14740242 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.24937137 0.14537891 0.26677571 0.2471273  0.27465602\n",
      " 0.25355629 0.14699652 0.26677571 0.2471273  0.27465602 0.25355629\n",
      " 0.14699652 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.17296206 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.17397301 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.17397301 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273\n",
      " 0.27465602 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602\n",
      " 0.25053121 0.20199319 0.26677571 0.2471273  0.27465602 0.25053121\n",
      " 0.20199319 0.26677571 0.2471273         nan        nan        nan\n",
      "        nan        nan 0.27465602 0.25053121 0.20199319 0.26677571\n",
      " 0.2471273  0.27465602 0.25053121 0.20199319 0.26677571 0.2471273 ]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2724595467673271\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6234413965087282\n",
      "GridSearchCV Runtime: 4.321814775466919 secs\n",
      "Efron R-squared: -0.34917147197442344\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.32786885245901637\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.28169014084507044\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.32051282051282054\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2911392405063291\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.28378378378378377\n",
      "Ave Test Precision: 0.30099896762140405\n",
      "Stdev Test Precision: 0.021617057678368582\n",
      "Ave Test Accuracy: 0.6455223880597015\n",
      "Stdev Test Accuracy: 0.018088357676926597\n",
      "Ave Test Specificity: 0.7485148514851485\n",
      "Ave Test Recall: 0.3303030303030303\n",
      "Ave Test NPV: 0.7738027551245351\n",
      "Ave Test F1-Score: 0.314279006874707\n",
      "Ave Test G-mean: 0.49646187714329437\n",
      "Ave Runtime: 0.007923555374145509\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with workingFamilyCount. 73 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27001717 0.24332925 0.09102079 0.24286099 0.2444425  0.27133808\n",
      " 0.07344136 0.02375    0.240807   0.2444425  0.25757038 0.07344136\n",
      " 0.02375    0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.24332925 0.09102079 0.240807   0.2444425  0.25757038\n",
      " 0.07344136 0.02375    0.240807   0.23668641 0.25757038 0.07344136\n",
      " 0.02375    0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.24332925 0.09102079 0.240807   0.22497664 0.25757038\n",
      " 0.07344136 0.02375    0.240807   0.22928698 0.25757038 0.07344136\n",
      " 0.02375    0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.24332925 0.09102079 0.240807   0.22928698 0.25757038\n",
      " 0.18285312 0.02375    0.240807   0.22928698 0.25757038 0.18285312\n",
      " 0.02375    0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.24332925 0.09102079 0.240807   0.22928698 0.25757038\n",
      " 0.23351648 0.06982323 0.240807   0.22928698 0.25757038 0.23351648\n",
      " 0.06982323 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09102079 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698\n",
      " 0.25757038 0.23582417 0.09181362 0.240807   0.22928698 0.25757038\n",
      " 0.23582417 0.09181362 0.240807   0.22928698 0.25757038 0.23582417\n",
      " 0.09181362 0.240807   0.22928698        nan        nan        nan\n",
      "        nan        nan 0.25757038 0.23582417 0.09181362 0.240807\n",
      " 0.22928698 0.25757038 0.23582417 0.09181362 0.240807   0.22928698]\n",
      "One or more of the train scores are non-finite: [0.27403194 0.25945632 0.11437926 0.27202179 0.23198163 0.25988291\n",
      " 0.07371821 0.02465374 0.25645463 0.23198163 0.25770653 0.07371821\n",
      " 0.02465374 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25945632 0.11437926 0.25645463 0.23198163 0.25770653\n",
      " 0.07371821 0.02465374 0.25645463 0.22408459 0.25770653 0.07371821\n",
      " 0.02465374 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25945632 0.11437926 0.25645463 0.22356627 0.25770653\n",
      " 0.07371821 0.02465374 0.25645463 0.22314433 0.25770653 0.07371821\n",
      " 0.02465374 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25945632 0.11437926 0.25645463 0.22314433 0.25770653\n",
      " 0.20093741 0.02465374 0.25645463 0.22314433 0.25770653 0.20093741\n",
      " 0.02465374 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25945632 0.11437926 0.25645463 0.22314433 0.25770653\n",
      " 0.25005062 0.09483707 0.25645463 0.22314433 0.25770653 0.25005062\n",
      " 0.09483707 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11437926 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.10876482 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.10876482 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433\n",
      " 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653\n",
      " 0.25648537 0.11605227 0.25645463 0.22314433 0.25770653 0.25648537\n",
      " 0.11605227 0.25645463 0.22314433        nan        nan        nan\n",
      "        nan        nan 0.25770653 0.25648537 0.11605227 0.25645463\n",
      " 0.22314433 0.25770653 0.25648537 0.11605227 0.25645463 0.22314433]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.27133808202670107\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.4800498753117207\n",
      "GridSearchCV Runtime: 4.505640268325806 secs\n",
      "Efron R-squared: -0.34917030625096923\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.22580645161290322\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.26666666666666666\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.19607843137254902\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.27906976744186046\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25308641975308643\n",
      "Ave Test Precision: 0.24414154736941315\n",
      "Stdev Test Precision: 0.03336769078942016\n",
      "Ave Test Accuracy: 0.5037313432835822\n",
      "Stdev Test Accuracy: 0.06900463436159303\n",
      "Ave Test Specificity: 0.5099009900990099\n",
      "Ave Test Recall: 0.48484848484848475\n",
      "Ave Test NPV: 0.7523196025310736\n",
      "Ave Test F1-Score: 0.3201292572809601\n",
      "Ave Test G-mean: 0.48295248368187044\n",
      "Ave Runtime: 0.0074039936065673825\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with residentsCount. 72 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.21493977 0.21814642 0.29943445 0.21493977 0.25294743 0.21493977\n",
      " 0.12344136 0.         0.21493977 0.25294743 0.23730719 0.12344136\n",
      " 0.         0.2391029  0.2538565         nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.21493977 0.21427035 0.29943445 0.21493977 0.25294743 0.23730719\n",
      " 0.12375781 0.         0.2391029  0.26428882 0.23730719 0.12375781\n",
      " 0.         0.2391029  0.2538565         nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.21493977 0.22505434 0.29943445 0.21493977 0.25294743 0.23730719\n",
      " 0.14169872 0.         0.2391029  0.25663428 0.23730719 0.14169872\n",
      " 0.         0.2391029  0.2538565         nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.21100484 0.23558609 0.29794627 0.21103655 0.25063776 0.23762448\n",
      " 0.15253521 0.025      0.2391029  0.25504227 0.23762448 0.15253521\n",
      " 0.025      0.2391029  0.25504227        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.22657022 0.17551338 0.29534817 0.22600159 0.25448271 0.23762448\n",
      " 0.15934556 0.35333333 0.2391029  0.25504227 0.23762448 0.15934556\n",
      " 0.35333333 0.2391029  0.25504227        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23229967 0.18024685 0.29175558 0.23689853 0.26371167 0.23272938\n",
      " 0.18096114 0.29461272 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.29461272 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18024685 0.31766056 0.2391029  0.26121929 0.23272938\n",
      " 0.18096114 0.31766056 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18096114 0.31766056 0.2391029  0.25883834 0.23272938\n",
      " 0.18096114 0.31766056 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834\n",
      " 0.23305724 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938\n",
      " 0.18096114 0.32099389 0.2391029  0.25883834 0.23272938 0.18096114\n",
      " 0.31766056 0.2391029  0.25883834        nan        nan        nan\n",
      "        nan        nan 0.23272938 0.18096114 0.32099389 0.2391029\n",
      " 0.25883834 0.23272938 0.18096114 0.31766056 0.2391029  0.25883834]\n",
      "One or more of the train scores are non-finite: [0.22420288 0.2236422  0.3020995  0.22420288 0.25180495 0.22420288\n",
      " 0.12281688 0.         0.22420288 0.25180495 0.24120377 0.12281688\n",
      " 0.         0.24176348 0.26256588        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.22420288 0.22338995 0.3020995  0.22420288 0.25180495 0.23993746\n",
      " 0.12291482 0.         0.24028389 0.25373772 0.24120377 0.12291482\n",
      " 0.         0.24176348 0.26256588        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.22408338 0.22335157 0.3020995  0.22402326 0.25180495 0.24106638\n",
      " 0.14454219 0.         0.24126054 0.26289177 0.24106638 0.14454219\n",
      " 0.         0.24126054 0.26256156        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.22249445 0.23277745 0.29942311 0.22254189 0.25121477 0.24060268\n",
      " 0.16309034 0.20733954 0.24129152 0.26217372 0.24060268 0.16309034\n",
      " 0.20733954 0.24129152 0.26217372        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.23584198 0.18657272 0.31450171 0.23702248 0.25956427 0.2406455\n",
      " 0.20163046 0.33806063 0.24129152 0.26215121 0.2406455  0.20168153\n",
      " 0.33806063 0.24129152 0.26215121        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24015815 0.19674384 0.31642852 0.24096069 0.26172847 0.24087133\n",
      " 0.19644567 0.32131888 0.24117372 0.26179596 0.24087133 0.19644567\n",
      " 0.32131888 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24082079 0.19633765 0.30504612 0.24121567 0.26179006 0.24087133\n",
      " 0.19644567 0.30447141 0.24117372 0.26179596 0.24087133 0.19644567\n",
      " 0.30447141 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24091444 0.19670904 0.30504612 0.24121567 0.26179596 0.24087133\n",
      " 0.19670904 0.30504612 0.24117372 0.26179596 0.24087133 0.19670904\n",
      " 0.30504612 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24087133 0.19670904 0.30356562 0.24121567 0.26179596 0.24087133\n",
      " 0.19670904 0.30356562 0.24117372 0.26179596 0.24087133 0.19670904\n",
      " 0.30504612 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24087133 0.19670904 0.30356562 0.24121567 0.26179596 0.24087133\n",
      " 0.19670904 0.30356562 0.24117372 0.26179596 0.24087133 0.19670904\n",
      " 0.30504612 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24087133 0.19670904 0.30356562 0.24121567 0.26179596 0.24087133\n",
      " 0.19670904 0.30356562 0.24117372 0.26179596 0.24087133 0.19670904\n",
      " 0.30504612 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24087133 0.19670904 0.30356562 0.24121567 0.26179596 0.24087133\n",
      " 0.19670904 0.30356562 0.24117372 0.26179596 0.24087133 0.19670904\n",
      " 0.30504612 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596\n",
      " 0.24087133 0.19670904 0.30356562 0.24121567 0.26179596 0.24087133\n",
      " 0.19670904 0.30356562 0.24117372 0.26179596 0.24087133 0.19670904\n",
      " 0.30504612 0.24121567 0.26179596        nan        nan        nan\n",
      "        nan        nan 0.24087133 0.19670904 0.30356562 0.24117372\n",
      " 0.26179596 0.24087133 0.19670904 0.30504612 0.24121567 0.26179596]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.35333333333333333\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.729426433915212\n",
      "GridSearchCV Runtime: 4.290892839431763 secs\n",
      "Efron R-squared: -0.17312167766396724\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.391304347826087\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.5\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.4\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4\n",
      "Ave Test Precision: 0.3982608695652174\n",
      "Stdev Test Precision: 0.07081753224552474\n",
      "Ave Test Accuracy: 0.7402985074626864\n",
      "Stdev Test Accuracy: 0.007737478116913322\n",
      "Ave Test Specificity: 0.9485148514851485\n",
      "Ave Test Recall: 0.10303030303030303\n",
      "Ave Test NPV: 0.7640751422326578\n",
      "Ave Test F1-Score: 0.15902587978970467\n",
      "Ave Test G-mean: 0.30432592506686246\n",
      "Ave Runtime: 0.006660270690917969\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyIncome. 71 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24609164 0.2834696  0.27184211 0.26540728 0.26540728 0.25036514\n",
      " 0.1225     0.0475     0.26540728 0.26540728 0.2520287  0.1225\n",
      " 0.0475     0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.25036514 0.2834696  0.27184211 0.26540728 0.26540728 0.25036514\n",
      " 0.1225     0.0475     0.26540728 0.26540728 0.2520287  0.1225\n",
      " 0.0475     0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.25036514 0.29180293 0.27184211 0.26540728 0.26540728 0.25112145\n",
      " 0.12102941 0.0475     0.26977934 0.26616359 0.2520287  0.12102941\n",
      " 0.0475     0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.25036514 0.28528179 0.27184211 0.26540728 0.26540728 0.2520287\n",
      " 0.19293665 0.0975     0.2706866  0.27275081 0.2520287  0.19293665\n",
      " 0.0975     0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2512724  0.33922783 0.29221725 0.26631454 0.26631454 0.2520287\n",
      " 0.33139046 0.20738801 0.2706866  0.27275081 0.2520287  0.33139046\n",
      " 0.20738801 0.2706866  0.27275081        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23953868 0.26984626 0.26647338 0.2520287\n",
      " 0.34123467 0.23391928 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23391928 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.26931594 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.26931594 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224\n",
      " 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224 0.2520287\n",
      " 0.34123467 0.23376945 0.26984626 0.27007224 0.2520287  0.34123467\n",
      " 0.23376945 0.26984626 0.27007224        nan        nan        nan\n",
      "        nan        nan 0.2520287  0.34123467 0.23376945 0.26984626\n",
      " 0.27007224 0.2520287  0.34123467 0.23376945 0.26984626 0.27007224]\n",
      "One or more of the train scores are non-finite: [0.25734902 0.27740934 0.30806605 0.2659201  0.2659201  0.25937202\n",
      " 0.12285319 0.04930748 0.2659201  0.2659201  0.26147392 0.12285319\n",
      " 0.04930748 0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.25937202 0.27740934 0.30806605 0.2659201  0.2659201  0.25937202\n",
      " 0.12285319 0.04930748 0.2659201  0.2659201  0.26147392 0.12285319\n",
      " 0.04930748 0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.25937202 0.28129598 0.30806605 0.2659201  0.2659201  0.2613433\n",
      " 0.1222614  0.04930748 0.27091955 0.26687707 0.26147392 0.1222614\n",
      " 0.04930748 0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.25937202 0.28218502 0.30806605 0.2659201  0.2659201  0.26147392\n",
      " 0.29681453 0.23680748 0.27100077 0.2716977  0.26147392 0.29681453\n",
      " 0.23680748 0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.25983315 0.28244277 0.30599652 0.2666605  0.2666605  0.26147392\n",
      " 0.28332143 0.3381712  0.27100077 0.27249068 0.26147392 0.28332143\n",
      " 0.3381712  0.27100077 0.27249068        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26053995 0.27746423 0.31372612 0.27030576 0.26917304 0.26138586\n",
      " 0.28455405 0.31330474 0.27030576 0.27196809 0.26147392 0.28455405\n",
      " 0.31330474 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27126128 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27126128 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809\n",
      " 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586\n",
      " 0.27742577 0.31362984 0.27030576 0.27196809 0.26138586 0.27742577\n",
      " 0.31362984 0.27030576 0.27196809        nan        nan        nan\n",
      "        nan        nan 0.26138586 0.27742577 0.31362984 0.27030576\n",
      " 0.27196809 0.26138586 0.27742577 0.31362984 0.27030576 0.27196809]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.34123467464930873\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.32917705735660846\n",
      "GridSearchCV Runtime: 4.337928533554077 secs\n",
      "Efron R-squared: -0.3619053607526317\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2555066079295154\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2838709677419355\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2828282828282828\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.255\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23387096774193547\n",
      "Ave Test Precision: 0.26221536524833383\n",
      "Stdev Test Precision: 0.021179773847642008\n",
      "Ave Test Accuracy: 0.41716417910447756\n",
      "Stdev Test Accuracy: 0.1320125840346531\n",
      "Ave Test Specificity: 0.31683168316831684\n",
      "Ave Test Recall: 0.7242424242424242\n",
      "Ave Test NPV: 0.7529494953987454\n",
      "Ave Test F1-Score: 0.3772747639423549\n",
      "Ave Test G-mean: 0.4260411353913523\n",
      "Ave Runtime: 0.007825279235839843\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with food. 70 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22070697 0.23867697 0.22114254 0.26786214 0.26786214 0.22494547\n",
      " 0.07344136 0.17344136 0.26786214 0.26786214 0.22186576 0.07344136\n",
      " 0.17344136 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22494547 0.23867697 0.22114254 0.26786214 0.26786214 0.22494547\n",
      " 0.07344136 0.17344136 0.26786214 0.26786214 0.22186576 0.07344136\n",
      " 0.17344136 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22494547 0.23867697 0.21958004 0.26786214 0.26786214 0.22494547\n",
      " 0.07344136 0.17344136 0.26478243 0.26786214 0.22186576 0.07344136\n",
      " 0.17344136 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22494547 0.23867697 0.22218861 0.26786214 0.26786214 0.22186576\n",
      " 0.31035767 0.17344136 0.26478243 0.26478243 0.22186576 0.31035767\n",
      " 0.17344136 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22494547 0.24596665 0.22427235 0.26478243 0.26786214 0.22186576\n",
      " 0.24867945 0.3627843  0.26478243 0.26478243 0.22186576 0.24867945\n",
      " 0.3627843  0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32480475 0.26478243 0.26478243 0.22186576\n",
      " 0.25495678 0.32650081 0.26478243 0.26478243 0.22186576 0.25495678\n",
      " 0.32650081 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243\n",
      " 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576\n",
      " 0.24743799 0.32376927 0.26478243 0.26478243 0.22186576 0.24743799\n",
      " 0.32376927 0.26478243 0.26478243        nan        nan        nan\n",
      "        nan        nan 0.22186576 0.24743799 0.32376927 0.26478243\n",
      " 0.26478243 0.22186576 0.24743799 0.32376927 0.26478243 0.26478243]\n",
      "One or more of the train scores are non-finite: [0.27293166 0.27203762 0.2289863  0.27405126 0.27405126 0.2708357\n",
      " 0.07371821 0.17177915 0.27405126 0.27405126 0.26731933 0.07371821\n",
      " 0.17177915 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26934312 0.27203762 0.2289863  0.27405126 0.27405126 0.2708357\n",
      " 0.07371821 0.17177915 0.27405126 0.27405126 0.26731933 0.07371821\n",
      " 0.17177915 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26934312 0.27203762 0.22996695 0.27405126 0.27405126 0.26746309\n",
      " 0.50571821 0.17177915 0.27154894 0.27405126 0.26731933 0.50571821\n",
      " 0.17177915 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26934312 0.27091939 0.23069112 0.27405126 0.27405126 0.26716148\n",
      " 0.25347926 0.37156967 0.27123273 0.27123273 0.26731933 0.25347926\n",
      " 0.37156967 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26903524 0.27238778 0.23070979 0.27344451 0.27405126 0.26731933\n",
      " 0.27415619 0.2363395  0.27123273 0.27123273 0.26731933 0.27415619\n",
      " 0.2363395  0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26716148 0.27380146 0.22906336 0.27123273 0.27155733 0.26731933\n",
      " 0.27519122 0.22908004 0.27123273 0.27123273 0.26731933 0.27519122\n",
      " 0.22908004 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273\n",
      " 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933\n",
      " 0.2742064  0.22890689 0.27123273 0.27123273 0.26731933 0.2742064\n",
      " 0.22890689 0.27123273 0.27123273        nan        nan        nan\n",
      "        nan        nan 0.26731933 0.2742064  0.22890689 0.27123273\n",
      " 0.27123273 0.26731933 0.2742064  0.22890689 0.27123273 0.27123273]\n",
      "invalid value encountered in scalar divide\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.36278429903429904\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.2456359102244389\n",
      "GridSearchCV Runtime: 4.464017629623413 secs\n",
      "Efron R-squared: -1.7783513337854702\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2608695652173913\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.14142764438676184\n",
      "Stdev Test Precision: 0.13104606857292384\n",
      "Ave Test Accuracy: 0.6395522388059701\n",
      "Stdev Test Accuracy: 0.22052207237899468\n",
      "Ave Test Specificity: 0.7752475247524753\n",
      "Ave Test Recall: 0.22424242424242422\n",
      "Ave Test NPV: 0.7536256779699038\n",
      "Ave Test F1-Score: 0.11653452409196982\n",
      "Ave Test G-mean: 0.09182818792766535\n",
      "Ave Runtime: 0.0073645591735839845\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with hygiene. 69 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25379797 0.26464849 0.23497413 0.26018894 0.29240658 0.25379797\n",
      " 0.12094136 0.09938272 0.25379797 0.28320685 0.24902536 0.12094136\n",
      " 0.09938272 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.25379797 0.26464849 0.23497413 0.25379797 0.27681587 0.25379797\n",
      " 0.12094136 0.09938272 0.25379797 0.27681587 0.24902536 0.12094136\n",
      " 0.09938272 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.25379797 0.27828486 0.23497413 0.25379797 0.27681587 0.24902536\n",
      " 0.17094136 0.09938272 0.24829006 0.27071291 0.24902536 0.17094136\n",
      " 0.09938272 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.25379797 0.26099089 0.20738189 0.25379797 0.27681587 0.24902536\n",
      " 0.17122839 0.09905033 0.24902536 0.26985869 0.24902536 0.17122839\n",
      " 0.09905033 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24828953 0.22122226 0.171297   0.24828953 0.27246151 0.24902536\n",
      " 0.21264968 0.13648078 0.24902536 0.26985869 0.24902536 0.21264968\n",
      " 0.13648078 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.17021005 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.17177278 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.17177278 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.1686768  0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.1686768  0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.16945247 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.16945247 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.16945247 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.16945247 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.16945247 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869\n",
      " 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536\n",
      " 0.2199816  0.16945247 0.24902536 0.26985869 0.24902536 0.2199816\n",
      " 0.16945247 0.24902536 0.26985869        nan        nan        nan\n",
      "        nan        nan 0.24902536 0.2199816  0.16945247 0.24902536\n",
      " 0.26985869 0.24902536 0.2199816  0.16945247 0.24902536 0.26985869]\n",
      "One or more of the train scores are non-finite: [0.27184806 0.26902998 0.22825881 0.27334539 0.29048509 0.27184806\n",
      " 0.12302569 0.09812895 0.27184806 0.27794305 0.26632203 0.12302569\n",
      " 0.09812895 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.27184806 0.26902998 0.22825881 0.27184806 0.27644573 0.27184806\n",
      " 0.12302569 0.09812895 0.27184806 0.27644573 0.26632203 0.12302569\n",
      " 0.09812895 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.27184806 0.26641394 0.2292207  0.27184806 0.27644573 0.26632203\n",
      " 0.15635902 0.09812895 0.26595859 0.27429497 0.26632203 0.15635902\n",
      " 0.09812895 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.27184806 0.26305439 0.20069737 0.27184806 0.27644573 0.26632203\n",
      " 0.20419283 0.13792092 0.26632203 0.27002058 0.26632203 0.20419283\n",
      " 0.13792092 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26618094 0.23041002 0.17319579 0.26618094 0.27279994 0.26632203\n",
      " 0.22972563 0.14676467 0.26632203 0.27002058 0.26632203 0.22972563\n",
      " 0.14676467 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17310655 0.26632203 0.27000176 0.26632203\n",
      " 0.23015013 0.17107996 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17107996 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17305364 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17305364 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17307091 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17307091 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17307091 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17307091 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17307091 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058\n",
      " 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203\n",
      " 0.23015013 0.17307091 0.26632203 0.27002058 0.26632203 0.23015013\n",
      " 0.17307091 0.26632203 0.27002058        nan        nan        nan\n",
      "        nan        nan 0.26632203 0.23015013 0.17307091 0.26632203\n",
      " 0.27002058 0.26632203 0.23015013 0.17307091 0.26632203 0.27002058]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2924065776970142\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.5224438902743143\n",
      "GridSearchCV Runtime: 4.3669703006744385 secs\n",
      "Efron R-squared: -0.3491643430882301\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.23484848484848486\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2222222222222222\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.296875\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.24242424242424243\n",
      "Ave Test Precision: 0.2392739898989899\n",
      "Stdev Test Precision: 0.03598817380081265\n",
      "Ave Test Accuracy: 0.5798507462686567\n",
      "Stdev Test Accuracy: 0.08053134416876992\n",
      "Ave Test Specificity: 0.6653465346534654\n",
      "Ave Test Recall: 0.31818181818181823\n",
      "Ave Test NPV: 0.747781710648281\n",
      "Ave Test F1-Score: 0.26426961926961934\n",
      "Ave Test G-mean: 0.442630007440229\n",
      "Ave Runtime: 0.006003952026367188\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseCleaning. 68 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25883009 0.19466016 0.17420361 0.25883009 0.25362049 0.25883009\n",
      " 0.09594136 0.07375    0.25883009 0.25362049 0.26032097 0.09594136\n",
      " 0.07375    0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.25883009 0.19466016 0.17420361 0.25883009 0.25362049 0.25883009\n",
      " 0.09594136 0.07375    0.25883009 0.25362049 0.26032097 0.09594136\n",
      " 0.07375    0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.25883009 0.19466016 0.1752381  0.25883009 0.25362049 0.26032097\n",
      " 0.09594136 0.07375    0.2615525  0.256974   0.26032097 0.09594136\n",
      " 0.07375    0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.25682815 0.19089105 0.16857143 0.25729624 0.25362049 0.26032097\n",
      " 0.16219136 0.07375    0.2615525  0.25387231 0.26032097 0.16219136\n",
      " 0.07375    0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.25656745 0.19598753 0.15715949 0.25939285 0.24018907 0.26032097\n",
      " 0.20517875 0.1675     0.2615525  0.25387231 0.26032097 0.20517875\n",
      " 0.1675     0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26085328 0.21355541 0.19587271 0.25873995 0.25452839 0.26032097\n",
      " 0.21355541 0.19920604 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19920604 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25430709 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231\n",
      " 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097\n",
      " 0.21355541 0.19587271 0.2615525  0.25387231 0.26032097 0.21355541\n",
      " 0.19587271 0.2615525  0.25387231        nan        nan        nan\n",
      "        nan        nan 0.26032097 0.21355541 0.19587271 0.2615525\n",
      " 0.25387231 0.26032097 0.21355541 0.19587271 0.2615525  0.25387231]\n",
      "One or more of the train scores are non-finite: [0.25688869 0.20314955 0.20350229 0.25688869 0.25188511 0.25688869\n",
      " 0.09851046 0.07368421 0.25688869 0.25278503 0.26671905 0.09851046\n",
      " 0.07368421 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.25688869 0.20314955 0.20350229 0.25688869 0.25278503 0.25681867\n",
      " 0.09851046 0.07368421 0.25681867 0.25278503 0.26671905 0.09851046\n",
      " 0.07368421 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.25667978 0.20343702 0.20330657 0.25667978 0.25278503 0.26671905\n",
      " 0.14851046 0.07368421 0.26723955 0.25615272 0.26671905 0.14851046\n",
      " 0.07368421 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.25804506 0.20605259 0.20888507 0.2575013  0.252715   0.26671905\n",
      " 0.21903108 0.07368421 0.26723955 0.26084006 0.26671905 0.21903108\n",
      " 0.07368421 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26056118 0.23688466 0.21439426 0.26133571 0.25424269 0.26671905\n",
      " 0.23771286 0.21041631 0.26723955 0.26084006 0.26671905 0.23771286\n",
      " 0.21041631 0.26723955 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.2654713  0.26550612 0.21328813 0.26611452 0.25900083 0.26671905\n",
      " 0.2654851  0.21134196 0.26712216 0.26084006 0.26671905 0.2654851\n",
      " 0.21134196 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26638066 0.26310415 0.20992275 0.26712216 0.26087086 0.26671905\n",
      " 0.26310415 0.21054331 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.21054331 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26671905 0.26310415 0.20992275 0.26712216 0.26088814 0.26671905\n",
      " 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.20992275 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905\n",
      " 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.20992275 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905\n",
      " 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.20992275 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905\n",
      " 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.20992275 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905\n",
      " 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.20992275 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006\n",
      " 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905\n",
      " 0.26310415 0.20992275 0.26712216 0.26084006 0.26671905 0.26310415\n",
      " 0.20992275 0.26712216 0.26084006        nan        nan        nan\n",
      "        nan        nan 0.26671905 0.26310415 0.20992275 0.26712216\n",
      " 0.26084006 0.26671905 0.26310415 0.20992275 0.26712216 0.26084006]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.26155249978779394\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.6047381546134664\n",
      "GridSearchCV Runtime: 4.236233234405518 secs\n",
      "Efron R-squared: -0.34916828272269473\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2549019607843137\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.27835051546391754\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.24175824175824176\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.23404255319148937\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3\n",
      "Ave Test Precision: 0.2618106542395925\n",
      "Stdev Test Precision: 0.027170249659907404\n",
      "Ave Test Accuracy: 0.5850746268656716\n",
      "Stdev Test Accuracy: 0.021980114141825602\n",
      "Ave Test Specificity: 0.6534653465346535\n",
      "Ave Test Recall: 0.37575757575757573\n",
      "Ave Test NPV: 0.7620807423408703\n",
      "Ave Test F1-Score: 0.30844415526120034\n",
      "Ave Test G-mean: 0.4949908535914968\n",
      "Ave Runtime: 0.007321548461914062\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with fare. 67 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.23426025 0.13545073 0.24588183 0.23426025 0.23887563 0.23988796\n",
      " 0.15166667 0.15479958 0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.15479958 0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.23426025 0.14261905 0.2289691  0.23426025 0.23887563 0.25592692\n",
      " 0.16119048 0.2038421  0.25088023 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.23151925 0.15140693 0.24822077 0.23151925 0.24001783 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.24455751 0.1747619  0.24576979 0.24455751 0.24272041 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.27105212 0.16190476 0.23064374 0.27105212 0.25353436 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.27441813 0.15666667 0.19967543 0.28118581 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513\n",
      " 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513 0.22473644\n",
      " 0.16619048 0.2038421  0.25412953 0.27051344 0.28951914 0.15666667\n",
      " 0.2038421  0.28951914 0.28442513        nan        nan        nan\n",
      "        nan        nan 0.22473644 0.16619048 0.2038421  0.25412953\n",
      " 0.27051344 0.28951914 0.15666667 0.2038421  0.28951914 0.28442513]\n",
      "One or more of the train scores are non-finite: [0.24205756 0.11975397 0.21464037 0.24205756 0.24220463 0.25516393\n",
      " 0.16954082 0.28793253 0.27680635 0.26334607 0.30185304 0.17676565\n",
      " 0.28793253 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.24205756 0.11716382 0.21505814 0.24205756 0.24220463 0.26266551\n",
      " 0.16021774 0.28371666 0.26231363 0.26334607 0.30185304 0.17676565\n",
      " 0.28371666 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.23991823 0.12664958 0.22896122 0.24085828 0.24242935 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27069079 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.25576426 0.15165947 0.25870406 0.25576426 0.24091985 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.28018316 0.16976304 0.28057304 0.28022261 0.2595521  0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.29912007 0.17681912 0.28097012 0.29935584 0.28038393 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30042424 0.17676565 0.28371666 0.30180954 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30185304 0.17676565 0.28371666 0.30255705 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498\n",
      " 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498 0.26550119\n",
      " 0.16744256 0.28375075 0.27680635 0.27790932 0.30185304 0.17676565\n",
      " 0.28375075 0.30255705 0.28211498        nan        nan        nan\n",
      "        nan        nan 0.26550119 0.16744256 0.28375075 0.27680635\n",
      " 0.27790932 0.30185304 0.17676565 0.28375075 0.30255705 0.28211498]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.28951914098972925\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.7069825436408977\n",
      "GridSearchCV Runtime: 4.594989776611328 secs\n",
      "Efron R-squared: -0.34898639755108696\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.15151515151515152\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.21739130434782608\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.24166666666666667\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2384937238493724\n",
      "Ave Test Precision: 0.21981336927580336\n",
      "Stdev Test Precision: 0.040025828773816614\n",
      "Ave Test Accuracy: 0.4388059701492537\n",
      "Stdev Test Accuracy: 0.21445650037872302\n",
      "Ave Test Specificity: 0.403960396039604\n",
      "Ave Test Recall: 0.5454545454545455\n",
      "Ave Test NPV: 0.695539950389547\n",
      "Ave Test F1-Score: 0.2760765286508996\n",
      "Ave Test G-mean: 0.29785410950910735\n",
      "Ave Runtime: 0.008019304275512696\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with parking. 66 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.2443672  0.27124112 0.22447406 0.2443672  0.2443672  0.2443672\n",
      " 0.08897707 0.09969136 0.2443672  0.2443672  0.27204502 0.09007597\n",
      " 0.09969136 0.26547744 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.2443672  0.27124112 0.22447406 0.2443672  0.2443672  0.24494345\n",
      " 0.08897707 0.09969136 0.24494345 0.24494345 0.27204502 0.09007597\n",
      " 0.09969136 0.26547744 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.2443672  0.2689684  0.22447406 0.2443672  0.2443672  0.27204502\n",
      " 0.19007597 0.09969136 0.26547744 0.26633093 0.27204502 0.19007597\n",
      " 0.09969136 0.26547744 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.24415743 0.26909486 0.23302469 0.24415743 0.24524982 0.27204502\n",
      " 0.24007597 0.09969136 0.26886206 0.26633093 0.27204502 0.24007597\n",
      " 0.09969136 0.26886206 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.24478859 0.30933718 0.28257974 0.25952073 0.25198024 0.27204502\n",
      " 0.30589724 0.34103335 0.26728311 0.26633093 0.27204502 0.30589724\n",
      " 0.34103335 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.2712353  0.30262768 0.35056739 0.27204502 0.26634559 0.27204502\n",
      " 0.30262768 0.34045431 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34045431 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.2638985  0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093\n",
      " 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502\n",
      " 0.30262768 0.34878794 0.26728311 0.26633093 0.27204502 0.30262768\n",
      " 0.34878794 0.26728311 0.26633093        nan        nan        nan\n",
      "        nan        nan 0.27204502 0.30262768 0.34878794 0.26728311\n",
      " 0.26633093 0.27204502 0.30262768 0.34878794 0.26728311 0.26633093]\n",
      "One or more of the train scores are non-finite: [0.24769496 0.26716439 0.20248286 0.24769496 0.24769496 0.24769496\n",
      " 0.09934148 0.09809494 0.24769496 0.24769496 0.2660345  0.10161709\n",
      " 0.09809494 0.26698393 0.2669056         nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.24769496 0.26716439 0.20248286 0.24769496 0.24769496 0.24760582\n",
      " 0.09934148 0.09809494 0.24760582 0.24760582 0.2660345  0.10161709\n",
      " 0.09809494 0.26698393 0.2669056         nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.24769496 0.26732322 0.20248286 0.24769496 0.24769496 0.2660345\n",
      " 0.46828376 0.09809494 0.26698393 0.2669056  0.2660345  0.46828376\n",
      " 0.09809494 0.26698393 0.2669056         nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.24695665 0.27144004 0.2085943  0.24695665 0.24763882 0.2660345\n",
      " 0.38292233 0.59791907 0.26658529 0.2669056  0.2660345  0.38292233\n",
      " 0.59791907 0.26658529 0.2669056         nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26150885 0.28259651 0.36911933 0.2626147  0.25081288 0.26599082\n",
      " 0.29078213 0.43506092 0.26668165 0.26687193 0.26599082 0.29078213\n",
      " 0.43506092 0.26668165 0.26687193        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26615734 0.28200875 0.3050342  0.26599082 0.26664882 0.26599082\n",
      " 0.28183421 0.31725231 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31725231 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26704662 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583\n",
      " 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082\n",
      " 0.28183421 0.31073678 0.26664172 0.26683583 0.26599082 0.28183421\n",
      " 0.31073678 0.26664172 0.26683583        nan        nan        nan\n",
      "        nan        nan 0.26599082 0.28183421 0.31073678 0.26664172\n",
      " 0.26683583 0.26599082 0.28183421 0.31073678 0.26664172 0.26683583]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.35056738714628083\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7518703241895262\n",
      "GridSearchCV Runtime: 4.430720567703247 secs\n",
      "Efron R-squared: -0.021235515652387127\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.375\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.29545454545454547\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.5416666666666666\n",
      "Ave Test Precision: 0.24242424242424238\n",
      "Stdev Test Precision: 0.23847096240039925\n",
      "Ave Test Accuracy: 0.7358208955223882\n",
      "Stdev Test Accuracy: 0.029663592409480286\n",
      "Ave Test Specificity: 0.9475247524752476\n",
      "Ave Test Recall: 0.08787878787878788\n",
      "Ave Test NPV: 0.7611389992836168\n",
      "Ave Test F1-Score: 0.12126672126672126\n",
      "Ave Test G-mean: 0.21008904625998212\n",
      "Ave Runtime: 0.008008337020874024\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with gasoline. 65 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25827039 0.02321429 0.07635802 0.27394437 0.26626412 0.25827039\n",
      " 0.02321429 0.02469136 0.27220826 0.24248909 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23244166        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.25746621 0.02280702 0.07400508 0.27522247 0.26452271 0.25690596\n",
      " 0.02321429 0.02469136 0.26926702 0.24336987 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.25654416 0.02333333 0.08251699 0.27654416 0.26366249 0.25719773\n",
      " 0.02321429 0.02469136 0.25934311 0.24639281 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2532803  0.02419355 0.08897707 0.27248115 0.26399277 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.25030223 0.02424242 0.02469136 0.24954538 0.2623776  0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2510964  0.02424242 0.02469136 0.24419432 0.23509497 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23461361 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189\n",
      " 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189 0.25719773\n",
      " 0.02321429 0.02469136 0.26065766 0.24189861 0.2515293  0.02424242\n",
      " 0.02469136 0.24382265 0.23339189        nan        nan        nan\n",
      "        nan        nan 0.25719773 0.02321429 0.02469136 0.26065766\n",
      " 0.24189861 0.2515293  0.02424242 0.02469136 0.24382265 0.23339189]\n",
      "One or more of the train scores are non-finite: [0.25855778 0.02613636 0.09073148 0.25151968 0.24709273 0.25855778\n",
      " 0.02613636 0.25048393 0.25042606 0.24920974 0.25060776 0.0246875\n",
      " 0.25048393 0.24595344 0.24256093        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25757408 0.02592593 0.08969351 0.25047028 0.24744062 0.25789611\n",
      " 0.02613636 0.27625115 0.25087363 0.24966169 0.25060776 0.0246875\n",
      " 0.27625115 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25662588 0.02591241 0.09304259 0.24926598 0.24748724 0.25704059\n",
      " 0.02613636 0.26791781 0.25284674 0.25015454 0.25060776 0.0246875\n",
      " 0.26791781 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25343654 0.02508772 0.12538412 0.24566496 0.24531312 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25113646 0.02468553 0.23446405 0.24775836 0.24405847 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25113419 0.0246875  0.27908997 0.24646313 0.24403673 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27908997 0.24595344 0.24321057 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467\n",
      " 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467 0.25704059\n",
      " 0.02613636 0.27432807 0.25400692 0.24853619 0.25060776 0.0246875\n",
      " 0.27432807 0.24595344 0.24298467        nan        nan        nan\n",
      "        nan        nan 0.25704059 0.02613636 0.27432807 0.25400692\n",
      " 0.24853619 0.25060776 0.0246875  0.27432807 0.24595344 0.24298467]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.0001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2765441574615398\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.0001, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.385286783042394\n",
      "GridSearchCV Runtime: 4.724778890609741 secs\n",
      "Efron R-squared: -0.3447105890161153\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.26066350710900477\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.19298245614035087\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.24861878453038674\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.24644549763033174\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.24878048780487805\n",
      "Ave Test Precision: 0.2394981466429904\n",
      "Stdev Test Precision: 0.02659556130848562\n",
      "Ave Test Accuracy: 0.4276119402985075\n",
      "Stdev Test Accuracy: 0.11149163819094386\n",
      "Ave Test Specificity: 0.35544554455445543\n",
      "Ave Test Recall: 0.6484848484848484\n",
      "Ave Test NPV: 0.7642530906445719\n",
      "Ave Test F1-Score: 0.33843623979729176\n",
      "Ave Test G-mean: 0.4208835336155188\n",
      "Ave Runtime: 0.007003021240234375\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with tuition. 64 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.23183073 0.09739055 0.14429487 0.23183073 0.22010912 0.23183073\n",
      " 0.04844136 0.0725     0.23183073 0.21998938 0.26451764 0.04844136\n",
      " 0.0725     0.26169099 0.25057988        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.23183073 0.09739055 0.14429487 0.23183073 0.22010912 0.23183073\n",
      " 0.04844136 0.0725     0.23520911 0.2219539  0.26451764 0.04844136\n",
      " 0.0725     0.26169099 0.25057988        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.23183073 0.09739055 0.14429487 0.23183073 0.22010912 0.26451764\n",
      " 0.04844136 0.0725     0.26169099 0.24248589 0.26451764 0.04844136\n",
      " 0.0725     0.26169099 0.25057988        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.23599151 0.10570682 0.14958333 0.23599151 0.22110196 0.26451764\n",
      " 0.15875    0.0725     0.26169099 0.25057988 0.26451764 0.15875\n",
      " 0.0725     0.26169099 0.25057988        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.24893497 0.17075376 0.43617063 0.25067789 0.21166582 0.26451764\n",
      " 0.22499267 0.37122294 0.26169099 0.25402191 0.26451764 0.22499267\n",
      " 0.37122294 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.34640386 0.26451764 0.25402191 0.26451764\n",
      " 0.24219697 0.34559368 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.34559368 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.34222551 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.34222551 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.34222551 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.32876397 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.33068705 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.33068705 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.31487508 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.31487508 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.31487508 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.31487508 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191\n",
      " 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764\n",
      " 0.24219697 0.31487508 0.26169099 0.25402191 0.26451764 0.24219697\n",
      " 0.31487508 0.26169099 0.25402191        nan        nan        nan\n",
      "        nan        nan 0.26451764 0.24219697 0.31487508 0.26169099\n",
      " 0.25402191 0.26451764 0.24219697 0.31487508 0.26169099 0.25402191]\n",
      "One or more of the train scores are non-finite: [0.23310758 0.09407757 0.23851104 0.23310758 0.23721101 0.23310758\n",
      " 0.04920298 0.07382271 0.23310758 0.23755242 0.26173823 0.04920298\n",
      " 0.07382271 0.26135432 0.26004765        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.23310758 0.09407757 0.23851104 0.23310758 0.23721101 0.23310758\n",
      " 0.04920298 0.07382271 0.23595968 0.23809542 0.26173823 0.04920298\n",
      " 0.07382271 0.26135432 0.26004765        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.23310758 0.09413918 0.26909712 0.23310758 0.23721101 0.26173823\n",
      " 0.04920298 0.07382271 0.26135432 0.25306727 0.26173823 0.04920298\n",
      " 0.07382271 0.26135432 0.26004765        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.23661837 0.13077268 0.28614416 0.23638037 0.23738106 0.26173823\n",
      " 0.13088497 0.07382271 0.26135432 0.26004765 0.26173823 0.13088497\n",
      " 0.07382271 0.26135432 0.26004765        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.24831876 0.2132835  0.32156428 0.24977083 0.237303   0.26182795\n",
      " 0.24671515 0.34375413 0.26145237 0.25978877 0.26182795 0.24671515\n",
      " 0.34375413 0.26145237 0.25978877        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.31650873 0.26182795 0.25910186 0.26182795\n",
      " 0.25921109 0.31846559 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.31846559 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.31177353 0.26145237 0.26000083 0.26182795\n",
      " 0.25921109 0.31177353 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.31177353 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.31036387 0.26145237 0.25990279 0.26182795\n",
      " 0.25921109 0.31088695 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.31088695 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279\n",
      " 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795\n",
      " 0.25921109 0.30514648 0.26145237 0.25990279 0.26182795 0.25921109\n",
      " 0.30514648 0.26145237 0.25990279        nan        nan        nan\n",
      "        nan        nan 0.26182795 0.25921109 0.30514648 0.26145237\n",
      " 0.25990279 0.26182795 0.25921109 0.30514648 0.26145237 0.25990279]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "invalid value encountered in scalar divide\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.4361706349206349\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.743142144638404\n",
      "GridSearchCV Runtime: 4.6912524700164795 secs\n",
      "Efron R-squared: -0.14551439806472977\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.13043478260869565\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.5\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.17534068786502272\n",
      "Stdev Test Precision: 0.2085303932824841\n",
      "Ave Test Accuracy: 0.6395522388059701\n",
      "Stdev Test Accuracy: 0.22156137040428409\n",
      "Ave Test Specificity: 0.7792079207920792\n",
      "Ave Test Recall: 0.2121212121212121\n",
      "Ave Test NPV: 0.751489731792167\n",
      "Ave Test F1-Score: 0.09840741517625687\n",
      "Ave Test G-mean: 0.06503152266274008\n",
      "Ave Runtime: 0.008513402938842774\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with allowance. 63 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.23915161 0.01111111 0.02469136 0.20563948 0.11380361 0.23915161\n",
      " 0.         0.02469136 0.20563948 0.24060616 0.22085467 0.\n",
      " 0.02469136 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.23915161 0.01111111 0.02469136 0.2315654  0.24060616 0.23915161\n",
      " 0.         0.02469136 0.2315654  0.24060616 0.22085467 0.\n",
      " 0.02469136 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.23425809 0.01111111 0.02469136 0.22509756 0.24231098 0.22085467\n",
      " 0.         0.02469136 0.21711914 0.23293405 0.22085467 0.\n",
      " 0.02469136 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.2350749  0.03119497 0.07469136 0.22178745 0.23579564 0.22085467\n",
      " 0.02407407 0.02469136 0.20449655 0.23293405 0.22085467 0.02407407\n",
      " 0.02469136 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22800567 0.06156009 0.12597403 0.20907094 0.23300876 0.22085467\n",
      " 0.06156009 0.12597403 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.12597403 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22261223 0.06156009 0.13264069 0.20369077 0.23177406 0.22085467\n",
      " 0.06156009 0.18264069 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.18264069 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405\n",
      " 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467\n",
      " 0.06156009 0.17708514 0.20449655 0.23293405 0.22085467 0.06156009\n",
      " 0.17708514 0.20449655 0.23293405        nan        nan        nan\n",
      "        nan        nan 0.22085467 0.06156009 0.17708514 0.20449655\n",
      " 0.23293405 0.22085467 0.06156009 0.17708514 0.20449655 0.23293405]\n",
      "One or more of the train scores are non-finite: [0.25301195 0.02176166 0.02458333 0.22168977 0.12709259 0.25301195\n",
      " 0.         0.02454924 0.22168977 0.2587137  0.24999426 0.\n",
      " 0.02454924 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.25301195 0.02176166 0.02458333 0.2479524  0.2587137  0.25301195\n",
      " 0.         0.02454924 0.2479524  0.2587137  0.24999426 0.\n",
      " 0.02454924 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.25071278 0.02176166 0.02458333 0.24681352 0.25747437 0.24999426\n",
      " 0.         0.02454924 0.24605186 0.25376518 0.24999426 0.\n",
      " 0.02454924 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.25141409 0.04908392 0.07681945 0.2468003  0.25671394 0.24999426\n",
      " 0.05130561 0.07458333 0.24644833 0.25376518 0.24999426 0.05130561\n",
      " 0.07458333 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.25091183 0.07905117 0.08474701 0.24718401 0.25416455 0.24999426\n",
      " 0.07980699 0.22641368 0.24644833 0.25376518 0.24999426 0.07980699\n",
      " 0.22641368 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.25031403 0.08070205 0.23884558 0.24679834 0.25412574 0.24999426\n",
      " 0.08070205 0.23733043 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.23733043 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518\n",
      " 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426\n",
      " 0.08070205 0.24310339 0.24644833 0.25376518 0.24999426 0.08070205\n",
      " 0.24310339 0.24644833 0.25376518        nan        nan        nan\n",
      "        nan        nan 0.24999426 0.08070205 0.24310339 0.24644833\n",
      " 0.25376518 0.24999426 0.08070205 0.24310339 0.24644833 0.25376518]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.0001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.24231097870381357\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.0001, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.5798004987531172\n",
      "GridSearchCV Runtime: 4.595154285430908 secs\n",
      "Efron R-squared: -0.3493949083895225\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.17073170731707318\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.26424870466321243\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2529411764705882\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.26424870466321243\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2236842105263158\n",
      "Ave Test Precision: 0.2351709007280804\n",
      "Stdev Test Precision: 0.039658459887510863\n",
      "Ave Test Accuracy: 0.48358208955223886\n",
      "Stdev Test Accuracy: 0.08527568543362812\n",
      "Ave Test Specificity: 0.4673267326732673\n",
      "Ave Test Recall: 0.5333333333333334\n",
      "Ave Test NPV: 0.7661055793285055\n",
      "Ave Test F1-Score: 0.3161354752426607\n",
      "Ave Test G-mean: 0.45042779345007755\n",
      "Ave Runtime: 0.006790685653686524\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with uniform. 62 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.19749313 0.09319998 0.01666667 0.19169179 0.19179782 0.19749313\n",
      " 0.07125    0.         0.19086843 0.22394068 0.18113248 0.07125\n",
      " 0.         0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.19749313 0.09319998 0.01666667 0.19169179 0.22394068 0.1905567\n",
      " 0.07125    0.         0.19104604 0.21847576 0.18113248 0.07125\n",
      " 0.         0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.19110597 0.07785334 0.01818182 0.18140676 0.22063858 0.18113248\n",
      " 0.11028481 0.         0.20347054 0.20982682 0.18113248 0.10320148\n",
      " 0.         0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18818315 0.06467836 0.01666667 0.1820372  0.21221764 0.18113248\n",
      " 0.08588751 0.13333333 0.2091815  0.20982682 0.18113248 0.08588751\n",
      " 0.13333333 0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18058744 0.06407379 0.11666667 0.19589869 0.20464434 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20314657 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682\n",
      " 0.18113248 0.08588751 0.35       0.20784816 0.20982682 0.18113248\n",
      " 0.08588751 0.35       0.20347054 0.20982682 0.18113248 0.08588751\n",
      " 0.35       0.20784816 0.20982682        nan        nan        nan\n",
      "        nan        nan 0.18113248 0.08588751 0.35       0.20347054\n",
      " 0.20982682 0.18113248 0.08588751 0.35       0.20784816 0.20982682]\n",
      "One or more of the train scores are non-finite: [0.23472969 0.11958355 0.05220779 0.19275756 0.21407182 0.23472969\n",
      " 0.07396122 0.         0.19198469 0.24042997 0.23895782 0.07396122\n",
      " 0.         0.20791805 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23472969 0.11958355 0.05220779 0.19275756 0.24042997 0.24168888\n",
      " 0.10729455 0.         0.20183193 0.24001078 0.23895782 0.10729455\n",
      " 0.         0.20791805 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.2312664  0.12279631 0.05441927 0.18707663 0.23951925 0.23895782\n",
      " 0.13933474 0.         0.20462025 0.2400415  0.23895782 0.13349288\n",
      " 0.         0.20791805 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23137596 0.13039311 0.07308612 0.18689335 0.23790163 0.23895782\n",
      " 0.13246526 0.40196078 0.20633856 0.2400415  0.23895782 0.13246526\n",
      " 0.40196078 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23433137 0.13122553 0.23507403 0.19647388 0.23793872 0.23895782\n",
      " 0.1334222  0.41834784 0.20480207 0.2400415  0.23895782 0.1334222\n",
      " 0.41834784 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20727682 0.23856999 0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415\n",
      " 0.23895782 0.1360196  0.41932896 0.20829643 0.2400415  0.23895782\n",
      " 0.1360196  0.41932896 0.20480207 0.2400415  0.23895782 0.1360196\n",
      " 0.41932896 0.20829643 0.2400415         nan        nan        nan\n",
      "        nan        nan 0.23895782 0.1360196  0.41932896 0.20480207\n",
      " 0.2400415  0.23895782 0.1360196  0.41932896 0.20829643 0.2400415 ]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.35\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.7094763092269327\n",
      "GridSearchCV Runtime: 4.494149684906006 secs\n",
      "Efron R-squared: -0.10704335245226826\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2702702702702703\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.37037037037037035\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25396825396825395\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.0\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23868312757201646\n",
      "Ave Test Precision: 0.2266584044361822\n",
      "Stdev Test Precision: 0.13676583833665623\n",
      "Ave Test Accuracy: 0.6164179104477612\n",
      "Stdev Test Accuracy: 0.19263274275123088\n",
      "Ave Test Specificity: 0.7247524752475247\n",
      "Ave Test Recall: 0.28484848484848485\n",
      "Ave Test NPV: 0.7426375745142544\n",
      "Ave Test F1-Score: 0.20653901339412525\n",
      "Ave Test G-mean: 0.28761252002017845\n",
      "Ave Runtime: 0.01000962257385254\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with otherEducation. 61 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22961435 0.24108772 0.15644835 0.23396037 0.26509081 0.23314023\n",
      " 0.1475     0.07313272 0.23597832 0.25939727 0.22670954 0.1475\n",
      " 0.07313272 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.23314023 0.24108772 0.15644835 0.23597832 0.26455243 0.23314023\n",
      " 0.1475     0.07313272 0.23597832 0.25939727 0.22670954 0.1475\n",
      " 0.07313272 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.23314023 0.240535   0.15644835 0.23597832 0.26455243 0.23314023\n",
      " 0.1475     0.07313272 0.23597832 0.26455243 0.22670954 0.1475\n",
      " 0.07313272 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.23314023 0.23773166 0.15644835 0.23597832 0.26455243 0.22670954\n",
      " 0.18193223 0.07313272 0.22621429 0.25942117 0.22670954 0.18193223\n",
      " 0.07313272 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22705328 0.24042865 0.20619343 0.22989136 0.25846547 0.22670954\n",
      " 0.17952207 0.1431782  0.22621429 0.25942117 0.22670954 0.17952207\n",
      " 0.1431782  0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21952676 0.22655803 0.25942117 0.22670954\n",
      " 0.2409337  0.19407222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.19407222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117\n",
      " 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954\n",
      " 0.2409337  0.21907222 0.22621429 0.25942117 0.22670954 0.2409337\n",
      " 0.21907222 0.22621429 0.25942117        nan        nan        nan\n",
      "        nan        nan 0.22670954 0.2409337  0.21907222 0.22621429\n",
      " 0.25942117 0.22670954 0.2409337  0.21907222 0.22621429 0.25942117]\n",
      "One or more of the train scores are non-finite: [0.2497371  0.24929083 0.2176507  0.24931631 0.24881983 0.25169437\n",
      " 0.14736842 0.07375221 0.25136004 0.25073864 0.25225756 0.14736842\n",
      " 0.07375221 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25169437 0.24929083 0.2176507  0.25136004 0.25065451 0.25169437\n",
      " 0.14736842 0.07375221 0.25136004 0.25073864 0.25225756 0.14736842\n",
      " 0.07375221 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25169437 0.24924073 0.2176507  0.25136004 0.25065451 0.25145629\n",
      " 0.17464115 0.07375221 0.25136004 0.25065451 0.25225756 0.17464115\n",
      " 0.07375221 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25169437 0.24962085 0.21654493 0.25136004 0.25065451 0.25225756\n",
      " 0.20564076 0.07375221 0.25210893 0.2516254  0.25225756 0.20564076\n",
      " 0.07375221 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25280079 0.25055943 0.21038245 0.25270454 0.25199902 0.25225756\n",
      " 0.22309179 0.18535531 0.25210893 0.25157928 0.25225756 0.22309179\n",
      " 0.18535531 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.2521729  0.25161031 0.2107624  0.25210893 0.25235006 0.25225756\n",
      " 0.2515829  0.20399491 0.25210893 0.25157928 0.25225756 0.2515829\n",
      " 0.20399491 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25121951 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138\n",
      " 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138 0.25225756\n",
      " 0.2518577  0.21074111 0.25210893 0.25157928 0.25225756 0.2518577\n",
      " 0.21074111 0.25210893 0.25134138        nan        nan        nan\n",
      "        nan        nan 0.25225756 0.2518577  0.21074111 0.25210893\n",
      " 0.25157928 0.25225756 0.2518577  0.21074111 0.25210893 0.25134138]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2650908137498526\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6172069825436409\n",
      "GridSearchCV Runtime: 4.538366079330444 secs\n",
      "Efron R-squared: -0.3491726069205505\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.3013698630136986\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2876712328767123\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.22388059701492538\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2569444444444444\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3235294117647059\n",
      "Ave Test Precision: 0.2786791098228973\n",
      "Stdev Test Precision: 0.038995354937258205\n",
      "Ave Test Accuracy: 0.6111940298507463\n",
      "Stdev Test Accuracy: 0.06856946110162625\n",
      "Ave Test Specificity: 0.695049504950495\n",
      "Ave Test Recall: 0.35454545454545455\n",
      "Ave Test NPV: 0.7671974865128453\n",
      "Ave Test F1-Score: 0.30500162141636705\n",
      "Ave Test G-mean: 0.48338157660770403\n",
      "Ave Runtime: 0.006384658813476563\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with emergency. 60 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24362231 0.3048146  0.14832308 0.24362231 0.24362231 0.24362231\n",
      " 0.09813272 0.07344136 0.24362231 0.24362231 0.24823343 0.09813272\n",
      " 0.07344136 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24362231 0.3048146  0.14832308 0.24362231 0.24362231 0.24362231\n",
      " 0.09813272 0.07344136 0.24362231 0.24362231 0.24823343 0.09813272\n",
      " 0.07344136 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24362231 0.3048146  0.14832308 0.24362231 0.24362231 0.24823343\n",
      " 0.24813272 0.07344136 0.24823343 0.24525275 0.24823343 0.24813272\n",
      " 0.07344136 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24362231 0.3048146  0.14832308 0.24362231 0.24362231 0.24823343\n",
      " 0.23146605 0.07344136 0.24823343 0.24641941 0.24823343 0.23146605\n",
      " 0.07344136 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24551362 0.25596009 0.18116547 0.24551362 0.24551362 0.24823343\n",
      " 0.290051   0.12852023 0.24823343 0.24641941 0.24823343 0.290051\n",
      " 0.12852023 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17582481 0.24752779 0.24641941 0.24823343\n",
      " 0.25596009 0.17682481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17682481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17582481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17582481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17057481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17057481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17057481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17057481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17057481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941\n",
      " 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343\n",
      " 0.25596009 0.17057481 0.24823343 0.24641941 0.24823343 0.25596009\n",
      " 0.17057481 0.24823343 0.24641941        nan        nan        nan\n",
      "        nan        nan 0.24823343 0.25596009 0.17057481 0.24823343\n",
      " 0.24641941 0.24823343 0.25596009 0.17057481 0.24823343 0.24641941]\n",
      "One or more of the train scores are non-finite: [0.24314208 0.23942567 0.15549678 0.24314208 0.24314208 0.24314208\n",
      " 0.09826745 0.07371821 0.24314208 0.24314208 0.24768942 0.09826745\n",
      " 0.07371821 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24314208 0.23942567 0.15549678 0.24314208 0.24314208 0.24314208\n",
      " 0.09826745 0.07371821 0.24314208 0.24314208 0.24768942 0.09826745\n",
      " 0.07371821 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24314208 0.23942567 0.15549678 0.24314208 0.24314208 0.24748505\n",
      " 0.17857435 0.07371821 0.24747103 0.24498825 0.24768942 0.17857435\n",
      " 0.07371821 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24314208 0.23969779 0.15549678 0.24314208 0.24314208 0.24768942\n",
      " 0.24148784 0.07375221 0.24768093 0.24692449 0.24768942 0.24148784\n",
      " 0.07375221 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24506451 0.24205973 0.2331001  0.24506451 0.24400422 0.24768942\n",
      " 0.24518544 0.15589581 0.24768093 0.24692449 0.24768942 0.24518544\n",
      " 0.15589581 0.24768093 0.24692449        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24737061 0.241635   0.22517502 0.24712915 0.24628106 0.24768942\n",
      " 0.24140575 0.22509755 0.24768093 0.24659988 0.24768942 0.24140575\n",
      " 0.22509755 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22812942 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22812942 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22797893 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22797893 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22797893 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22797893 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22797893 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22797893 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22797893 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22797893 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22797893 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22797893 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988\n",
      " 0.24768942 0.241635   0.22797893 0.24768093 0.24659988 0.24768942\n",
      " 0.241635   0.22797893 0.24768093 0.24659988 0.24768942 0.241635\n",
      " 0.22797893 0.24768093 0.24659988        nan        nan        nan\n",
      "        nan        nan 0.24768942 0.241635   0.22797893 0.24768093\n",
      " 0.24659988 0.24768942 0.241635   0.22797893 0.24768093 0.24659988]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.30481460434358987\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.5972568578553616\n",
      "GridSearchCV Runtime: 4.548198461532593 secs\n",
      "Efron R-squared: -0.34916963728635575\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2388888888888889\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.26582278481012656\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.22404371584699453\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.19575107790920201\n",
      "Stdev Test Precision: 0.11049074424610782\n",
      "Ave Test Accuracy: 0.5567164179104478\n",
      "Stdev Test Accuracy: 0.16147277584832945\n",
      "Ave Test Specificity: 0.6188118811881188\n",
      "Ave Test Recall: 0.3666666666666667\n",
      "Ave Test NPV: 0.7430113565100396\n",
      "Ave Test F1-Score: 0.24294395671578073\n",
      "Ave Test G-mean: 0.35871829660766386\n",
      "Ave Runtime: 0.010720348358154297\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with medicine. 59 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27703768 0.26408678 0.27255194 0.27703768 0.27703768 0.27703768\n",
      " 0.20219136 0.09844136 0.27703768 0.27703768 0.28411582 0.20219136\n",
      " 0.09844136 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.27703768 0.26408678 0.27549312 0.27703768 0.27703768 0.27703768\n",
      " 0.20219136 0.09844136 0.27703768 0.27703768 0.28411582 0.20219136\n",
      " 0.09844136 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.27703768 0.26408678 0.27549312 0.27703768 0.27703768 0.28411582\n",
      " 0.20219136 0.09844136 0.28150191 0.27986796 0.28411582 0.20219136\n",
      " 0.09844136 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.27472039 0.27231329 0.27500332 0.27472039 0.27703768 0.28411582\n",
      " 0.22069302 0.09620578 0.28150191 0.28150191 0.28411582 0.22069302\n",
      " 0.09620578 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.27370858 0.2751261  0.27588681 0.27437146 0.2727562  0.28065255\n",
      " 0.2761052  0.30370767 0.28150191 0.28150191 0.28065255 0.2761052\n",
      " 0.30370767 0.28150191 0.28150191        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.27875908 0.27758097 0.26768379 0.28150191 0.27937656 0.28065255\n",
      " 0.27758097 0.26980981 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26980981 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28109898 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121\n",
      " 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255\n",
      " 0.27758097 0.26852776 0.28150191 0.27819121 0.28065255 0.27758097\n",
      " 0.26852776 0.28150191 0.27819121        nan        nan        nan\n",
      "        nan        nan 0.28065255 0.27758097 0.26852776 0.28150191\n",
      " 0.27819121 0.28065255 0.27758097 0.26852776 0.28150191 0.27819121]\n",
      "One or more of the train scores are non-finite: [0.2779338  0.25848598 0.25436941 0.2779338  0.2779338  0.2779338\n",
      " 0.19949241 0.09823345 0.2779338  0.2779338  0.28170213 0.19928608\n",
      " 0.09823345 0.28433758 0.28422363        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.2779338  0.25848598 0.25436941 0.2779338  0.2779338  0.2779338\n",
      " 0.19925099 0.09823345 0.2779338  0.2779338  0.28170213 0.19928608\n",
      " 0.09823345 0.28433758 0.28422363        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.2779338  0.25847031 0.25436941 0.2779338  0.2779338  0.28170213\n",
      " 0.23564972 0.19823345 0.28433758 0.28231264 0.28170213 0.23564972\n",
      " 0.19823345 0.28433758 0.28422363        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27598508 0.2620282  0.25441456 0.27574367 0.2779338  0.28170213\n",
      " 0.26950983 0.4820651  0.28411485 0.28422363 0.28170213 0.26950983\n",
      " 0.4820651  0.28411485 0.28422363        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.2754909  0.26077026 0.36560638 0.27588176 0.27477941 0.27959101\n",
      " 0.26127643 0.36417613 0.28388674 0.28411268 0.27959101 0.26127643\n",
      " 0.36417613 0.28388674 0.28411268        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27734264 0.2608411  0.31662053 0.28377159 0.28234059 0.27959101\n",
      " 0.26093962 0.29965528 0.28377159 0.28366105 0.27959101 0.26093962\n",
      " 0.29965528 0.28377159 0.28366105        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.2787257  0.26093962 0.29912548 0.28377159 0.28343416 0.27959101\n",
      " 0.26093962 0.29974555 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29974555 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101\n",
      " 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29912548 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101\n",
      " 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29912548 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101\n",
      " 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29912548 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101\n",
      " 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29912548 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101\n",
      " 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29912548 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931\n",
      " 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101\n",
      " 0.26093962 0.29912548 0.28377159 0.28354931 0.27959101 0.26093962\n",
      " 0.29912548 0.28377159 0.28354931        nan        nan        nan\n",
      "        nan        nan 0.27959101 0.26093962 0.29912548 0.28377159\n",
      " 0.28354931 0.27959101 0.26093962 0.29912548 0.28377159 0.28354931]\n",
      "invalid value encountered in scalar divide\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "invalid value encountered in scalar divide\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3037076689150119\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.2456359102244389\n",
      "GridSearchCV Runtime: 4.7885260581970215 secs\n",
      "Efron R-squared: -0.6119213644407446\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.5\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2317596566523605\n",
      "Ave Test Precision: 0.24485939401703924\n",
      "Stdev Test Precision: 0.1769381437264725\n",
      "Ave Test Accuracy: 0.45746268656716416\n",
      "Stdev Test Accuracy: 0.2709736329145468\n",
      "Ave Test Specificity: 0.42079207920792083\n",
      "Ave Test Recall: 0.5696969696969697\n",
      "Ave Test NPV: 0.7228166526673988\n",
      "Ave Test F1-Score: 0.24175320643948606\n",
      "Ave Test G-mean: 0.09568677045764229\n",
      "Ave Runtime: 0.009827423095703124\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with water. 58 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.30084728 0.26722619 0.31690021 0.30084728 0.30084728 0.30084728\n",
      " 0.24563272 0.02469136 0.30084728 0.30084728 0.30270794 0.24563272\n",
      " 0.02469136 0.30161138 0.30862137        nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.30084728 0.26722619 0.31690021 0.30084728 0.30084728 0.30084728\n",
      " 0.24563272 0.02469136 0.30084728 0.30084728 0.30270794 0.24563272\n",
      " 0.02469136 0.30161138 0.30862137        nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.30084728 0.26722619 0.32244716 0.30084728 0.30084728 0.30270794\n",
      " 0.24563272 0.02469136 0.30161138 0.30084728 0.30270794 0.24563272\n",
      " 0.02469136 0.30161138 0.30862137        nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.30084728 0.26890075 0.32231729 0.30084728 0.30084728 0.30270794\n",
      " 0.25789579 0.02469136 0.30161138 0.30862137 0.30270794 0.25789579\n",
      " 0.02469136 0.30161138 0.30862137        nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28017711 0.30968742 0.3019967  0.30084728 0.30270794\n",
      " 0.28110355 0.2766358  0.30161138 0.3066983  0.30270794 0.28110355\n",
      " 0.2766358  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30391978 0.30425113 0.3019967\n",
      " 0.28927676 0.38760422 0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.38760422 0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983\n",
      " 0.3019967  0.28927676 0.3996856  0.30161138 0.3066983  0.3019967\n",
      " 0.28927676 0.3996856  0.30161138 0.3066983  0.3019967  0.28927676\n",
      " 0.3996856  0.30161138 0.3066983         nan        nan        nan\n",
      "        nan        nan 0.3019967  0.28927676 0.3996856  0.30161138\n",
      " 0.3066983  0.3019967  0.28927676 0.3996856  0.30161138 0.3066983 ]\n",
      "One or more of the train scores are non-finite: [0.2993445  0.26921295 0.31304315 0.2993445  0.2993445  0.2993445\n",
      " 0.24563587 0.02454924 0.2993445  0.2993445  0.30092026 0.24563587\n",
      " 0.02454924 0.30209065 0.30197741        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.2993445  0.26927297 0.31304315 0.2993445  0.2993445  0.2993445\n",
      " 0.24563587 0.02454924 0.2993445  0.2993445  0.30092026 0.24563587\n",
      " 0.02454924 0.30209065 0.30197741        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.2993445  0.26950675 0.31337255 0.2993445  0.2993445  0.30092026\n",
      " 0.24563587 0.02454924 0.30209065 0.29957592 0.30092026 0.24563587\n",
      " 0.02454924 0.30209065 0.30197741        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.2993445  0.27088958 0.30847678 0.2993445  0.2993445  0.30092026\n",
      " 0.25929405 0.12454924 0.30209065 0.30197741 0.30092026 0.25929405\n",
      " 0.12454924 0.30209065 0.30197741        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28136321 0.31505016 0.30036814 0.29967903 0.30092026\n",
      " 0.28173623 0.35459817 0.30209065 0.30156868 0.30092026 0.28173623\n",
      " 0.35459817 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28635351 0.31498771 0.30077686 0.30129698 0.30036814\n",
      " 0.28646615 0.32299389 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.32299389 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868\n",
      " 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814\n",
      " 0.28646615 0.31498771 0.30209065 0.30156868 0.30036814 0.28646615\n",
      " 0.31498771 0.30209065 0.30156868        nan        nan        nan\n",
      "        nan        nan 0.30036814 0.28646615 0.31498771 0.30209065\n",
      " 0.30156868 0.30036814 0.28646615 0.31498771 0.30209065 0.30156868]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3996855953375717\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.6770573566084788\n",
      "GridSearchCV Runtime: 4.859527349472046 secs\n",
      "Efron R-squared: -0.18618241173675965\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.21739130434782608\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.16666666666666666\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3018867924528302\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.5\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2553191489361702\n",
      "Ave Test Precision: 0.2882527824806986\n",
      "Stdev Test Precision: 0.12836659146313203\n",
      "Ave Test Accuracy: 0.6985074626865672\n",
      "Stdev Test Accuracy: 0.04441709295508351\n",
      "Ave Test Specificity: 0.8831683168316831\n",
      "Ave Test Recall: 0.13333333333333336\n",
      "Ave Test NPV: 0.7572640010459165\n",
      "Ave Test F1-Score: 0.1638450194547622\n",
      "Ave Test G-mean: 0.31578677779821496\n",
      "Ave Runtime: 0.008326339721679687\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with electricity. 57 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.13456819 0.06832298 0.10737292 0.12812031 0.20795319 0.24354221\n",
      " 0.0725     0.09844136 0.2212469  0.24285977 0.22830941 0.0725\n",
      " 0.09844136 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.24330171 0.06776794 0.1068202  0.25616763 0.26854679 0.2445783\n",
      " 0.0725     0.09844136 0.25548535 0.27085052 0.22830941 0.0725\n",
      " 0.09844136 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.2435127  0.06743668 0.1068202  0.25448665 0.26756998 0.23069036\n",
      " 0.07280063 0.09844136 0.24799519 0.26290011 0.22830941 0.07280063\n",
      " 0.09844136 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.24287845 0.06949424 0.10345012 0.25340943 0.26756998 0.22830941\n",
      " 0.06702151 0.09844136 0.25005408 0.26645323 0.22830941 0.06702151\n",
      " 0.09844136 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.23507194 0.06834248 0.13968689 0.2524277  0.26763699 0.22830941\n",
      " 0.0655042  0.13768355 0.25005408 0.26645323 0.22830941 0.0655042\n",
      " 0.13768355 0.25005408 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22873458 0.11760655 0.18919476 0.25069511 0.27156738 0.22830941\n",
      " 0.11807563 0.18606777 0.25069511 0.26645323 0.22830941 0.11807563\n",
      " 0.18606777 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26755617 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323\n",
      " 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941\n",
      " 0.11760655 0.22766665 0.25069511 0.26645323 0.22830941 0.11760655\n",
      " 0.22766665 0.25069511 0.26645323        nan        nan        nan\n",
      "        nan        nan 0.22830941 0.11760655 0.22766665 0.25069511\n",
      " 0.26645323 0.22830941 0.11760655 0.22766665 0.25069511 0.26645323]\n",
      "One or more of the train scores are non-finite: [0.15474158 0.07597141 0.0980444  0.12377978 0.1952009  0.2580131\n",
      " 0.07382271 0.09823345 0.22870244 0.22097938 0.24632226 0.07382271\n",
      " 0.09823345 0.24377289 0.23971829        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.25570475 0.07582219 0.09804562 0.25274009 0.24600411 0.25529638\n",
      " 0.07385691 0.09823345 0.25265878 0.24642723 0.24632226 0.07385691\n",
      " 0.09823345 0.24377289 0.23971829        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.25508241 0.07582219 0.09804562 0.25144994 0.24522893 0.24449084\n",
      " 0.07378135 0.09833564 0.24796166 0.24118465 0.24632226 0.07378135\n",
      " 0.09833564 0.24377289 0.23971829        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.25456412 0.0754023  0.09671405 0.2507434  0.24522893 0.24632226\n",
      " 0.07234392 0.09822825 0.24377289 0.23976884 0.24632226 0.07234392\n",
      " 0.09822825 0.24377289 0.23976884        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.25124798 0.07285838 0.1459082  0.24777424 0.24413689 0.24643316\n",
      " 0.07195599 0.29679458 0.24377289 0.23986972 0.24643316 0.07195599\n",
      " 0.29679458 0.24377289 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24654888 0.12295523 0.23062    0.24398949 0.24165306 0.24643316\n",
      " 0.12320939 0.27139703 0.24382803 0.23986972 0.24643316 0.12320939\n",
      " 0.27139703 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.24055666 0.24643316\n",
      " 0.12285289 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316\n",
      " 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316\n",
      " 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316\n",
      " 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316\n",
      " 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316\n",
      " 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972\n",
      " 0.24643316 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316\n",
      " 0.12295523 0.27100182 0.24382803 0.23986972 0.24643316 0.12285289\n",
      " 0.27100182 0.24382803 0.23986972        nan        nan        nan\n",
      "        nan        nan 0.24643316 0.12295523 0.27100182 0.24382803\n",
      " 0.23986972 0.24643316 0.12285289 0.27100182 0.24382803 0.23986972]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.27156738349006126\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.5498753117206983\n",
      "GridSearchCV Runtime: 4.608592510223389 secs\n",
      "Efron R-squared: -0.3555064199923801\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.216\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.22330097087378642\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25210084033613445\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.22772277227722773\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.24561403508771928\n",
      "Ave Test Precision: 0.23294772371497358\n",
      "Stdev Test Precision: 0.01528776158500623\n",
      "Ave Test Accuracy: 0.5298507462686567\n",
      "Stdev Test Accuracy: 0.023599087016181915\n",
      "Ave Test Specificity: 0.5732673267326733\n",
      "Ave Test Recall: 0.396969696969697\n",
      "Ave Test NPV: 0.7441635303436533\n",
      "Ave Test F1-Score: 0.2931592798866548\n",
      "Ave Test G-mean: 0.47561255982375117\n",
      "Ave Runtime: 0.005601024627685547\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with rent. 56 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.21453101 0.12824409 0.11690572 0.21453101 0.18640601 0.21453101\n",
      " 0.         0.09969136 0.21453101 0.18640601 0.22769151 0.\n",
      " 0.09969136 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.21453101 0.12583029 0.11690572 0.21453101 0.20932268 0.21162008\n",
      " 0.04       0.09969136 0.24075663 0.21050787 0.22769151 0.04\n",
      " 0.09969136 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.21152025 0.12755081 0.11690572 0.21152025 0.20896803 0.22769151\n",
      " 0.17714286 0.09969136 0.22769151 0.23222058 0.22769151 0.17714286\n",
      " 0.09969136 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.21345188 0.12878119 0.11592533 0.21345188 0.20795737 0.22769151\n",
      " 0.15285548 0.14905063 0.22769151 0.23801047 0.22769151 0.15285548\n",
      " 0.14905063 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.21843661 0.16392831 0.1885759  0.23071164 0.21719924 0.22769151\n",
      " 0.16812158 0.1949561  0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.1949561  0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16713344 0.22828943 0.22769151 0.23226817 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16713344 0.26162276 0.22769151 0.23324856 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047\n",
      " 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151\n",
      " 0.16812158 0.26162276 0.22769151 0.23801047 0.22769151 0.16812158\n",
      " 0.26162276 0.22769151 0.23801047        nan        nan        nan\n",
      "        nan        nan 0.22769151 0.16812158 0.26162276 0.22769151\n",
      " 0.23801047 0.22769151 0.16812158 0.26162276 0.22769151 0.23801047]\n",
      "One or more of the train scores are non-finite: [0.21597925 0.16611704 0.20401966 0.21597925 0.19533705 0.21597925\n",
      " 0.         0.09809494 0.21597925 0.19533705 0.22650063 0.\n",
      " 0.09809494 0.22928213 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.21597925 0.16611704 0.20401966 0.21597925 0.22152753 0.22384554\n",
      " 0.126      0.09809494 0.22395148 0.22177182 0.22650063 0.126\n",
      " 0.09809494 0.22928213 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.21547441 0.16639596 0.18735299 0.21547441 0.2241393  0.22650063\n",
      " 0.20018544 0.09809494 0.22928213 0.23154032 0.22650063 0.20018544\n",
      " 0.09809494 0.22928213 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.21734971 0.16551967 0.19838389 0.21734971 0.22251917 0.22650063\n",
      " 0.26206217 0.29620715 0.22791524 0.23275055 0.22650063 0.26206217\n",
      " 0.29620715 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22410146 0.19490465 0.31705466 0.22377456 0.22431032 0.22650063\n",
      " 0.24681458 0.31294728 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31294728 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22503123 0.24546136 0.30425484 0.22503123 0.22903937 0.22650063\n",
      " 0.24681458 0.30253674 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.30253674 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24546136 0.31182406 0.22791524 0.23182747 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055\n",
      " 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063\n",
      " 0.24681458 0.31182406 0.22791524 0.23275055 0.22650063 0.24681458\n",
      " 0.31182406 0.22791524 0.23275055        nan        nan        nan\n",
      "        nan        nan 0.22650063 0.24681458 0.31182406 0.22791524\n",
      " 0.23275055 0.22650063 0.24681458 0.31182406 0.22791524 0.23275055]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.26162276200250884\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.7418952618453866\n",
      "GridSearchCV Runtime: 4.605366468429565 secs\n",
      "Efron R-squared: -0.07047011104286072\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.46153846153846156\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.42857142857142855\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3076923076923077\n",
      "Ave Test Precision: 0.3633699633699634\n",
      "Stdev Test Precision: 0.07733189809993642\n",
      "Ave Test Accuracy: 0.7447761194029849\n",
      "Stdev Test Accuracy: 0.006243731541299092\n",
      "Ave Test Specificity: 0.9693069306930692\n",
      "Ave Test Recall: 0.05757575757575757\n",
      "Ave Test NPV: 0.7589708816345524\n",
      "Ave Test F1-Score: 0.09738891695126947\n",
      "Ave Test G-mean: 0.22534962951358578\n",
      "Ave Runtime: 0.008904695510864258\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with repair. 55 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.26786339 0.26786339 0.27696409 0.26786339 0.26786339 0.30469697\n",
      " 0.32722222 0.26898009 0.30469697 0.2980303  0.30469697 0.32722222\n",
      " 0.26898009 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.26786339 0.26786339 0.28982123 0.26786339 0.26786339 0.30469697\n",
      " 0.32722222 0.26909973 0.30469697 0.2980303  0.30469697 0.32722222\n",
      " 0.26909973 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.28660173 0.28660173 0.2790853  0.28660173 0.26786339 0.30469697\n",
      " 0.32722222 0.26687751 0.30469697 0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.29772727 0.29772727 0.26034071 0.29772727 0.2812987  0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.30469697 0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.29772727 0.29530303 0.26088485 0.29772727 0.29772727 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.31136364 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697\n",
      " 0.30469697 0.32722222 0.26687751 0.30469697 0.30469697 0.30469697\n",
      " 0.32722222 0.26687751 0.2980303  0.2980303  0.30469697 0.32722222\n",
      " 0.26687751 0.30469697 0.30469697        nan        nan        nan\n",
      "        nan        nan 0.30469697 0.32722222 0.26687751 0.2980303\n",
      " 0.2980303  0.30469697 0.32722222 0.26687751 0.30469697 0.30469697]\n",
      "One or more of the train scores are non-finite: [0.27100136 0.27100136 0.2586463  0.27100136 0.27100136 0.28755412\n",
      " 0.29923365 0.34632661 0.28755412 0.28644528 0.28755412 0.29923365\n",
      " 0.34632661 0.28755412 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.27100136 0.27100136 0.26086892 0.27100136 0.27100136 0.28740222\n",
      " 0.29868538 0.34089897 0.28755412 0.28644528 0.28740222 0.29868538\n",
      " 0.34089897 0.28755412 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28428323 0.28428323 0.26725007 0.28428323 0.27100136 0.28740222\n",
      " 0.29868538 0.3384327  0.28740222 0.28644528 0.28740222 0.29868538\n",
      " 0.3384327  0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28589184 0.28589184 0.2983294  0.28589184 0.27757888 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28740222 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28589184 0.28936177 0.32869123 0.28589184 0.28589184 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29955861 0.33956495 0.28716172 0.28608023 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222\n",
      " 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222 0.28740222\n",
      " 0.29868538 0.33935546 0.28644528 0.28644528 0.28740222 0.29868538\n",
      " 0.33935546 0.28740222 0.28740222        nan        nan        nan\n",
      "        nan        nan 0.28740222 0.29868538 0.33935546 0.28644528\n",
      " 0.28644528 0.28740222 0.29868538 0.33935546 0.28740222 0.28740222]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.32722222222222225\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7069825436408977\n",
      "GridSearchCV Runtime: 4.671872615814209 secs\n",
      "Efron R-squared: -0.322449966543668\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2727272727272727\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.36363636363636365\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.21428571428571427\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.15384615384615385\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3076923076923077\n",
      "Ave Test Precision: 0.26243756243756244\n",
      "Stdev Test Precision: 0.08140295137508442\n",
      "Ave Test Accuracy: 0.7052238805970149\n",
      "Stdev Test Accuracy: 0.018279774199874463\n",
      "Ave Test Specificity: 0.9009900990099011\n",
      "Ave Test Recall: 0.10606060606060605\n",
      "Ave Test NPV: 0.7551613364384039\n",
      "Ave Test F1-Score: 0.15043310066436802\n",
      "Ave Test G-mean: 0.30636184996106863\n",
      "Ave Runtime: 0.008925485610961913\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with cinema. 54 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27054528 0.26776102 0.22644098 0.27054528 0.27054528 0.27054528\n",
      " 0.0725     0.09469136 0.27054528 0.27054528 0.28673775 0.0725\n",
      " 0.09135802 0.30285286 0.28472961        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27054528 0.26776102 0.22644098 0.27054528 0.27054528 0.27170358\n",
      " 0.0725     0.09469136 0.27054528 0.27054528 0.28673775 0.0725\n",
      " 0.09135802 0.30285286 0.28472961        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27054528 0.26776102 0.22644098 0.27054528 0.27054528 0.28673775\n",
      " 0.26582317 0.09135802 0.30285286 0.28472961 0.28673775 0.26582317\n",
      " 0.09135802 0.30285286 0.28472961        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.26780132 0.27475768 0.19310765 0.26780132 0.27054528 0.27909069\n",
      " 0.27255934 0.12885802 0.29232918 0.28472961 0.27909069 0.27255934\n",
      " 0.12885802 0.29232918 0.28472961        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27739796 0.29563318 0.25977431 0.27761867 0.27743786 0.27909069\n",
      " 0.30374901 0.20538101 0.28673775 0.28034364 0.27909069 0.30374901\n",
      " 0.20538101 0.28673775 0.28034364        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.2750677  0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194\n",
      " 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069\n",
      " 0.31140247 0.26281691 0.27909069 0.27836194 0.27909069 0.31140247\n",
      " 0.26281691 0.27909069 0.27836194        nan        nan        nan\n",
      "        nan        nan 0.27909069 0.31140247 0.26281691 0.27909069\n",
      " 0.27836194 0.27909069 0.31140247 0.26281691 0.27909069 0.27836194]\n",
      "One or more of the train scores are non-finite: [0.27343909 0.27916496 0.27162953 0.27343909 0.27343909 0.27343909\n",
      " 0.07382271 0.10215114 0.27343909 0.27343909 0.28128833 0.07382271\n",
      " 0.10467727 0.29252973 0.29444698        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.27343909 0.27916496 0.27162953 0.27343909 0.27343909 0.27430715\n",
      " 0.17382271 0.10215114 0.27343909 0.27343909 0.28128833 0.17382271\n",
      " 0.10467727 0.29252973 0.29444698        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.27343909 0.27916496 0.27162953 0.27343909 0.27343909 0.28128833\n",
      " 0.49374892 0.20467727 0.29252973 0.29444698 0.28128833 0.49374892\n",
      " 0.20467727 0.29252973 0.29444698        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.27614715 0.28342558 0.27879074 0.27614715 0.27343909 0.28009999\n",
      " 0.31048781 0.53922272 0.28991991 0.29444698 0.28009999 0.31048781\n",
      " 0.53922272 0.28991991 0.29444698        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.27923676 0.29016958 0.26279997 0.27889477 0.27925172 0.28009999\n",
      " 0.30132882 0.38630113 0.28307956 0.29136289 0.28009999 0.30132882\n",
      " 0.38630113 0.28307956 0.29136289        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.27992724 0.29332931 0.26429987 0.28172253 0.28236627 0.28009999\n",
      " 0.29370965 0.36467168 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36467168 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292\n",
      " 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999\n",
      " 0.29370965 0.36236293 0.28189123 0.28969292 0.28009999 0.29370965\n",
      " 0.36236293 0.28189123 0.28969292        nan        nan        nan\n",
      "        nan        nan 0.28009999 0.29370965 0.36236293 0.28189123\n",
      " 0.28969292 0.28009999 0.29370965 0.36236293 0.28189123 0.28969292]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "invalid value encountered in scalar divide\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "invalid value encountered in scalar divide\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.31140247007894073\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.6670822942643392\n",
      "GridSearchCV Runtime: 4.458004951477051 secs\n",
      "Efron R-squared: -0.2956320121003564\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.375\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.30597014925373134\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.46153846153846156\n",
      "Ave Test Precision: 0.32700918484500574\n",
      "Stdev Test Precision: 0.09203525331426364\n",
      "Ave Test Accuracy: 0.5022388059701492\n",
      "Stdev Test Accuracy: 0.24416218056155595\n",
      "Ave Test Specificity: 0.47128712871287126\n",
      "Ave Test Recall: 0.5969696969696969\n",
      "Ave Test NPV: 0.7866522999973395\n",
      "Ave Test F1-Score: 0.3336214739076145\n",
      "Ave Test G-mean: 0.27142169529888394\n",
      "Ave Runtime: 0.007134246826171875\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with dineOut. 53 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25212008 0.19656579 0.09843335 0.25212008 0.25212008 0.25836629\n",
      " 0.14625    0.09938272 0.2567063  0.25934126 0.26852693 0.14625\n",
      " 0.09938272 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.25212008 0.19656579 0.09843335 0.25212008 0.25212008 0.25445933\n",
      " 0.14625    0.09938272 0.26852693 0.26033841 0.26852693 0.14625\n",
      " 0.09938272 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.25685241 0.19673447 0.09843335 0.25685241 0.26021731 0.26852693\n",
      " 0.15075    0.09938272 0.26852693 0.26852693 0.26852693 0.15075\n",
      " 0.09938272 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.25616404 0.29324002 0.09905804 0.25616404 0.25811024 0.26852693\n",
      " 0.21483999 0.09938272 0.26852693 0.26852693 0.26852693 0.21483999\n",
      " 0.09938272 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26933098 0.24875073 0.18124312 0.27029872 0.26798947 0.26852693\n",
      " 0.25253755 0.17560005 0.26852693 0.26852693 0.26852693 0.25253755\n",
      " 0.17560005 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.3193323  0.34372609 0.26852693 0.26864003 0.26852693\n",
      " 0.32025184 0.39339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.39339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.39339337 0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.39339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.39339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.39339337 0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.39339337 0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.3902565  0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.3902565  0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.3902565  0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.3902565  0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693\n",
      " 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693\n",
      " 0.32025184 0.3902565  0.26852693 0.26852693 0.26852693 0.32025184\n",
      " 0.3902565  0.26852693 0.26852693        nan        nan        nan\n",
      "        nan        nan 0.26852693 0.32025184 0.3902565  0.26852693\n",
      " 0.26852693 0.26852693 0.32025184 0.3902565  0.26852693 0.26852693]\n",
      "One or more of the train scores are non-finite: [0.2528304  0.20023111 0.09727893 0.2528304  0.2528304  0.25824666\n",
      " 0.14750693 0.09812895 0.25871207 0.25798377 0.26819894 0.14750693\n",
      " 0.09812895 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.2528304  0.20023111 0.09727893 0.2528304  0.2528304  0.26607426\n",
      " 0.14750693 0.09812895 0.26819894 0.25838446 0.26819894 0.14750693\n",
      " 0.09812895 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.25679526 0.20446646 0.09727893 0.25679526 0.26035095 0.26819894\n",
      " 0.15174111 0.09812895 0.26819894 0.26819894 0.26819894 0.15174111\n",
      " 0.09812895 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.25686927 0.24041688 0.09718618 0.25686927 0.25835329 0.26819894\n",
      " 0.22726003 0.09812895 0.26819894 0.26819894 0.26819894 0.22726003\n",
      " 0.09812895 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.27009128 0.32646413 0.19096961 0.27030177 0.26716475 0.26819894\n",
      " 0.32725985 0.24032938 0.26819894 0.26819894 0.26819894 0.32725985\n",
      " 0.24032938 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30387564 0.32747447 0.26819894 0.2696211  0.26819894\n",
      " 0.30399233 0.30934013 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30934013 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30335722 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30335722 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30335722 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30335722 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30335722 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30316783 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30316783 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30316783 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30316783 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894\n",
      " 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894\n",
      " 0.30399233 0.30316783 0.26819894 0.26819894 0.26819894 0.30399233\n",
      " 0.30316783 0.26819894 0.26819894        nan        nan        nan\n",
      "        nan        nan 0.26819894 0.30399233 0.30316783 0.26819894\n",
      " 0.26819894 0.26819894 0.30399233 0.30316783 0.26819894 0.26819894]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3933933663709828\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.25561097256857856\n",
      "GridSearchCV Runtime: 4.385900497436523 secs\n",
      "Efron R-squared: -0.9694003550382793\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.24609375\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.30612244897959184\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.4117647058823529\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2222222222222222\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.35\n",
      "Ave Test Precision: 0.30724062541683345\n",
      "Stdev Test Precision: 0.07700968621779661\n",
      "Ave Test Accuracy: 0.6238805970149254\n",
      "Stdev Test Accuracy: 0.199712284696006\n",
      "Ave Test Specificity: 0.7257425742574257\n",
      "Ave Test Recall: 0.31212121212121213\n",
      "Ave Test NPV: 0.7611986702552921\n",
      "Ave Test F1-Score: 0.23604753009459611\n",
      "Ave Test G-mean: 0.32517984198485406\n",
      "Ave Runtime: 0.006801319122314453\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with leisure. 52 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24590355 0.22560591 0.16421673 0.24085076 0.24724523 0.24590355\n",
      " 0.12319851 0.         0.24221374 0.24724523 0.27214045 0.12088272\n",
      " 0.         0.25246785 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.24590355 0.22560591 0.16421673 0.24221374 0.24464648 0.24590355\n",
      " 0.12319851 0.         0.24221374 0.24464648 0.27214045 0.12088272\n",
      " 0.         0.25246785 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.24590355 0.22560591 0.16421673 0.24221374 0.24464648 0.26090496\n",
      " 0.12088272 0.         0.25238875 0.24290045 0.27214045 0.12088272\n",
      " 0.         0.25246785 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.25029594 0.23057667 0.16878816 0.24172451 0.24464648 0.27214045\n",
      " 0.29528748 0.1        0.25246785 0.26204622 0.27214045 0.29528748\n",
      " 0.1        0.25246785 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.26084919 0.22011784 0.16878816 0.24852151 0.25867546 0.26423569\n",
      " 0.21581542 0.21607143 0.25403307 0.26204622 0.26423569 0.21581542\n",
      " 0.21607143 0.25403307 0.26204622        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.26259113 0.22214213 0.20255291 0.25082755 0.25872266 0.2638219\n",
      " 0.22214213 0.14255291 0.253326   0.26027811 0.2638219  0.22214213\n",
      " 0.14255291 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.24999266 0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811\n",
      " 0.2638219  0.22259462 0.19906638 0.253326   0.26027811 0.2638219\n",
      " 0.22259462 0.19906638 0.253326   0.26027811 0.2638219  0.22259462\n",
      " 0.19906638 0.253326   0.26027811        nan        nan        nan\n",
      "        nan        nan 0.2638219  0.22259462 0.19906638 0.253326\n",
      " 0.26027811 0.2638219  0.22259462 0.19906638 0.253326   0.26027811]\n",
      "One or more of the train scores are non-finite: [0.24803383 0.22690808 0.20685484 0.24805236 0.24736634 0.24803383\n",
      " 0.12304779 0.         0.24792011 0.24736634 0.26365807 0.1250453\n",
      " 0.         0.26469351 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.24803383 0.22690808 0.20685484 0.24792011 0.24766315 0.24803383\n",
      " 0.22304779 0.         0.24792011 0.24766315 0.26365807 0.2250453\n",
      " 0.         0.26469351 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.24803383 0.22690808 0.20685484 0.24792011 0.24766315 0.26027013\n",
      " 0.34587863 0.         0.26410437 0.25170813 0.26365807 0.34587863\n",
      " 0.         0.26469351 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.25209562 0.22823696 0.20671549 0.25213915 0.24766315 0.26365807\n",
      " 0.24253461 0.475      0.26469351 0.26341731 0.26365807 0.24253461\n",
      " 0.475      0.26469351 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26102205 0.23183243 0.2103165  0.26026459 0.2532693  0.26378117\n",
      " 0.24372824 0.20457418 0.26425056 0.26341731 0.26378117 0.24372824\n",
      " 0.20457418 0.26425056 0.26341731        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26339015 0.23363069 0.19851198 0.26211968 0.26104941 0.26350056\n",
      " 0.23373943 0.40362839 0.2635985  0.26295811 0.26350056 0.23373943\n",
      " 0.40362839 0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.39763052 0.26358225 0.26295811 0.26350056\n",
      " 0.23356884 0.49763052 0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.49763052 0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.4974116  0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.4974116  0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.4974116  0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.4974116  0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.4974116  0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811\n",
      " 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056\n",
      " 0.23356884 0.4974116  0.2635985  0.26295811 0.26350056 0.23356884\n",
      " 0.4974116  0.2635985  0.26295811        nan        nan        nan\n",
      "        nan        nan 0.26350056 0.23356884 0.4974116  0.2635985\n",
      " 0.26295811 0.26350056 0.23356884 0.4974116  0.2635985  0.26295811]\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n",
      "invalid value encountered in scalar divide\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2952874779541447\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.2456359102244389\n",
      "GridSearchCV Runtime: 4.617961168289185 secs\n",
      "Efron R-squared: -0.3568273144824228\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.24436090225563908\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.21428571428571427\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2462686567164179\n",
      "Ave Test Precision: 0.23949051733812143\n",
      "Stdev Test Precision: 0.014114108778543692\n",
      "Ave Test Accuracy: 0.3358208955223881\n",
      "Stdev Test Accuracy: 0.20024489350744387\n",
      "Ave Test Specificity: 0.1792079207920792\n",
      "Ave Test Recall: 0.8151515151515152\n",
      "Ave Test NPV: 0.625\n",
      "Ave Test F1-Score: 0.34097091640865923\n",
      "Ave Test G-mean: 0.0708887824545379\n",
      "Ave Runtime: 0.008588933944702148\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with personalCare. 51 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25432036 0.27085759 0.24127941 0.25432036 0.25432036 0.25432036\n",
      " 0.07375    0.12344136 0.25432036 0.25432036 0.26883361 0.07375\n",
      " 0.12344136 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.25432036 0.27085759 0.24127941 0.25432036 0.25432036 0.25432036\n",
      " 0.07375    0.12344136 0.25432036 0.25195293 0.26883361 0.07375\n",
      " 0.12344136 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.25432036 0.27085759 0.24127941 0.25432036 0.25432036 0.26883361\n",
      " 0.12375    0.12344136 0.26883361 0.26161245 0.26883361 0.12375\n",
      " 0.12344136 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.25482456 0.26936545 0.28127941 0.25500331 0.25432036 0.26883361\n",
      " 0.32508127 0.29010802 0.26883361 0.26883361 0.26883361 0.32508127\n",
      " 0.29010802 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26746641 0.26685534 0.28127941 0.26746641 0.26480578 0.26883361\n",
      " 0.33858161 0.27844136 0.26883361 0.26883361 0.26883361 0.33858161\n",
      " 0.27844136 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.27398236 0.25880978 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.25880978 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.25880978 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.25838138 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.25838138 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.25838138 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.25730446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.25730446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.25730446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361\n",
      " 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361\n",
      " 0.2763671  0.26030446 0.26883361 0.26883361 0.26883361 0.2763671\n",
      " 0.26030446 0.26883361 0.26883361        nan        nan        nan\n",
      "        nan        nan 0.26883361 0.2763671  0.26030446 0.26883361\n",
      " 0.26883361 0.26883361 0.2763671  0.26030446 0.26883361 0.26883361]\n",
      "One or more of the train scores are non-finite: [0.25514949 0.27869038 0.2984899  0.25514949 0.25514949 0.25514949\n",
      " 0.07368421 0.12274868 0.25514949 0.25514949 0.26923137 0.07368421\n",
      " 0.12274868 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.25514949 0.27869038 0.2984899  0.25514949 0.25514949 0.25514949\n",
      " 0.07368421 0.12274868 0.25514949 0.25559018 0.26923137 0.07368421\n",
      " 0.12274868 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.25514949 0.27869038 0.2984899  0.25514949 0.25514949 0.26923137\n",
      " 0.16315789 0.12274868 0.26923137 0.26689283 0.26923137 0.16315789\n",
      " 0.12274868 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.25553089 0.28219012 0.29416659 0.25660959 0.25514949 0.26923137\n",
      " 0.37018754 0.25322487 0.26923137 0.26923137 0.26923137 0.37018754\n",
      " 0.25322487 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26821545 0.30384638 0.29376118 0.26821545 0.26645015 0.26923137\n",
      " 0.31714727 0.30904954 0.26923137 0.26923137 0.26923137 0.31714727\n",
      " 0.30904954 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26911115 0.3100908  0.28855114 0.26923137 0.26901826 0.26923137\n",
      " 0.3114073  0.28973827 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28973827 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28734225 0.26923137 0.26912917 0.26923137\n",
      " 0.3114073  0.28734225 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28734225 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28734225 0.26923137 0.26923137 0.26923137\n",
      " 0.3114073  0.28734225 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28734225 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137\n",
      " 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137\n",
      " 0.3114073  0.28489908 0.26923137 0.26923137 0.26923137 0.3114073\n",
      " 0.28489908 0.26923137 0.26923137        nan        nan        nan\n",
      "        nan        nan 0.26923137 0.3114073  0.28489908 0.26923137\n",
      " 0.26923137 0.26923137 0.3114073  0.28489908 0.26923137 0.26923137]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.33858160792371317\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.5548628428927681\n",
      "GridSearchCV Runtime: 4.637185573577881 secs\n",
      "Efron R-squared: -0.3519979861474811\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2830188679245283\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.4186046511627907\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.22699386503067484\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2222222222222222\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23893805309734514\n",
      "Ave Test Precision: 0.27795553188751226\n",
      "Stdev Test Precision: 0.08220928942484357\n",
      "Ave Test Accuracy: 0.5552238805970149\n",
      "Stdev Test Accuracy: 0.11257283971491158\n",
      "Ave Test Specificity: 0.6\n",
      "Ave Test Recall: 0.41818181818181815\n",
      "Ave Test NPV: 0.7543480805553012\n",
      "Ave Test F1-Score: 0.31761710531634585\n",
      "Ave Test G-mean: 0.4861078075843257\n",
      "Ave Runtime: 0.007601404190063476\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with clothing. 50 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24750769 0.24857716 0.24267477 0.24750769 0.2581215  0.24750769\n",
      " 0.09813272 0.1        0.24750769 0.2581215  0.22356655 0.09813272\n",
      " 0.1        0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.24750769 0.24857716 0.24267477 0.24750769 0.2581215  0.24750769\n",
      " 0.09813272 0.1        0.24750769 0.2581215  0.22356655 0.09813272\n",
      " 0.1        0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.24750769 0.24857716 0.24267477 0.24750769 0.2581215  0.23099014\n",
      " 0.19813272 0.1        0.2243602  0.26206887 0.22356655 0.19813272\n",
      " 0.1        0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.24546669 0.24941049 0.24393424 0.24546669 0.26010241 0.22356655\n",
      " 0.34035494 0.09807692 0.2243602  0.24029313 0.22356655 0.34035494\n",
      " 0.09807692 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22695    0.23749933 0.25288568 0.22893679 0.24875605 0.22356655\n",
      " 0.23307319 0.15095238 0.2243602  0.23467599 0.22356655 0.23307319\n",
      " 0.15095238 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.22356655 0.22947057 0.22356655\n",
      " 0.23596803 0.31775264 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31775264 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599\n",
      " 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655\n",
      " 0.23596803 0.31813725 0.2243602  0.23467599 0.22356655 0.23596803\n",
      " 0.31813725 0.2243602  0.23467599        nan        nan        nan\n",
      "        nan        nan 0.22356655 0.23596803 0.31813725 0.2243602\n",
      " 0.23467599 0.22356655 0.23596803 0.31813725 0.2243602  0.23467599]\n",
      "One or more of the train scores are non-finite: [0.25001205 0.27203125 0.16498253 0.25001205 0.24779599 0.25001205\n",
      " 0.09826745 0.09806094 0.25001205 0.24779599 0.22463815 0.09826745\n",
      " 0.09806094 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.25001205 0.27203125 0.16498253 0.25001205 0.24779599 0.25001205\n",
      " 0.19826745 0.09806094 0.25001205 0.24779599 0.22463815 0.19826745\n",
      " 0.09806094 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.25001205 0.27203125 0.16492787 0.25001205 0.24779599 0.22834081\n",
      " 0.19124991 0.09806094 0.22463907 0.24441942 0.22463815 0.19124991\n",
      " 0.09806094 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.24776137 0.27262963 0.16552741 0.24776137 0.24934436 0.22463815\n",
      " 0.38402634 0.19747533 0.22463907 0.22896095 0.22463815 0.38402634\n",
      " 0.19747533 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.23240157 0.26831405 0.2643078  0.23098767 0.23915618 0.22455054\n",
      " 0.27717073 0.44529656 0.22463907 0.22658    0.22455054 0.27717073\n",
      " 0.44529656 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26205685 0.40579101 0.22455054 0.22822454 0.22455054\n",
      " 0.26866379 0.40655713 0.22463907 0.22658    0.22455054 0.26866379\n",
      " 0.40655713 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.40557996 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658\n",
      " 0.22455054 0.26430761 0.41891329 0.22463907 0.22658    0.22455054\n",
      " 0.26430761 0.41891329 0.22463907 0.22658    0.22455054 0.26430761\n",
      " 0.41891329 0.22463907 0.22658           nan        nan        nan\n",
      "        nan        nan 0.22455054 0.26430761 0.41891329 0.22463907\n",
      " 0.22658    0.22455054 0.26430761 0.41891329 0.22463907 0.22658   ]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.34035493827160496\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.743142144638404\n",
      "GridSearchCV Runtime: 4.587321043014526 secs\n",
      "Efron R-squared: -0.3301035039364515\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.29411764705882354\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.17391304347826086\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2727272727272727\n",
      "Ave Test Precision: 0.14815159265287142\n",
      "Stdev Test Precision: 0.14264171047123156\n",
      "Ave Test Accuracy: 0.7223880597014926\n",
      "Stdev Test Accuracy: 0.04649978381992352\n",
      "Ave Test Specificity: 0.9425742574257425\n",
      "Ave Test Recall: 0.048484848484848485\n",
      "Ave Test NPV: 0.7516074700178519\n",
      "Ave Test F1-Score: 0.06825222969801284\n",
      "Ave Test G-mean: 0.15791600065457903\n",
      "Ave Runtime: 0.006893777847290039\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with mobileLoad. 49 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.28265159 0.26110281 0.17860161 0.28265159 0.28265159 0.28265159\n",
      " 0.12219136 0.14719136 0.28265159 0.28265159 0.30665091 0.12219136\n",
      " 0.14719136 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.28265159 0.26110281 0.17860161 0.28265159 0.28265159 0.28265159\n",
      " 0.12219136 0.14719136 0.28265159 0.28326135 0.30665091 0.12219136\n",
      " 0.14719136 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.28265159 0.26295511 0.17860161 0.28265159 0.28265159 0.30665091\n",
      " 0.12219136 0.14719136 0.30797366 0.30253024 0.30665091 0.12219136\n",
      " 0.14719136 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.28533517 0.26295205 0.19556798 0.28533517 0.28405403 0.30665091\n",
      " 0.23697937 0.14719136 0.30797366 0.30253024 0.30665091 0.23697937\n",
      " 0.14719136 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30426997 0.26993437 0.33052469 0.30426997 0.28827635 0.30665091\n",
      " 0.27774586 0.17860161 0.30797366 0.30253024 0.30665091 0.27774586\n",
      " 0.17860161 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28616215 0.24755316 0.30665091 0.30253024 0.30665091\n",
      " 0.28299549 0.25014806 0.30797366 0.30253024 0.30665091 0.28299549\n",
      " 0.25014806 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024\n",
      " 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091\n",
      " 0.28505898 0.25098029 0.30797366 0.30253024 0.30665091 0.28505898\n",
      " 0.25098029 0.30797366 0.30253024        nan        nan        nan\n",
      "        nan        nan 0.30665091 0.28505898 0.25098029 0.30797366\n",
      " 0.30253024 0.30665091 0.28505898 0.25098029 0.30797366 0.30253024]\n",
      "One or more of the train scores are non-finite: [0.28307638 0.27839136 0.20152228 0.28307638 0.28307638 0.28307638\n",
      " 0.12288719 0.14740242 0.28307638 0.28307638 0.30604687 0.12288719\n",
      " 0.14740242 0.30649813 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.28307638 0.27839136 0.20152228 0.28307638 0.28307638 0.28307638\n",
      " 0.12288719 0.14740242 0.28307638 0.28349189 0.30604687 0.12288719\n",
      " 0.14740242 0.30649813 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.28307638 0.27850622 0.20152228 0.28307638 0.28307638 0.30604687\n",
      " 0.12288719 0.14740242 0.30649813 0.30651448 0.30604687 0.12288719\n",
      " 0.14740242 0.30649813 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.28540879 0.28055705 0.26326814 0.28557756 0.28434539 0.30604687\n",
      " 0.25366757 0.14740242 0.30649813 0.30651448 0.30604687 0.25366757\n",
      " 0.14740242 0.30649813 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30408467 0.28616186 0.37898701 0.30408467 0.28923159 0.30604687\n",
      " 0.28898453 0.29152744 0.30627022 0.30651448 0.30604687 0.28898453\n",
      " 0.29152744 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30582361 0.2938237  0.27864938 0.30604968 0.30541997 0.30604687\n",
      " 0.29443981 0.27860324 0.30627022 0.30651448 0.30604687 0.29443981\n",
      " 0.27860324 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26581211 0.30627022 0.30651448 0.30604687\n",
      " 0.29504535 0.26533539 0.30627022 0.30651448 0.30604687 0.29504535\n",
      " 0.26533539 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539\n",
      " 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539 0.29504535\n",
      " 0.26573892 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539\n",
      " 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539 0.29504535\n",
      " 0.26573892 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539\n",
      " 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539 0.29504535\n",
      " 0.26573892 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539\n",
      " 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539 0.29504535\n",
      " 0.26573892 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539\n",
      " 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539 0.29504535\n",
      " 0.26573892 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448\n",
      " 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539\n",
      " 0.29504535 0.26573892 0.30627022 0.30651448 0.30593539 0.29504535\n",
      " 0.26573892 0.30627022 0.30651448        nan        nan        nan\n",
      "        nan        nan 0.30593539 0.29504535 0.26573892 0.30627022\n",
      " 0.30651448 0.30593539 0.29504535 0.26573892 0.30627022 0.30651448]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3305246913580247\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.756857855361596\n",
      "GridSearchCV Runtime: 4.235983371734619 secs\n",
      "Efron R-squared: -0.11633107109075258\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.5\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.18994413407821228\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.328125\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.20361382681564244\n",
      "Stdev Test Precision: 0.21590034584454101\n",
      "Ave Test Accuracy: 0.6537313432835822\n",
      "Stdev Test Accuracy: 0.17909281678458255\n",
      "Ave Test Specificity: 0.8118811881188119\n",
      "Ave Test Recall: 0.16969696969696968\n",
      "Ave Test NPV: 0.7364081265400235\n",
      "Ave Test F1-Score: 0.12600794163819373\n",
      "Ave Test G-mean: 0.20090065766814585\n",
      "Ave Runtime: 0.00631861686706543\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with internet. 48 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25345238 0.25345238 0.28576279 0.25345238 0.25345238 0.25345238\n",
      " 0.28694444 0.28540866 0.23845238 0.25595238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.25345238 0.26392857 0.30441358 0.25345238 0.25345238 0.27916667\n",
      " 0.25345238 0.28540866 0.26456349 0.2552381  0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.25916667 0.26686508 0.27024691 0.25916667 0.25583333 0.25345238\n",
      " 0.27527778 0.28540866 0.25345238 0.26734127 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.25916667 0.29912698 0.29897707 0.25916667 0.25916667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.26416667 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.33916667 0.33635802 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762\n",
      " 0.26416667 0.28916667 0.28540866 0.26416667 0.26416667 0.25345238\n",
      " 0.26456349 0.28540866 0.25345238 0.25345238 0.27603175 0.30238095\n",
      " 0.28540866 0.2831746  0.29904762        nan        nan        nan\n",
      "        nan        nan 0.25345238 0.26456349 0.28540866 0.25345238\n",
      " 0.25345238 0.27603175 0.30238095 0.28540866 0.2831746  0.29904762]\n",
      "One or more of the train scores are non-finite: [0.2735763  0.2735763  0.27064891 0.2735763  0.2735763  0.2735763\n",
      " 0.2978488  0.36147026 0.28123396 0.2818759  0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.2735763  0.28756448 0.27957894 0.2735763  0.2735763  0.28795097\n",
      " 0.2735763  0.36147026 0.27961885 0.29435407 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27667154 0.30089298 0.28462036 0.27667154 0.2753079  0.2735763\n",
      " 0.28435781 0.36147026 0.28008912 0.28174701 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27667154 0.31357225 0.29400494 0.27667154 0.27667154 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.2809265  0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32428708 0.33760254 0.27708034 0.27648635 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32718931 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32804297 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32877467 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32877467 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32877467 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32877467 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32877467 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538\n",
      " 0.27708034 0.32877467 0.36147026 0.27769718 0.27708034 0.2735763\n",
      " 0.27792924 0.36147026 0.2735763  0.27742245 0.32503055 0.33490006\n",
      " 0.36147026 0.32837576 0.3223538         nan        nan        nan\n",
      "        nan        nan 0.2735763  0.27792924 0.36147026 0.2735763\n",
      " 0.27742245 0.32503055 0.33490006 0.36147026 0.32837576 0.3223538 ]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.33916666666666667\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7331670822942643\n",
      "GridSearchCV Runtime: 4.652786731719971 secs\n",
      "Efron R-squared: -0.3352571957954411\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.5454545454545454\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.5\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.4482758620689655\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2571428571428571\n",
      "Ave Test Precision: 0.40731751007613076\n",
      "Stdev Test Precision: 0.12912130114588072\n",
      "Ave Test Accuracy: 0.7328358208955225\n",
      "Stdev Test Accuracy: 0.02787290954773594\n",
      "Ave Test Specificity: 0.9346534653465346\n",
      "Ave Test Recall: 0.11515151515151514\n",
      "Ave Test NPV: 0.7637535862906284\n",
      "Ave Test F1-Score: 0.17075706614870328\n",
      "Ave Test G-mean: 0.32043096007558375\n",
      "Ave Runtime: 0.008431339263916015\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with vehicleLoan. 47 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.3025     0.3025     0.28552469 0.3025     0.3025     0.42916667\n",
      " 0.45       0.35874199 0.42916667 0.42916667 0.42916667 0.45\n",
      " 0.35874199 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.3025     0.3025     0.29385802 0.3025     0.3025     0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.32416667 0.3375     0.32885802 0.32416667 0.3025     0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.42916667 0.32885802 0.42916667 0.3575     0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.42916667 0.30969136 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.42916667 0.30969136 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30874199 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667\n",
      " 0.42916667 0.4375     0.30776828 0.42916667 0.42916667 0.42916667\n",
      " 0.4375     0.30776828 0.42916667 0.42916667 0.42916667 0.4375\n",
      " 0.30776828 0.42916667 0.42916667        nan        nan        nan\n",
      "        nan        nan 0.42916667 0.4375     0.30776828 0.42916667\n",
      " 0.42916667 0.42916667 0.4375     0.30776828 0.42916667 0.42916667]\n",
      "One or more of the train scores are non-finite: [0.33312914 0.33312914 0.29945734 0.33312914 0.33312914 0.40663038\n",
      " 0.45368924 0.4667004  0.40663038 0.40663038 0.40663038 0.45368924\n",
      " 0.4667004  0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.33312914 0.33312914 0.3187877  0.33312914 0.33312914 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.36340997 0.3735933  0.34407513 0.36340997 0.33312914 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40164901 0.40663038 0.35171862 0.40440569 0.37618542 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.42557596 0.41072652 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.43057596 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038\n",
      " 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038\n",
      " 0.44674479 0.41628208 0.40663038 0.40663038 0.40663038 0.44674479\n",
      " 0.41628208 0.40663038 0.40663038        nan        nan        nan\n",
      "        nan        nan 0.40663038 0.44674479 0.41628208 0.40663038\n",
      " 0.40663038 0.40663038 0.44674479 0.41628208 0.40663038 0.40663038]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.45\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7468827930174564\n",
      "GridSearchCV Runtime: 5.1894073486328125 secs\n",
      "Efron R-squared: -0.31026822721953695\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.23076923076923078\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.5454545454545454\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4166666666666667\n",
      "Ave Test Precision: 0.35524475524475524\n",
      "Stdev Test Precision: 0.1294248243233685\n",
      "Ave Test Accuracy: 0.7432835820895523\n",
      "Stdev Test Accuracy: 0.011005881495337775\n",
      "Ave Test Specificity: 0.9673267326732674\n",
      "Ave Test Recall: 0.05757575757575758\n",
      "Ave Test NPV: 0.7585594540478023\n",
      "Ave Test F1-Score: 0.09822683645468455\n",
      "Ave Test G-mean: 0.22789511905128673\n",
      "Ave Runtime: 0.011220979690551757\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with informalLenders. 46 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22869546 0.09083333 0.09859726 0.20028711 0.22553133 0.22944792\n",
      " 0.07083333 0.09767448 0.20028711 0.22553133 0.2399706  0.03333333\n",
      " 0.09767448 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.22869546 0.08690476 0.09756085 0.20028711 0.22553133 0.22944792\n",
      " 0.07083333 0.09798079 0.20028711 0.22553133 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.23005036 0.08690476 0.09831331 0.21940476 0.23684628 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25053133 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.23584683 0.05       0.09798765 0.24047021 0.23865516 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09797903 0.24523212 0.23865516 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09766433 0.24459398 0.25532183 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733\n",
      " 0.2399706  0.03333333 0.09798079 0.24459398 0.25633997 0.22869546\n",
      " 0.03333333 0.09798079 0.20103957 0.25886467 0.2399706  0.03333333\n",
      " 0.09798079 0.24459398 0.2646733         nan        nan        nan\n",
      "        nan        nan 0.22869546 0.03333333 0.09798079 0.20103957\n",
      " 0.25886467 0.2399706  0.03333333 0.09798079 0.24459398 0.2646733 ]\n",
      "One or more of the train scores are non-finite: [0.24067471 0.09506111 0.09822167 0.23787841 0.24045567 0.24034427\n",
      " 0.09425466 0.49831583 0.23787841 0.24045567 0.25401405 0.10640437\n",
      " 0.49831583 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.24067471 0.10014517 0.09800951 0.23787841 0.24045567 0.24034427\n",
      " 0.09789743 0.49842221 0.24058372 0.24391417 0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.24851699 0.10401775 0.09806637 0.24996559 0.2468563  0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2419725  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25201832 0.10164069 0.09833626 0.25493519 0.25095694 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25339708 0.10303324 0.28145805 0.25693553 0.251888   0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10362659 0.4984225  0.25715111 0.25204925 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006\n",
      " 0.25401405 0.10640437 0.49842221 0.25557798 0.25217416 0.24067471\n",
      " 0.10177346 0.49842221 0.23754796 0.2473518  0.25401405 0.10640437\n",
      " 0.49842221 0.25557798 0.25124006        nan        nan        nan\n",
      "        nan        nan 0.24067471 0.10177346 0.49842221 0.23754796\n",
      " 0.2473518  0.25401405 0.10640437 0.49842221 0.25557798 0.25124006]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2646733036185091\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.7231920199501247\n",
      "GridSearchCV Runtime: 5.006442070007324 secs\n",
      "Efron R-squared: -0.35398508327840394\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.15\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2388663967611336\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25301204819277107\n",
      "Ave Test Precision: 0.21837568899078094\n",
      "Stdev Test Precision: 0.04368221698058891\n",
      "Ave Test Accuracy: 0.4544776119402985\n",
      "Stdev Test Accuracy: 0.23423505605187847\n",
      "Ave Test Specificity: 0.4138613861386139\n",
      "Ave Test Recall: 0.5787878787878789\n",
      "Ave Test NPV: 0.7636455628105071\n",
      "Ave Test F1-Score: 0.26341294722814\n",
      "Ave Test G-mean: 0.2365069405488125\n",
      "Ave Runtime: 0.006855106353759766\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with companyLoan. 45 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.29119048 0.30119048 0.31619048 0.29119048 0.29119048 0.27833333\n",
      " 0.34       0.23       0.29119048 0.30587302 0.38190476 0.38166667\n",
      " 0.23       0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.29119048 0.3359127  0.33234127 0.29119048 0.29119048 0.30119048\n",
      " 0.37611111 0.21857143 0.28301587 0.285      0.38190476 0.38166667\n",
      " 0.21857143 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.3409127  0.37638889 0.37162698 0.3359127  0.31710317 0.30785714\n",
      " 0.27888889 0.21222222 0.26833333 0.37420635 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.35138889 0.35055556 0.37555556 0.35138889 0.35138889 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.36940476 0.37333333 0.31222222 0.38190476 0.35138889 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.37333333 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476\n",
      " 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476 0.30785714\n",
      " 0.29555556 0.21222222 0.26833333 0.35055556 0.38190476 0.38166667\n",
      " 0.21222222 0.35690476 0.38190476        nan        nan        nan\n",
      "        nan        nan 0.30785714 0.29555556 0.21222222 0.26833333\n",
      " 0.35055556 0.38190476 0.38166667 0.21222222 0.35690476 0.38190476]\n",
      "One or more of the train scores are non-finite: [0.29168357 0.29423541 0.29404047 0.29168357 0.29168357 0.30881349\n",
      " 0.34430847 0.38039425 0.29168357 0.30593111 0.3596441  0.36830088\n",
      " 0.38039425 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.29168357 0.32071662 0.31354822 0.29168357 0.29168357 0.29841056\n",
      " 0.34949227 0.37617788 0.31412438 0.31579334 0.3596441  0.36830088\n",
      " 0.37617788 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.32287019 0.3353991  0.33266286 0.32407989 0.3219805  0.30548771\n",
      " 0.33485959 0.3763184  0.30149126 0.3313945  0.3596441  0.36830088\n",
      " 0.3763184  0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.33358092 0.34801606 0.34120557 0.33358092 0.33430556 0.30548771\n",
      " 0.32349595 0.3763184  0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.3763184  0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.36039931 0.37066854 0.34477697 0.36193696 0.33358092 0.30548771\n",
      " 0.32349595 0.3763184  0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.3763184  0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3605099  0.37917536 0.3597902  0.35880542 0.36128761 0.30548771\n",
      " 0.32349595 0.37693305 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37693305 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37693305 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37693305 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37545672 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37545672 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37545672 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37545672 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37545672 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37545672 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37545672 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37545672 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37545672 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37545672 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761\n",
      " 0.3596441  0.36830088 0.37693305 0.3570888  0.36128761 0.30548771\n",
      " 0.32349595 0.37545672 0.30149126 0.33444144 0.3596441  0.36830088\n",
      " 0.37545672 0.3570888  0.36128761        nan        nan        nan\n",
      "        nan        nan 0.30548771 0.32349595 0.37545672 0.30149126\n",
      " 0.33444144 0.3596441  0.36830088 0.37545672 0.3570888  0.36128761]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.38190476190476186\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.7381546134663342\n",
      "GridSearchCV Runtime: 5.343757629394531 secs\n",
      "Efron R-squared: -0.3450163426149604\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.1\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.1111111111111111\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.23715415019762845\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.26666666666666666\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.42857142857142855\n",
      "Ave Test Precision: 0.22870067130936697\n",
      "Stdev Test Precision: 0.13402645744441252\n",
      "Ave Test Accuracy: 0.6313432835820896\n",
      "Stdev Test Accuracy: 0.20961051300967495\n",
      "Ave Test Specificity: 0.7653465346534654\n",
      "Ave Test Recall: 0.2212121212121212\n",
      "Ave Test NPV: 0.7221564509050763\n",
      "Ave Test F1-Score: 0.1397751635561678\n",
      "Ave Test G-mean: 0.20469644730637687\n",
      "Ave Runtime: 0.008201265335083007\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with privateLoans. 44 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24422637 0.         0.12827272 0.22008844 0.17567255 0.24422637\n",
      " 0.         0.12188272 0.24422637 0.23067255 0.23380233 0.\n",
      " 0.12188272 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.24297187 0.         0.12859018 0.24297187 0.25108071 0.24679677\n",
      " 0.         0.12188272 0.24442809 0.23873867 0.23380233 0.\n",
      " 0.12188272 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.24437091 0.         0.12699102 0.24437091 0.24784642 0.23380233\n",
      " 0.         0.12154262 0.23351695 0.22918826 0.23380233 0.\n",
      " 0.12154262 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.24166242 0.         0.12406315 0.24166242 0.24911086 0.23380233\n",
      " 0.         0.12245962 0.23351695 0.22906215 0.23380233 0.\n",
      " 0.12245962 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23111161 0.         0.12238865 0.23337248 0.23637776 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22906215 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22906215        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23409558 0.         0.12205597 0.23351695 0.22647169 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22782347 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925\n",
      " 0.23380233 0.         0.12205597 0.23351695 0.22862925 0.23380233\n",
      " 0.         0.12205597 0.23351695 0.22862925 0.23380233 0.\n",
      " 0.12205597 0.23351695 0.22862925        nan        nan        nan\n",
      "        nan        nan 0.23380233 0.         0.12205597 0.23351695\n",
      " 0.22862925 0.23380233 0.         0.12205597 0.23351695 0.22862925]\n",
      "One or more of the train scores are non-finite: [0.25941592 0.         0.13154794 0.23207779 0.18362803 0.25941592\n",
      " 0.         0.12292119 0.25941592 0.23746098 0.24889402 0.\n",
      " 0.12292119 0.24745294 0.25004889        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.25698887 0.         0.13190373 0.25708196 0.26516051 0.24931848\n",
      " 0.         0.12292119 0.2531278  0.25712209 0.24889402 0.\n",
      " 0.12292119 0.24745294 0.25004889        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.25633817 0.         0.13188792 0.25633817 0.26163532 0.24898491\n",
      " 0.         0.12318429 0.24770359 0.251564   0.24898491 0.\n",
      " 0.12318429 0.24770359 0.25004889        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.25586401 0.         0.12711313 0.25586401 0.26193029 0.24907515\n",
      " 0.         0.32229279 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.32229279 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.25164977 0.         0.22218018 0.25157422 0.25708223 0.24907515\n",
      " 0.         0.35550195 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.35550195 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24872129 0.         0.25578404 0.24827076 0.2491633  0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30571395 0.24757271 0.25041518 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30560829 0.24770359 0.25009048 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30560829 0.24770359 0.25009048 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30560829 0.24770359 0.25009048 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30560829 0.24770359 0.25009048 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30560829 0.24770359 0.25009048 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048\n",
      " 0.24907515 0.         0.30560829 0.24770359 0.25009048 0.24907515\n",
      " 0.         0.30560829 0.24770359 0.25009048 0.24907515 0.\n",
      " 0.30560829 0.24770359 0.25009048        nan        nan        nan\n",
      "        nan        nan 0.24907515 0.         0.30560829 0.24770359\n",
      " 0.25009048 0.24907515 0.         0.30560829 0.24770359 0.25009048]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-05, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2510807142769716\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-05, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.48129675810473815\n",
      "GridSearchCV Runtime: 5.712975978851318 secs\n",
      "Efron R-squared: -0.34910235283244817\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2804878048780488\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2621951219512195\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.211864406779661\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.23\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.20175438596491227\n",
      "Ave Test Precision: 0.23726034391476833\n",
      "Stdev Test Precision: 0.03334927211996916\n",
      "Ave Test Accuracy: 0.5\n",
      "Stdev Test Accuracy: 0.032954331590775564\n",
      "Ave Test Specificity: 0.504950495049505\n",
      "Ave Test Recall: 0.484848484848485\n",
      "Ave Test NPV: 0.7556063936063936\n",
      "Ave Test F1-Score: 0.3156632326407077\n",
      "Ave Test G-mean: 0.48071283471666815\n",
      "Ave Runtime: 0.007311630249023438\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with governmentLoans. 43 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.30319597 0.25319597 0.27113683 0.30319597 0.30319597 0.32001082\n",
      " 0.19809524 0.18238367 0.33212454 0.31299756 0.32759019 0.19809524\n",
      " 0.18238367 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.30319597 0.25319597 0.27311402 0.30319597 0.30319597 0.32001082\n",
      " 0.19809524 0.31905033 0.30823565 0.33264042 0.32759019 0.19809524\n",
      " 0.31905033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.3196176  0.27755411 0.27697713 0.3196176  0.30319597 0.32425685\n",
      " 0.19809524 0.32405033 0.31279915 0.31803724 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.34147908 0.28314574 0.32925235 0.34147908 0.32199856 0.32981241\n",
      " 0.19809524 0.32405033 0.32930708 0.32279915 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.34223665 0.26107143 0.32440748 0.32223665 0.34223665 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.20809524 0.32405033 0.3266811  0.3266811  0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.20809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019\n",
      " 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019 0.32536797\n",
      " 0.19809524 0.32405033 0.30430708 0.33708486 0.32759019 0.19809524\n",
      " 0.32405033 0.32759019 0.32759019        nan        nan        nan\n",
      "        nan        nan 0.32536797 0.19809524 0.32405033 0.30430708\n",
      " 0.33708486 0.32759019 0.19809524 0.32405033 0.32759019 0.32759019]\n",
      "One or more of the train scores are non-finite: [0.29805773 0.27067678 0.27872465 0.29805773 0.29805773 0.3114852\n",
      " 0.31340409 0.3167918  0.31106446 0.31180764 0.32446574 0.31340409\n",
      " 0.3167918  0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.29805773 0.27081113 0.27885333 0.29805773 0.29805773 0.3114852\n",
      " 0.31219479 0.2937685  0.31460312 0.31100367 0.32446574 0.31219479\n",
      " 0.2937685  0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.29779328 0.27300515 0.28991115 0.29779328 0.29805773 0.31945792\n",
      " 0.31219479 0.29645595 0.30895261 0.31987676 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.31868533 0.29384367 0.2944938  0.31868533 0.30138353 0.31595652\n",
      " 0.31219479 0.29645595 0.30584218 0.3164527  0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32482938 0.30078667 0.29806777 0.32555122 0.32392088 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.30915112 0.29645595 0.32489187 0.32504554 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.30996413 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431\n",
      " 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431 0.31375872\n",
      " 0.31219479 0.29645595 0.30389774 0.31320147 0.32446574 0.31219479\n",
      " 0.29645595 0.32446574 0.32509431        nan        nan        nan\n",
      "        nan        nan 0.31375872 0.31219479 0.29645595 0.30389774\n",
      " 0.31320147 0.32446574 0.31219479 0.29645595 0.32446574 0.32509431]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.3422366522366523\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7206982543640897\n",
      "GridSearchCV Runtime: 6.4482996463775635 secs\n",
      "Efron R-squared: -0.34803469919102414\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2222222222222222\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.4\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3548387096774194\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.36363636363636365\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25\n",
      "Ave Test Precision: 0.31813945910720104\n",
      "Stdev Test Precision: 0.07739679206608645\n",
      "Ave Test Accuracy: 0.7238805970149254\n",
      "Stdev Test Accuracy: 0.008750775671312369\n",
      "Ave Test Specificity: 0.9227722772277229\n",
      "Ave Test Recall: 0.11515151515151516\n",
      "Ave Test NPV: 0.7615479429790593\n",
      "Ave Test F1-Score: 0.1679839380630558\n",
      "Ave Test G-mean: 0.32004536454318994\n",
      "Ave Runtime: 0.009618759155273438\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with smoking. 42 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22418914 0.22418914 0.23405033 0.22418914 0.22418914 0.22589147\n",
      " 0.24126984 0.09874199 0.23452242 0.23801893 0.23617077 0.24126984\n",
      " 0.09874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.22418914 0.22418914 0.24041144 0.22418914 0.22418914 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.2195177  0.21979548 0.265296   0.2195177  0.22418914 0.22589147\n",
      " 0.24126984 0.24874199 0.23005814 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.2268759  0.23342352 0.34934889 0.2268759  0.22043651 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.24770924 0.24274892 0.09874199 0.24770924 0.24770924 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.14874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242\n",
      " 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242 0.22753982\n",
      " 0.24126984 0.24874199 0.23452242 0.23452242 0.23617077 0.24126984\n",
      " 0.24874199 0.23617077 0.23452242        nan        nan        nan\n",
      "        nan        nan 0.22753982 0.24126984 0.24874199 0.23452242\n",
      " 0.23452242 0.23617077 0.24126984 0.24874199 0.23617077 0.23452242]\n",
      "One or more of the train scores are non-finite: [0.22035238 0.22035238 0.22594855 0.22035238 0.22035238 0.22355581\n",
      " 0.25010772 0.69749364 0.22306956 0.22317126 0.22214523 0.25061138\n",
      " 0.69749364 0.2231764  0.22390563        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22035238 0.22035238 0.23015995 0.22035238 0.22035238 0.22339164\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22214523 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.21496167 0.21187651 0.24415089 0.21496167 0.22035238 0.22329763\n",
      " 0.24232978 0.62666031 0.22365779 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.21930218 0.22291601 0.57712229 0.22000361 0.20998117 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22477394 0.23201537 0.6643698  0.22580948 0.22355329 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22204354 0.24232978 0.62666031 0.22328434 0.22500576 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22429177 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22363311 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22363311 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22363311 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22363311 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22363311 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311\n",
      " 0.22252585 0.24232978 0.62666031 0.2231764  0.22363311 0.22313347\n",
      " 0.24232978 0.62666031 0.22306956 0.22363311 0.22188705 0.24232978\n",
      " 0.62666031 0.2231764  0.22363311        nan        nan        nan\n",
      "        nan        nan 0.22313347 0.24232978 0.62666031 0.22306956\n",
      " 0.22363311 0.22188705 0.24232978 0.62666031 0.2231764  0.22363311]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.34934889227126675\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.756857855361596\n",
      "GridSearchCV Runtime: 6.491811037063599 secs\n",
      "Efron R-squared: -0.25165590734465093\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.5\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2727272727272727\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.5\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2413793103448276\n",
      "Ave Test Precision: 0.3028213166144201\n",
      "Stdev Test Precision: 0.20865065766212348\n",
      "Ave Test Accuracy: 0.7313432835820896\n",
      "Stdev Test Accuracy: 0.030656113293199595\n",
      "Ave Test Specificity: 0.9514851485148516\n",
      "Ave Test Recall: 0.05757575757575758\n",
      "Ave Test NPV: 0.7555062164856292\n",
      "Ave Test F1-Score: 0.08314824494391057\n",
      "Ave Test G-mean: 0.19001360003003093\n",
      "Ave Runtime: 0.009649848937988282\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with alcohol. 41 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.32       0.32       0.28635802 0.32       0.32       0.42333333\n",
      " 0.22333333 0.09874199 0.42333333 0.40666667 0.42333333 0.22333333\n",
      " 0.09874199 0.42333333 0.42333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.32       0.32       0.28635802 0.32       0.32       0.42333333\n",
      " 0.32333333 0.24874199 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24874199 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.32       0.32       0.27302469 0.32       0.32       0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.32333333 0.32333333 0.20302469 0.32333333 0.33666667 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.32333333 0.32333333 0.19969136 0.32333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333\n",
      " 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333\n",
      " 0.32333333 0.24776828 0.42333333 0.32333333 0.42333333 0.32333333\n",
      " 0.24776828 0.42333333 0.32333333        nan        nan        nan\n",
      "        nan        nan 0.42333333 0.32333333 0.24776828 0.42333333\n",
      " 0.32333333 0.42333333 0.32333333 0.24776828 0.42333333 0.32333333]\n",
      "One or more of the train scores are non-finite: [0.29761626 0.29761626 0.27800812 0.29761626 0.29761626 0.34589372\n",
      " 0.33707946 0.69809494 0.33886758 0.33690679 0.34589372 0.33707946\n",
      " 0.69809494 0.34033816 0.33704254        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.29761626 0.29761626 0.27378314 0.29761626 0.29761626 0.33577295\n",
      " 0.33329556 0.50357113 0.34033816 0.3208971  0.33577295 0.33329556\n",
      " 0.50357113 0.34033816 0.3208971         nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.29761626 0.29280073 0.29142828 0.29761626 0.29761626 0.32658103\n",
      " 0.32825354 0.45238066 0.33478261 0.31799855 0.32658103 0.32825354\n",
      " 0.45238066 0.33478261 0.31799855        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.28644791 0.29747213 0.31289205 0.28644791 0.29512757 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31496824 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31496824        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.30681124 0.33518165 0.42476161 0.30873432 0.29282082 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.30836102 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465\n",
      " 0.32221739 0.32825354 0.45238066 0.33114625 0.31272465 0.32221739\n",
      " 0.32825354 0.45238066 0.33478261 0.31272465 0.32221739 0.32825354\n",
      " 0.45238066 0.33478261 0.31272465        nan        nan        nan\n",
      "        nan        nan 0.32221739 0.32825354 0.45238066 0.33478261\n",
      " 0.31272465 0.32221739 0.32825354 0.45238066 0.33478261 0.31272465]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.42333333333333334\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7418952618453866\n",
      "GridSearchCV Runtime: 6.837020397186279 secs\n",
      "Efron R-squared: -0.3435802879785226\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.1111111111111111\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.16666666666666666\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.4166666666666667\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.14285714285714285\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.16666666666666666\n",
      "Ave Test Precision: 0.2007936507936508\n",
      "Stdev Test Precision: 0.12281088912338513\n",
      "Ave Test Accuracy: 0.7373134328358208\n",
      "Stdev Test Accuracy: 0.0067783216882779706\n",
      "Ave Test Specificity: 0.9693069306930692\n",
      "Ave Test Recall: 0.02727272727272727\n",
      "Ave Test NPV: 0.7531056294668017\n",
      "Ave Test F1-Score: 0.04756492214026461\n",
      "Ave Test G-mean: 0.1510851941747013\n",
      "Ave Runtime: 0.01307525634765625\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with gambling. 40 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.29880952 0.22380952 0.29016755 0.29880952 0.29880952 0.32\n",
      " 0.13333333 0.09874199 0.30333333 0.29880952 0.345      0.13333333\n",
      " 0.09874199 0.32       0.345             nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.29880952 0.22380952 0.28064374 0.29880952 0.29880952 0.32333333\n",
      " 0.23333333 0.24874199 0.32333333 0.29       0.34       0.23333333\n",
      " 0.24874199 0.34       0.29              nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.29880952 0.22714286 0.33635802 0.29880952 0.29880952 0.32\n",
      " 0.225      0.19874199 0.34       0.28666667 0.33666667 0.225\n",
      " 0.19874199 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.38666667 0.29166667 0.33135802 0.38666667 0.31547619 0.32\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.38666667 0.245      0.23207532 0.38666667 0.38666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667\n",
      " 0.33666667 0.225      0.23110161 0.33666667 0.28666667 0.33666667\n",
      " 0.225      0.23110161 0.34       0.28666667 0.33666667 0.225\n",
      " 0.23110161 0.34       0.28666667        nan        nan        nan\n",
      "        nan        nan 0.33666667 0.225      0.23110161 0.34\n",
      " 0.28666667 0.33666667 0.225      0.23110161 0.34       0.28666667]\n",
      "One or more of the train scores are non-finite: [0.29554617 0.27054617 0.27592044 0.29554617 0.29554617 0.32879277\n",
      " 0.31731075 0.69809494 0.32874927 0.30156246 0.33137897 0.31731075\n",
      " 0.69809494 0.32864765 0.32619948        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.29554617 0.27054617 0.27852304 0.29554617 0.29554617 0.32998004\n",
      " 0.29445105 0.44738066 0.3278848  0.33088176 0.33271175 0.29445105\n",
      " 0.44738066 0.32778318 0.33088176        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.29554617 0.27356131 0.28739506 0.29554617 0.29554617 0.32752117\n",
      " 0.2926978  0.34990254 0.32778318 0.32312377 0.33025287 0.2926978\n",
      " 0.34990254 0.32778318 0.32312377        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.30394319 0.29376326 0.29005634 0.30394319 0.29531014 0.32752117\n",
      " 0.2926978  0.33339905 0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.33339905 0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.32304772 0.29724326 0.32603666 0.32482747 0.30782041 0.33025287\n",
      " 0.2926978  0.33932762 0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.33932762 0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.29566223 0.33883097 0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.33883097 0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.33883097 0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.33883097 0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.3361643  0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.3361643  0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.3361643  0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.3361643  0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.3361643  0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401\n",
      " 0.33025287 0.2926978  0.3361643  0.32741954 0.32134401 0.33025287\n",
      " 0.2926978  0.3361643  0.32778318 0.32134401 0.33025287 0.2926978\n",
      " 0.3361643  0.32778318 0.32134401        nan        nan        nan\n",
      "        nan        nan 0.33025287 0.2926978  0.3361643  0.32778318\n",
      " 0.32134401 0.33025287 0.2926978  0.3361643  0.32778318 0.32134401]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.38666666666666666\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7381546134663342\n",
      "GridSearchCV Runtime: 7.025540590286255 secs\n",
      "Efron R-squared: -0.349867009459641\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.125\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.23577235772357724\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.14285714285714285\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.150725900116144\n",
      "Stdev Test Precision: 0.10066067144462562\n",
      "Ave Test Accuracy: 0.6402985074626865\n",
      "Stdev Test Accuracy: 0.20779595801964662\n",
      "Ave Test Specificity: 0.7881188118811882\n",
      "Ave Test Recall: 0.18787878787878787\n",
      "Ave Test NPV: 0.7276588162795059\n",
      "Ave Test F1-Score: 0.0960546426299851\n",
      "Ave Test G-mean: 0.13209079637177706\n",
      "Ave Runtime: 0.011400508880615234\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with smallLottery. 39 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.30166667 0.30166667 0.26802469 0.30166667 0.30166667 0.3\n",
      " 0.21190476 0.09874199 0.35       0.26833333 0.3        0.21190476\n",
      " 0.09874199 0.3        0.3               nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.30166667 0.30166667 0.29969136 0.30166667 0.30166667 0.3\n",
      " 0.31190476 0.09874199 0.3        0.25       0.3        0.31190476\n",
      " 0.09874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.30166667 0.385      0.21159612 0.30166667 0.30166667 0.3\n",
      " 0.31190476 0.09874199 0.3        0.23333333 0.3        0.31190476\n",
      " 0.09874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.35       0.4        0.14969136 0.35       0.36833333 0.3\n",
      " 0.31190476 0.19874199 0.3        0.23333333 0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.35       0.3        0.09969136 0.3        0.35       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.09874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.09874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.19874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.19874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.19874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.19874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.19874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25\n",
      " 0.3        0.31190476 0.19874199 0.3        0.25       0.3\n",
      " 0.31190476 0.19874199 0.3        0.25       0.3        0.31190476\n",
      " 0.19874199 0.3        0.25              nan        nan        nan\n",
      "        nan        nan 0.3        0.31190476 0.19874199 0.3\n",
      " 0.25       0.3        0.31190476 0.19874199 0.3        0.25      ]\n",
      "One or more of the train scores are non-finite: [0.25041557 0.25041557 0.24824634 0.25041557 0.25041557 0.25110421\n",
      " 0.3051079  0.69809494 0.25379999 0.2517117  0.25110421 0.3051079\n",
      " 0.69809494 0.25903899 0.25412651        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25041557 0.25041557 0.24756076 0.25041557 0.25041557 0.25110421\n",
      " 0.27997322 0.56476161 0.2527864  0.25426912 0.25110421 0.27997322\n",
      " 0.56476161 0.25445307 0.25426912        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25041557 0.23570301 0.26684786 0.25041557 0.25041557 0.25110421\n",
      " 0.2763847  0.54799025 0.2527864  0.25142424 0.25110421 0.2763847\n",
      " 0.54799025 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.23395066 0.25037161 0.36416637 0.23395066 0.24187976 0.25110421\n",
      " 0.2763847  0.54465692 0.25445307 0.25142424 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.24932213 0.269425   0.54476161 0.25041247 0.24005181 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25169945 0.2763847  0.54465692 0.25445307 0.24982718 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944\n",
      " 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944 0.25110421\n",
      " 0.2763847  0.54465692 0.2527864  0.25090944 0.25110421 0.2763847\n",
      " 0.54465692 0.25445307 0.25090944        nan        nan        nan\n",
      "        nan        nan 0.25110421 0.2763847  0.54465692 0.2527864\n",
      " 0.25090944 0.25110421 0.2763847  0.54465692 0.25445307 0.25090944]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.4\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.001, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7369077306733167\n",
      "GridSearchCV Runtime: 6.933996677398682 secs\n",
      "Efron R-squared: -0.34558835951592304\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2222222222222222\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.14285714285714285\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.14285714285714285\n",
      "Ave Test Precision: 0.10158730158730159\n",
      "Stdev Test Precision: 0.0982333234715289\n",
      "Ave Test Accuracy: 0.7388059701492538\n",
      "Stdev Test Accuracy: 0.008343537229476828\n",
      "Ave Test Specificity: 0.9762376237623762\n",
      "Ave Test Recall: 0.012121212121212121\n",
      "Ave Test NPV: 0.7515184469298781\n",
      "Ave Test F1-Score: 0.02162557077625571\n",
      "Ave Test G-mean: 0.08270682080933188\n",
      "Ave Runtime: 0.013116931915283203\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with otherVices. 38 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.26099611 0.26682245 0.2329219  0.26099611 0.26099611 0.26099611\n",
      " 0.0975     0.09969136 0.26099611 0.26099611 0.32645677 0.0975\n",
      " 0.09969136 0.3284154  0.33647913        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.26099611 0.26682245 0.2329219  0.26099611 0.26099611 0.30889344\n",
      " 0.0975     0.09969136 0.31545244 0.26099611 0.32645677 0.0975\n",
      " 0.09969136 0.3284154  0.33647913        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.26099611 0.27275835 0.23571879 0.26099611 0.26099611 0.32645677\n",
      " 0.28083333 0.09969136 0.3284154  0.33647913 0.32645677 0.28083333\n",
      " 0.09969136 0.3284154  0.33647913        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.27104097 0.3178464  0.22157149 0.27104097 0.26560071 0.32645677\n",
      " 0.40372909 0.18302469 0.32388324 0.33647913 0.32645677 0.40372909\n",
      " 0.18302469 0.32388324 0.33647913        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.31910679 0.39158509 0.22534071 0.31620842 0.27731516 0.32645677\n",
      " 0.40151942 0.21782789 0.32626419 0.33533067 0.32645677 0.40151942\n",
      " 0.21782789 0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32883772 0.41818609 0.22505766 0.32626419 0.3316155  0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32883772 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067\n",
      " 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677\n",
      " 0.41818609 0.2253663  0.32626419 0.33533067 0.32645677 0.41818609\n",
      " 0.2253663  0.32626419 0.33533067        nan        nan        nan\n",
      "        nan        nan 0.32645677 0.41818609 0.2253663  0.32626419\n",
      " 0.33533067 0.32645677 0.41818609 0.2253663  0.32626419 0.33533067]\n",
      "One or more of the train scores are non-finite: [0.2577556  0.25997953 0.2457579  0.2577556  0.2577556  0.2577556\n",
      " 0.09833795 0.09809494 0.2577556  0.2577556  0.32336281 0.09833795\n",
      " 0.09809494 0.32292046 0.32667809        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.2577556  0.25997953 0.2457579  0.2577556  0.2577556  0.31291582\n",
      " 0.14833795 0.09809494 0.31858088 0.2577556  0.32336281 0.14833795\n",
      " 0.09809494 0.32292046 0.32667809        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.2577556  0.26130795 0.24485335 0.2577556  0.2577556  0.32336281\n",
      " 0.30067006 0.25476161 0.32292046 0.32667809 0.32336281 0.30067006\n",
      " 0.25476161 0.32292046 0.32667809        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.26619824 0.28780795 0.24875714 0.26619824 0.25781535 0.32291979\n",
      " 0.28248515 0.31320483 0.31744442 0.32667809 0.32291979 0.28248515\n",
      " 0.31320483 0.31744442 0.32667809        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.30041964 0.28680107 0.32214868 0.30061341 0.27384967 0.32074527\n",
      " 0.28754376 0.30043999 0.31555701 0.32313527 0.32074527 0.28754376\n",
      " 0.30043999 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.31862311 0.28928592 0.30798899 0.31534671 0.31881519 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.31885785 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527\n",
      " 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527\n",
      " 0.28928592 0.30598801 0.31555701 0.32313527 0.32074527 0.28928592\n",
      " 0.30598801 0.31555701 0.32313527        nan        nan        nan\n",
      "        nan        nan 0.32074527 0.28928592 0.30598801 0.31555701\n",
      " 0.32313527 0.32074527 0.28928592 0.30598801 0.31555701 0.32313527]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "invalid value encountered in scalar divide\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "invalid value encountered in scalar divide\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': ADASYN(random_state=0)}\n",
      "Best Validation Score (Precision): 0.41818609022556397\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', ADASYN(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.2456359102244389\n",
      "GridSearchCV Runtime: 6.4639341831207275 secs\n",
      "Efron R-squared: -0.37380200701271993\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2462686567164179\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3076923076923077\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2543859649122807\n",
      "Ave Test Precision: 0.2775897838741515\n",
      "Stdev Test Precision: 0.040354517943496794\n",
      "Ave Test Accuracy: 0.49850746268656715\n",
      "Stdev Test Accuracy: 0.2421868880643179\n",
      "Ave Test Specificity: 0.4900990099009901\n",
      "Ave Test Recall: 0.5242424242424242\n",
      "Ave Test NPV: 0.7592944460842189\n",
      "Ave Test F1-Score: 0.27782370598824646\n",
      "Ave Test G-mean: 0.2156041540326256\n",
      "Ave Runtime: 0.014240121841430664\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with savings. 37 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "910 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "208 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "598 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.28297619 0.                nan 0.24011905 0.19011905 0.28297619\n",
      " 0.                nan 0.24011905 0.19011905 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.24672619 0.                nan 0.20386905 0.21380326 0.28297619\n",
      " 0.                nan 0.24011905 0.21380326 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107\n",
      " 0.28297619 0.                nan 0.24011905 0.23572107 0.28297619\n",
      " 0.                nan 0.24011905 0.23572107 0.28297619 0.\n",
      "        nan 0.28297619 0.23572107        nan        nan        nan\n",
      "        nan        nan 0.28297619 0.                nan 0.24011905\n",
      " 0.23572107 0.28297619 0.                nan 0.28297619 0.23572107]\n",
      "One or more of the train scores are non-finite: [0.29082237 0.                nan 0.26373904 0.23628806 0.29082237\n",
      " 0.                nan 0.26373904 0.23628806 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.28947611 0.                nan 0.26239278 0.26058016 0.29082237\n",
      " 0.                nan 0.26373904 0.26058016 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087\n",
      " 0.29082237 0.                nan 0.26373904 0.28506087 0.29082237\n",
      " 0.                nan 0.26373904 0.28506087 0.29082237 0.\n",
      "        nan 0.29082237 0.28506087        nan        nan        nan\n",
      "        nan        nan 0.29082237 0.                nan 0.26373904\n",
      " 0.28506087 0.29082237 0.                nan 0.29082237 0.28506087]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2829761904761905\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7256857855361596\n",
      "GridSearchCV Runtime: 6.964975118637085 secs\n",
      "Efron R-squared: -0.3491715141764158\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2777777777777778\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.0\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.1126984126984127\n",
      "Stdev Test Precision: 0.15434416555597125\n",
      "Ave Test Accuracy: 0.7455223880597015\n",
      "Stdev Test Accuracy: 0.013033021788487286\n",
      "Ave Test Specificity: 0.9821782178217822\n",
      "Ave Test Recall: 0.021212121212121213\n",
      "Ave Test NPV: 0.7543966603762795\n",
      "Ave Test F1-Score: 0.03476842791911285\n",
      "Ave Test G-mean: 0.0876293264991058\n",
      "Ave Runtime: 0.009058284759521484\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanOthers. 36 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "910 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "208 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "598 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.23936508 0.03222222        nan 0.33936508 0.08079365 0.23936508\n",
      " 0.                nan 0.33936508 0.08079365 0.25936508 0.\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.05222222        nan 0.33936508 0.12545119 0.25936508\n",
      " 0.                nan 0.33936508 0.10079365 0.25936508 0.\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.05222222        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.                nan 0.33936508 0.16707281 0.25936508 0.\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.05222222        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.                nan 0.33936508 0.16707281 0.25936508 0.\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281\n",
      " 0.25936508 0.15936508        nan 0.33936508 0.16707281 0.25936508\n",
      " 0.15936508        nan 0.33936508 0.16707281 0.25936508 0.15936508\n",
      "        nan 0.33936508 0.16707281        nan        nan        nan\n",
      "        nan        nan 0.25936508 0.15936508        nan 0.33936508\n",
      " 0.16707281 0.25936508 0.15936508        nan 0.33936508 0.16707281]\n",
      "One or more of the train scores are non-finite: [0.26838504 0.09575013        nan 0.29226564 0.18650841 0.26838504\n",
      " 0.                nan 0.29226564 0.18650841 0.29296519 0.\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.12639529        nan 0.29226564 0.23513727 0.29296519\n",
      " 0.                nan 0.29226564 0.21108856 0.29296519 0.\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.12639529        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.                nan 0.29226564 0.28410767 0.29296519 0.\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.12639529        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.                nan 0.29226564 0.28410767 0.29296519 0.\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767\n",
      " 0.29296519 0.21212956        nan 0.29226564 0.28410767 0.29296519\n",
      " 0.21212956        nan 0.29226564 0.28410767 0.29296519 0.21212956\n",
      "        nan 0.29226564 0.28410767        nan        nan        nan\n",
      "        nan        nan 0.29296519 0.21212956        nan 0.29226564\n",
      " 0.28410767 0.29296519 0.21212956        nan 0.29226564 0.28410767]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.33936507936507937\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7169576059850374\n",
      "GridSearchCV Runtime: 6.775078296661377 secs\n",
      "Efron R-squared: -0.34917193557156656\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4166666666666667\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.35714285714285715\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.34615384615384615\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.35714285714285715\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4166666666666667\n",
      "Ave Test Precision: 0.3787545787545788\n",
      "Stdev Test Precision: 0.03489840113765008\n",
      "Ave Test Accuracy: 0.7298507462686568\n",
      "Stdev Test Accuracy: 0.00817496354485325\n",
      "Ave Test Specificity: 0.9198019801980198\n",
      "Ave Test Recall: 0.1484848484848485\n",
      "Ave Test NPV: 0.7677559499616132\n",
      "Ave Test F1-Score: 0.21312570665022096\n",
      "Ave Test G-mean: 0.36948706056345093\n",
      "Ave Runtime: 0.013111591339111328\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with payInsurance. 35 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.15726284 0.         0.         0.08054711 0.         0.26114724\n",
      " 0.         0.         0.26114724 0.1806671  0.26114724 0.\n",
      " 0.         0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.         0.         0.26114724 0.26114724 0.26114724\n",
      " 0.         0.         0.26114724 0.26114724 0.26114724 0.\n",
      " 0.         0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.         0.         0.26114724 0.26114724 0.26114724\n",
      " 0.         0.         0.26114724 0.26114724 0.26114724 0.\n",
      " 0.         0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.         0.         0.26114724 0.26114724 0.26114724\n",
      " 0.         0.         0.26114724 0.26114724 0.26114724 0.\n",
      " 0.         0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.11374646 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.11374646 0.21433873 0.26114724 0.26114724 0.26114724 0.11374646\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724\n",
      " 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724\n",
      " 0.26114724 0.21433873 0.26114724 0.26114724 0.26114724 0.26114724\n",
      " 0.21433873 0.26114724 0.26114724        nan        nan        nan\n",
      "        nan        nan 0.26114724 0.26114724 0.21433873 0.26114724\n",
      " 0.26114724 0.26114724 0.26114724 0.21433873 0.26114724 0.26114724]\n",
      "One or more of the train scores are non-finite: [0.15638915 0.         0.         0.07790897 0.         0.26070874\n",
      " 0.         0.         0.26070874 0.18272748 0.26070874 0.\n",
      " 0.         0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.         0.         0.26070874 0.26070874 0.26070874\n",
      " 0.         0.         0.26070874 0.26070874 0.26070874 0.\n",
      " 0.         0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.         0.         0.26070874 0.26070874 0.26070874\n",
      " 0.         0.         0.26070874 0.26070874 0.26070874 0.\n",
      " 0.         0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.         0.         0.26070874 0.26070874 0.26070874\n",
      " 0.         0.         0.26070874 0.26070874 0.26070874 0.\n",
      " 0.         0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.13224755 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.13224755 0.20803208 0.26070874 0.26070874 0.26070874 0.13224755\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874\n",
      " 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874\n",
      " 0.26070874 0.20803208 0.26070874 0.26070874 0.26070874 0.26070874\n",
      " 0.20803208 0.26070874 0.26070874        nan        nan        nan\n",
      "        nan        nan 0.26070874 0.26070874 0.20803208 0.26070874\n",
      " 0.26070874 0.26070874 0.26070874 0.20803208 0.26070874 0.26070874]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2611472392843984\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.4476309226932668\n",
      "GridSearchCV Runtime: 6.80575156211853 secs\n",
      "Efron R-squared: -0.3491678595981724\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.26666666666666666\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.26737967914438504\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23125\n",
      "Ave Test Precision: 0.20305926916221032\n",
      "Stdev Test Precision: 0.11447077407247937\n",
      "Ave Test Accuracy: 0.4977611940298508\n",
      "Stdev Test Accuracy: 0.1431494241335313\n",
      "Ave Test Specificity: 0.48415841584158414\n",
      "Ave Test Recall: 0.5393939393939394\n",
      "Ave Test NPV: 0.7687106345377489\n",
      "Ave Test F1-Score: 0.29485579711375637\n",
      "Ave Test G-mean: 0.3898449514301894\n",
      "Ave Runtime: 0.008896112442016602\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanSSS. 34 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "780 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "624 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22341625 0.025             nan 0.21008292 0.13882623 0.24495471\n",
      " 0.025             nan 0.21008292 0.13882623 0.24495471 0.025\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.025             nan 0.21008292 0.18422306 0.24495471\n",
      " 0.025             nan 0.21008292 0.18422306 0.24495471 0.025\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.025             nan 0.21008292 0.18422306 0.24495471\n",
      " 0.025             nan 0.21008292 0.18422306 0.24495471 0.025\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.025             nan 0.21008292 0.18422306 0.24495471\n",
      " 0.025             nan 0.21008292 0.18422306 0.24495471 0.025\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.06184211        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.04078947        nan 0.21008292 0.18422306 0.24495471 0.04078947\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.13099795        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.13099795        nan 0.21008292 0.18422306 0.24495471 0.13099795\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306\n",
      " 0.24495471 0.15266462        nan 0.21008292 0.18422306 0.24495471\n",
      " 0.15266462        nan 0.21008292 0.18422306 0.24495471 0.15266462\n",
      "        nan 0.21008292 0.18422306        nan        nan        nan\n",
      "        nan        nan 0.24495471 0.15266462        nan 0.21008292\n",
      " 0.18422306 0.24495471 0.15266462        nan 0.21008292 0.18422306]\n",
      "One or more of the train scores are non-finite: [0.22706693 0.02451524        nan 0.22828644 0.15234286 0.25179811\n",
      " 0.02451524        nan 0.22828644 0.15234286 0.25179811 0.02451524\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.02451524        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.02451524        nan 0.22828644 0.20153897 0.25179811 0.02451524\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.02451524        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.02451524        nan 0.22828644 0.20153897 0.25179811 0.02451524\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.02451524        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.02451524        nan 0.22828644 0.20153897 0.25179811 0.02451524\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.07639024        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.05076524        nan 0.22828644 0.20153897 0.25179811 0.05076524\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15247972        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15247972        nan 0.22828644 0.20153897 0.25179811 0.15247972\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897\n",
      " 0.25179811 0.15113522        nan 0.22828644 0.20153897 0.25179811\n",
      " 0.15113522        nan 0.22828644 0.20153897 0.25179811 0.15113522\n",
      "        nan 0.22828644 0.20153897        nan        nan        nan\n",
      "        nan        nan 0.25179811 0.15113522        nan 0.22828644\n",
      " 0.20153897 0.25179811 0.15113522        nan 0.22828644 0.20153897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.24495471161105836\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.3566084788029925\n",
      "GridSearchCV Runtime: 6.282716989517212 secs\n",
      "Efron R-squared: -0.34917100700766235\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.23880597014925373\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.20634920634920634\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.18571428571428572\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25\n",
      "Ave Test Precision: 0.23331674958540632\n",
      "Stdev Test Precision: 0.038881895317645854\n",
      "Ave Test Accuracy: 0.5694029850746269\n",
      "Stdev Test Accuracy: 0.11818996719198696\n",
      "Ave Test Specificity: 0.6475247524752475\n",
      "Ave Test Recall: 0.3303030303030303\n",
      "Ave Test NPV: 0.7451967178152107\n",
      "Ave Test F1-Score: 0.25701570614166597\n",
      "Ave Test G-mean: 0.4160454402909529\n",
      "Ave Runtime: 0.016042518615722656\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with payFamilySupport. 33 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "\n",
      "780 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "624 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.07807018 0.                nan 0.1061176  0.         0.26579155\n",
      " 0.                nan 0.26579155 0.24079155 0.26579155 0.\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.                nan 0.26579155 0.26579155 0.26579155\n",
      " 0.                nan 0.26579155 0.26579155 0.26579155 0.\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.                nan 0.26579155 0.26579155 0.26579155\n",
      " 0.                nan 0.26579155 0.26579155 0.26579155 0.\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.                nan 0.26579155 0.26579155 0.26579155\n",
      " 0.                nan 0.26579155 0.26579155 0.26579155 0.\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155\n",
      " 0.26579155 0.26579155        nan 0.26579155 0.26579155 0.26579155\n",
      " 0.26579155        nan 0.26579155 0.26579155 0.26579155 0.26579155\n",
      "        nan 0.26579155 0.26579155        nan        nan        nan\n",
      "        nan        nan 0.26579155 0.26579155        nan 0.26579155\n",
      " 0.26579155 0.26579155 0.26579155        nan 0.26579155 0.26579155]\n",
      "One or more of the train scores are non-finite: [0.0800134  0.                nan 0.10641855 0.         0.26605232\n",
      " 0.                nan 0.26605232 0.23927747 0.26605232 0.\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.                nan 0.26605232 0.26605232 0.26605232\n",
      " 0.                nan 0.26605232 0.26605232 0.26605232 0.\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.                nan 0.26605232 0.26605232 0.26605232\n",
      " 0.                nan 0.26605232 0.26605232 0.26605232 0.\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.                nan 0.26605232 0.26605232 0.26605232\n",
      " 0.                nan 0.26605232 0.26605232 0.26605232 0.\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232\n",
      " 0.26605232 0.26605232        nan 0.26605232 0.26605232 0.26605232\n",
      " 0.26605232        nan 0.26605232 0.26605232 0.26605232 0.26605232\n",
      "        nan 0.26605232 0.26605232        nan        nan        nan\n",
      "        nan        nan 0.26605232 0.26605232        nan 0.26605232\n",
      " 0.26605232 0.26605232 0.26605232        nan 0.26605232 0.26605232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2657915475780755\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.43640897755610975\n",
      "GridSearchCV Runtime: 6.43879771232605 secs\n",
      "Efron R-squared: -0.34916565739628935\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25925925925925924\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.288135593220339\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2617801047120419\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2541436464088398\n",
      "Ave Test Precision: 0.26266372072009603\n",
      "Stdev Test Precision: 0.014951161430659526\n",
      "Ave Test Accuracy: 0.4298507462686567\n",
      "Stdev Test Accuracy: 0.025086173602942703\n",
      "Ave Test Specificity: 0.3326732673267327\n",
      "Ave Test Recall: 0.7272727272727273\n",
      "Ave Test NPV: 0.7886334523402065\n",
      "Ave Test F1-Score: 0.3858555739079204\n",
      "Ave Test G-mean: 0.4912617142364824\n",
      "Ave Runtime: 0.014802312850952149\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanPagIbig. 32 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.03333333 0.03333333        nan 0.03333333 0.09333333 0.03333333\n",
      " 0.                nan 0.03333333 0.09333333 0.07907692 0.\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.11949359 0.03333333        nan 0.08069136 0.09333333 0.056\n",
      " 0.                nan 0.056      0.09333333 0.07907692 0.\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.                nan 0.056      0.11641026 0.07907692 0.\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.                nan 0.056      0.11641026 0.07907692 0.\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026\n",
      " 0.07907692 0.03333333        nan 0.056      0.11641026 0.07907692\n",
      " 0.03333333        nan 0.056      0.11641026 0.07907692 0.03333333\n",
      "        nan 0.08069136 0.11641026        nan        nan        nan\n",
      "        nan        nan 0.07907692 0.03333333        nan 0.056\n",
      " 0.11641026 0.07907692 0.03333333        nan 0.08069136 0.11641026]\n",
      "One or more of the train scores are non-finite: [0.23215995 0.05631579        nan 0.20358852 0.159807   0.23215995\n",
      " 0.                nan 0.20358852 0.159807   0.28148467 0.\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.27028166 0.05631579        nan 0.2228186  0.159807   0.2568408\n",
      " 0.                nan 0.22826937 0.159807   0.28148467 0.\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.05631579        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.                nan 0.22826937 0.18445088 0.28148467 0.\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.05631579        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.                nan 0.22826937 0.18445088 0.28148467 0.\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.14631579        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.08631579        nan 0.22826937 0.18445088 0.28148467 0.08631579\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088\n",
      " 0.28148467 0.20215995        nan 0.22826937 0.18445088 0.28148467\n",
      " 0.20215995        nan 0.22826937 0.18445088 0.28148467 0.20215995\n",
      "        nan 0.2528186  0.18445088        nan        nan        nan\n",
      "        nan        nan 0.28148467 0.20215995        nan 0.22826937\n",
      " 0.18445088 0.28148467 0.20215995        nan 0.2528186  0.18445088]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-05, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.11949358974358974\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-05, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.2581047381546135\n",
      "GridSearchCV Runtime: 6.623358726501465 secs\n",
      "Efron R-squared: -0.34917112067307077\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.14285714285714285\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.24806201550387597\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.23846153846153847\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2471042471042471\n",
      "Ave Test Precision: 0.22529698878536086\n",
      "Stdev Test Precision: 0.04629707141061084\n",
      "Ave Test Accuracy: 0.3567164179104477\n",
      "Stdev Test Accuracy: 0.2117122222449494\n",
      "Ave Test Specificity: 0.21980198019801983\n",
      "Ave Test Recall: 0.7757575757575758\n",
      "Ave Test NPV: 0.7407471264367815\n",
      "Ave Test F1-Score: 0.3190892493619334\n",
      "Ave Test G-mean: 0.16433163933532752\n",
      "Ave Runtime: 0.010653114318847657\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanGSIS. 31 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.                nan 0.34202381 0.34202381 0.34202381 0.\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.32291667 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.                nan 0.34202381 0.34202381 0.34202381 0.\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.                nan 0.34202381 0.34202381 0.34202381 0.\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.                nan 0.34202381 0.34202381 0.34202381 0.\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381\n",
      " 0.34202381 0.12285714        nan 0.34202381 0.34202381 0.34202381\n",
      " 0.12285714        nan 0.34202381 0.34202381 0.34202381 0.12285714\n",
      "        nan 0.34202381 0.34202381        nan        nan        nan\n",
      "        nan        nan 0.34202381 0.12285714        nan 0.34202381\n",
      " 0.34202381 0.34202381 0.12285714        nan 0.34202381 0.34202381]\n",
      "One or more of the train scores are non-finite: [0.32278992 0.09406699        nan 0.32278992 0.32278992 0.32278992\n",
      " 0.                nan 0.32278992 0.32278992 0.32278992 0.\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.31653457 0.09406699        nan 0.32278992 0.32278992 0.32278992\n",
      " 0.                nan 0.32278992 0.32278992 0.32278992 0.\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.09406699        nan 0.32278992 0.32278992 0.32278992\n",
      " 0.                nan 0.32278992 0.32278992 0.32278992 0.\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.09406699        nan 0.32278992 0.32278992 0.32278992\n",
      " 0.                nan 0.32278992 0.32278992 0.32278992 0.\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.09406699        nan 0.32278992 0.32278992 0.32278992\n",
      " 0.09406699        nan 0.32278992 0.32278992 0.32278992 0.09406699\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992\n",
      " 0.32278992 0.1291547         nan 0.32278992 0.32278992 0.32278992\n",
      " 0.1291547         nan 0.32278992 0.32278992 0.32278992 0.1291547\n",
      "        nan 0.32278992 0.32278992        nan        nan        nan\n",
      "        nan        nan 0.32278992 0.1291547         nan 0.32278992\n",
      " 0.32278992 0.32278992 0.1291547         nan 0.32278992 0.32278992]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.3420238095238095\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.726932668329177\n",
      "GridSearchCV Runtime: 6.3191564083099365 secs\n",
      "Efron R-squared: -0.349171715202381\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.35\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2727272727272727\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4\n",
      "Ave Test Precision: 0.3283549783549783\n",
      "Stdev Test Precision: 0.051329743836055444\n",
      "Ave Test Accuracy: 0.7276119402985074\n",
      "Stdev Test Accuracy: 0.008750775671312359\n",
      "Ave Test Specificity: 0.9316831683168317\n",
      "Ave Test Recall: 0.10303030303030303\n",
      "Ave Test NPV: 0.760743992229473\n",
      "Ave Test F1-Score: 0.15606447568489804\n",
      "Ave Test G-mean: 0.30746008147610804\n",
      "Ave Runtime: 0.010916709899902344\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with loanPersonal. 30 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.35       0.                nan 0.35       0.3        0.35\n",
      " 0.                nan 0.35       0.3        0.35       0.\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.                nan 0.34969136 0.3        0.35\n",
      " 0.                nan 0.35       0.3        0.35       0.\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.                nan 0.35       0.3        0.35\n",
      " 0.                nan 0.35       0.32368421 0.35       0.\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.03333333        nan 0.35       0.32368421 0.35\n",
      " 0.                nan 0.35       0.32368421 0.35       0.\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.18333333        nan 0.35       0.32368421 0.35\n",
      " 0.18333333        nan 0.35       0.32368421 0.35       0.18333333\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.25              nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.3               nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.3               nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.3               nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421\n",
      " 0.35       0.25              nan 0.35       0.32368421 0.35\n",
      " 0.3               nan 0.35       0.32368421 0.35       0.25\n",
      "        nan 0.35       0.32368421        nan        nan        nan\n",
      "        nan        nan 0.35       0.3               nan 0.35\n",
      " 0.32368421 0.35       0.3               nan 0.35       0.32368421]\n",
      "One or more of the train scores are non-finite: [0.34779107 0.                nan 0.34779107 0.31621212 0.34779107\n",
      " 0.                nan 0.34779107 0.31621212 0.34779107 0.\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.                nan 0.32715857 0.31621212 0.34779107\n",
      " 0.                nan 0.34779107 0.31621212 0.34779107 0.\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.                nan 0.34779107 0.31621212 0.34779107\n",
      " 0.                nan 0.34779107 0.34053645 0.34779107 0.\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.035             nan 0.34779107 0.34053645 0.34779107\n",
      " 0.                nan 0.34779107 0.34053645 0.34779107 0.\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.10015152        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.10015152        nan 0.34779107 0.34053645 0.34779107 0.10015152\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.28287879        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.31621212        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.31621212        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.31621212        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645\n",
      " 0.34779107 0.28287879        nan 0.34779107 0.34053645 0.34779107\n",
      " 0.31621212        nan 0.34779107 0.34053645 0.34779107 0.28287879\n",
      "        nan 0.34779107 0.34053645        nan        nan        nan\n",
      "        nan        nan 0.34779107 0.31621212        nan 0.34779107\n",
      " 0.34053645 0.34779107 0.31621212        nan 0.34779107 0.34053645]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.35\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7456359102244389\n",
      "GridSearchCV Runtime: 6.311387062072754 secs\n",
      "Efron R-squared: -0.3491715150183212\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.18181818181818182\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.6\n",
      "Ave Test Precision: 0.32303030303030306\n",
      "Stdev Test Precision: 0.16764999177716466\n",
      "Ave Test Accuracy: 0.7425373134328359\n",
      "Stdev Test Accuracy: 0.01087864159486063\n",
      "Ave Test Specificity: 0.9742574257425742\n",
      "Ave Test Recall: 0.03333333333333334\n",
      "Ave Test NPV: 0.7551639783529711\n",
      "Ave Test F1-Score: 0.059825406452537665\n",
      "Ave Test G-mean: 0.1771968772153508\n",
      "Ave Runtime: 0.011108922958374023\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasPensioner. 29 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "975 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "130 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "260 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "585 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.0684771  0.02469136        nan 0.07441789 0.0521978  0.1618558\n",
      " 0.02469136        nan 0.16822178 0.0521978  0.22254674 0.02469136\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.02469136        nan 0.21913087 0.14636352 0.18725263\n",
      " 0.02469136        nan 0.16822178 0.15239527 0.22254674 0.02469136\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.02469136        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.02469136        nan 0.21913087 0.19959269 0.22254674 0.02469136\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.02469136        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.02469136        nan 0.21913087 0.19959269 0.22254674 0.02469136\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.0265625         nan 0.21913087 0.19959269 0.18725263\n",
      " 0.0265625         nan 0.21913087 0.19959269 0.22254674 0.0265625\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269\n",
      " 0.18725263 0.05628833        nan 0.21913087 0.19959269 0.18725263\n",
      " 0.05628833        nan 0.21913087 0.19959269 0.22254674 0.05628833\n",
      "        nan 0.21913087 0.19959269        nan        nan        nan\n",
      "        nan        nan 0.18725263 0.05628833        nan 0.21913087\n",
      " 0.19959269 0.22254674 0.05628833        nan 0.21913087 0.19959269]\n",
      "One or more of the train scores are non-finite: [0.09917277 0.02454924        nan 0.09954993 0.04797748 0.19846212\n",
      " 0.02454924        nan 0.19880329 0.04797748 0.24599383 0.02454924\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.02454924        nan 0.24800822 0.14718676 0.22306389\n",
      " 0.02454924        nan 0.19880329 0.14652584 0.24599383 0.02454924\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.02454924        nan 0.24800822 0.19612667 0.22306389\n",
      " 0.02454924        nan 0.24800822 0.19612667 0.24599383 0.02454924\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.02454924        nan 0.24800822 0.19612667 0.22306389\n",
      " 0.02454924        nan 0.24800822 0.19612667 0.24599383 0.02454924\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.02446809        nan 0.24800822 0.19612667 0.22306389\n",
      " 0.02446809        nan 0.24800822 0.19612667 0.24599383 0.02446809\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667\n",
      " 0.22306389 0.0747542         nan 0.24800822 0.19612667 0.22306389\n",
      " 0.0747542         nan 0.24800822 0.19612667 0.24599383 0.0747542\n",
      "        nan 0.24800822 0.19612667        nan        nan        nan\n",
      "        nan        nan 0.22306389 0.0747542         nan 0.24800822\n",
      " 0.19612667 0.24599383 0.0747542         nan 0.24800822 0.19612667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2225467444686368\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.35785536159601\n",
      "GridSearchCV Runtime: 6.367269515991211 secs\n",
      "Efron R-squared: -0.3491713601429467\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2350230414746544\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.16666666666666666\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2727272727272727\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.24193548387096775\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.21311475409836064\n",
      "Ave Test Precision: 0.22589344376758444\n",
      "Stdev Test Precision: 0.0393865782196834\n",
      "Ave Test Accuracy: 0.5656716417910448\n",
      "Stdev Test Accuracy: 0.13547910907339883\n",
      "Ave Test Specificity: 0.6445544554455445\n",
      "Ave Test Recall: 0.3242424242424242\n",
      "Ave Test NPV: 0.7390832716914387\n",
      "Ave Test F1-Score: 0.24619617383496034\n",
      "Ave Test G-mean: 0.39397032977849544\n",
      "Ave Runtime: 0.014866256713867187\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasPrivateEmployee. 28 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.02727273 0.12219136 0.17219136 0.         0.02991453 0.12442131\n",
      " 0.12219136 0.17219136 0.22178444 0.02991453 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.20543651 0.12219136 0.17219136 0.22178444 0.17887471 0.19986833\n",
      " 0.12219136 0.17219136 0.22178444 0.20281837 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12219136 0.17219136 0.22178444 0.20281837 0.19986833\n",
      " 0.12219136 0.17219136 0.22178444 0.20281837 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12219136 0.17219136 0.22178444 0.20281837 0.19986833\n",
      " 0.12219136 0.17219136 0.22178444 0.20281837 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12219136 0.17219136 0.22178444 0.20281837 0.19986833\n",
      " 0.12219136 0.17219136 0.22178444 0.20281837 0.22294525 0.12219136\n",
      " 0.17219136 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.10889882 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.10889882 0.17688219 0.22178444 0.20281837 0.22294525 0.10889882\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837\n",
      " 0.19986833 0.12254079 0.17688219 0.22178444 0.20281837 0.19986833\n",
      " 0.12254079 0.17688219 0.22178444 0.20281837 0.22294525 0.12254079\n",
      " 0.17688219 0.22947674 0.20281837        nan        nan        nan\n",
      "        nan        nan 0.19986833 0.12254079 0.17688219 0.22178444\n",
      " 0.20281837 0.22294525 0.12254079 0.17688219 0.22947674 0.20281837]\n",
      "One or more of the train scores are non-finite: [0.04736842 0.12288719 0.17191766 0.         0.0474116  0.14704803\n",
      " 0.12288719 0.17191766 0.22392204 0.0474116  0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22302655 0.12288719 0.17191766 0.22392204 0.19657363 0.22153071\n",
      " 0.12288719 0.17191766 0.22392204 0.22153363 0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.12288719 0.17191766 0.22392204 0.22153363 0.22153071\n",
      " 0.12288719 0.17191766 0.22392204 0.22153363 0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.12288719 0.17191766 0.22392204 0.22153363 0.22153071\n",
      " 0.12288719 0.17191766 0.22392204 0.22153363 0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.12288719 0.17191766 0.22392204 0.22153363 0.22153071\n",
      " 0.12288719 0.17191766 0.22392204 0.22153363 0.24411135 0.12288719\n",
      " 0.17191766 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11895355 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11895355 0.17368407 0.22392204 0.22153363 0.24411135 0.11895355\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363\n",
      " 0.22153071 0.11341356 0.17368407 0.22392204 0.22153363 0.22153071\n",
      " 0.11341356 0.17368407 0.22392204 0.22153363 0.24411135 0.11341356\n",
      " 0.17368407 0.24865322 0.22153363        nan        nan        nan\n",
      "        nan        nan 0.22153071 0.11341356 0.17368407 0.22392204\n",
      " 0.22153363 0.24411135 0.11341356 0.17368407 0.24865322 0.22153363]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.22947674419304326\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.3179551122194514\n",
      "GridSearchCV Runtime: 6.282754182815552 secs\n",
      "Efron R-squared: -0.3491714504618819\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2145748987854251\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.3448275862068966\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.21982758620689655\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2647058823529412\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.23529411764705882\n",
      "Ave Test Precision: 0.25584601423984366\n",
      "Stdev Test Precision: 0.053430398639459815\n",
      "Ave Test Accuracy: 0.5194029850746269\n",
      "Stdev Test Accuracy: 0.24837128176702294\n",
      "Ave Test Specificity: 0.5594059405940593\n",
      "Ave Test Recall: 0.39696969696969686\n",
      "Ave Test NPV: 0.6477046198803521\n",
      "Ave Test F1-Score: 0.24629326838979212\n",
      "Ave Test G-mean: 0.3005806224694716\n",
      "Ave Runtime: 0.007671213150024414\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasBusiness. 27 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.01666667 0.         0.025      0.         0.01666667 0.03903509\n",
      " 0.         0.025      0.         0.01666667 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.24521431 0.         0.025      0.24420077 0.06634323 0.18843962\n",
      " 0.         0.025      0.17102509 0.06559536 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.         0.025      0.19670077 0.21408065 0.18843962\n",
      " 0.         0.025      0.19670077 0.21408065 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.         0.025      0.19670077 0.21408065 0.18843962\n",
      " 0.         0.025      0.19670077 0.21408065 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.         0.025      0.19670077 0.21408065 0.18843962\n",
      " 0.         0.025      0.19670077 0.21408065 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.         0.025      0.19670077 0.21408065 0.18843962\n",
      " 0.         0.025      0.19670077 0.21408065 0.18843962 0.\n",
      " 0.025      0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065\n",
      " 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962\n",
      " 0.02236842 0.0542471  0.19670077 0.21408065 0.18843962 0.02236842\n",
      " 0.0542471  0.19670077 0.21408065        nan        nan        nan\n",
      "        nan        nan 0.18843962 0.02236842 0.0542471  0.19670077\n",
      " 0.21408065 0.18843962 0.02236842 0.0542471  0.19670077 0.21408065]\n",
      "One or more of the train scores are non-finite: [0.06805556 0.         0.02451524 0.04849806 0.04513889 0.09309259\n",
      " 0.         0.02451524 0.04849806 0.04513889 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.24727836 0.         0.02451524 0.24760596 0.09466014 0.2416157\n",
      " 0.         0.02451524 0.22212889 0.09473082 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.         0.02451524 0.24679654 0.24336471 0.2416157\n",
      " 0.         0.02451524 0.24679654 0.24336471 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.         0.02451524 0.24679654 0.24336471 0.2416157\n",
      " 0.         0.02451524 0.24679654 0.24336471 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.         0.02451524 0.24679654 0.24336471 0.2416157\n",
      " 0.         0.02451524 0.24679654 0.24336471 0.2416157  0.\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.0255814  0.02451524 0.24679654 0.24336471 0.2416157\n",
      " 0.0255814  0.02451524 0.24679654 0.24336471 0.2416157  0.0255814\n",
      " 0.02451524 0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471\n",
      " 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471 0.2416157\n",
      " 0.05061843 0.0451222  0.24679654 0.24336471 0.2416157  0.05061843\n",
      " 0.0451222  0.24679654 0.24336471        nan        nan        nan\n",
      "        nan        nan 0.2416157  0.05061843 0.0451222  0.24679654\n",
      " 0.24336471 0.2416157  0.05061843 0.0451222  0.24679654 0.24336471]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-05, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.24521431392706322\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-05, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.2817955112219451\n",
      "GridSearchCV Runtime: 6.619903564453125 secs\n",
      "Efron R-squared: -0.34916731933938094\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.23293172690763053\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.236\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.29411764705882354\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.29411764705882354\n",
      "Ave Test Precision: 0.2685762613479127\n",
      "Stdev Test Precision: 0.03134558296682887\n",
      "Ave Test Accuracy: 0.5388059701492537\n",
      "Stdev Test Accuracy: 0.2551485880038287\n",
      "Ave Test Specificity: 0.5831683168316831\n",
      "Ave Test Recall: 0.403030303030303\n",
      "Ave Test NPV: 0.6922175445765297\n",
      "Ave Test F1-Score: 0.22411331593548017\n",
      "Ave Test G-mean: 0.25267476404609096\n",
      "Ave Runtime: 0.009448099136352538\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasFreelancer. 26 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.         0.04875           nan 0.         0.         0.09709085\n",
      " 0.04875           nan 0.09512141 0.         0.22453706 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.07243421        nan 0.22453706 0.17209926 0.1984501\n",
      " 0.07243421        nan 0.22453706 0.16904371 0.22453706 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.07243421        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.07243421        nan 0.22453706 0.24919459 0.22453706 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.07243421        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.07243421        nan 0.22453706 0.24919459 0.22453706 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.07243421        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.07243421        nan 0.22453706 0.24919459 0.22453706 0.07243421\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.09417334        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.09417334        nan 0.22453706 0.24919459 0.22453706 0.09417334\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11778445        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11070112        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459\n",
      " 0.22453706 0.11778445        nan 0.22453706 0.24919459 0.22453706\n",
      " 0.11070112        nan 0.22453706 0.24919459 0.22453706 0.11778445\n",
      "        nan 0.23882277 0.24919459        nan        nan        nan\n",
      "        nan        nan 0.22453706 0.11070112        nan 0.22453706\n",
      " 0.24919459 0.22453706 0.11070112        nan 0.23882277 0.24919459]\n",
      "One or more of the train scores are non-finite: [0.         0.04524197        nan 0.         0.         0.09999804\n",
      " 0.04524197        nan 0.10022808 0.         0.22435173 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.07031843        nan 0.22435173 0.17476926 0.19954084\n",
      " 0.07031843        nan 0.22435173 0.17511243 0.22435173 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.07031843        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.07031843        nan 0.22435173 0.24931368 0.22435173 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.07031843        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.07031843        nan 0.22435173 0.24931368 0.22435173 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.07031843        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.07031843        nan 0.22435173 0.24931368 0.22435173 0.07031843\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.09558318        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.09558318        nan 0.22435173 0.24931368 0.22435173 0.09558318\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.12065917        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.11721755        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368\n",
      " 0.22435173 0.12065917        nan 0.22435173 0.24931368 0.22435173\n",
      " 0.11721755        nan 0.22435173 0.24931368 0.22435173 0.12065917\n",
      "        nan 0.24589019 0.24931368        nan        nan        nan\n",
      "        nan        nan 0.22435173 0.11721755        nan 0.22435173\n",
      " 0.24931368 0.22435173 0.11721755        nan 0.24589019 0.24931368]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.24919459136576155\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.29800498753117205\n",
      "GridSearchCV Runtime: 6.163161754608154 secs\n",
      "Efron R-squared: -0.34917143231517644\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.25311203319502074\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.26141078838174275\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25210084033613445\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.24481327800829875\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2625\n",
      "Ave Test Precision: 0.25478738798423933\n",
      "Stdev Test Precision: 0.007294879003087672\n",
      "Ave Test Accuracy: 0.31417910447761194\n",
      "Stdev Test Accuracy: 0.013297410209854815\n",
      "Ave Test Specificity: 0.11386138613861385\n",
      "Ave Test Recall: 0.9272727272727274\n",
      "Ave Test NPV: 0.8274603174603176\n",
      "Ave Test F1-Score: 0.39973679168221377\n",
      "Ave Test G-mean: 0.324780082136426\n",
      "Ave Runtime: 0.008667659759521485\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasGovtEmployee. 25 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22270722 0.08865947        nan 0.26361631 0.23634359 0.22270722\n",
      " 0.                nan 0.26361631 0.23634359 0.2399486  0.\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.08865947        nan 0.26361631 0.23634359 0.2399486\n",
      " 0.                nan 0.26361631 0.26098127 0.2399486  0.\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.08865947        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.                nan 0.26361631 0.26098127 0.2399486  0.\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.14032614        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.                nan 0.26361631 0.26098127 0.2399486  0.\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127\n",
      " 0.2399486  0.19917781        nan 0.26361631 0.26098127 0.2399486\n",
      " 0.19917781        nan 0.26361631 0.26098127 0.2399486  0.19917781\n",
      "        nan 0.26361631 0.26098127        nan        nan        nan\n",
      "        nan        nan 0.2399486  0.19917781        nan 0.26361631\n",
      " 0.26098127 0.2399486  0.19917781        nan 0.26361631 0.26098127]\n",
      "One or more of the train scores are non-finite: [0.24356555 0.10942141        nan 0.26821344 0.24141605 0.24356555\n",
      " 0.                nan 0.26821344 0.24141605 0.26822072 0.\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.10942141        nan 0.26821344 0.24141605 0.26822072\n",
      " 0.                nan 0.26821344 0.26531763 0.26822072 0.\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.10942141        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.                nan 0.26821344 0.26531763 0.26822072 0.\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.16335038        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.                nan 0.26821344 0.26531763 0.26822072 0.\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763\n",
      " 0.26822072 0.21635467        nan 0.26821344 0.26531763 0.26822072\n",
      " 0.21635467        nan 0.26821344 0.26531763 0.26822072 0.21635467\n",
      "        nan 0.26821344 0.26531763        nan        nan        nan\n",
      "        nan        nan 0.26822072 0.21635467        nan 0.26821344\n",
      " 0.26531763 0.26822072 0.21635467        nan 0.26821344 0.26531763]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2636163131673967\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6596009975062345\n",
      "GridSearchCV Runtime: 5.032087802886963 secs\n",
      "Efron R-squared: -0.3491729897248048\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.36538461538461536\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.27586206896551724\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3275862068965517\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3103448275862069\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2903225806451613\n",
      "Ave Test Precision: 0.3139000598956105\n",
      "Stdev Test Precision: 0.03483315960478191\n",
      "Ave Test Accuracy: 0.673134328358209\n",
      "Stdev Test Accuracy: 0.01858193969849065\n",
      "Ave Test Specificity: 0.803960396039604\n",
      "Ave Test Recall: 0.2727272727272727\n",
      "Ave Test NPV: 0.7717843016386705\n",
      "Ave Test F1-Score: 0.2916245215965008\n",
      "Ave Test G-mean: 0.46809626526532977\n",
      "Ave Runtime: 0.006918001174926758\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseHasOFW. 24 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.29961993 0.14510802 0.         0.29961993 0.29961993 0.29961993\n",
      " 0.09844136 0.         0.29961993 0.29961993 0.29961993 0.09844136\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.14510802 0.         0.29961993 0.29961993 0.29961993\n",
      " 0.09844136 0.         0.29961993 0.29961993 0.29961993 0.09844136\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.14510802 0.         0.29961993 0.29961993 0.29961993\n",
      " 0.09844136 0.         0.29961993 0.29961993 0.29961993 0.09844136\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.14510802 0.         0.29961993 0.29961993 0.29961993\n",
      " 0.09844136 0.         0.29961993 0.29961993 0.29961993 0.09844136\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.         0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.         0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.         0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993\n",
      " 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993\n",
      " 0.13918123 0.24568054 0.29961993 0.29961993 0.29961993 0.13918123\n",
      " 0.24568054 0.29961993 0.29961993        nan        nan        nan\n",
      "        nan        nan 0.29961993 0.13918123 0.24568054 0.29961993\n",
      " 0.29961993 0.29961993 0.13918123 0.24568054 0.29961993 0.29961993]\n",
      "One or more of the train scores are non-finite: [0.30323976 0.16032457 0.         0.30323976 0.30323976 0.30323976\n",
      " 0.09823345 0.         0.30323976 0.30323976 0.30323976 0.09823345\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.16032457 0.         0.30323976 0.30323976 0.30323976\n",
      " 0.09823345 0.         0.30323976 0.30323976 0.30323976 0.09823345\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.16032457 0.         0.30323976 0.30323976 0.30323976\n",
      " 0.09823345 0.         0.30323976 0.30323976 0.30323976 0.09823345\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.16032457 0.         0.30323976 0.30323976 0.30323976\n",
      " 0.09823345 0.         0.30323976 0.30323976 0.30323976 0.09823345\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.         0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.         0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.         0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976\n",
      " 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976\n",
      " 0.14973624 0.24176801 0.30323976 0.30323976 0.30323976 0.14973624\n",
      " 0.24176801 0.30323976 0.30323976        nan        nan        nan\n",
      "        nan        nan 0.30323976 0.14973624 0.24176801 0.30323976\n",
      " 0.30323976 0.30323976 0.14973624 0.24176801 0.30323976 0.30323976]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2996199340112383\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6346633416458853\n",
      "GridSearchCV Runtime: 4.955168724060059 secs\n",
      "Efron R-squared: -0.3491763006524897\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.20512820512820512\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.38666666666666666\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2328767123287671\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2558139534883721\n",
      "Ave Test Precision: 0.26609710752240223\n",
      "Stdev Test Precision: 0.07021873047074419\n",
      "Ave Test Accuracy: 0.6134328358208955\n",
      "Stdev Test Accuracy: 0.04403933619050918\n",
      "Ave Test Specificity: 0.7089108910891089\n",
      "Ave Test Recall: 0.32121212121212117\n",
      "Ave Test NPV: 0.7615295046437669\n",
      "Ave Test F1-Score: 0.29067240528485866\n",
      "Ave Test G-mean: 0.47496608666078843\n",
      "Ave Runtime: 0.008432435989379882\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseOnlyFamily. 23 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.31037195 0.14936077        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.                nan 0.31037195 0.31037195 0.31037195 0.\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.14936077        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.                nan 0.31037195 0.31037195 0.31037195 0.\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.14936077        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.                nan 0.31037195 0.31037195 0.31037195 0.\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.14936077        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.                nan 0.31037195 0.31037195 0.31037195 0.\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.14936077        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.14936077        nan 0.31037195 0.31037195 0.31037195 0.14936077\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.14936077        nan 0.31037195 0.31037195 0.31037195\n",
      " 0.14936077        nan 0.31037195 0.31037195 0.31037195 0.14936077\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195\n",
      " 0.31037195 0.1626941         nan 0.31037195 0.31037195 0.31037195\n",
      " 0.1626941         nan 0.31037195 0.31037195 0.31037195 0.1626941\n",
      "        nan 0.31037195 0.31037195        nan        nan        nan\n",
      "        nan        nan 0.31037195 0.1626941         nan 0.31037195\n",
      " 0.31037195 0.31037195 0.1626941         nan 0.31037195 0.31037195]\n",
      "One or more of the train scores are non-finite: [0.31918216 0.12503367        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.                nan 0.31918216 0.31918216 0.31918216 0.\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.12503367        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.                nan 0.31918216 0.31918216 0.31918216 0.\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.12503367        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.                nan 0.31918216 0.31918216 0.31918216 0.\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.12503367        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.                nan 0.31918216 0.31918216 0.31918216 0.\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.12503367        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.12503367        nan 0.31918216 0.31918216 0.31918216 0.12503367\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.12503367        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.12503367        nan 0.31918216 0.31918216 0.31918216 0.12503367\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216\n",
      " 0.31918216 0.15880851        nan 0.31918216 0.31918216 0.31918216\n",
      " 0.15880851        nan 0.31918216 0.31918216 0.31918216 0.15880851\n",
      "        nan 0.31918216 0.31918216        nan        nan        nan\n",
      "        nan        nan 0.31918216 0.15880851        nan 0.31918216\n",
      " 0.31918216 0.31918216 0.15880851        nan 0.31918216 0.31918216]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.3103719517496607\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6795511221945137\n",
      "GridSearchCV Runtime: 5.177691459655762 secs\n",
      "Efron R-squared: -0.34917428083529933\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.24193548387096775\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2698412698412698\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.4230769230769231\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2978723404255319\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.25396825396825395\n",
      "Ave Test Precision: 0.2973388542365893\n",
      "Stdev Test Precision: 0.07334338096633872\n",
      "Ave Test Accuracy: 0.6649253731343284\n",
      "Stdev Test Accuracy: 0.03818023057398948\n",
      "Ave Test Specificity: 0.7990099009900989\n",
      "Ave Test Recall: 0.2545454545454545\n",
      "Ave Test NPV: 0.7661005067693928\n",
      "Ave Test F1-Score: 0.27333437470568334\n",
      "Ave Test G-mean: 0.4497861963040686\n",
      "Ave Runtime: 0.008329534530639648\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with houseExtendedFamily. 22 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.28341881 0.30843881 0.22833333 0.28341881 0.2621076  0.28341881\n",
      " 0.04969136 0.         0.28341881 0.26152789 0.29806797 0.04969136\n",
      " 0.         0.29892314 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.28397153 0.30843881 0.22833333 0.28397153 0.26086501 0.28397153\n",
      " 0.04969136 0.         0.28397153 0.26086501 0.29806797 0.04969136\n",
      " 0.         0.29892314 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.28465181 0.31389007 0.22833333 0.28465181 0.26086501 0.28957046\n",
      " 0.04969136 0.         0.29892314 0.2661736  0.29806797 0.04969136\n",
      " 0.         0.29892314 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.28875481 0.34712071 0.35833333 0.2896083  0.26235391 0.29806797\n",
      " 0.24135802 0.         0.29816683 0.27554335 0.29806797 0.24135802\n",
      " 0.         0.29816683 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.28947671 0.42689708 0.43807692 0.28998094 0.26387487 0.29806797\n",
      " 0.32367242 0.3        0.29816683 0.27554335 0.29806797 0.32367242\n",
      " 0.3        0.29816683 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29440482 0.41445744 0.34813897 0.29129142 0.2730491  0.29544649\n",
      " 0.41029576 0.34261822 0.30113126 0.27554335 0.29667801 0.41029576\n",
      " 0.34261822 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35744222 0.29838401 0.27554335 0.29544649\n",
      " 0.40960895 0.35839424 0.29838401 0.27554335 0.29667801 0.40960895\n",
      " 0.35839424 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35592707 0.29838401 0.27554335 0.29544649\n",
      " 0.41491198 0.35592707 0.29838401 0.27554335 0.29667801 0.40960895\n",
      " 0.35592707 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35592707 0.29838401 0.27554335 0.29544649\n",
      " 0.41491198 0.35592707 0.29838401 0.27554335 0.29667801 0.40960895\n",
      " 0.35592707 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35592707 0.29838401 0.27554335 0.29544649\n",
      " 0.41491198 0.35592707 0.29838401 0.27554335 0.29667801 0.41491198\n",
      " 0.35592707 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35592707 0.29838401 0.27554335 0.29544649\n",
      " 0.41491198 0.35592707 0.29838401 0.27554335 0.29667801 0.41491198\n",
      " 0.35592707 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35592707 0.29838401 0.27554335 0.29544649\n",
      " 0.41491198 0.35592707 0.29838401 0.27554335 0.29667801 0.41491198\n",
      " 0.35592707 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335\n",
      " 0.29544649 0.41491198 0.35592707 0.29838401 0.27554335 0.29544649\n",
      " 0.41491198 0.35592707 0.29838401 0.27554335 0.29667801 0.41491198\n",
      " 0.35592707 0.30113126 0.27554335        nan        nan        nan\n",
      "        nan        nan 0.29544649 0.41491198 0.35592707 0.29838401\n",
      " 0.27554335 0.29667801 0.41491198 0.35592707 0.30113126 0.27554335]\n",
      "One or more of the train scores are non-finite: [0.2856378  0.2933594  0.34086814 0.2856378  0.25202424 0.2856378\n",
      " 0.04906447 0.         0.2856378  0.252463   0.29533539 0.04906447\n",
      " 0.         0.29778371 0.26544085        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.28580001 0.2933594  0.34441699 0.28580001 0.25250385 0.28580001\n",
      " 0.04906447 0.         0.28580001 0.25250385 0.29533539 0.04906447\n",
      " 0.         0.29778371 0.26544085        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.2863845  0.29306972 0.34441699 0.2863845  0.2524486  0.29332285\n",
      " 0.04906447 0.         0.2981182  0.25522607 0.29533539 0.04906447\n",
      " 0.         0.29778371 0.26544085        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29001797 0.2955035  0.32130187 0.29103067 0.25320988 0.29559315\n",
      " 0.31316811 0.         0.2982698  0.26515107 0.29559315 0.31316811\n",
      " 0.         0.2982698  0.26515107        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.28735149 0.30015886 0.33957052 0.28806455 0.26142481 0.29544123\n",
      " 0.30131265 0.31071078 0.29900407 0.26515107 0.29544123 0.30276379\n",
      " 0.31071078 0.29900407 0.26515107        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29393078 0.30698773 0.33156927 0.2974003  0.26237361 0.29520762\n",
      " 0.31067495 0.33565592 0.29870723 0.26442213 0.29536663 0.30845721\n",
      " 0.33565592 0.29870723 0.26437569        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29445094 0.31039453 0.32183095 0.29826983 0.26451487 0.29520762\n",
      " 0.31184991 0.32383637 0.29860028 0.26442213 0.29536663 0.31142982\n",
      " 0.32383637 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29520762 0.31039453 0.32131088 0.29860028 0.26442213 0.29520762\n",
      " 0.31039453 0.32232494 0.29860028 0.26442213 0.29536663 0.31144303\n",
      " 0.32232494 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29520762 0.31039453 0.32131088 0.29860028 0.26442213 0.29520762\n",
      " 0.31039453 0.32131088 0.29860028 0.26442213 0.29536663 0.31144303\n",
      " 0.32131088 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29520762 0.31039453 0.32131088 0.29860028 0.26442213 0.29520762\n",
      " 0.31039453 0.32131088 0.29860028 0.26442213 0.29536663 0.31102109\n",
      " 0.32131088 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29520762 0.31039453 0.32131088 0.29860028 0.26442213 0.29520762\n",
      " 0.31039453 0.32131088 0.29860028 0.26442213 0.29536663 0.31102109\n",
      " 0.32131088 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29520762 0.31039453 0.32131088 0.29860028 0.26442213 0.29520762\n",
      " 0.31039453 0.32131088 0.29860028 0.26442213 0.29536663 0.31102109\n",
      " 0.32131088 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213\n",
      " 0.29520762 0.31039453 0.32131088 0.29860028 0.26442213 0.29520762\n",
      " 0.31039453 0.32131088 0.29860028 0.26442213 0.29536663 0.31102109\n",
      " 0.32131088 0.29870723 0.26442213        nan        nan        nan\n",
      "        nan        nan 0.29520762 0.31039453 0.32131088 0.29860028\n",
      " 0.26442213 0.29536663 0.31102109 0.32131088 0.29870723 0.26442213]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.43807692307692303\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7394014962593516\n",
      "GridSearchCV Runtime: 5.95715594291687 secs\n",
      "Efron R-squared: -0.18553956103525415\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.23076923076923078\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.5\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.5\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.14285714285714285\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2727272727272727\n",
      "Ave Test Precision: 0.32927072927072926\n",
      "Stdev Test Precision: 0.16274727390343235\n",
      "Ave Test Accuracy: 0.741044776119403\n",
      "Stdev Test Accuracy: 0.011975228758120283\n",
      "Ave Test Specificity: 0.9742574257425742\n",
      "Ave Test Recall: 0.02727272727272727\n",
      "Ave Test NPV: 0.7540082079190025\n",
      "Ave Test F1-Score: 0.048018446939284566\n",
      "Ave Test G-mean: 0.15672304849874294\n",
      "Ave Runtime: 0.0109893798828125\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyUtilityBills. 21 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24188518 0.21063518 0.2535305  0.24188518 0.24188518 0.2559535\n",
      " 0.26767399 0.32306798 0.2517402  0.2690517  0.2649534  0.26767399\n",
      " 0.32306798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.24188518 0.21682616 0.25344155 0.24188518 0.24188518 0.2517402\n",
      " 0.25690476 0.28806798 0.25884546 0.2716158  0.2649534  0.26767399\n",
      " 0.28806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.24262702 0.20896304 0.26746362 0.23854409 0.24340034 0.26352926\n",
      " 0.26767399 0.27806798 0.26352926 0.2690517  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.23753447 0.22209955 0.27719136 0.23414265 0.23428799 0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.26315    0.25357143 0.27905033 0.24266585 0.24210507 0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.27005495 0.27806798 0.25239289 0.26419397 0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405\n",
      " 0.2649534  0.26767399 0.27806798 0.25239289 0.2690517  0.26620783\n",
      " 0.26767399 0.27806798 0.26142399 0.2716158  0.2649534  0.26767399\n",
      " 0.27806798 0.26642399 0.27144405        nan        nan        nan\n",
      "        nan        nan 0.26620783 0.26767399 0.27806798 0.26142399\n",
      " 0.2716158  0.2649534  0.26767399 0.27806798 0.26642399 0.27144405]\n",
      "One or more of the train scores are non-finite: [0.24056735 0.21732792 0.24289783 0.24056735 0.24056735 0.2582042\n",
      " 0.32344983 0.34163165 0.25915094 0.26076208 0.26522162 0.32344983\n",
      " 0.34163165 0.2676002  0.26734693        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.24056735 0.22122198 0.24250732 0.24056735 0.24056735 0.25843653\n",
      " 0.32073301 0.30848347 0.25830426 0.25742391 0.26522162 0.32307946\n",
      " 0.30848347 0.2676002  0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.24504329 0.2161901  0.25047042 0.24504618 0.24311069 0.25999065\n",
      " 0.32307946 0.30848347 0.26112692 0.26027291 0.26522162 0.32307946\n",
      " 0.30848347 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.24076939 0.2396737  0.29723856 0.24104709 0.2398313  0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.2603177  0.32579065 0.31011153 0.25073555 0.24466804 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32323322 0.30718292 0.25765249 0.25850629 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26162012 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26095249 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26095249 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26095249 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26095249 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26095249 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776\n",
      " 0.26522162 0.32509966 0.30711092 0.25765249 0.26095249 0.25637824\n",
      " 0.32307946 0.30711092 0.26454418 0.25742391 0.26522162 0.32307946\n",
      " 0.30711092 0.26735748 0.26685776        nan        nan        nan\n",
      "        nan        nan 0.25637824 0.32307946 0.30711092 0.26454418\n",
      " 0.25742391 0.26522162 0.32307946 0.30711092 0.26735748 0.26685776]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.32306798140131476\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7481296758104738\n",
      "GridSearchCV Runtime: 6.426344156265259 secs\n",
      "Efron R-squared: -0.01570082486467328\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.42857142857142855\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.36363636363636365\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.36363636363636365\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4444444444444444\n",
      "Ave Test Precision: 0.38672438672438675\n",
      "Stdev Test Precision: 0.04743284246433938\n",
      "Ave Test Accuracy: 0.7470149253731343\n",
      "Stdev Test Accuracy: 0.004087481772426625\n",
      "Ave Test Specificity: 0.9752475247524753\n",
      "Ave Test Recall: 0.048484848484848485\n",
      "Ave Test NPV: 0.7582930318062935\n",
      "Ave Test F1-Score: 0.08512723250543382\n",
      "Ave Test G-mean: 0.2120873200015206\n",
      "Ave Runtime: 0.015245532989501953\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyVices. 20 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.27559308 0.26685048 0.29864253 0.27559308 0.27559308 0.27559308\n",
      " 0.14563272 0.         0.27559308 0.27559308 0.27902625 0.14563272\n",
      " 0.         0.28040213 0.27753464        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.27559308 0.26685048 0.29944899 0.27559308 0.27559308 0.27559308\n",
      " 0.24563272 0.         0.27559308 0.27559308 0.27902625 0.24563272\n",
      " 0.         0.28040213 0.27753464        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.27018457 0.26747012 0.30197587 0.27113154 0.27629821 0.27902625\n",
      " 0.23433121 0.         0.28040213 0.26985997 0.27902625 0.23175768\n",
      " 0.         0.28040213 0.27753464        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.26721534 0.25836767 0.32592671 0.26721534 0.26725493 0.27902625\n",
      " 0.27161781 0.05       0.28210343 0.27753464 0.27902625 0.27161781\n",
      " 0.05       0.28210343 0.27753464        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.27903616 0.27133802 0.34142485 0.276148   0.27536741 0.27784107\n",
      " 0.28351995 0.37452381 0.28169809 0.27607186 0.27784107 0.28351995\n",
      " 0.37452381 0.28169809 0.27607186        nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28497493 0.28372634 0.35722222 0.28560985 0.27801488 0.28308099\n",
      " 0.28141536 0.38029762 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.38029762 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28081476 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.37009921 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.37009921 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.36340803 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.36340803 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.36340803 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.36340803 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.36340803 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262\n",
      " 0.28308099 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099\n",
      " 0.28141536 0.36340803 0.28103142 0.2768262  0.28308099 0.28141536\n",
      " 0.36340803 0.28103142 0.2768262         nan        nan        nan\n",
      "        nan        nan 0.28308099 0.28141536 0.36340803 0.28103142\n",
      " 0.2768262  0.28308099 0.28141536 0.36340803 0.28103142 0.2768262 ]\n",
      "One or more of the train scores are non-finite: [0.26930146 0.26762252 0.3089438  0.26930146 0.26930146 0.26930146\n",
      " 0.14757493 0.         0.26930146 0.26930146 0.27898251 0.14757493\n",
      " 0.         0.27847205 0.28029106        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.26937379 0.26762252 0.30865565 0.26937379 0.26930146 0.26937379\n",
      " 0.2792416  0.         0.26937379 0.26930146 0.27898251 0.2792416\n",
      " 0.         0.27847205 0.28029106        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.268036   0.26820301 0.31026667 0.268036   0.26845701 0.27909351\n",
      " 0.29406433 0.         0.27859526 0.27456115 0.27898251 0.29435159\n",
      " 0.         0.27847205 0.28029106        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.26531896 0.26769462 0.31037838 0.26569284 0.2652553  0.27875237\n",
      " 0.27557168 0.4452381  0.27831342 0.28015878 0.27875237 0.27557168\n",
      " 0.4452381  0.27831342 0.28015878        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27537047 0.27233797 0.33821835 0.27450209 0.27123192 0.2790534\n",
      " 0.27624563 0.42440367 0.27928318 0.28077078 0.2790534  0.27624563\n",
      " 0.42440367 0.27928318 0.28077078        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.28008782 0.27602334 0.34323621 0.27908157 0.27867439 0.27863227\n",
      " 0.27739138 0.35313799 0.27908564 0.28019789 0.27863227 0.27739138\n",
      " 0.35313799 0.27908564 0.28019789        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27943248 0.2768604  0.34449406 0.27904163 0.27977057 0.27911561\n",
      " 0.27744748 0.34405652 0.27896561 0.28034557 0.27851643 0.27744748\n",
      " 0.34405652 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27911561 0.2770838  0.34570448 0.27926877 0.28020991 0.27911561\n",
      " 0.27730812 0.34506968 0.27896561 0.28034557 0.27851643 0.27730812\n",
      " 0.34374078 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27911561 0.27730812 0.34570448 0.27896561 0.28020991 0.27911561\n",
      " 0.27730812 0.34570448 0.27896561 0.28034557 0.27851643 0.27730812\n",
      " 0.34437558 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27911561 0.27730812 0.34570448 0.27896561 0.28034557 0.27911561\n",
      " 0.27730812 0.34570448 0.27896561 0.28034557 0.27851643 0.27730812\n",
      " 0.34437558 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27911561 0.27730812 0.34570448 0.27896561 0.28034557 0.27911561\n",
      " 0.27730812 0.34570448 0.27896561 0.28034557 0.27851643 0.27730812\n",
      " 0.34437558 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27911561 0.27730812 0.34570448 0.27896561 0.28034557 0.27911561\n",
      " 0.27730812 0.34570448 0.27896561 0.28034557 0.27851643 0.27730812\n",
      " 0.34437558 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557\n",
      " 0.27911561 0.27730812 0.34570448 0.27896561 0.28034557 0.27911561\n",
      " 0.27730812 0.34570448 0.27896561 0.28034557 0.27851643 0.27730812\n",
      " 0.34437558 0.27896561 0.28034557        nan        nan        nan\n",
      "        nan        nan 0.27911561 0.27730812 0.34570448 0.27896561\n",
      " 0.28034557 0.27851643 0.27730812 0.34437558 0.27896561 0.28034557]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.380297619047619\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7119700748129676\n",
      "GridSearchCV Runtime: 5.957436561584473 secs\n",
      "Efron R-squared: -0.1850071501594559\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.3225806451612903\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.5714285714285714\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.5\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3157894736842105\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2857142857142857\n",
      "Ave Test Precision: 0.39910259519767155\n",
      "Stdev Test Precision: 0.12799425979410642\n",
      "Ave Test Accuracy: 0.7283582089552239\n",
      "Stdev Test Accuracy: 0.030473905953873746\n",
      "Ave Test Specificity: 0.9287128712871286\n",
      "Ave Test Recall: 0.11515151515151514\n",
      "Ave Test NPV: 0.7626080945065177\n",
      "Ave Test F1-Score: 0.1653454233927248\n",
      "Ave Test G-mean: 0.3139468889712652\n",
      "Ave Runtime: 0.009794473648071289\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyExpenses. 19 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.206957   0.2070859  0.36390693 0.206957   0.27026184 0.206957\n",
      " 0.10944878 0.         0.20645866 0.2697635  0.20121743 0.11718034\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20645866 0.20486816 0.36390693 0.20645866 0.2697635  0.20577725\n",
      " 0.11843335 0.         0.20694236 0.26944344 0.20121743 0.11748097\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20787322 0.21053384 0.36525974 0.20787322 0.27117806 0.20746609\n",
      " 0.18497553 0.         0.20632572 0.26860801 0.20121743 0.18607443\n",
      " 0.         0.19714027 0.27106964        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20912052 0.2129899  0.3552381  0.20872158 0.27294574 0.20317587\n",
      " 0.22124032 0.         0.20258582 0.27071881 0.20121743 0.22124032\n",
      " 0.         0.19760485 0.27071881        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20154865 0.25548036 0.37285714 0.20119575 0.26782754 0.20317587\n",
      " 0.23358949 0.36666667 0.19997585 0.27000482 0.20121743 0.23358949\n",
      " 0.36666667 0.19760485 0.27000482        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20169311 0.25746124 0.38333333 0.1998966  0.26986083 0.20317587\n",
      " 0.25898576 0.4        0.19997585 0.27063283 0.20121743 0.25898576\n",
      " 0.4        0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.38333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.38333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283]\n",
      "One or more of the train scores are non-finite: [0.21406456 0.21753168 0.34579767 0.21433556 0.25980589 0.21485609\n",
      " 0.11555348 0.         0.21524006 0.2605446  0.21190105 0.11858785\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21524006 0.21779676 0.3463148  0.21524006 0.26046086 0.21693595\n",
      " 0.18597355 0.         0.21607005 0.26201811 0.21190105 0.1852493\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21499148 0.21400501 0.34650148 0.21507196 0.26053989 0.21511944\n",
      " 0.2913795  0.         0.21487681 0.26369047 0.21176171 0.29039832\n",
      " 0.         0.21053761 0.26395912        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2174422  0.22071031 0.35897384 0.2172269  0.26169073 0.21319033\n",
      " 0.28575602 0.16666667 0.21490766 0.26352371 0.21179116 0.28575602\n",
      " 0.16666667 0.2106657  0.26352371        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21301237 0.24906234 0.38825152 0.21245925 0.25972252 0.21316659\n",
      " 0.26871203 0.47803207 0.21228805 0.26231435 0.21176742 0.26930026\n",
      " 0.47803207 0.21081978 0.26231435        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21130223 0.26264816 0.41770404 0.21152949 0.26028815 0.21310005\n",
      " 0.26431071 0.42410912 0.21241002 0.2625036  0.2117339  0.26431071\n",
      " 0.42410912 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21178291 0.26243359 0.41615745 0.21108015 0.26219953 0.21310005\n",
      " 0.26431071 0.41599638 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41599638 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26210887 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.39999999999999997\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7518703241895262\n",
      "GridSearchCV Runtime: 6.300202369689941 secs\n",
      "Efron R-squared: -0.15776698146763035\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3870967741935484\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2727272727272727\n",
      "Ave Test Precision: 0.32910766652702134\n",
      "Stdev Test Precision: 0.059785972659199466\n",
      "Ave Test Accuracy: 0.7358208955223879\n",
      "Stdev Test Accuracy: 0.007177382112563941\n",
      "Ave Test Specificity: 0.9504950495049505\n",
      "Ave Test Recall: 0.0787878787878788\n",
      "Ave Test NPV: 0.759685019948753\n",
      "Ave Test F1-Score: 0.12191105693004774\n",
      "Ave Test G-mean: 0.2613548711958696\n",
      "Ave Runtime: 0.010165739059448241\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlySoloNetIncome. 18 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1105 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "182 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "364 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "559 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.07694209 0.                nan 0.07421482 0.         0.25989423\n",
      " 0.                nan 0.25989423 0.1774121  0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.                nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.                nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.                nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.07478773        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.07478773        nan 0.25989423 0.25989423 0.25989423 0.07478773\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.25989423        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.25989423        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423]\n",
      "One or more of the train scores are non-finite: [0.07770881 0.                nan 0.07804872 0.         0.25887731\n",
      " 0.                nan 0.25887731 0.18165174 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.                nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.                nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.                nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.07799125        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.07799125        nan 0.25887731 0.25887731 0.25887731 0.07799125\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.25887731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.25887731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.259894230383722\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.415211970074813\n",
      "GridSearchCV Runtime: 6.119722366333008 secs\n",
      "Efron R-squared: -0.3491673138540201\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2736842105263158\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25263157894736843\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.22941176470588234\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.28888888888888886\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2512820512820513\n",
      "Ave Test Precision: 0.25917969887010134\n",
      "Stdev Test Precision: 0.022827600872137695\n",
      "Ave Test Accuracy: 0.42164179104477606\n",
      "Stdev Test Accuracy: 0.03099486516014207\n",
      "Ave Test Specificity: 0.32178217821782173\n",
      "Ave Test Recall: 0.7272727272727272\n",
      "Ave Test NPV: 0.7844531528484562\n",
      "Ave Test F1-Score: 0.38200032588432253\n",
      "Ave Test G-mean: 0.48221180258827284\n",
      "Ave Runtime: 0.015737438201904298\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positiveMonthlySoloNetIncome. 17 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22669875 0.23653443 0.23010684 0.22669875 0.23660771 0.22669875\n",
      " 0.20939724 0.         0.22669875 0.23660771 0.24001419 0.2147074\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.22669875 0.23602161 0.23010684 0.22669875 0.23660771 0.2404398\n",
      " 0.21500803 0.         0.24285524 0.24691285 0.24001419 0.21500803\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.22956664 0.23467117 0.23189255 0.22956664 0.23660771 0.24096413\n",
      " 0.24062621 0.         0.23992435 0.2461551  0.23949354 0.24062621\n",
      " 0.         0.23992435 0.24888676        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.24003546 0.24115818 0.25315129 0.24003546 0.24378851 0.23748021\n",
      " 0.24270697 0.         0.23375239 0.24922126 0.23748021 0.24270697\n",
      " 0.         0.23375239 0.24922126        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23049107 0.24537943 0.24583333 0.23296233 0.23747562 0.23663853\n",
      " 0.24188832 0.42333333 0.23169206 0.24742561 0.23663853 0.24188832\n",
      " 0.42333333 0.23169206 0.24742561        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23783161 0.23921981 0.24345238 0.23200984 0.24163056 0.23663853\n",
      " 0.2421235  0.25119048 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.25119048 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24526455 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957]\n",
      "One or more of the train scores are non-finite: [0.23477526 0.23770697 0.29560901 0.23477526 0.24626193 0.23477526\n",
      " 0.219768   0.         0.23477526 0.24647661 0.2455207  0.22102263\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.23464593 0.23797083 0.29560901 0.23485901 0.24641156 0.24556157\n",
      " 0.26114871 0.         0.24533465 0.25108246 0.2455207  0.26114871\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.23676889 0.23825403 0.29807227 0.23697656 0.24683148 0.24417298\n",
      " 0.24111749 0.         0.24446346 0.2501251  0.24476588 0.24111749\n",
      " 0.         0.24446346 0.25065147        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24353191 0.23967955 0.30121849 0.24338633 0.24618009 0.24286218\n",
      " 0.24167568 0.095      0.24396914 0.2503354  0.24286218 0.24167568\n",
      " 0.095      0.24396914 0.2503354         nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.23543138 0.24041165 0.32269832 0.23569409 0.24570293 0.24142589\n",
      " 0.2394545  0.31088804 0.24285622 0.24972114 0.24152145 0.2394545\n",
      " 0.31088804 0.24290218 0.24972114        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24052581 0.23980302 0.33679572 0.24101142 0.24929717 0.24107228\n",
      " 0.23959051 0.34914591 0.24289462 0.24985954 0.24116783 0.23955247\n",
      " 0.34914591 0.24289462 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24089904 0.23929204 0.3369035  0.24249328 0.24991287 0.24111965\n",
      " 0.23935736 0.33823487 0.24289462 0.24985954 0.2412152  0.23935736\n",
      " 0.33823487 0.24289462 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24126215 0.23941048 0.3369035  0.24249328 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24990407        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.42333333333333334\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7468827930174564\n",
      "GridSearchCV Runtime: 6.170670986175537 secs\n",
      "Efron R-squared: -0.20525021574232527\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.38461538461538464\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.4117647058823529\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2916666666666667\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.4444444444444444\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3333333333333333\n",
      "Ave Test Precision: 0.3731649069884364\n",
      "Stdev Test Precision: 0.061094901194505016\n",
      "Ave Test Accuracy: 0.7365671641791045\n",
      "Stdev Test Accuracy: 0.013086326803544922\n",
      "Ave Test Specificity: 0.9485148514851485\n",
      "Ave Test Recall: 0.08787878787878789\n",
      "Ave Test NPV: 0.7609078069397102\n",
      "Ave Test F1-Score: 0.1400672684711117\n",
      "Ave Test G-mean: 0.2868883094391115\n",
      "Ave Runtime: 0.01091470718383789\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyNetIncome. 16 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.09629953 0.                nan 0.15781469 0.14781469 0.09629953\n",
      " 0.                nan 0.1813441  0.14781469 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.                nan 0.19118519 0.22013531 0.19128534\n",
      " 0.                nan 0.1813441  0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.                nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.                nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.03068182        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531]\n",
      "One or more of the train scores are non-finite: [0.12870734 0.                nan 0.1505025  0.1512757  0.12870734\n",
      " 0.                nan 0.17506669 0.1512757  0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.                nan 0.19943274 0.22476988 0.22687699\n",
      " 0.                nan 0.17506669 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.                nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.                nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.05240275        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.22195442571788887\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.6907730673316709\n",
      "GridSearchCV Runtime: 6.431148529052734 secs\n",
      "Efron R-squared: -0.34917143678365536\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.21621621621621623\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24669603524229075\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2409090909090909\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2565217391304348\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2297872340425532\n",
      "Ave Test Precision: 0.2380260631081172\n",
      "Stdev Test Precision: 0.015565330252285143\n",
      "Ave Test Accuracy: 0.3888059701492537\n",
      "Stdev Test Accuracy: 0.16168819361247286\n",
      "Ave Test Specificity: 0.2881188118811881\n",
      "Ave Test Recall: 0.696969696969697\n",
      "Ave Test NPV: 0.7372670173215744\n",
      "Ave Test F1-Score: 0.33313487430823757\n",
      "Ave Test G-mean: 0.34361955132076955\n",
      "Ave Runtime: 0.01027069091796875\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positiveMonthlyFamilyNetIncome. 15 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.21850877 0.22681418 0.33063007 0.25359649 0.26797559 0.21850877\n",
      " 0.19594136 0.         0.25359649 0.26797559 0.23118435 0.19594136\n",
      " 0.         0.25455179 0.28232675        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.21904017 0.22810789 0.33063007 0.25412789 0.26797559 0.22675879\n",
      " 0.19499199 0.         0.25729216 0.26937657 0.23118435 0.19499199\n",
      " 0.         0.25455179 0.28232675        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.21912339 0.21761647 0.34063007 0.25627872 0.268507   0.23253476\n",
      " 0.19046581 0.         0.25068354 0.27891113 0.23118435 0.19046581\n",
      " 0.         0.25455179 0.28232675        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22787433 0.23231653 0.36298319 0.25882005 0.26885671 0.22942997\n",
      " 0.20539879 0.05       0.25455179 0.28232675 0.22942997 0.20539879\n",
      " 0.05       0.25455179 0.28232675        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22181008 0.20984022 0.40857143 0.257009   0.27549028 0.2261919\n",
      " 0.21858358 0.41785714 0.25618208 0.28162192 0.22785102 0.21858358\n",
      " 0.41785714 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.23187888 0.21679889 0.41829365 0.25276443 0.2777616  0.2261919\n",
      " 0.21919334 0.41412698 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.41412698 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22823505 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.43365079 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.43365079 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.42698413 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.42698413 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.42698413 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.42698413 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.42698413 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192\n",
      " 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192 0.2261919\n",
      " 0.21919334 0.42698413 0.25682725 0.28162192 0.22785102 0.21919334\n",
      " 0.42698413 0.25374965 0.28162192        nan        nan        nan\n",
      "        nan        nan 0.2261919  0.21919334 0.42698413 0.25682725\n",
      " 0.28162192 0.22785102 0.21919334 0.42698413 0.25374965 0.28162192]\n",
      "One or more of the train scores are non-finite: [0.25930101 0.23055323 0.3362354  0.27536749 0.26290519 0.25924638\n",
      " 0.1965714  0.         0.27531286 0.26284902 0.26901286 0.1965714\n",
      " 0.         0.28356117 0.2757061         nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.25935288 0.23058684 0.3362354  0.27563527 0.2627396  0.26223427\n",
      " 0.19663719 0.         0.27672691 0.26514506 0.26901286 0.19663719\n",
      " 0.         0.28356117 0.2757061         nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.2595868  0.23380997 0.33181331 0.2762616  0.2633811  0.26775278\n",
      " 0.23869682 0.         0.28338591 0.27040037 0.26901286 0.22917301\n",
      " 0.         0.28356117 0.2757061         nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.26202197 0.24687007 0.3382123  0.27949116 0.26644983 0.26880455\n",
      " 0.28174169 0.3952381  0.28332307 0.27572434 0.2689841  0.28174169\n",
      " 0.3952381  0.28318943 0.27572434        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.26693919 0.26489394 0.33605412 0.28484823 0.27270896 0.26744138\n",
      " 0.28100176 0.39687637 0.28288093 0.27665697 0.26992009 0.28100176\n",
      " 0.39687637 0.28317719 0.27665697        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27007994 0.27862952 0.35468991 0.2820084  0.27540482 0.26806877\n",
      " 0.27911858 0.37929045 0.28244533 0.27691334 0.27040912 0.27911858\n",
      " 0.37929045 0.28317719 0.27691334        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27037414 0.28012028 0.3706446  0.28267927 0.27584733 0.26806803\n",
      " 0.27911858 0.36928415 0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.36928415 0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27040912 0.27911858 0.369257   0.28317719 0.27680581 0.26806803\n",
      " 0.27911858 0.369257   0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.369257   0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27040912 0.27911858 0.369257   0.28317719 0.27680581 0.26806803\n",
      " 0.27911858 0.369257   0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.369257   0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27040912 0.27911858 0.369257   0.28317719 0.27680581 0.26806803\n",
      " 0.27911858 0.369257   0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.369257   0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27040912 0.27911858 0.369257   0.28317719 0.27680581 0.26806803\n",
      " 0.27911858 0.369257   0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.369257   0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27040912 0.27911858 0.369257   0.28317719 0.27680581 0.26806803\n",
      " 0.27911858 0.369257   0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.369257   0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581\n",
      " 0.27040912 0.27911858 0.369257   0.28317719 0.27680581 0.26806803\n",
      " 0.27911858 0.369257   0.28244533 0.27680581 0.27040912 0.27911858\n",
      " 0.369257   0.28317719 0.27680581        nan        nan        nan\n",
      "        nan        nan 0.26806803 0.27911858 0.369257   0.28244533\n",
      " 0.27680581 0.27040912 0.27911858 0.369257   0.28317719 0.27680581]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.4336507936507936\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7344139650872819\n",
      "GridSearchCV Runtime: 5.750536918640137 secs\n",
      "Efron R-squared: -0.18485833269145102\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4375\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2903225806451613\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.47619047619047616\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2631578947368421\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3103448275862069\n",
      "Ave Test Precision: 0.35550315583173725\n",
      "Stdev Test Precision: 0.09500571165355737\n",
      "Ave Test Accuracy: 0.726865671641791\n",
      "Stdev Test Accuracy: 0.02016306878523329\n",
      "Ave Test Specificity: 0.9247524752475247\n",
      "Ave Test Recall: 0.12121212121212119\n",
      "Ave Test NPV: 0.7630344790915451\n",
      "Ave Test F1-Score: 0.17866090362633433\n",
      "Ave Test G-mean: 0.33229385584329857\n",
      "Ave Runtime: 0.013065958023071289\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlySoloNetIncomeWithSavings. 14 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "910 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "104 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "208 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "598 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.15952403 0.                nan 0.12931614 0.06       0.15985116\n",
      " 0.                nan 0.2274904  0.06       0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20463497 0.                nan 0.2274904  0.22905116 0.20721634\n",
      " 0.                nan 0.2274904  0.26063011 0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.                nan 0.2274904  0.26063011 0.20721634\n",
      " 0.                nan 0.2274904  0.26063011 0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.                nan 0.2274904  0.26063011 0.20721634\n",
      " 0.                nan 0.2274904  0.26063011 0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.00869565        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.                nan 0.2274904  0.26063011 0.20721634 0.\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.03488613        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.03488613        nan 0.2274904  0.26063011 0.20721634 0.03488613\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.05746677        nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.0805437         nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011\n",
      " 0.20721634 0.05746677        nan 0.2274904  0.26063011 0.20721634\n",
      " 0.0805437         nan 0.2274904  0.26063011 0.20721634 0.05746677\n",
      "        nan 0.2274904  0.26063011        nan        nan        nan\n",
      "        nan        nan 0.20721634 0.0805437         nan 0.2274904\n",
      " 0.26063011 0.20721634 0.0805437         nan 0.2274904  0.26063011]\n",
      "One or more of the train scores are non-finite: [0.19705997 0.                nan 0.14920796 0.0462963  0.19705746\n",
      " 0.                nan 0.24849319 0.0462963  0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.24793666 0.                nan 0.24849319 0.22035867 0.2468814\n",
      " 0.                nan 0.24849319 0.24447839 0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.                nan 0.24849319 0.24447839 0.2468814\n",
      " 0.                nan 0.24849319 0.24447839 0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.                nan 0.24849319 0.24447839 0.2468814\n",
      " 0.                nan 0.24849319 0.24447839 0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.02597403        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.                nan 0.24849319 0.24447839 0.2468814  0.\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.07657224        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.07657224        nan 0.24849319 0.24447839 0.2468814  0.07657224\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.10161665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.12661665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839\n",
      " 0.2468814  0.10161665        nan 0.24849319 0.24447839 0.2468814\n",
      " 0.12661665        nan 0.24849319 0.24447839 0.2468814  0.10161665\n",
      "        nan 0.24849319 0.24447839        nan        nan        nan\n",
      "        nan        nan 0.2468814  0.12661665        nan 0.24849319\n",
      " 0.24447839 0.2468814  0.12661665        nan 0.24849319 0.24447839]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2606301088307362\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.6384039900249376\n",
      "GridSearchCV Runtime: 5.720377683639526 secs\n",
      "Efron R-squared: -0.3491714791572853\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.16393442622950818\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24390243902439024\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.22613065326633167\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2660098522167488\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.24537037037037038\n",
      "Ave Test Precision: 0.22906954822146983\n",
      "Stdev Test Precision: 0.039056173430170205\n",
      "Ave Test Accuracy: 0.41044776119402987\n",
      "Stdev Test Accuracy: 0.10865835655642565\n",
      "Ave Test Specificity: 0.3346534653465346\n",
      "Ave Test Recall: 0.6424242424242423\n",
      "Ave Test NPV: 0.7473074268726443\n",
      "Ave Test F1-Score: 0.32869603203544856\n",
      "Ave Test G-mean: 0.40319526001692435\n",
      "Ave Runtime: 0.006829738616943359\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positiveMonthlySoloNetIncomeWithSavings. 13 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22986345 0.23214016 0.2847619  0.22986345 0.24767587 0.22986345\n",
      " 0.22520965 0.         0.22986345 0.24767587 0.24458784 0.22013491\n",
      " 0.         0.24621365 0.23911882        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.22986345 0.23214016 0.2847619  0.22986345 0.24767587 0.24268308\n",
      " 0.22013491 0.         0.24346855 0.25298827 0.24458784 0.22013491\n",
      " 0.         0.24621365 0.23911882        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.2286578  0.22870567 0.28642857 0.2286578  0.24716708 0.24499511\n",
      " 0.21452606 0.         0.24750399 0.25731386 0.24499511 0.21452606\n",
      " 0.         0.24750399 0.23911882        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.22722573 0.24322594 0.29386447 0.22812343 0.24654304 0.24520716\n",
      " 0.24629621 0.         0.24192381 0.23881744 0.24520716 0.24629621\n",
      " 0.         0.24192381 0.23881744        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24622302 0.2514528  0.37138889 0.24526385 0.24543963 0.24419163\n",
      " 0.25044663 0.3        0.24108347 0.23565463 0.24419163 0.25044663\n",
      " 0.3        0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24044137 0.24719686 0.39166667 0.2398269  0.23938392 0.24459076\n",
      " 0.24659763 0.385      0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.385      0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.2398269  0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463\n",
      " 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076\n",
      " 0.24659763 0.39166667 0.24108347 0.23565463 0.24459076 0.24659763\n",
      " 0.39166667 0.24108347 0.23565463        nan        nan        nan\n",
      "        nan        nan 0.24459076 0.24659763 0.39166667 0.24108347\n",
      " 0.23565463 0.24459076 0.24659763 0.39166667 0.24108347 0.23565463]\n",
      "One or more of the train scores are non-finite: [0.23006765 0.23143067 0.32636769 0.23006765 0.24486765 0.23006765\n",
      " 0.2187965  0.         0.23006765 0.2449588  0.24093331 0.22052129\n",
      " 0.         0.24101197 0.25910633        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.22956441 0.23188958 0.32636769 0.22968991 0.2448964  0.2396491\n",
      " 0.22078445 0.         0.24009199 0.24594458 0.24093331 0.22078445\n",
      " 0.         0.24101197 0.25910633        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.22922844 0.22963666 0.32728336 0.22895784 0.24538274 0.24113362\n",
      " 0.26638747 0.         0.24086041 0.25312776 0.24102277 0.26638747\n",
      " 0.         0.24086041 0.25904913        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.2290762  0.23442361 0.33684931 0.2298772  0.24653106 0.24224361\n",
      " 0.24339603 0.04166667 0.24193358 0.25837921 0.24224361 0.24339603\n",
      " 0.04166667 0.24198152 0.25837921        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24716679 0.24428605 0.35021016 0.2435348  0.25192142 0.24125951\n",
      " 0.24210305 0.38818761 0.24191817 0.25850841 0.24125951 0.24210305\n",
      " 0.38818761 0.24191817 0.25850841        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24149578 0.24354476 0.35728609 0.24252058 0.26069963 0.24130573\n",
      " 0.24163862 0.37093914 0.24191817 0.25944955 0.24135079 0.24159978\n",
      " 0.37093914 0.24196504 0.25944955        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24129341 0.24249379 0.35854353 0.242004   0.25952269 0.24135079\n",
      " 0.24210365 0.36207813 0.24196504 0.25934383 0.24135079 0.24206394\n",
      " 0.36207813 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24135079 0.24252417 0.35854353 0.24201209 0.25967176 0.24135079\n",
      " 0.24252417 0.35926817 0.24196504 0.25934383 0.24135079 0.24248446\n",
      " 0.36030984 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24135079 0.24252417 0.35854353 0.24201209 0.25938768 0.24135079\n",
      " 0.24252417 0.35854353 0.24196504 0.25934383 0.24135079 0.24248446\n",
      " 0.35926817 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24135079 0.24252417 0.35854353 0.24201209 0.25938768 0.24135079\n",
      " 0.24252417 0.35854353 0.24196504 0.25934383 0.24135079 0.24248446\n",
      " 0.35926817 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24135079 0.24252417 0.35854353 0.24201209 0.25938768 0.24135079\n",
      " 0.24252417 0.35854353 0.24196504 0.25934383 0.24135079 0.24248446\n",
      " 0.35926817 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24135079 0.24252417 0.35854353 0.24201209 0.25938768 0.24135079\n",
      " 0.24252417 0.35854353 0.24196504 0.25934383 0.24135079 0.24248446\n",
      " 0.35926817 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383\n",
      " 0.24135079 0.24252417 0.35854353 0.24201209 0.25938768 0.24135079\n",
      " 0.24252417 0.35854353 0.24196504 0.25934383 0.24135079 0.24248446\n",
      " 0.35926817 0.24196504 0.25934383        nan        nan        nan\n",
      "        nan        nan 0.24135079 0.24252417 0.35854353 0.24196504\n",
      " 0.25934383 0.24135079 0.24248446 0.35926817 0.24196504 0.25934383]\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': None, 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.3916666666666667\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, penalty=None,\n",
      "                                    random_state=0))])\n",
      "Train Score of Best Model: 0.7468827930174564\n",
      "GridSearchCV Runtime: 5.657336711883545 secs\n",
      "Efron R-squared: -0.19239166458690704\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.3888888888888889\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.48148148148148145\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3181818181818182\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2972972972972973\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.24242424242424243\n",
      "Ave Test Precision: 0.3456547456547457\n",
      "Stdev Test Precision: 0.09228737001797055\n",
      "Ave Test Accuracy: 0.714179104477612\n",
      "Stdev Test Accuracy: 0.0279975096750807\n",
      "Ave Test Specificity: 0.895049504950495\n",
      "Ave Test Recall: 0.16060606060606059\n",
      "Ave Test NPV: 0.7654072763350948\n",
      "Ave Test F1-Score: 0.21519808166222196\n",
      "Ave Test G-mean: 0.37535935900961476\n",
      "Ave Runtime: 0.011381912231445312\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyNetIncomeWithSavings. 12 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.08428571 0.                nan 0.18731602 0.13969697 0.08428571\n",
      " 0.                nan 0.18731602 0.13969697 0.16555448 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.16305448 0.                nan 0.19301046 0.16330808 0.13222115\n",
      " 0.                nan 0.21092713 0.1853669  0.16555448 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.                nan 0.21092713 0.1853669  0.13222115\n",
      " 0.                nan 0.21092713 0.1853669  0.16555448 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.                nan 0.21092713 0.1853669  0.13222115\n",
      " 0.                nan 0.21092713 0.1853669  0.16555448 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.03095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.                nan 0.21092713 0.1853669  0.16555448 0.\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669\n",
      " 0.13222115 0.05095238        nan 0.21092713 0.1853669  0.13222115\n",
      " 0.05095238        nan 0.21092713 0.1853669  0.16555448 0.05095238\n",
      "        nan 0.21092713 0.1853669         nan        nan        nan\n",
      "        nan        nan 0.13222115 0.05095238        nan 0.21092713\n",
      " 0.1853669  0.16555448 0.05095238        nan 0.21092713 0.1853669 ]\n",
      "One or more of the train scores are non-finite: [0.15684432 0.                nan 0.22872903 0.15206002 0.15684432\n",
      " 0.                nan 0.22872903 0.15206002 0.22973629 0.\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.2027584  0.                nan 0.25403095 0.17664018 0.20592677\n",
      " 0.                nan 0.25330918 0.20137462 0.22973629 0.\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.                nan 0.25330918 0.20137462 0.20592677\n",
      " 0.                nan 0.25330918 0.20137462 0.22973629 0.\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.                nan 0.25330918 0.20137462 0.20592677\n",
      " 0.                nan 0.25330918 0.20137462 0.22973629 0.\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.05255754        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.                nan 0.25330918 0.20137462 0.22973629 0.\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462\n",
      " 0.20592677 0.07827183        nan 0.25330918 0.20137462 0.20592677\n",
      " 0.07827183        nan 0.25330918 0.20137462 0.22973629 0.07827183\n",
      "        nan 0.25330918 0.20137462        nan        nan        nan\n",
      "        nan        nan 0.20592677 0.07827183        nan 0.25330918\n",
      " 0.20137462 0.22973629 0.07827183        nan 0.25330918 0.20137462]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.21092712842712844\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.7082294264339152\n",
      "GridSearchCV Runtime: 5.773653030395508 secs\n",
      "Efron R-squared: -0.349171453717809\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.15384615384615385\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24583333333333332\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.23809523809523808\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2489451476793249\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.22950819672131148\n",
      "Ave Test Precision: 0.22324561393507233\n",
      "Stdev Test Precision: 0.039517994141190345\n",
      "Ave Test Accuracy: 0.3716417910447761\n",
      "Stdev Test Accuracy: 0.17704427659305888\n",
      "Ave Test Specificity: 0.26237623762376244\n",
      "Ave Test Recall: 0.7060606060606062\n",
      "Ave Test NPV: 0.7108062474631389\n",
      "Ave Test F1-Score: 0.318735414723443\n",
      "Ave Test G-mean: 0.2866295334225509\n",
      "Ave Runtime: 0.008520841598510742\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positiveMonthlyFamilyNetIncomeWithSavings. 11 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.25434951 0.25520923 0.225      0.25434951 0.27867813 0.25434951\n",
      " 0.19480898 0.         0.25434951 0.28028389 0.24786284 0.16320925\n",
      " 0.         0.24753768 0.26561708        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.25305081 0.25574687 0.225      0.25305081 0.28028389 0.24990829\n",
      " 0.1635257  0.         0.24958313 0.27489173 0.24786284 0.1635257\n",
      " 0.         0.24753768 0.26561708        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.25244597 0.25021517 0.225      0.25244597 0.27898519 0.24926181\n",
      " 0.24058748 0.         0.24668806 0.26560945 0.24926181 0.24058748\n",
      " 0.         0.24668806 0.26561708        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.25784982 0.24820251 0.22424242 0.25598117 0.28186634 0.24354307\n",
      " 0.22192713 0.02727273 0.24517347 0.26571534 0.24354307 0.22192713\n",
      " 0.02727273 0.24517347 0.26645758        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24923461 0.23492195 0.28524031 0.25074392 0.27714601 0.24533351\n",
      " 0.23843614 0.026      0.24936379 0.26738271 0.24533351 0.23843614\n",
      " 0.026      0.24936379 0.26738271        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24803107 0.23569464 0.18310767 0.25021694 0.26662391 0.24710207\n",
      " 0.2352906  0.20013793 0.24796617 0.26820939 0.24710207 0.2352906\n",
      " 0.20013793 0.24796617 0.26820939        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24611172 0.2352906  0.18286275 0.24882269 0.2679871  0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24611172 0.2352906  0.18286275 0.24882269 0.26709332 0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332\n",
      " 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934\n",
      " 0.2352906  0.18286275 0.24841865 0.26709332 0.24750934 0.2352906\n",
      " 0.18286275 0.24841865 0.26709332        nan        nan        nan\n",
      "        nan        nan 0.24750934 0.2352906  0.18286275 0.24841865\n",
      " 0.26709332 0.24750934 0.2352906  0.18286275 0.24841865 0.26709332]\n",
      "One or more of the train scores are non-finite: [0.25574251 0.25754297 0.27709265 0.25574251 0.24615161 0.25572399\n",
      " 0.19907031 0.         0.25572399 0.24614778 0.2517971  0.17338213\n",
      " 0.         0.25184939 0.24227824        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25544126 0.25681424 0.277169   0.2553694  0.24614778 0.2519456\n",
      " 0.25691782 0.         0.25200956 0.24592487 0.2517971  0.25691782\n",
      " 0.         0.25184939 0.24227824        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.2558974  0.25558272 0.27703558 0.2558974  0.24612235 0.25206091\n",
      " 0.22067608 0.05       0.25200038 0.24538622 0.25206091 0.22067608\n",
      " 0.05       0.25214658 0.24223482        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25644744 0.25389216 0.27080839 0.25653683 0.24612162 0.25159616\n",
      " 0.2736352  0.12520107 0.25130479 0.24332844 0.25159616 0.2736352\n",
      " 0.12520107 0.25124709 0.2426575         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25356324 0.25154653 0.23767531 0.25295123 0.24441498 0.25216639\n",
      " 0.2538636  0.36515508 0.25214024 0.24272142 0.25216639 0.2538636\n",
      " 0.36515508 0.25214024 0.24272142        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25183186 0.25112269 0.29970809 0.25128917 0.24257828 0.2520732\n",
      " 0.25121301 0.30680013 0.25219421 0.24288575 0.2520732  0.25121301\n",
      " 0.30680013 0.25219421 0.24288575        nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25218116 0.25124171 0.29725365 0.25202742 0.24298121 0.25228095\n",
      " 0.24957504 0.29725365 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.29725365 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25228095 0.24957504 0.30433609 0.25202742 0.2429277  0.25228095\n",
      " 0.24957504 0.30433609 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.30433609 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25228095 0.24957504 0.30433609 0.25202742 0.2429277  0.25228095\n",
      " 0.24957504 0.30433609 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.30433609 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25228095 0.24957504 0.30433609 0.25202742 0.2429277  0.25228095\n",
      " 0.24957504 0.30433609 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.30433609 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25228095 0.24957504 0.30433609 0.25202742 0.2429277  0.25228095\n",
      " 0.24957504 0.30433609 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.30433609 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25228095 0.24957504 0.30433609 0.25202742 0.2429277  0.25228095\n",
      " 0.24957504 0.30433609 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.30433609 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277\n",
      " 0.25228095 0.24957504 0.30433609 0.25202742 0.2429277  0.25228095\n",
      " 0.24957504 0.30433609 0.25202742 0.2429277  0.25222921 0.24957504\n",
      " 0.30433609 0.25202742 0.2429277         nan        nan        nan\n",
      "        nan        nan 0.25228095 0.24957504 0.30433609 0.25202742\n",
      " 0.2429277  0.25222921 0.24957504 0.30433609 0.25202742 0.2429277 ]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.2852403100775194\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6159600997506235\n",
      "GridSearchCV Runtime: 5.9270994663238525 secs\n",
      "Efron R-squared: -0.3214371255595845\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2289156626506024\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.375\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.0\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.0\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.12078313253012048\n",
      "Stdev Test Precision: 0.17326609238958235\n",
      "Ave Test Accuracy: 0.7134328358208955\n",
      "Stdev Test Accuracy: 0.07135553819687412\n",
      "Ave Test Specificity: 0.9247524752475247\n",
      "Ave Test Recall: 0.06666666666666668\n",
      "Ave Test NPV: 0.7516682096721817\n",
      "Ave Test F1-Score: 0.06722292762561219\n",
      "Ave Test G-mean: 0.1308040199019298\n",
      "Ave Runtime: 0.009148979187011718\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyIncome - basicMonthlySalary. 10 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "715 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "637 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.1524111  0.02469136        nan 0.12260069 0.23857484 0.17771413\n",
      " 0.02469136        nan 0.21316419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.23403989 0.02469136        nan 0.23816419 0.23857484 0.20271413\n",
      " 0.02469136        nan 0.21316419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.02469136        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.02469136        nan 0.23816419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.02469136        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.02469136        nan 0.23816419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.02469136        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.02469136        nan 0.23816419 0.23857484 0.22847171 0.02469136\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.11431857        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.13472673        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484\n",
      " 0.22847171 0.11431857        nan 0.23816419 0.23857484 0.22847171\n",
      " 0.13472673        nan 0.23816419 0.23857484 0.22847171 0.11431857\n",
      "        nan 0.23816419 0.23857484        nan        nan        nan\n",
      "        nan        nan 0.22847171 0.13472673        nan 0.23816419\n",
      " 0.23857484 0.22847171 0.13472673        nan 0.23816419 0.23857484]\n",
      "One or more of the train scores are non-finite: [0.17406968 0.02454924        nan 0.12267077 0.24416029 0.19859948\n",
      " 0.02454924        nan 0.2219619  0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24716329 0.02454924        nan 0.24653004 0.24416029 0.22316762\n",
      " 0.02454924        nan 0.2219619  0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.02454924        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.02454924        nan 0.24653004 0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.02454924        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.02454924        nan 0.24653004 0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.02454924        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.02454924        nan 0.24653004 0.24416029 0.24763775 0.02454924\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.14996007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.17496007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029\n",
      " 0.24763775 0.14996007        nan 0.24653004 0.24416029 0.24763775\n",
      " 0.17496007        nan 0.24653004 0.24416029 0.24763775 0.14996007\n",
      "        nan 0.24653004 0.24416029        nan        nan        nan\n",
      "        nan        nan 0.24763775 0.17496007        nan 0.24653004\n",
      " 0.24416029 0.24763775 0.17496007        nan 0.24653004 0.24416029]\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.23857484303136473\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6159600997506235\n",
      "GridSearchCV Runtime: 5.998752593994141 secs\n",
      "Efron R-squared: -0.349171642865564\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2345679012345679\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.0\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2289156626506024\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.0\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.0\n",
      "Ave Test Precision: 0.09269671277703406\n",
      "Stdev Test Precision: 0.1269459315003977\n",
      "Ave Test Accuracy: 0.6880597014925373\n",
      "Stdev Test Accuracy: 0.08996329788014193\n",
      "Ave Test Specificity: 0.8752475247524754\n",
      "Ave Test Recall: 0.11515151515151516\n",
      "Ave Test NPV: 0.7511606154801941\n",
      "Ave Test F1-Score: 0.10270739168150482\n",
      "Ave Test G-mean: 0.17803022046464823\n",
      "Ave Runtime: 0.008625507354736328\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positive monthlyFamilyIncome - basicMonthlySalary. 9 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.206957   0.2070859  0.36390693 0.206957   0.27026184 0.206957\n",
      " 0.10944878 0.         0.20645866 0.2697635  0.20121743 0.11718034\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20645866 0.20486816 0.36390693 0.20645866 0.2697635  0.20577725\n",
      " 0.11843335 0.         0.20694236 0.26944344 0.20121743 0.11748097\n",
      " 0.         0.19902289 0.27215022        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20787322 0.21053384 0.36525974 0.20787322 0.27117806 0.20746609\n",
      " 0.18497553 0.         0.20632572 0.26860801 0.20121743 0.18607443\n",
      " 0.         0.19714027 0.27106964        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20912052 0.2129899  0.3552381  0.20872158 0.27294574 0.20317587\n",
      " 0.22124032 0.         0.20258582 0.27071881 0.20121743 0.22124032\n",
      " 0.         0.19760485 0.27071881        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20154865 0.25548036 0.37285714 0.20119575 0.26782754 0.20317587\n",
      " 0.23358949 0.36666667 0.19997585 0.27000482 0.20121743 0.23358949\n",
      " 0.36666667 0.19760485 0.27000482        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20169311 0.25746124 0.38333333 0.1998966  0.26986083 0.20317587\n",
      " 0.25898576 0.4        0.19997585 0.27063283 0.20121743 0.25898576\n",
      " 0.4        0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.38333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.38333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283\n",
      " 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283 0.20317587\n",
      " 0.2593281  0.37333333 0.19997585 0.27063283 0.20121743 0.2593281\n",
      " 0.37333333 0.19760485 0.27063283        nan        nan        nan\n",
      "        nan        nan 0.20317587 0.2593281  0.37333333 0.19997585\n",
      " 0.27063283 0.20121743 0.2593281  0.37333333 0.19760485 0.27063283]\n",
      "One or more of the train scores are non-finite: [0.21406456 0.21753168 0.34579767 0.21433556 0.25980589 0.21485609\n",
      " 0.11555348 0.         0.21524006 0.2605446  0.21190105 0.11858785\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21524006 0.21779676 0.3463148  0.21524006 0.26046086 0.21693595\n",
      " 0.18597355 0.         0.21607005 0.26201811 0.21190105 0.1852493\n",
      " 0.         0.21053761 0.26405948        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21499148 0.21400501 0.34650148 0.21507196 0.26053989 0.21511944\n",
      " 0.2913795  0.         0.21487681 0.26369047 0.21176171 0.29039832\n",
      " 0.         0.21053761 0.26395912        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2174422  0.22071031 0.35897384 0.2172269  0.26169073 0.21319033\n",
      " 0.28575602 0.16666667 0.21490766 0.26352371 0.21179116 0.28575602\n",
      " 0.16666667 0.2106657  0.26352371        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21301237 0.24906234 0.38825152 0.21245925 0.25972252 0.21316659\n",
      " 0.26871203 0.47803207 0.21228805 0.26231435 0.21176742 0.26930026\n",
      " 0.47803207 0.21081978 0.26231435        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21130223 0.26264816 0.41770404 0.21152949 0.26028815 0.21310005\n",
      " 0.26431071 0.42410912 0.21241002 0.2625036  0.2117339  0.26431071\n",
      " 0.42410912 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.21178291 0.26243359 0.41615745 0.21108015 0.26219953 0.21310005\n",
      " 0.26431071 0.41599638 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41599638 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26210887 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773\n",
      " 0.2117339  0.26480506 0.41280247 0.21098925 0.26206357 0.21310005\n",
      " 0.26431071 0.41280247 0.21241002 0.26226944 0.2117339  0.26480506\n",
      " 0.41280247 0.21094175 0.26229773        nan        nan        nan\n",
      "        nan        nan 0.21310005 0.26431071 0.41280247 0.21241002\n",
      " 0.26226944 0.2117339  0.26480506 0.41280247 0.21094175 0.26229773]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.39999999999999997\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7518703241895262\n",
      "GridSearchCV Runtime: 5.826650142669678 secs\n",
      "Efron R-squared: -0.15776698146763035\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.3870967741935484\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.3\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2727272727272727\n",
      "Ave Test Precision: 0.32910766652702134\n",
      "Stdev Test Precision: 0.059785972659199466\n",
      "Ave Test Accuracy: 0.7358208955223879\n",
      "Stdev Test Accuracy: 0.007177382112563941\n",
      "Ave Test Specificity: 0.9504950495049505\n",
      "Ave Test Recall: 0.0787878787878788\n",
      "Ave Test NPV: 0.759685019948753\n",
      "Ave Test F1-Score: 0.12191105693004774\n",
      "Ave Test G-mean: 0.2613548711958696\n",
      "Ave Runtime: 0.010606908798217773\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with basicMonthlySalary - monthlyExpenses. 8 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1105 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "182 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "364 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "559 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.07694209 0.                nan 0.07421482 0.         0.25989423\n",
      " 0.                nan 0.25989423 0.1774121  0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.                nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.                nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.                nan 0.25989423 0.25989423 0.25989423\n",
      " 0.                nan 0.25989423 0.25989423 0.25989423 0.\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.07478773        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.07478773        nan 0.25989423 0.25989423 0.25989423 0.07478773\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.22527885        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.25989423        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423\n",
      " 0.25989423 0.22527885        nan 0.25989423 0.25989423 0.25989423\n",
      " 0.25989423        nan 0.25989423 0.25989423 0.25989423 0.22527885\n",
      "        nan 0.25989423 0.25989423        nan        nan        nan\n",
      "        nan        nan 0.25989423 0.25989423        nan 0.25989423\n",
      " 0.25989423 0.25989423 0.25989423        nan 0.25989423 0.25989423]\n",
      "One or more of the train scores are non-finite: [0.07770881 0.                nan 0.07804872 0.         0.25887731\n",
      " 0.                nan 0.25887731 0.18165174 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.                nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.                nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.                nan 0.25887731 0.25887731 0.25887731\n",
      " 0.                nan 0.25887731 0.25887731 0.25887731 0.\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.07799125        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.07799125        nan 0.25887731 0.25887731 0.25887731 0.07799125\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.23387731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.25887731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731\n",
      " 0.25887731 0.23387731        nan 0.25887731 0.25887731 0.25887731\n",
      " 0.25887731        nan 0.25887731 0.25887731 0.25887731 0.23387731\n",
      "        nan 0.25887731 0.25887731        nan        nan        nan\n",
      "        nan        nan 0.25887731 0.25887731        nan 0.25887731\n",
      " 0.25887731 0.25887731 0.25887731        nan 0.25887731 0.25887731]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': SMOTE(random_state=0)}\n",
      "Best Validation Score (Precision): 0.259894230383722\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', SMOTE(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.415211970074813\n",
      "GridSearchCV Runtime: 4.646442651748657 secs\n",
      "Efron R-squared: -0.3491673138540201\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.2736842105263158\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.25263157894736843\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.22941176470588234\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.28888888888888886\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2512820512820513\n",
      "Ave Test Precision: 0.25917969887010134\n",
      "Stdev Test Precision: 0.022827600872137695\n",
      "Ave Test Accuracy: 0.42164179104477606\n",
      "Stdev Test Accuracy: 0.03099486516014207\n",
      "Ave Test Specificity: 0.32178217821782173\n",
      "Ave Test Recall: 0.7272727272727272\n",
      "Ave Test NPV: 0.7844531528484562\n",
      "Ave Test F1-Score: 0.38200032588432253\n",
      "Ave Test G-mean: 0.48221180258827284\n",
      "Ave Runtime: 0.007780933380126953\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positive basicMonthlySalary - monthlyExpenses. 7 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.22669875 0.23653443 0.23010684 0.22669875 0.23660771 0.22669875\n",
      " 0.20939724 0.         0.22669875 0.23660771 0.24001419 0.2147074\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.22669875 0.23602161 0.23010684 0.22669875 0.23660771 0.2404398\n",
      " 0.21500803 0.         0.24285524 0.24691285 0.24001419 0.21500803\n",
      " 0.         0.2419944  0.2496445         nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.22956664 0.23467117 0.23189255 0.22956664 0.23660771 0.24096413\n",
      " 0.24062621 0.         0.23992435 0.2461551  0.23949354 0.24062621\n",
      " 0.         0.23992435 0.24888676        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.24003546 0.24115818 0.25315129 0.24003546 0.24378851 0.23748021\n",
      " 0.24270697 0.         0.23375239 0.24922126 0.23748021 0.24270697\n",
      " 0.         0.23375239 0.24922126        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23049107 0.24537943 0.24583333 0.23296233 0.23747562 0.23663853\n",
      " 0.24188832 0.42333333 0.23169206 0.24742561 0.23663853 0.24188832\n",
      " 0.42333333 0.23169206 0.24742561        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23783161 0.23921981 0.24345238 0.23200984 0.24163056 0.23663853\n",
      " 0.2421235  0.25119048 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.25119048 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24526455 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957\n",
      " 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853\n",
      " 0.2421235  0.24345238 0.23083185 0.24657957 0.23663853 0.2421235\n",
      " 0.24345238 0.23083185 0.24657957        nan        nan        nan\n",
      "        nan        nan 0.23663853 0.2421235  0.24345238 0.23083185\n",
      " 0.24657957 0.23663853 0.2421235  0.24345238 0.23083185 0.24657957]\n",
      "One or more of the train scores are non-finite: [0.23477526 0.23770697 0.29560901 0.23477526 0.24626193 0.23477526\n",
      " 0.219768   0.         0.23477526 0.24647661 0.2455207  0.22102263\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.23464593 0.23797083 0.29560901 0.23485901 0.24641156 0.24556157\n",
      " 0.26114871 0.         0.24533465 0.25108246 0.2455207  0.26114871\n",
      " 0.         0.24535924 0.25084637        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.23676889 0.23825403 0.29807227 0.23697656 0.24683148 0.24417298\n",
      " 0.24111749 0.         0.24446346 0.2501251  0.24476588 0.24111749\n",
      " 0.         0.24446346 0.25065147        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24353191 0.23967955 0.30121849 0.24338633 0.24618009 0.24286218\n",
      " 0.24167568 0.095      0.24396914 0.2503354  0.24286218 0.24167568\n",
      " 0.095      0.24396914 0.2503354         nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.23543138 0.24041165 0.32269832 0.23569409 0.24570293 0.24142589\n",
      " 0.2394545  0.31088804 0.24285622 0.24972114 0.24152145 0.2394545\n",
      " 0.31088804 0.24290218 0.24972114        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24052581 0.23980302 0.33679572 0.24101142 0.24929717 0.24107228\n",
      " 0.23959051 0.34914591 0.24289462 0.24985954 0.24116783 0.23955247\n",
      " 0.34914591 0.24289462 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24089904 0.23929204 0.3369035  0.24249328 0.24991287 0.24111965\n",
      " 0.23935736 0.33823487 0.24289462 0.24985954 0.2412152  0.23935736\n",
      " 0.33823487 0.24289462 0.24985954        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24126215 0.23941048 0.3369035  0.24249328 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24990407        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002\n",
      " 0.24121459 0.23941048 0.3369035  0.24263945 0.24995002 0.24111965\n",
      " 0.23935736 0.3369035  0.24289462 0.24995002 0.2412152  0.23935736\n",
      " 0.3369035  0.24289462 0.24995002        nan        nan        nan\n",
      "        nan        nan 0.24111965 0.23935736 0.3369035  0.24289462\n",
      " 0.24995002 0.2412152  0.23935736 0.3369035  0.24289462 0.24995002]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': EditedNearestNeighbours()}\n",
      "Best Validation Score (Precision): 0.42333333333333334\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', EditedNearestNeighbours()),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.7468827930174564\n",
      "GridSearchCV Runtime: 4.463063478469849 secs\n",
      "Efron R-squared: -0.20525021574232527\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.38461538461538464\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.4117647058823529\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2916666666666667\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.4444444444444444\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.3333333333333333\n",
      "Ave Test Precision: 0.3731649069884364\n",
      "Stdev Test Precision: 0.061094901194505016\n",
      "Ave Test Accuracy: 0.7365671641791045\n",
      "Stdev Test Accuracy: 0.013086326803544922\n",
      "Ave Test Specificity: 0.9485148514851485\n",
      "Ave Test Recall: 0.08787878787878789\n",
      "Ave Test NPV: 0.7609078069397102\n",
      "Ave Test F1-Score: 0.1400672684711117\n",
      "Ave Test G-mean: 0.2868883094391115\n",
      "Ave Runtime: 0.008012199401855468\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyFamilyIncome - monthlyExpenses. 6 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "845 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "78 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1187, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "156 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "611 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.09629953 0.                nan 0.15781469 0.14781469 0.09629953\n",
      " 0.                nan 0.1813441  0.14781469 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.                nan 0.19118519 0.22013531 0.19128534\n",
      " 0.                nan 0.1813441  0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.                nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.                nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.03068182        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.                nan 0.20528776 0.22013531 0.217952   0.\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.08775253        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531\n",
      " 0.19128534 0.08775253        nan 0.20528776 0.22013531 0.19128534\n",
      " 0.11441919        nan 0.20528776 0.22013531 0.217952   0.08775253\n",
      "        nan 0.22195443 0.22013531        nan        nan        nan\n",
      "        nan        nan 0.19128534 0.11441919        nan 0.20528776\n",
      " 0.22013531 0.217952   0.11441919        nan 0.22195443 0.22013531]\n",
      "One or more of the train scores are non-finite: [0.12870734 0.                nan 0.1505025  0.1512757  0.12870734\n",
      " 0.                nan 0.17506669 0.1512757  0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.                nan 0.19943274 0.22476988 0.22687699\n",
      " 0.                nan 0.17506669 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.                nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.                nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.05240275        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.                nan 0.19958898 0.22476988 0.25187699 0.\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.12979481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988\n",
      " 0.22687699 0.12979481        nan 0.19958898 0.22476988 0.22687699\n",
      " 0.15479481        nan 0.19958898 0.22476988 0.25187699 0.12979481\n",
      "        nan 0.22536217 0.22476988        nan        nan        nan\n",
      "        nan        nan 0.22687699 0.15479481        nan 0.19958898\n",
      " 0.22476988 0.25187699 0.15479481        nan 0.22536217 0.22476988]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cholesky', 'resampling': RandomOverSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.22195442571788887\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomOverSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='newton-cholesky'))])\n",
      "Train Score of Best Model: 0.6907730673316709\n",
      "GridSearchCV Runtime: 4.940269708633423 secs\n",
      "Efron R-squared: -0.34917143678365536\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.21621621621621623\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24669603524229075\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2409090909090909\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2565217391304348\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.2297872340425532\n",
      "Ave Test Precision: 0.2380260631081172\n",
      "Stdev Test Precision: 0.015565330252285143\n",
      "Ave Test Accuracy: 0.3888059701492537\n",
      "Stdev Test Accuracy: 0.16168819361247286\n",
      "Ave Test Specificity: 0.2881188118811881\n",
      "Ave Test Recall: 0.696969696969697\n",
      "Ave Test NPV: 0.7372670173215744\n",
      "Ave Test F1-Score: 0.33313487430823757\n",
      "Ave Test G-mean: 0.34361955132076955\n",
      "Ave Runtime: 0.007558679580688477\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with positive monthlyFamilyIncome - monthlyExpenses. 5 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.21177073 0.17680636 0.13440412 0.2258301  0.24398084 0.21101315\n",
      " 0.16944224 0.02469136 0.2258301  0.24398084 0.219761   0.14563272\n",
      " 0.02469136 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.21067302 0.17680636 0.13440412 0.22548997 0.24398084 0.21067302\n",
      " 0.14563272 0.02469136 0.22548997 0.24398084 0.219761   0.14563272\n",
      " 0.02469136 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.21050879 0.17680636 0.13241473 0.22659379 0.24364071 0.21182931\n",
      " 0.14563272 0.02469136 0.21565835 0.24606506 0.219761   0.14563272\n",
      " 0.02469136 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.2157896  0.17705548 0.12916798 0.22429175 0.24407788 0.21854519\n",
      " 0.14271605 0.02647059 0.21641752 0.24126761 0.219761   0.14271605\n",
      " 0.02647059 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.22012154 0.18545195 0.17179843 0.2157354  0.24076848 0.219761\n",
      " 0.19466351 0.11190476 0.21817191 0.24386924 0.219761   0.19466351\n",
      " 0.11190476 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.21755999 0.18734212 0.18509512 0.21763354 0.24418627 0.219761\n",
      " 0.18689014 0.18608459 0.21817191 0.24386924 0.219761   0.18897347\n",
      " 0.18608459 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18856833 0.21817191 0.24261893 0.219761\n",
      " 0.18653926 0.18856833 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18856833 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924\n",
      " 0.219761   0.18653926 0.18930907 0.21817191 0.24386924 0.219761\n",
      " 0.18653926 0.18930907 0.21817191 0.24386924 0.219761   0.18653926\n",
      " 0.18930907 0.21817191 0.24386924        nan        nan        nan\n",
      "        nan        nan 0.219761   0.18653926 0.18930907 0.21817191\n",
      " 0.24386924 0.219761   0.18653926 0.18930907 0.21817191 0.24386924]\n",
      "One or more of the train scores are non-finite: [0.23960513 0.20150029 0.21154485 0.25572029 0.26284364 0.24018906\n",
      " 0.16838418 0.02454924 0.25612311 0.26284364 0.24204582 0.14757493\n",
      " 0.02454924 0.26007573 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24027096 0.20150029 0.21154485 0.25612434 0.26284364 0.24041757\n",
      " 0.14757493 0.02454924 0.25627095 0.26284364 0.24204582 0.14757493\n",
      " 0.02454924 0.26007573 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.23997661 0.20150029 0.2121425  0.25589505 0.26306458 0.23978231\n",
      " 0.14757493 0.02454924 0.257159   0.26322902 0.24204582 0.14757493\n",
      " 0.02454924 0.26007573 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24007467 0.20606076 0.21191338 0.25698938 0.26322278 0.24218319\n",
      " 0.15056781 0.05066038 0.2600003  0.2651445  0.24204582 0.15056781\n",
      " 0.05066038 0.26007573 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24157305 0.20100473 0.21479849 0.25808951 0.26370923 0.24208858\n",
      " 0.20104837 0.22259473 0.25948243 0.26491422 0.24208858 0.20104837\n",
      " 0.22259473 0.25999406 0.26491422        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24090943 0.20131456 0.21815866 0.25843224 0.26431975 0.24160947\n",
      " 0.20601454 0.21685128 0.25948243 0.26524178 0.24160947 0.20631732\n",
      " 0.21652203 0.25948243 0.26524178        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21815037 0.25905654 0.26484588 0.24166765\n",
      " 0.20318846 0.21810933 0.25893919 0.26516277 0.24166765 0.20344\n",
      " 0.21810933 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21879701 0.25893919 0.26516277 0.24166765 0.20318846\n",
      " 0.21879701 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277\n",
      " 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765\n",
      " 0.20318846 0.21870714 0.25893919 0.26516277 0.24166765 0.20318846\n",
      " 0.21870714 0.25893919 0.26516277        nan        nan        nan\n",
      "        nan        nan 0.24166765 0.20318846 0.21870714 0.25893919\n",
      " 0.26516277 0.24166765 0.20318846 0.21870714 0.25893919 0.26516277]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.0001, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.2460650573029935\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.0001, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.5211970074812967\n",
      "GridSearchCV Runtime: 5.296264410018921 secs\n",
      "Efron R-squared: -0.3492278520428962\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.21897810218978103\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2827586206896552\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.25\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.25296442687747034\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.27205882352941174\n",
      "Ave Test Precision: 0.25535199465726366\n",
      "Stdev Test Precision: 0.024431770614269228\n",
      "Ave Test Accuracy: 0.44253731343283587\n",
      "Stdev Test Accuracy: 0.09674906165521746\n",
      "Ave Test Specificity: 0.3722772277227723\n",
      "Ave Test Recall: 0.6575757575757575\n",
      "Ave Test NPV: 0.7860544281015329\n",
      "Ave Test F1-Score: 0.36352726111507294\n",
      "Ave Test G-mean: 0.45427003050056314\n",
      "Ave Runtime: 0.010802412033081054\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with basicMonthlySalary / monthlyFamilyIncome. 4 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.24124527 0.2468773  0.06401515 0.24285287 0.25914687 0.24124527\n",
      " 0.24563272 0.         0.24285287 0.25914687 0.22687907 0.24563272\n",
      " 0.         0.25844118 0.24251432        nan        nan        nan\n",
      "        nan        nan 0.22807425 0.24526745 0.         0.25672579\n",
      " 0.23991661 0.22807425 0.24526745 0.         0.25672579 0.23949314\n",
      " 0.24072263 0.2468773  0.06401515 0.24355799 0.25985199 0.24072263\n",
      " 0.24563272 0.         0.25427266 0.25985199 0.22687907 0.24563272\n",
      " 0.         0.25844118 0.24251432        nan        nan        nan\n",
      "        nan        nan 0.22807425 0.24526745 0.         0.25672579\n",
      " 0.23991661 0.22807425 0.24526745 0.         0.25672579 0.23949314\n",
      " 0.23893252 0.24782359 0.05761905 0.24513692 0.25927575 0.22799496\n",
      " 0.24499169 0.         0.25844118 0.263373   0.22687907 0.24499169\n",
      " 0.         0.25844118 0.24251432        nan        nan        nan\n",
      "        nan        nan 0.22807425 0.24526745 0.         0.25672579\n",
      " 0.23991661 0.22807425 0.24526745 0.         0.25672579 0.23949314\n",
      " 0.23792278 0.24904558 0.05333333 0.25082948 0.26084849 0.22687907\n",
      " 0.24396114 0.         0.25751526 0.24123227 0.22687907 0.24396114\n",
      " 0.         0.25751526 0.24123227        nan        nan        nan\n",
      "        nan        nan 0.22807425 0.24526745 0.         0.25672579\n",
      " 0.23991661 0.22807425 0.24526745 0.         0.25672579 0.23949314\n",
      " 0.23498808 0.24471313 0.         0.25338691 0.25587628 0.22807425\n",
      " 0.2430361  0.         0.25672579 0.23665574 0.22807425 0.24267427\n",
      " 0.         0.25751526 0.23623227        nan        nan        nan\n",
      "        nan        nan 0.22807425 0.24526745 0.         0.25672579\n",
      " 0.23991661 0.22807425 0.24526745 0.         0.25672579 0.23949314\n",
      " 0.22443716 0.24562977 0.         0.25501968 0.24831473 0.22807425\n",
      " 0.24417144 0.         0.25672579 0.23991661 0.22807425 0.24417144\n",
      " 0.         0.25672579 0.23949314        nan        nan        nan\n",
      "        nan        nan 0.22807425 0.24526745 0.         0.25672579\n",
      " 0.23991661 0.22807425 0.24526745 0.         0.25672579 0.23949314\n",
      " 0.22807425 0.24526745 0.         0.25672579 0.24291661 0.22807425\n",
      " 0.24492023 0.         0.25672579 0.23991661 0.22807425 0.24492023\n",
      " 0.         0.25672579 0.23949314        nan        nan        nan\n",
      "        nan        nan 0.22807425 0.24526745 0.         0.25672579\n",
      " 0.23991661 0.22807425 0.24526745 0.         0.25672579 0.23949314\n",
      " 0.22807425 0.24526745 0.         0.25672579 0.23991661 0.22807425\n",
      " 0.24526745 0.         0.25672579 0.23991661 0.22807425 0.24526745\n",
      " 0.         0.25672579 0.23949314        nan        nan        nan\n",
      "        nan        nan 0.22807425 0.24526745 0.         0.25672579\n",
      " 0.23991661 0.22807425 0.24526745 0.         0.25672579 0.23949314\n",
      " 0.22807425 0.24526745 0.         0.25672579 0.23991661 0.22807425\n",
      " 0.24526745 0.         0.25672579 0.23991661 0.22807425 0.24526745\n",
      " 0.         0.25672579 0.23949314        nan        nan        nan\n",
      "        nan        nan 0.22807425 0.24526745 0.         0.25672579\n",
      " 0.23991661 0.22807425 0.24526745 0.         0.25672579 0.23949314\n",
      " 0.22807425 0.24526745 0.         0.25672579 0.23991661 0.22807425\n",
      " 0.24526745 0.         0.25672579 0.23991661 0.22807425 0.24526745\n",
      " 0.         0.25672579 0.23949314        nan        nan        nan\n",
      "        nan        nan 0.22807425 0.24526745 0.         0.25672579\n",
      " 0.23991661 0.22807425 0.24526745 0.         0.25672579 0.23949314\n",
      " 0.22807425 0.24526745 0.         0.25672579 0.23991661 0.22807425\n",
      " 0.24526745 0.         0.25672579 0.23991661 0.22807425 0.24526745\n",
      " 0.         0.25672579 0.23949314        nan        nan        nan\n",
      "        nan        nan 0.22807425 0.24526745 0.         0.25672579\n",
      " 0.23991661 0.22807425 0.24526745 0.         0.25672579 0.23949314\n",
      " 0.22807425 0.24526745 0.         0.25672579 0.23991661 0.22807425\n",
      " 0.24526745 0.         0.25672579 0.23991661 0.22807425 0.24526745\n",
      " 0.         0.25672579 0.23949314        nan        nan        nan\n",
      "        nan        nan 0.22807425 0.24526745 0.         0.25672579\n",
      " 0.23991661 0.22807425 0.24526745 0.         0.25672579 0.23949314\n",
      " 0.22807425 0.24526745 0.         0.25672579 0.23991661 0.22807425\n",
      " 0.24526745 0.         0.25672579 0.23991661 0.22807425 0.24526745\n",
      " 0.         0.25672579 0.23949314        nan        nan        nan\n",
      "        nan        nan 0.22807425 0.24526745 0.         0.25672579\n",
      " 0.23991661 0.22807425 0.24526745 0.         0.25672579 0.23949314]\n",
      "One or more of the train scores are non-finite: [0.24514503 0.24026637 0.17757641 0.24486417 0.2439662  0.24507501\n",
      " 0.24563587 0.         0.24486417 0.24403315 0.25513061 0.24563587\n",
      " 0.         0.26101461 0.25431804        nan        nan        nan\n",
      "        nan        nan 0.25432534 0.24387216 0.24912698 0.26114191\n",
      " 0.25313634 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634\n",
      " 0.245075   0.24026637 0.1778187  0.24499999 0.24424206 0.245075\n",
      " 0.24563587 0.         0.25252563 0.24424206 0.25513061 0.24563587\n",
      " 0.         0.26101461 0.25431804        nan        nan        nan\n",
      "        nan        nan 0.25432534 0.24387216 0.24912698 0.26114191\n",
      " 0.25313634 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634\n",
      " 0.24518209 0.24067698 0.1788132  0.24510063 0.24409949 0.25487407\n",
      " 0.24563317 0.         0.26104216 0.25479731 0.25513061 0.24563317\n",
      " 0.         0.26088271 0.25431804        nan        nan        nan\n",
      "        nan        nan 0.25432534 0.24387216 0.24912698 0.26114191\n",
      " 0.25313634 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634\n",
      " 0.2480149  0.24085315 0.26940402 0.24845667 0.2445132  0.25477956\n",
      " 0.24482619 0.57333333 0.2609387  0.25454673 0.25477956 0.24482619\n",
      " 0.57333333 0.2609387  0.25454673        nan        nan        nan\n",
      "        nan        nan 0.25432534 0.24387216 0.24912698 0.26114191\n",
      " 0.25313634 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634\n",
      " 0.25086622 0.24314532 0.23704236 0.24831272 0.24879931 0.25443442\n",
      " 0.24312648 0.305      0.26150042 0.25355955 0.25443442 0.24312648\n",
      " 0.305      0.26150042 0.25355955        nan        nan        nan\n",
      "        nan        nan 0.25432534 0.24387216 0.24912698 0.26114191\n",
      " 0.25313634 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634\n",
      " 0.25423697 0.24434715 0.24749251 0.25795199 0.2508305  0.25432534\n",
      " 0.24367979 0.25139971 0.26114191 0.25300757 0.25432534 0.24367979\n",
      " 0.25139971 0.26114191 0.25300757        nan        nan        nan\n",
      "        nan        nan 0.25432534 0.24387216 0.24912698 0.26114191\n",
      " 0.25313634 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634\n",
      " 0.25437625 0.24402734 0.24912698 0.26041269 0.25303657 0.25432534\n",
      " 0.24387216 0.24912698 0.26114191 0.25313634 0.25432534 0.24387216\n",
      " 0.24912698 0.26114191 0.25313634        nan        nan        nan\n",
      "        nan        nan 0.25432534 0.24387216 0.24912698 0.26114191\n",
      " 0.25313634 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634\n",
      " 0.25411436 0.24387216 0.24912698 0.26101906 0.2535457  0.25432534\n",
      " 0.24387216 0.24912698 0.26114191 0.25313634 0.25432534 0.24387216\n",
      " 0.24912698 0.26114191 0.25313634        nan        nan        nan\n",
      "        nan        nan 0.25432534 0.24387216 0.24912698 0.26114191\n",
      " 0.25313634 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634\n",
      " 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634 0.25432534\n",
      " 0.24387216 0.24912698 0.26114191 0.25313634 0.25432534 0.24387216\n",
      " 0.24912698 0.26114191 0.25313634        nan        nan        nan\n",
      "        nan        nan 0.25432534 0.24387216 0.24912698 0.26114191\n",
      " 0.25313634 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634\n",
      " 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634 0.25432534\n",
      " 0.24387216 0.24912698 0.26114191 0.25313634 0.25432534 0.24387216\n",
      " 0.24912698 0.26114191 0.25313634        nan        nan        nan\n",
      "        nan        nan 0.25432534 0.24387216 0.24912698 0.26114191\n",
      " 0.25313634 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634\n",
      " 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634 0.25432534\n",
      " 0.24387216 0.24912698 0.26114191 0.25313634 0.25432534 0.24387216\n",
      " 0.24912698 0.26114191 0.25313634        nan        nan        nan\n",
      "        nan        nan 0.25432534 0.24387216 0.24912698 0.26114191\n",
      " 0.25313634 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634\n",
      " 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634 0.25432534\n",
      " 0.24387216 0.24912698 0.26114191 0.25313634 0.25432534 0.24387216\n",
      " 0.24912698 0.26114191 0.25313634        nan        nan        nan\n",
      "        nan        nan 0.25432534 0.24387216 0.24912698 0.26114191\n",
      " 0.25313634 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634\n",
      " 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634 0.25432534\n",
      " 0.24387216 0.24912698 0.26114191 0.25313634 0.25432534 0.24387216\n",
      " 0.24912698 0.26114191 0.25313634        nan        nan        nan\n",
      "        nan        nan 0.25432534 0.24387216 0.24912698 0.26114191\n",
      " 0.25313634 0.25432534 0.24387216 0.24912698 0.26114191 0.25313634]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 0.0001, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.26337299816497517\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.0001, max_iter=1000, random_state=0))])\n",
      "Train Score of Best Model: 0.5897755610972568\n",
      "GridSearchCV Runtime: 6.178497076034546 secs\n",
      "Efron R-squared: -0.34902483868897627\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.21739130434782608\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2644628099173554\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.19480519480519481\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.27380952380952384\n",
      "Ave Test Precision: 0.24723662371883712\n",
      "Stdev Test Precision: 0.039125403541355325\n",
      "Ave Test Accuracy: 0.5798507462686567\n",
      "Stdev Test Accuracy: 0.030610663137304036\n",
      "Ave Test Specificity: 0.6544554455445544\n",
      "Ave Test Recall: 0.35151515151515156\n",
      "Ave Test NPV: 0.7561287574168787\n",
      "Ave Test F1-Score: 0.2886155227613034\n",
      "Ave Test G-mean: 0.47387637889711465\n",
      "Ave Runtime: 0.008386564254760743\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyExpenses / monthlyFamilyIncome. 3 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n",
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n",
      "Best Hyperparameters: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.30900044400044396\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.7169576059850374\n",
      "GridSearchCV Runtime: 6.3577094078063965 secs\n",
      "Efron R-squared: -0.34877735832282775\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.3333333333333333\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.4230769230769231\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.2653061224489796\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2857142857142857\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.30434782608695654\n",
      "Ave Test Precision: 0.32235569813209564\n",
      "Stdev Test Precision: 0.06161737635234825\n",
      "Ave Test Accuracy: 0.7156716417910447\n",
      "Stdev Test Accuracy: 0.02784792262245567\n",
      "Ave Test Specificity: 0.9069306930693068\n",
      "Ave Test Recall: 0.1303030303030303\n",
      "Ave Test NPV: 0.7613973668038968\n",
      "Ave Test F1-Score: 0.18066178788615322\n",
      "Ave Test G-mean: 0.3380297902399014\n",
      "Ave Runtime: 0.009598302841186523\n",
      "\n",
      "=====================\n",
      "\n",
      "\n",
      "Done with monthlyVices / monthlyFamilyIncome. 2 columns left\n",
      "\n",
      "\n",
      "=====================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.21063518 0.         0.09969136 0.24188518 0.24188518 0.21063518\n",
      " 0.         0.09969136 0.24188518 0.24188518 0.19853535 0.\n",
      " 0.09969136 0.12600733 0.11119769        nan        nan        nan\n",
      "        nan        nan 0.27953546 0.07151515 0.14797903 0.27614802\n",
      " 0.2615386  0.27255134 0.08333333 0.14797903 0.21779304 0.25338384\n",
      " 0.24407022 0.         0.09969136 0.24981858 0.24188518 0.20864434\n",
      " 0.         0.09969136 0.24536206 0.24188518 0.19853535 0.\n",
      " 0.09969136 0.12600733 0.11119769        nan        nan        nan\n",
      "        nan        nan 0.27953546 0.07151515 0.14797903 0.27614802\n",
      " 0.2615386  0.27255134 0.08333333 0.14797903 0.21779304 0.25338384\n",
      " 0.22953996 0.         0.09969136 0.24155142 0.23827818 0.20207707\n",
      " 0.         0.09969136 0.24155142 0.23827818 0.19853535 0.\n",
      " 0.09969136 0.12600733 0.11119769        nan        nan        nan\n",
      "        nan        nan 0.27953546 0.07151515 0.14797903 0.27614802\n",
      " 0.2615386  0.27255134 0.08333333 0.14797903 0.21779304 0.25338384\n",
      " 0.24874989 0.         0.09969136 0.25808584 0.24138329 0.24003394\n",
      " 0.         0.09969136 0.10368108 0.24763329 0.19853535 0.\n",
      " 0.09969136 0.12600733 0.11119769        nan        nan        nan\n",
      "        nan        nan 0.27953546 0.07151515 0.14797903 0.27614802\n",
      " 0.2615386  0.27255134 0.08333333 0.14797903 0.21779304 0.25338384\n",
      " 0.29188145 0.         0.09969136 0.11953102 0.30900044 0.19853535\n",
      " 0.         0.09969136 0.1224359  0.11119769 0.19853535 0.\n",
      " 0.09969136 0.1224359  0.11119769        nan        nan        nan\n",
      "        nan        nan 0.27953546 0.07151515 0.14797903 0.27614802\n",
      " 0.2615386  0.27255134 0.08333333 0.14797903 0.21779304 0.25338384\n",
      " 0.20670996 0.1        0.09969136 0.13286436 0.14453102 0.22170996\n",
      " 0.1        0.09969136 0.11965812 0.11953102 0.22170996 0.1\n",
      " 0.09969136 0.11965812 0.11953102        nan        nan        nan\n",
      "        nan        nan 0.27953546 0.07151515 0.14797903 0.27614802\n",
      " 0.2615386  0.27255134 0.08333333 0.14797903 0.21779304 0.25338384\n",
      " 0.28853896 0.06666667 0.09874199 0.13839549 0.19214286 0.28853896\n",
      " 0.06666667 0.09874199 0.13839549 0.19214286 0.28414336 0.06666667\n",
      " 0.09874199 0.13839549 0.19214286        nan        nan        nan\n",
      "        nan        nan 0.27953546 0.07151515 0.14797903 0.27614802\n",
      " 0.2615386  0.27255134 0.08333333 0.14797903 0.21779304 0.25338384\n",
      " 0.27622655 0.07222222 0.09874199 0.21422161 0.25843407 0.27622655\n",
      " 0.07222222 0.09874199 0.21422161 0.25843407 0.28813131 0.07222222\n",
      " 0.09874199 0.21779304 0.26321429        nan        nan        nan\n",
      "        nan        nan 0.27953546 0.07151515 0.14797903 0.27614802\n",
      " 0.2615386  0.27255134 0.08333333 0.14797903 0.21779304 0.25338384\n",
      " 0.27223388 0.07151515 0.09905033 0.26137529 0.28078759 0.27223388\n",
      " 0.07151515 0.09905033 0.26137529 0.28078759 0.27255134 0.08333333\n",
      " 0.09905033 0.21779304 0.26849822        nan        nan        nan\n",
      "        nan        nan 0.27953546 0.07151515 0.14797903 0.27614802\n",
      " 0.2615386  0.27255134 0.08333333 0.14797903 0.21779304 0.25338384\n",
      " 0.28175769 0.07151515 0.19901568 0.27614802 0.2665386  0.28175769\n",
      " 0.07151515 0.19901568 0.27614802 0.2665386  0.27255134 0.08333333\n",
      " 0.19901568 0.21779304 0.25338384        nan        nan        nan\n",
      "        nan        nan 0.27953546 0.07151515 0.14797903 0.27614802\n",
      " 0.2615386  0.27255134 0.08333333 0.14797903 0.21779304 0.25338384\n",
      " 0.27953546 0.07151515 0.14797903 0.27614802 0.2615386  0.28175769\n",
      " 0.07151515 0.14797903 0.27614802 0.2615386  0.27255134 0.08333333\n",
      " 0.14797903 0.21779304 0.25338384        nan        nan        nan\n",
      "        nan        nan 0.27953546 0.07151515 0.14797903 0.27614802\n",
      " 0.2615386  0.27255134 0.08333333 0.14797903 0.21779304 0.25338384\n",
      " 0.27953546 0.07151515 0.14797903 0.27614802 0.2615386  0.27953546\n",
      " 0.07151515 0.14797903 0.27614802 0.2615386  0.27255134 0.08333333\n",
      " 0.14797903 0.21779304 0.25338384        nan        nan        nan\n",
      "        nan        nan 0.27953546 0.07151515 0.14797903 0.27614802\n",
      " 0.2615386  0.27255134 0.08333333 0.14797903 0.21779304 0.25338384\n",
      " 0.27953546 0.07151515 0.14797903 0.27614802 0.2615386  0.27953546\n",
      " 0.07151515 0.14797903 0.27614802 0.2615386  0.27255134 0.08333333\n",
      " 0.14797903 0.21779304 0.25338384        nan        nan        nan\n",
      "        nan        nan 0.27953546 0.07151515 0.14797903 0.27614802\n",
      " 0.2615386  0.27255134 0.08333333 0.14797903 0.21779304 0.25338384]\n",
      "One or more of the train scores are non-finite: [0.21732792 0.9        0.09809494 0.24056735 0.24056735 0.21732792\n",
      " 0.         0.09809494 0.24056735 0.24056735 0.22491971 0.\n",
      " 0.09809494 0.22119649 0.16685587        nan        nan        nan\n",
      "        nan        nan 0.28507317 0.25220621 0.37338141 0.27125825\n",
      " 0.2733428  0.28709512 0.25847141 0.37338141 0.25233531 0.2885973\n",
      " 0.24594822 0.9        0.09809494 0.2425668  0.24056735 0.2172391\n",
      " 0.         0.09809494 0.24256319 0.24056735 0.22491971 0.\n",
      " 0.09809494 0.22119649 0.16685587        nan        nan        nan\n",
      "        nan        nan 0.28507317 0.25220621 0.37338141 0.27125825\n",
      " 0.2733428  0.28709512 0.25847141 0.37338141 0.25233531 0.2885973\n",
      " 0.2411584  0.9        0.09809494 0.24031516 0.23804219 0.21750166\n",
      " 0.         0.09809494 0.24031516 0.23804219 0.22491971 0.\n",
      " 0.09809494 0.22119649 0.16685587        nan        nan        nan\n",
      "        nan        nan 0.28507317 0.25220621 0.37338141 0.27125825\n",
      " 0.2733428  0.28709512 0.25847141 0.37338141 0.25233531 0.2885973\n",
      " 0.25632118 0.9        0.09809494 0.27191304 0.24750991 0.22552941\n",
      " 0.4        0.09809494 0.21589212 0.24704519 0.22491971 0.4\n",
      " 0.09809494 0.22119649 0.16685587        nan        nan        nan\n",
      "        nan        nan 0.28507317 0.25220621 0.37338141 0.27125825\n",
      " 0.2733428  0.28709512 0.25847141 0.37338141 0.25233531 0.2885973\n",
      " 0.27805005 0.9        0.09809494 0.2009915  0.30865744 0.22491971\n",
      " 0.9        0.09809494 0.21094887 0.16991143 0.22491971 0.9\n",
      " 0.09809494 0.21094887 0.17074476        nan        nan        nan\n",
      "        nan        nan 0.28507317 0.25220621 0.37338141 0.27125825\n",
      " 0.2733428  0.28709512 0.25847141 0.37338141 0.25233531 0.2885973\n",
      " 0.24164418 0.825      0.69809494 0.19325682 0.200485   0.23912182\n",
      " 0.825      0.69809494 0.17630789 0.1908281  0.23948917 0.825\n",
      " 0.69809494 0.17671275 0.1908281         nan        nan        nan\n",
      "        nan        nan 0.28507317 0.25220621 0.37338141 0.27125825\n",
      " 0.2733428  0.28709512 0.25847141 0.37338141 0.25233531 0.2885973\n",
      " 0.28650422 0.50205128 0.69809494 0.21418266 0.2209103  0.28844709\n",
      " 0.50205128 0.69809494 0.21062535 0.21693103 0.28392884 0.50205128\n",
      " 0.69809494 0.20771529 0.21738525        nan        nan        nan\n",
      "        nan        nan 0.28507317 0.25220621 0.37338141 0.27125825\n",
      " 0.2733428  0.28709512 0.25847141 0.37338141 0.25233531 0.2885973\n",
      " 0.29321069 0.25658645 0.69809494 0.25477723 0.28458168 0.29321069\n",
      " 0.25658645 0.69809494 0.25477723 0.28057301 0.29389446 0.26655264\n",
      " 0.69809494 0.24846993 0.273542          nan        nan        nan\n",
      "        nan        nan 0.28507317 0.25220621 0.37338141 0.27125825\n",
      " 0.2733428  0.28709512 0.25847141 0.37338141 0.25233531 0.2885973\n",
      " 0.28414077 0.25827011 0.63805228 0.26933213 0.28240802 0.28324603\n",
      " 0.25827011 0.63805228 0.26933213 0.28240802 0.28987081 0.26694556\n",
      " 0.63805228 0.2527739  0.2920775         nan        nan        nan\n",
      "        nan        nan 0.28507317 0.25220621 0.37338141 0.27125825\n",
      " 0.2733428  0.28709512 0.25847141 0.37338141 0.25233531 0.2885973\n",
      " 0.28507317 0.25220621 0.38330839 0.27197282 0.27528545 0.28507317\n",
      " 0.25220621 0.38330839 0.27197282 0.27528545 0.28709512 0.26114998\n",
      " 0.38330839 0.25233531 0.2885973         nan        nan        nan\n",
      "        nan        nan 0.28507317 0.25220621 0.37338141 0.27125825\n",
      " 0.2733428  0.28709512 0.25847141 0.37338141 0.25233531 0.2885973\n",
      " 0.28507317 0.25220621 0.36790522 0.27166397 0.27367894 0.28507317\n",
      " 0.25220621 0.36790522 0.27125825 0.27367894 0.28709512 0.25847141\n",
      " 0.36790522 0.25233531 0.2885973         nan        nan        nan\n",
      "        nan        nan 0.28507317 0.25220621 0.37338141 0.27125825\n",
      " 0.2733428  0.28709512 0.25847141 0.37338141 0.25233531 0.2885973\n",
      " 0.28507317 0.25220621 0.36671474 0.27125825 0.2730032  0.28507317\n",
      " 0.25220621 0.36671474 0.27125825 0.2733428  0.28709512 0.25847141\n",
      " 0.36671474 0.25233531 0.2885973         nan        nan        nan\n",
      "        nan        nan 0.28507317 0.25220621 0.37338141 0.27125825\n",
      " 0.2733428  0.28709512 0.25847141 0.37338141 0.25233531 0.2885973\n",
      " 0.28507317 0.25220621 0.37338141 0.27125825 0.2730032  0.28507317\n",
      " 0.25220621 0.37338141 0.27125825 0.2733428  0.28709512 0.25847141\n",
      " 0.37338141 0.25233531 0.2885973         nan        nan        nan\n",
      "        nan        nan 0.28507317 0.25220621 0.37338141 0.27125825\n",
      " 0.2733428  0.28709512 0.25847141 0.37338141 0.25233531 0.2885973 ]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    }
   ],
   "source": [
    "res_df = pd.DataFrame(columns=[\n",
    "    'Variable',\n",
    "    'Mean Test Precision', 'Std Test Precision', 'Val Precision of Best Model',\n",
    "    'Train Precision of Best Model', 'Mean Test Accuracy', 'Std Test Accuracy',\n",
    "    'Efron R2'\n",
    "])\n",
    "\n",
    "for index, col in enumerate(columns):\n",
    "    df_2 = df.copy().loc[:, ['userId', 'lastFirstName', col, 'home_ownership_class']]\n",
    "    t01_logreg_gs, t01_logreg_be, t01_logreg_model_info, t01_logreg_metrics_df = logreg_class2(df_2, 'precision',\n",
    "                                                                'home_ownership_class', scaler, random_state=0)\n",
    "    new_row = {\n",
    "        'Variable': col,\n",
    "        'Mean Test Precision': t01_logreg_model_info['average_test_precision'],\n",
    "        'Std Test Precision': t01_logreg_model_info['stdev_test_precision'],\n",
    "        'Val Precision of Best Model': t01_logreg_model_info['best_cv_score'],\n",
    "        'Train Precision of Best Model': t01_logreg_model_info['train_score'],\n",
    "        'Mean Test Accuracy': t01_logreg_model_info['average_test_accuracy'],\n",
    "        'Std Test Accuracy': t01_logreg_model_info['stdev_test_accuracy'],\n",
    "        'Efron R2': t01_logreg_model_info['efron']\n",
    "    }\n",
    "    res_df = res_df.append(new_row, ignore_index=True)\n",
    "    print(f'\\n=====================\\n\\n\\nDone with {col}. {len(columns) - index + 1} columns left\\n\\n\\n=====================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Mean Test Precision</th>\n",
       "      <th>Std Test Precision</th>\n",
       "      <th>Val Precision of Best Model</th>\n",
       "      <th>Train Precision of Best Model</th>\n",
       "      <th>Mean Test Accuracy</th>\n",
       "      <th>Std Test Accuracy</th>\n",
       "      <th>Efron R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>vehicleLoan</td>\n",
       "      <td>0.407318</td>\n",
       "      <td>0.129121</td>\n",
       "      <td>0.339167</td>\n",
       "      <td>0.733167</td>\n",
       "      <td>0.732836</td>\n",
       "      <td>0.027873</td>\n",
       "      <td>-0.335257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>monthlyExpenses</td>\n",
       "      <td>0.399103</td>\n",
       "      <td>0.127994</td>\n",
       "      <td>0.380298</td>\n",
       "      <td>0.711970</td>\n",
       "      <td>0.728358</td>\n",
       "      <td>0.030474</td>\n",
       "      <td>-0.185007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>monthlyFamilyIncome</td>\n",
       "      <td>0.398261</td>\n",
       "      <td>0.070818</td>\n",
       "      <td>0.353333</td>\n",
       "      <td>0.729426</td>\n",
       "      <td>0.740299</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>-0.173122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>monthlyVices</td>\n",
       "      <td>0.386724</td>\n",
       "      <td>0.047433</td>\n",
       "      <td>0.323068</td>\n",
       "      <td>0.748130</td>\n",
       "      <td>0.747015</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>-0.015701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>payInsurance</td>\n",
       "      <td>0.378755</td>\n",
       "      <td>0.034898</td>\n",
       "      <td>0.339365</td>\n",
       "      <td>0.716958</td>\n",
       "      <td>0.729851</td>\n",
       "      <td>0.008175</td>\n",
       "      <td>-0.349172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>monthlyFamilyNetIncome</td>\n",
       "      <td>0.373165</td>\n",
       "      <td>0.061095</td>\n",
       "      <td>0.423333</td>\n",
       "      <td>0.746883</td>\n",
       "      <td>0.736567</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>-0.205250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>preferredNetDisposableIncomeId</td>\n",
       "      <td>0.368088</td>\n",
       "      <td>0.030940</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>0.638404</td>\n",
       "      <td>0.663433</td>\n",
       "      <td>0.022911</td>\n",
       "      <td>-0.349172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>repair</td>\n",
       "      <td>0.363370</td>\n",
       "      <td>0.077332</td>\n",
       "      <td>0.261623</td>\n",
       "      <td>0.741895</td>\n",
       "      <td>0.744776</td>\n",
       "      <td>0.006244</td>\n",
       "      <td>-0.070470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>monthlySoloNetIncomeWithSavings</td>\n",
       "      <td>0.355503</td>\n",
       "      <td>0.095006</td>\n",
       "      <td>0.433651</td>\n",
       "      <td>0.734414</td>\n",
       "      <td>0.726866</td>\n",
       "      <td>0.020163</td>\n",
       "      <td>-0.184858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>informalLenders</td>\n",
       "      <td>0.355245</td>\n",
       "      <td>0.129425</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.746883</td>\n",
       "      <td>0.743284</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>-0.310268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Variable  Mean Test Precision  Std Test Precision  \\\n",
       "29                      vehicleLoan             0.407318            0.129121   \n",
       "57                  monthlyExpenses             0.399103            0.127994   \n",
       "5               monthlyFamilyIncome             0.398261            0.070818   \n",
       "56                     monthlyVices             0.386724            0.047433   \n",
       "41                     payInsurance             0.378755            0.034898   \n",
       "60           monthlyFamilyNetIncome             0.373165            0.061095   \n",
       "2    preferredNetDisposableIncomeId             0.368088            0.030940   \n",
       "21                           repair             0.363370            0.077332   \n",
       "62  monthlySoloNetIncomeWithSavings             0.355503            0.095006   \n",
       "30                  informalLenders             0.355245            0.129425   \n",
       "\n",
       "    Val Precision of Best Model  Train Precision of Best Model  \\\n",
       "29                     0.339167                       0.733167   \n",
       "57                     0.380298                       0.711970   \n",
       "5                      0.353333                       0.729426   \n",
       "56                     0.323068                       0.748130   \n",
       "41                     0.339365                       0.716958   \n",
       "60                     0.423333                       0.746883   \n",
       "2                      0.330400                       0.638404   \n",
       "21                     0.261623                       0.741895   \n",
       "62                     0.433651                       0.734414   \n",
       "30                     0.450000                       0.746883   \n",
       "\n",
       "    Mean Test Accuracy  Std Test Accuracy  Efron R2  \n",
       "29            0.732836           0.027873 -0.335257  \n",
       "57            0.728358           0.030474 -0.185007  \n",
       "5             0.740299           0.007737 -0.173122  \n",
       "56            0.747015           0.004087 -0.015701  \n",
       "41            0.729851           0.008175 -0.349172  \n",
       "60            0.736567           0.013086 -0.205250  \n",
       "2             0.663433           0.022911 -0.349172  \n",
       "21            0.744776           0.006244 -0.070470  \n",
       "62            0.726866           0.020163 -0.184858  \n",
       "30            0.743284           0.011006 -0.310268  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_res_df = res_df.copy().drop(70)\n",
    "sorted_res_df = sorted_res_df.sort_values(by='Mean Test Precision', ascending=False)\n",
    "# sorted_res_df[sorted_res_df['Variable'] == 'basicMonthlySalary']\n",
    "sorted_res_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Mean Test Precision</th>\n",
       "      <th>Std Test Precision</th>\n",
       "      <th>Val Precision of Best Model</th>\n",
       "      <th>Train Precision of Best Model</th>\n",
       "      <th>Mean Test Accuracy</th>\n",
       "      <th>Std Test Accuracy</th>\n",
       "      <th>Efron R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Variable, Mean Test Precision, Std Test Precision, Val Precision of Best Model, Train Precision of Best Model, Mean Test Accuracy, Std Test Accuracy, Efron R2]\n",
       "Index: []"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_res_df[sorted_res_df['Efron R2'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 390 candidates, totalling 3900 fits\n",
      "Best Hyperparameters: {'classifier__C': 1e-06, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'resampling': RandomUnderSampler(random_state=0)}\n",
      "Best Validation Score (Precision): 0.34054818973297235\n",
      "Best Estimator: Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('resampling', RandomUnderSampler(random_state=0)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=1e-06, max_iter=1000, random_state=0,\n",
      "                                    solver='liblinear'))])\n",
      "Train Score of Best Model: 0.6870324189526185\n",
      "GridSearchCV Runtime: 3.9100213050842285 secs\n",
      "Efron R-squared: -0.34917038707679815\n",
      "Run 1 - Random State: 0\n",
      "Test Precision: 0.4444444444444444\n",
      "Run 2 - Random State: 1\n",
      "Test Precision: 0.24390243902439024\n",
      "Run 3 - Random State: 2\n",
      "Test Precision: 0.38461538461538464\n",
      "Run 4 - Random State: 3\n",
      "Test Precision: 0.2013888888888889\n",
      "Run 5 - Random State: 4\n",
      "Test Precision: 0.4074074074074074\n",
      "Ave Test Precision: 0.3363517128761031\n",
      "Stdev Test Precision: 0.10703289733453612\n",
      "Ave Test Accuracy: 0.6216417910447761\n",
      "Stdev Test Accuracy: 0.13683401056341998\n",
      "Ave Test Specificity: 0.700990099009901\n",
      "Ave Test Recall: 0.3787878787878788\n",
      "Ave Test NPV: 0.7677009838630747\n",
      "Ave Test F1-Score: 0.3398601022329836\n",
      "Ave Test G-mean: 0.5042736311717393\n",
      "Ave Runtime: 0.003125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "650 fits failed out of a total of 3900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "650 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Carlo\\mambaforge\\envs\\myenv\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "One or more of the test scores are non-finite: [0.32713188 0.07777778 0.         0.32565732 0.34054819 0.28504347\n",
      " 0.025      0.         0.3039592  0.34054819 0.30465025 0.025\n",
      " 0.         0.32504055 0.33174553        nan        nan        nan\n",
      "        nan        nan 0.27558778 0.24054286 0.24038946 0.26564217\n",
      " 0.28236463 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463\n",
      " 0.28504347 0.07777778 0.         0.3039592  0.32555506 0.28504347\n",
      " 0.025      0.         0.3039592  0.32555506 0.30465025 0.025\n",
      " 0.         0.32504055 0.33174553        nan        nan        nan\n",
      "        nan        nan 0.27558778 0.24054286 0.24038946 0.26564217\n",
      " 0.28236463 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463\n",
      " 0.28504347 0.07777778 0.         0.3039592  0.32555506 0.30465025\n",
      " 0.025      0.         0.32504055 0.32555506 0.30465025 0.025\n",
      " 0.         0.32504055 0.33174553        nan        nan        nan\n",
      "        nan        nan 0.27558778 0.24054286 0.24038946 0.26564217\n",
      " 0.28236463 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463\n",
      " 0.28937248 0.07777778 0.         0.30976277 0.32555506 0.30465025\n",
      " 0.02142857 0.         0.32504055 0.33174553 0.30465025 0.02142857\n",
      " 0.         0.32504055 0.33174553        nan        nan        nan\n",
      "        nan        nan 0.27558778 0.24054286 0.24038946 0.26564217\n",
      " 0.28236463 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463\n",
      " 0.30223646 0.10777778 0.         0.30892944 0.33174553 0.30223646\n",
      " 0.10777778 0.         0.30892944 0.33174553 0.30223646 0.10777778\n",
      " 0.         0.30892944 0.33174553        nan        nan        nan\n",
      "        nan        nan 0.27558778 0.24054286 0.24038946 0.26564217\n",
      " 0.28236463 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463\n",
      " 0.26798409 0.16718254 0.24038946 0.26914292 0.28345159 0.26798409\n",
      " 0.16718254 0.24038946 0.26914292 0.28345159 0.26798409 0.16718254\n",
      " 0.24038946 0.26914292 0.28345159        nan        nan        nan\n",
      "        nan        nan 0.27558778 0.24054286 0.24038946 0.26564217\n",
      " 0.28236463 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463\n",
      " 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778\n",
      " 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778 0.24054286\n",
      " 0.24038946 0.26564217 0.28236463        nan        nan        nan\n",
      "        nan        nan 0.27558778 0.24054286 0.24038946 0.26564217\n",
      " 0.28236463 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463\n",
      " 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778\n",
      " 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778 0.24054286\n",
      " 0.24038946 0.26564217 0.28236463        nan        nan        nan\n",
      "        nan        nan 0.27558778 0.24054286 0.24038946 0.26564217\n",
      " 0.28236463 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463\n",
      " 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778\n",
      " 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778 0.24054286\n",
      " 0.24038946 0.26564217 0.28236463        nan        nan        nan\n",
      "        nan        nan 0.27558778 0.24054286 0.24038946 0.26564217\n",
      " 0.28236463 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463\n",
      " 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778\n",
      " 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778 0.24054286\n",
      " 0.24038946 0.26564217 0.28236463        nan        nan        nan\n",
      "        nan        nan 0.27558778 0.24054286 0.24038946 0.26564217\n",
      " 0.28236463 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463\n",
      " 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778\n",
      " 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778 0.24054286\n",
      " 0.24038946 0.26564217 0.28236463        nan        nan        nan\n",
      "        nan        nan 0.27558778 0.24054286 0.24038946 0.26564217\n",
      " 0.28236463 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463\n",
      " 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778\n",
      " 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778 0.24054286\n",
      " 0.24038946 0.26564217 0.28236463        nan        nan        nan\n",
      "        nan        nan 0.27558778 0.24054286 0.24038946 0.26564217\n",
      " 0.28236463 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463\n",
      " 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778\n",
      " 0.24054286 0.24038946 0.26564217 0.28236463 0.27558778 0.24054286\n",
      " 0.24038946 0.26564217 0.28236463        nan        nan        nan\n",
      "        nan        nan 0.27558778 0.24054286 0.24038946 0.26564217\n",
      " 0.28236463 0.27558778 0.24054286 0.24038946 0.26564217 0.28236463]\n",
      "One or more of the train scores are non-finite: [0.32167404 0.09572363 0.         0.32184245 0.33179045 0.31177079\n",
      " 0.02451524 0.         0.31731609 0.33179045 0.33317353 0.02451524\n",
      " 0.         0.33855042 0.33075821        nan        nan        nan\n",
      "        nan        nan 0.29416529 0.28925727 0.24073418 0.29511938\n",
      " 0.28658818 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818\n",
      " 0.31177079 0.09572363 0.         0.31731609 0.31947188 0.31177079\n",
      " 0.02451524 0.         0.31731609 0.31947188 0.33317353 0.02451524\n",
      " 0.         0.33855042 0.33075821        nan        nan        nan\n",
      "        nan        nan 0.29416529 0.28925727 0.24073418 0.29511938\n",
      " 0.28658818 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818\n",
      " 0.31177079 0.09572363 0.         0.31731609 0.31947188 0.33317353\n",
      " 0.02451524 0.         0.33855042 0.31947188 0.33317353 0.02451524\n",
      " 0.         0.33855042 0.33075821        nan        nan        nan\n",
      "        nan        nan 0.29416529 0.28925727 0.24073418 0.29511938\n",
      " 0.28658818 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818\n",
      " 0.32303441 0.09572363 0.         0.3284113  0.31947188 0.33317353\n",
      " 0.02584856 0.         0.33855042 0.33075821 0.33317353 0.02584856\n",
      " 0.         0.33855042 0.33075821        nan        nan        nan\n",
      "        nan        nan 0.29416529 0.28925727 0.24073418 0.29511938\n",
      " 0.28658818 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818\n",
      " 0.32631995 0.13059205 0.         0.3333665  0.33075821 0.32631995\n",
      " 0.13059205 0.         0.3333665  0.33075821 0.32631995 0.13059205\n",
      " 0.         0.3333665  0.33075821        nan        nan        nan\n",
      "        nan        nan 0.29416529 0.28925727 0.24073418 0.29511938\n",
      " 0.28658818 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818\n",
      " 0.30175786 0.21818363 0.24073418 0.30923802 0.29385325 0.30175786\n",
      " 0.21818363 0.24073418 0.30923802 0.29385325 0.30175786 0.21818363\n",
      " 0.24073418 0.30923802 0.29385325        nan        nan        nan\n",
      "        nan        nan 0.29416529 0.28925727 0.24073418 0.29511938\n",
      " 0.28658818 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818\n",
      " 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529\n",
      " 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529 0.28925727\n",
      " 0.24073418 0.29511938 0.28658818        nan        nan        nan\n",
      "        nan        nan 0.29416529 0.28925727 0.24073418 0.29511938\n",
      " 0.28658818 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818\n",
      " 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529\n",
      " 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529 0.28925727\n",
      " 0.24073418 0.29511938 0.28658818        nan        nan        nan\n",
      "        nan        nan 0.29416529 0.28925727 0.24073418 0.29511938\n",
      " 0.28658818 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818\n",
      " 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529\n",
      " 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529 0.28925727\n",
      " 0.24073418 0.29511938 0.28658818        nan        nan        nan\n",
      "        nan        nan 0.29416529 0.28925727 0.24073418 0.29511938\n",
      " 0.28658818 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818\n",
      " 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529\n",
      " 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529 0.28925727\n",
      " 0.24073418 0.29511938 0.28658818        nan        nan        nan\n",
      "        nan        nan 0.29416529 0.28925727 0.24073418 0.29511938\n",
      " 0.28658818 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818\n",
      " 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529\n",
      " 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529 0.28925727\n",
      " 0.24073418 0.29511938 0.28658818        nan        nan        nan\n",
      "        nan        nan 0.29416529 0.28925727 0.24073418 0.29511938\n",
      " 0.28658818 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818\n",
      " 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529\n",
      " 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529 0.28925727\n",
      " 0.24073418 0.29511938 0.28658818        nan        nan        nan\n",
      "        nan        nan 0.29416529 0.28925727 0.24073418 0.29511938\n",
      " 0.28658818 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818\n",
      " 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529\n",
      " 0.28925727 0.24073418 0.29511938 0.28658818 0.29416529 0.28925727\n",
      " 0.24073418 0.29511938 0.28658818        nan        nan        nan\n",
      "        nan        nan 0.29416529 0.28925727 0.24073418 0.29511938\n",
      " 0.28658818 0.29416529 0.28925727 0.24073418 0.29511938 0.28658818]\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Test Precision</th>\n",
       "      <th>Std Test Precision</th>\n",
       "      <th>Val Precision of Best Model</th>\n",
       "      <th>Train Precision of Best Model</th>\n",
       "      <th>Mean Test Accuracy</th>\n",
       "      <th>Std Test Accuracy</th>\n",
       "      <th>Efron R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.336352</td>\n",
       "      <td>0.107033</td>\n",
       "      <td>0.340548</td>\n",
       "      <td>0.687032</td>\n",
       "      <td>0.621642</td>\n",
       "      <td>0.136834</td>\n",
       "      <td>-0.34917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Mean Test Precision  Std Test Precision  Val Precision of Best Model  \\\n",
       "0             0.336352            0.107033                     0.340548   \n",
       "\n",
       "   Train Precision of Best Model  Mean Test Accuracy  Std Test Accuracy  \\\n",
       "0                       0.687032            0.621642           0.136834   \n",
       "\n",
       "   Efron R2  \n",
       "0  -0.34917  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = df.copy().loc[:, ['userId', 'lastFirstName', 'basicMonthlySalary', 'home_ownership_class']]\n",
    "bin_edges = [0, df_2['basicMonthlySalary'].quantile(0.25), df_2['basicMonthlySalary'].quantile(0.75), df_2['basicMonthlySalary'].max()]\n",
    "\n",
    "# Create binned column\n",
    "df_2['salary_bin'] = pd.cut(df_2['basicMonthlySalary'],\n",
    "                            bins=bin_edges,\n",
    "                            labels=['0-25th percentile', '25th-75th percentile', 'above 75th percentile'],\n",
    "                            include_lowest=True)\n",
    "df_2 = pd.get_dummies(df_2, columns=['salary_bin'], prefix=\"\", prefix_sep=\"\", drop_first=True).drop(columns='basicMonthlySalary')\n",
    "df_2\n",
    "\n",
    "res_df = pd.DataFrame(columns=[\n",
    "    'Mean Test Precision', 'Std Test Precision', 'Val Precision of Best Model',\n",
    "    'Train Precision of Best Model', 'Mean Test Accuracy', 'Std Test Accuracy',\n",
    "    'Efron R2'\n",
    "])\n",
    "t01_logreg_gs, t01_logreg_be, t01_logreg_model_info, t01_logreg_metrics_df = logreg_class2(df_2, 'precision',\n",
    "                                                            'home_ownership_class', scaler, random_state=0)\n",
    "\n",
    "new_row = {\n",
    "    'Mean Test Precision': t01_logreg_model_info['average_test_precision'],\n",
    "    'Std Test Precision': t01_logreg_model_info['stdev_test_precision'],\n",
    "    'Val Precision of Best Model': t01_logreg_model_info['best_cv_score'],\n",
    "    'Train Precision of Best Model': t01_logreg_model_info['train_score'],\n",
    "    'Mean Test Accuracy': t01_logreg_model_info['average_test_accuracy'],\n",
    "    'Std Test Accuracy': t01_logreg_model_info['stdev_test_accuracy'],\n",
    "    'Efron R2': t01_logreg_model_info['efron']\n",
    "}\n",
    "res_df = res_df.append(new_row, ignore_index=True)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>lastFirstName</th>\n",
       "      <th>home_ownership_class</th>\n",
       "      <th>25th-75th percentile</th>\n",
       "      <th>above 75th percentile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>370</td>\n",
       "      <td>IBALI, HOWARD</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1025</td>\n",
       "      <td>PATALINGHOG, KIMBERLY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1105</td>\n",
       "      <td>TEMILLOSO, DENNIS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1719</td>\n",
       "      <td>OSCARES, ELMER</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2081</td>\n",
       "      <td>LEGASPI, MANNY</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>12850</td>\n",
       "      <td>LIPANGO, ARVIN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>12852</td>\n",
       "      <td>TEJADA, JERIC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>12853</td>\n",
       "      <td>RAMOS, RHEALYN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>12854</td>\n",
       "      <td>BURGOS, LIEZEL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>12857</td>\n",
       "      <td>SEÑERES, JENNY MAE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1070 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId          lastFirstName  home_ownership_class  \\\n",
       "0        370          IBALI, HOWARD                     1   \n",
       "1       1025  PATALINGHOG, KIMBERLY                     0   \n",
       "2       1105      TEMILLOSO, DENNIS                     0   \n",
       "3       1719         OSCARES, ELMER                     1   \n",
       "4       2081         LEGASPI, MANNY                     0   \n",
       "...      ...                    ...                   ...   \n",
       "1065   12850         LIPANGO, ARVIN                     0   \n",
       "1066   12852          TEJADA, JERIC                     0   \n",
       "1067   12853         RAMOS, RHEALYN                     0   \n",
       "1068   12854         BURGOS, LIEZEL                     0   \n",
       "1069   12857     SEÑERES, JENNY MAE                     0   \n",
       "\n",
       "      25th-75th percentile  above 75th percentile  \n",
       "0                        1                      0  \n",
       "1                        0                      0  \n",
       "2                        1                      0  \n",
       "3                        1                      0  \n",
       "4                        1                      0  \n",
       "...                    ...                    ...  \n",
       "1065                     1                      0  \n",
       "1066                     0                      0  \n",
       "1067                     0                      0  \n",
       "1068                     0                      1  \n",
       "1069                     0                      1  \n",
       "\n",
       "[1070 rows x 5 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
